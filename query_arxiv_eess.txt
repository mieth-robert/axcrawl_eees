
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Bayesian Model Parameter Learning in Linear Inverse Problems with Application in EEG Focal Source Imaging
Authors: Alexandra Koulouri, Ville Rimpilainen
Abstract: Inverse problems can be described as limited-data problems in which the signal of interest cannot be observed directly. A physics-based forward model that relates the signal with the observations is typically needed. Unfortunately, unknown model parameters and imperfect forward models can undermine the signal recovery. Even though supervised machine learning offers promising avenues to improve the robustness of the solutions, we have to rely on model-based learning when there is no access to ground truth for the training. Here, we studied a linear inverse problem that included an unknown non-linear model parameter and utilized a Bayesian model-based learning approach that allowed signal recovery and subsequently estimation of the model parameter. This approach, called Bayesian Approximation Error approach, employed a simplified model of the physics of the problem augmented with an approximation error term that compensated for the simplification. An error subspace was spanned with the help of the eigenvectors of the approximation error covariance matrix which allowed, alongside the primary signal, simultaneous estimation of the induced error. The estimated error and signal were then used to determine the unknown model parameter. For the model parameter estimation, we tested different approaches: a conditional Gaussian regression, an iterative (model-based) optimization, and a Gaussian process that was modeled with the help of physics-informed learning. In addition, alternating optimization was used as a reference method. As an example application, we focused on the problem of reconstructing brain activity from EEG recordings under the condition that the electrical conductivity of the patient's skull was unknown in the model. Our results demonstrated clear improvements in EEG source localization accuracy and provided feasible estimates for the unknown model parameter, skull conductivity.

Paper number 2:
Title: Linea alba 3D morphometric variability by CT scan exploration
Authors: P. Gueroult (LBA), V. Joppin (LBA), K. Chaumoitre (ADES), M. Di Bisceglie, C. Masson (LBA), T. Bege (LBA)
Abstract: Purpose: The width of the Linea alba, which is often gauged by inter-rectus distance, is a key risk factor for incisional hernia and recurrence. Previous studies provided limited descriptions with no consideration for width, location variability, or curvature. We aimed to offer a comprehensive 3D anatomical analysis of the Linea alba, emphasizing its variations across diverse demographics. Methods: Using open source software, 2D sagittal plane and 3D reconstructions were performed on 117 patients' CT scans. Linea alba length, curvature assessed by the sagitta (the longest perpendicular segment between xipho-pubic line and the Linea alba), and continuous width along the height were measured. Results: The Linea alba had a rhombus shape, with a maximum width at the umbilicus of 4.4$\pm$1.9 cm and a larger width above the umbilicus than below. Its length was 37.5$\pm$3.6 cm, which increased with body mass index (BMI) (p$\leq$0.001), and was shorter in women (p$\leq$0.001). The sagitta was 2.6$\pm$2.2 cm, three times higher in the obese group (p$\leq$0.001), majorated with age (p=0.009), but was independent of gender (p=0.212). Linea alba width increased with both age and BMI (p$\leq$0.001, p=0.002), being notably wider in women halfway between the umbilicus and pubis (p=0.007). Conclusion: This study provides an exhaustive 3D description of Linea alba's anatomical variability, presenting new considerations for curvature. This method provides a patient-specific anatomy description of the Linea alba. Further studies are needed to determine whether 3D reconstruction correlates with pathologies, such as hernias and diastasis recti.

Paper number 3:
Title: A Learnt Half-Quadratic Splitting-Based Algorithm for Fast and High-Quality Industrial Cone-beam CT Reconstruction
Authors: Aniket Pramanik, Singanallur V. Venkatakrishnan, Obaidullah Rahman, Amirkoushyar Ziabari
Abstract: Industrial X-ray cone-beam CT (XCT) scanners are widely used for scientific imaging and non-destructive characterization. Industrial CBCT scanners use large detectors containing millions of pixels and the subsequent 3D reconstructions can be of the order of billions of voxels. In order to obtain high-quality reconstruction when using typical analytic algorithms, the scan involves collecting a large number of projections/views which results in large measurement times - limiting the utility of the technique. Model-based iterative reconstruction (MBIR) algorithms can produce high-quality reconstructions from fast sparse-view CT scans, but are computationally expensive and hence are avoided in practice. Single-step deep-learning (DL) based methods have demonstrated that it is possible to obtain fast and high-quality reconstructions from sparse-view data but they do not generalize well to out-of-distribution scenarios. In this work, we propose a half-quadratic splitting-based algorithm that uses convolutional neural networks (CNN) in order to obtain high-quality reconstructions from large sparse-view cone-beam CT (CBCT) measurements while overcoming the challenges with typical approaches. The algorithm alternates between the application of a CNN and a conjugate gradient (CG) step enforcing data-consistency (DC). The proposed method outperforms other methods on the publicly available Walnuts data-set.

Paper number 4:
Title: Hybridization of Attention UNet with Repeated Atrous Spatial Pyramid Pooling for Improved Brain Tumour Segmentation
Authors: Satyaki Roy Chowdhury, Golrokh Mirzaei
Abstract: Brain tumors are highly heterogeneous in terms of their spatial and scaling characteristics, making tumor segmentation in medical images a difficult task that might result in wrong diagnosis and therapy. Automation of a task like tumor segmentation is expected to enhance objectivity, repeatability and at the same time reducing turn around time. Conventional convolutional neural networks (CNNs) exhibit sub-par performance as a result of their inability to accurately represent the range of tumor sizes and forms. Developing on that, UNets have been a commonly used solution for semantic segmentation, and it uses a downsampling-upsampling approach to segment tumors. This paper proposes a novel architecture that integrates Attention-UNet with repeated Atrous Spatial Pyramid Pooling (ASPP). ASPP effectively captures multi-scale contextual information through parallel atrous convolutions with varying dilation rates. This allows for efficient expansion of the receptive field while maintaining fine details. The attention provides the necessary context by incorporating local characteristics with their corresponding global dependencies. This integration significantly enhances semantic segmentation performance. Our approach demonstrates significant improvements over UNet, Attention UNet and Attention UNet with Spatial Pyramid Pooling allowing to set a new benchmark for tumor segmentation tasks.

Paper number 5:
Title: A Novel Scene Coupling Semantic Mask Network for Remote Sensing Image Segmentation
Authors: Xiaowen Ma, Rongrong Lian, Zhenkai Wu, Renxiang Guan, Tingfeng Hong, Mengjiao Zhao, Mengting Ma, Jiangtao Nie, Zhenhong Du, Siyang Song, Wei Zhang
Abstract: As a common method in the field of computer vision, spatial attention mechanism has been widely used in semantic segmentation of remote sensing images due to its outstanding long-range dependency modeling capability. However, remote sensing images are usually characterized by complex backgrounds and large intra-class variance that would degrade their analysis performance. While vanilla spatial attention mechanisms are based on dense affine operations, they tend to introduce a large amount of background contextual information and lack of consideration for intrinsic spatial correlation. To deal with such limitations, this paper proposes a novel scene-Coupling semantic mask network, which reconstructs the vanilla attention with scene coupling and local global semantic masks strategies. Specifically, scene coupling module decomposes scene information into global representations and object distributions, which are then embedded in the attention affinity processes. This Strategy effectively utilizes the intrinsic spatial correlation between features so that improve the process of attention modeling. Meanwhile, local global semantic masks module indirectly correlate pixels with the global semantic masks by using the local semantic mask as an intermediate sensory element, which reduces the background contextual interference and mitigates the effect of intra-class variance. By combining the above two strategies, we propose the model SCSM, which not only can efficiently segment various geospatial objects in complex scenarios, but also possesses inter-clean and elegant mathematical representations. Experimental results on four benchmark datasets demonstrate the the effectiveness of the above two strategies for improving the attention modeling of remote sensing images. The dataset and code are available at this https URL

Paper number 6:
Title: Need for Speed: A Comprehensive Benchmark of JPEG Decoders in Python
Authors: Vladimir Iglovikov
Abstract: Image loading represents a critical bottleneck in modern machine learning pipelines, particularly in computer vision tasks where JPEG remains the dominant format. This study presents a systematic performance analysis of nine popular Python JPEG decoding libraries on different computing architectures. We benchmark traditional image processing libraries (Pillow, OpenCV), machine learning frameworks (TensorFlow, PyTorch), and specialized decoders (jpeg4py, kornia-rs) on both ARM64 (Apple M4 Max) and x86\_64 (AMD Threadripper) platforms. Our findings reveal that modern implementations using libjpeg-turbo achieve up to 1.5x faster decoding speeds compared to traditional approaches. We provide evidence-based recommendations for choosing optimal JPEG decoders across different scenarios, from high-throughput training pipelines to real-time applications. This comprehensive analysis helps practitioners make informed decisions about image loading infrastructure, potentially reducing training times and improving system efficiency.

Paper number 7:
Title: UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior
Authors: I-Hsiang Chen, Wei-Ting Chen, Yu-Wei Liu, Yuan-Chun Chiang, Sy-Yen Kuo, Ming-Hsuan Yang
Abstract: Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios.

Paper number 8:
Title: Revisiting Data Augmentation for Ultrasound Images
Authors: Adam Tupper, Christian Gagné
Abstract: Data augmentation is a widely used and effective technique to improve the generalization performance of deep neural networks. Yet, despite often facing limited data availability when working with medical images, it is frequently underutilized. This appears to come from a gap in our collective understanding of the efficacy of different augmentation techniques across different tasks and modalities. One modality where this is especially true is ultrasound imaging. This work addresses this gap by analyzing the effectiveness of different augmentation techniques at improving model performance across a wide range of ultrasound image analysis tasks. To achieve this, we introduce a new standardized benchmark of 14 ultrasound image classification and semantic segmentation tasks from 10 different sources and covering 11 body regions. Our results demonstrate that many of the augmentations commonly used for tasks on natural images are also effective on ultrasound images, even more so than augmentations developed specifically for ultrasound images in some cases. We also show that diverse augmentation using TrivialAugment, which is widely used for natural images, is also effective for ultrasound images. Moreover, our proposed methodology represents a structured approach for assessing various data augmentations that can be applied to other contexts and modalities.

Paper number 9:
Title: Symbolic Control for Autonomous Docking of Marine Surface Vessels
Authors: Elizabeth Dietrich, Emir Cem Gezer, Bingzhuo Zhong, Murat Arcak, Majid Zamani, Roger Skjetne, Asgeir Johan Sørensen
Abstract: Docking marine surface vessels remains a largely manual task due to its safety-critical nature. In this paper, we develop a hierarchical symbolic control architecture for autonomous docking maneuvers of a dynamic positioning vessel, to provide formal safety guarantees. At the upper-level, we treat the vessel's desired surge, sway, and yaw velocities as control inputs and synthesize a symbolic controller in real-time. The desired velocities are then transmitted to and executed by the vessel's low-level velocity feedback control loop. Given a synthesized symbolic controller, we investigate methods to optimize the performance of the proposed control scheme for the docking task. The efficacy of this methodology is evaluated on a low-fidelity simulation model of a marine surface vessel in the presence of static and dynamic obstacles and, for the first time, through physical experiments on a scaled model vessel.

Paper number 10:
Title: Time-Constrained Model Predictive Control for Autonomous Satellite Rendezvous, Proximity Operations, and Docking
Authors: Gabriel Behrendt, Matthew Hale, Alexander Soderlund, Sean Phillips, Evan Kain
Abstract: This paper presents a time-constrained model predictive control strategy for the six degree-of-freedom autonomous rendezvous, proximity, operations and docking problem between a controllable "deputy" satellite and an uncontrolled "chief" satellite. The objective is to achieve a docking configuration defined by both the translational and attitudinal states of the deputy relative to the chief, whose dynamics are respectively governed by both the Clohessy-Wiltshire equations and Euler's second law of motion. The proposed control strategy explicitly addresses computational time constraints that are common to state-of-the-art space vehicles. Thus, a time-constrained model predictive control strategy is implemented on a space-grade processor. Although suboptimal with regards to energy consumption when compared to conventional optimal RPO trajectories, it is empirically demonstrated via numerical simulations that the deputy spacecraft still achieves a successful docking configuration while subject to computational time constraints.

Paper number 11:
Title: Distributed Multiple Testing with False Discovery Rate Control in the Presence of Byzantines
Authors: Daofu Zhang, Mehrdad Pournaderi, Yu Xiang, Pramod Varshney
Abstract: This work studies distributed multiple testing with false discovery rate (FDR) control in the presence of Byzantine attacks, where an adversary captures a fraction of the nodes and corrupts their reported p-values. We focus on two baseline attack models: an oracle model with the full knowledge of which hypotheses are true nulls, and a practical attack model that leverages the Benjamini-Hochberg (BH) procedure locally to classify which p-values follow the true null hypotheses. We provide a thorough characterization of how both attack models affect the global FDR, which in turn motivates counter-attack strategies and stronger attack models. Our extensive simulation studies confirm the theoretical results, highlight key design trade-offs under attacks and countermeasures, and provide insights into more sophisticated attacks.

Paper number 12:
Title: Generative Data Augmentation Challenge: Synthesis of Room Acoustics for Speaker Distance Estimation
Authors: Jackie Lin, Georg Götz, Hermes Sampedro Llopis, Haukur Hafsteinsson, Steinar Guðjónsson, Daniel Gert Nielsen, Finnur Pind, Paris Smaragdis, Dinesh Manocha, John Hershey, Trausti Kristjansson, Minje Kim
Abstract: This paper describes the synthesis of the room acoustics challenge as a part of the generative data augmentation workshop at ICASSP 2025. The challenge defines a unique generative task that is designed to improve the quantity and diversity of the room impulse responses dataset so that it can be used for spatially sensitive downstream tasks: speaker distance estimation. The challenge identifies the technical difficulty in measuring or simulating many rooms' acoustic characteristics precisely. As a solution, it proposes generative data augmentation as an alternative that can potentially be used to improve various downstream tasks. The challenge website, dataset, and evaluation code are available at this https URL.

Paper number 13:
Title: Comparative Withholding Behavior Analysis of Historical Energy Storage Bids in California
Authors: Neal Ma, Ningkun Zheng, Ning Qi, Bolun Xu
Abstract: The rapid growth of battery energy storage in wholesale electricity markets calls for a deeper understanding of storage operators' bidding strategies and their market impacts. This study examines energy storage bidding data from the California Independent System Operator (CAISO) between July 1, 2023, and October 1, 2024, with a primary focus on economic withholding strategies. Our analysis reveals that storage bids are closely aligned with day-ahead and real-time market clearing prices, with notable bid inflation during price spikes. Statistical tests demonstrate a strong correlation between price spikes and capacity withholding, indicating that operators can anticipate price surges and use market volatility to increase profitability. Comparisons with optimal hindsight bids further reveal a clear daily periodic bidding pattern, highlighting extensive economic withholding. These results underscore potential market inefficiencies and highlight the need for refined regulatory measures to address economic withholding as storage capacity in the market continues to grow.

Paper number 14:
Title: Scalable dataset acquisition for data-driven lensless imaging
Authors: Clara S. Hung, Leyla A. Kabuli, Vasilisa Ponomarenko, Laura Waller
Abstract: Data-driven developments in lensless imaging, such as machine learning-based reconstruction algorithms, require large datasets. In this work, we introduce a data acquisition pipeline that can capture from multiple lensless imaging systems in parallel, under the same imaging conditions, and paired with computational ground truth registration. We provide an open-access 25,000 image dataset with two lensless imagers, a reproducible hardware setup, and open-source camera synchronization code. Experimental datasets from our system can enable data-driven developments in lensless imaging, such as machine learning-based reconstruction algorithms and end-to-end system design.

Paper number 15:
Title: Joint Beamforming and Position Optimization for Fluid RIS-aided ISAC Systems
Authors: Junjie Ye, Peichang Zhang, Xiao-Peng Li, Lei Huang, YuanweiLiu
Abstract: A fluid reconfigurable intelligent surface (fRIS)-aided integrated sensing and communications (ISAC) system is proposed to enhance multi-target sensing and multi-user communication. Unlike the conventional RIS, the fRIS incorporates movable elements whose positions can be flexibly adjusted to provide extra spatial degrees of freedom. In this system, a joint optimization problem is formulated to minimize sensing beampattern mismatch and communication symbol estimation error by optimizing the symbol estimator, transmit beamformer, fRIS phase shifts, and element positions. To solve this problem, an algorithm based on alternating minimization is devised, where subproblems are solved leveraging augmented Lagrangian method, quadratic programming, semidefinite-relaxation, and majorization-minimization techniques. A key challenge exists that the fRIS element positions affect both the incident and reflective channels, leading to the high-order composite functions regarding the positions. As a remedy, it is proved that the high-order terms can be transformed to linear and linear-difference forms using the characteristics of fRIS and structural channels, which facilitates the position optimization. Numerical results validate the effectiveness of the proposed scheme as compared to the conventional RIS-aided ISAC systems and other benchmarks.

Paper number 16:
Title: Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond Voxel and Volumetric Embedding
Authors: Tianyuan Yao, Zhiyuan Li, Praitayini Kanakaraj, Derek B. Archer, Kurt Schilling, Lori Beason-Held, Susan Resnick, Bennett A. Landman, Yuankai Huo
Abstract: Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in neuroimaging. It is arguably the sole noninvasive technique for examining the microstructural properties and structural connectivity of the brain. Recent years have seen the emergence of machine learning and data-driven approaches that enhance the speed, accuracy, and consistency of dMRI data analysis. However, traditional deep learning models often fell short, as they typically utilize pixel-level or volumetric patch-level embeddings similar to those used in structural MRI, and do not account for the unique distribution of various gradient encodings. In this paper, we propose a novel method called Polyhedra Encoding Transformer (PE-Transformer) for dMRI, designed specifically to handle spherical signals. Our approach involves projecting an icosahedral polygon onto a unit sphere to resample signals from predetermined directions. These resampled signals are then transformed into embeddings, which are processed by a transformer encoder that incorporates orientational information reflective of the icosahedral structure. Through experimental validation with various gradient encoding protocols, our method demonstrates superior accuracy in estimating multi-compartment models and Fiber Orientation Distributions (FOD), outperforming both conventional CNN architectures and standard transformers.

Paper number 17:
Title: Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization
Authors: Peirong Liu, Ana Lawry Aguila, Juan E. Iglesias
Abstract: Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA's effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology. Code is available at this https URL.

Paper number 18:
Title: Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech Enhancement
Authors: Jae-Sung Bae, Anastasia Kuznetsova, Dinesh Manocha, John Hershey, Trausti Kristjansson, Minje Kim
Abstract: This paper presents a new challenge that calls for zero-shot text-to-speech (TTS) systems to augment speech data for the downstream task, personalized speech enhancement (PSE), as part of the Generative Data Augmentation workshop at ICASSP 2025. Collecting high-quality personalized data is challenging due to privacy concerns and technical difficulties in recording audio from the test scene. To address these issues, synthetic data generation using generative models has gained significant attention. In this challenge, participants are tasked first with building zero-shot TTS systems to augment personalized data. Subsequently, PSE systems are asked to be trained with this augmented personalized dataset. Through this challenge, we aim to investigate how the quality of augmented data generated by zero-shot TTS models affects PSE model performance. We also provide baseline experiments using open-source zero-shot TTS models to encourage participation and benchmark advancements. Our baseline code implementation and checkpoints are available online.

Paper number 19:
Title: Scalable Evaluation Framework for Foundation Models in Musculoskeletal MRI Bridging Computational Innovation with Clinical Utility
Authors: Gabrielle Hoyer, Michelle W Tong, Rupsa Bhattacharjee, Valentina Pedoia, Sharmila Majumdar
Abstract: Foundation models hold transformative potential for medical imaging, but their clinical utility requires rigorous evaluation to address their strengths and limitations. This study introduces an evaluation framework for assessing the clinical impact and translatability of SAM, MedSAM, and SAM2, using musculoskeletal MRI as a case study. We tested these models across zero-shot and finetuned paradigms to assess their ability to process diverse anatomical structures and effectuate clinically reliable biomarkers, including cartilage thickness, muscle volume, and disc height. We engineered a modular pipeline emphasizing scalability, clinical relevance, and workflow integration, reducing manual effort and aligning validation with end-user expectations. Hierarchical modeling revealed how dataset mixing, anatomical complexity, and MRI acquisition parameters influence performance, providing insights into the role of imaging refinements in improving segmentation accuracy. This work demonstrates how clinically focused evaluations can connect computational advancements with tangible applications, creating a pathway for foundation models to address medical challenges. By emphasizing interdisciplinary collaboration and aligning technical innovation with clinical priorities, our framework provides a roadmap for advancing machine learning technologies into scalable and impactful biomedical solutions.

Paper number 20:
Title: ROMA: ROtary and Movable Antenna
Authors: Jiayi Zhang, Wenhui Yi, Bokai Xu, Zhe Wang, Huahua Xiao, Bo Ai
Abstract: The rotary and movable antenna (ROMA) architecture represents a next-generation multi-antenna technology that enables flexible adjustment of antenna position and array rotation angles of the transceiver. In this letter, we propose a ROMA-aided multi-user MIMO communication system to fully enhance the efficiency and reliability of system transmissions. By deploying ROMA panels at both the transmitter and receiver sides, and jointly optimizing the three-dimensional (3D) rotation angles of each ROMA panel and the relative positions of antenna elements based on the spatial distribution of users and channel state information (CSI), we can achieve the objective of maximizing the average spectral efficiency (SE). Subsequently, we conduct a detailed analysis of the average SE performance of the system under the consideration of maximum ratio (MR) precoding. Due to the non-convexity of the optimization problem in the ROMA multi-user MIMO system, we propose an efficient solution based on an alternating optimization (AO) algorithm. Finally, simulation results demonstrate that the AO-based ROMA architecture can significantly improve the average SE. Furthermore, the performance improvement becomes more pronounced as the size of the movable region and the transmission power increase.

Paper number 21:
Title: Deep Multi-modal Neural Receiver for 6G Vehicular Communication
Authors: Osama Saleem, Mohammed Alfaqawi, Pierre Merdrignac, Abdelaziz Bensrhair, Soheyb Ribouh
Abstract: Deep Learning (DL) based neural receiver models are used to jointly optimize PHY of baseline receiver for cellular vehicle to everything (C-V2X) system in next generation (6G) communication, however, there has been no exploration of how varying training parameters affect the model's efficiency. Additionally, a comprehensive evaluation of its performance on multi-modal data remains largely unexplored. To address this, we propose a neural receiver designed to optimize Bit Error Rate (BER) for vehicle to network (V2N) uplink scenario in 6G network. We train multiple neural receivers by changing its trainable parameters and use the best fit model as proposition for large scale deployment. Our proposed neural receiver gets signal in frequency domain at the base station (BS) as input and generates optimal log likelihood ratio (LLR) at the output. It estimates the channel based on the received signal, equalizes and demodulates the higher order modulated signal. Later, to evaluate multi-modality of the proposed model, we test it across diverse V2X data flows (e.g., image, video, gps, lidar cloud points and radar detection signal). Results from simulation clearly indicates that our proposed multi-modal neural receiver outperforms state-of-the-art receiver architectures by achieving high performance at low Signal to Noise Ratio (SNR).

Paper number 22:
Title: Radio Map Estimation via Latent Domain Plug-and-Play Denoising
Authors: Le Xu, Lei Cheng, Junting Chen, Wenqiang Pu, Xiao Fu
Abstract: Radio map estimation (RME), also known as spectrum cartography, aims to reconstruct the strength of radio interference across different domains (e.g., space and frequency) from sparsely sampled measurements. To tackle this typical inverse problem, state-of-the-art RME methods rely on handcrafted or data-driven structural information of radio maps. However, the former often struggles to model complex radio frequency (RF) environments and the latter requires excessive training -- making it hard to quickly adapt to in situ sensing tasks. This work presents a spatio-spectral RME approach based on plug-and-play (PnP) denoising, a technique from computational imaging. The idea is to leverage the observation that the denoising operations of signals like natural images and radio maps are similar -- despite the nontrivial differences of the signals themselves. Hence, sophisticated denoisers designed for or learned from natural images can be directly employed to assist RME, avoiding using radio map data for training. Unlike conventional PnP methods that operate directly in the data domain, the proposed method exploits the underlying physical structure of radio maps and proposes an ADMM algorithm that denoises in a latent domain. This design significantly improves computational efficiency and enhances noise robustness. Theoretical aspects, e.g., recoverability of the complete radio map and convergence of the ADMM algorithm are analyzed. Synthetic and real data experiments are conducted to demonstrate the effectiveness of our approach.

Paper number 23:
Title: Risk and Vulnerability Assessment of Energy-Transportation Infrastructure Systems to Extreme Weather
Authors: Jiawei Wang, Qinglai Guo, Hongbin Sun
Abstract: The interaction between extreme weather events and interdependent critical infrastructure systems involves complex spatiotemporal dynamics. Multi-type emergency decisions within energy-transportation infrastructures significantly influence system performance throughout the extreme weather process. A comprehensive assessment of these factors faces challenges in model complexity and heterogeneity between energy and transportation systems. This paper proposes an assessment framework that accommodates multiple types of emergency decisions. It integrates the heterogeneous energy and transportation infrastructures in the form of a network flow model to simulate and quantify the impact of extreme weather events on the energy-transportation infrastructure system. Based on this framework, a targeted method for identifying system vulnerabilities is further introduced, utilizing a neural network surrogate that achieves privacy protection and evaluation acceleration while maintaining consideration of system interdependencies. Numerical experiments demonstrate that the proposed framework and method can reveal the risk levels faced by urban infrastructure systems, identify weak points that should be prioritized for reinforcement, and strike a balance between accuracy and evaluation speed.

Paper number 24:
Title: Leveraging Digital Twin and Machine Learning Techniques for Anomaly Detection in Power Electronics Dominated Grid
Authors: Ildar N. Idrisov, Divine Okeke, Abdullatif Albaseer, Mohamed Abdallah, Federico M. Ibanez
Abstract: Modern power grids are transitioning towards power electronics-dominated grids (PEDG) due to the increasing integration of renewable energy sources and energy storage systems. This shift introduces complexities in grid operation and increases vulnerability to cyberattacks. This research explores the application of digital twin (DT) technology and machine learning (ML) techniques for anomaly detection in PEDGs. A DT can accurately track and simulate the behavior of the physical grid in real-time, providing a platform for monitoring and analyzing grid operations, with extended amount of data about dynamic power flow along the whole power system. By integrating ML algorithms, the DT can learn normal grid behavior and effectively identify anomalies that deviate from established patterns, enabling early detection of potential cyberattacks or system faults. This approach offers a comprehensive and proactive strategy for enhancing cybersecurity and ensuring the stability and reliability of PEDGs.

Paper number 25:
Title: Integrated 6G TN and NTN Localization: Challenges, Opportunities, and Advancements
Authors: Sharief Saleh, Pinjun Zheng, Xing Liu, Hui Chen, Musa Furkan Keskin, Basuki Priyanto, Martin Beale, Yasaman Ettefagh, Gonzalo Seco-Granados, Tareq Y. Al-Naffouri, Henk Wymeersch
Abstract: The rapid evolution of cellular networks has introduced groundbreaking technologies, including large and distributed antenna arrays and reconfigurable intelligent surfaces in terrestrial networks (TNs), as well as aerial and space-based nodes in non-terrestrial networks (NTNs). These advancements enable applications beyond traditional communication, such as high-precision localization and sensing. While integrating TN and NTN enablers will lead to unparalleled opportunities for seamless global localization, such integration attempts are expected to face several challenges. To understand these opportunities and challenges, we first examine the distinctive characteristics of the key 6G enablers, evaluating their roles in localization from both technical and practical perspectives. Next, to identify developments driving TN-NTN localization, we review the latest standardization and industrial innovation progress. Finally, we discuss the opportunities and challenges of TN-NTN integration, illustrating its potential through two numerical case studies.

Paper number 26:
Title: Interference Prediction Using Gaussian Process Regression and Management Framework for Critical Services in Local 6G Networks
Authors: Syed Luqman Shah, Nurul Huda Mahmood, Matti Latva-aho
Abstract: Interference prediction and resource allocation are critical challenges in mission-critical applications where stringent latency and reliability constraints must be met. This paper proposes a novel Gaussian process regression (GPR)-based framework for predictive interference management and resource allocation in future 6G networks. Firstly, the received interference power is modeled as a Gaussian process, enabling both the prediction of future interference values and their corresponding estimation of uncertainty bounds. Differently from conventional machine learning methods that extract patterns from a given set of data without any prior belief, a Gaussian process assigns probability distributions to different functions that possibly represent the data set which can be further updates using Bayes' rule as more data points are observed. For instance, unlike deep neural networks, the GPR model requires only a few sample points to update its prior beliefs in real-time. Furthermore, we propose a proactive resource allocation scheme that dynamically adjusts resources according to predicted interference. The performance of the proposed approach is evaluated against two benchmarks prediction schemes, a moving average-based estimator and the ideal genie-aided estimator. The GPR-based method outperforms the moving average-based estimator and achieves near-optimal performance, closely matching the genie-aided benchmark.

Paper number 27:
Title: Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop and Fault-Ride-Through capabilities of PV inverters
Authors: Carina Lehmal, Ziqian Zhang, Philipp Hackl, Robert Schürhuber
Abstract: The deployment of PV inverters is rapidly expanding across Europe, where these devices must increasingly comply with stringent grid this http URL study presents a benchmark analysis of four PV inverter manufacturers, focusing on their Fault Ride Through capabilities under varying grid strengths, voltage dips, and fault durations, parameters critical for grid operators during fault this http URL findings highlight the influence of different inverter controls on key metrics such as total harmonic distortion of current and voltage signals, as well as system stability following grid this http URL, the study evaluates transient stability using two distinct testing this http URL first approach employs the current standard method, which is testing with an ideal voltage source. The second utilizes a Power Hardware in the Loop methodology with a benchmark CIGRE grid this http URL results reveal that while testing with an ideal voltage source is cost-effective and convenient in the short term, it lacks the ability to capture the dynamic interactions and feedback loops of physical grid this http URL limitation can obscure critical real world factors, potentially leading to unexpected inverter behavior and operational challenges in grids with high PV this http URL study underscores the importance of re-evaluating conventional testing methods and incorporating Power Hardware in the Loop structures to achieve test results that more closely align with real-world conditions.

Paper number 28:
Title: Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement
Authors: Chenxu Wu, Qingpeng Kong, Zihang Jiang, S. Kevin Zhou
Abstract: Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks.

Paper number 29:
Title: A Dual-Polarization Feature Fusion Network for Radar Automatic Target Recognition Based On HRRP Sequence
Authors: Yangbo Zhou, Sen Liu, Hong-Wei Gao, Hai lin, Guohua Wei, Xiaoqing Wang, Xiao-Min Pan
Abstract: Recent advances in radar automatic target recognition (RATR) techniques utilizing deep neural networks have demonstrated remarkable performance, largely due to their robust generalization capabilities. To address the challenge for applications with polarimetric HRRP sequences, a dual-polarization feature fusion network (DPFFN) is proposed along with a novel two-stage feature fusion strategy. Moreover, a specific fusion loss function is developed, which enables the adaptive generation of comprehensive multi-modal representations from polarimetric HRRP sequences. Experimental results demonstrate that the proposed network significantly improves performance in radar target recognition tasks, thus validating its effectiveness. The PyTorch implementation of our proposed DPFFN is available at this https URL.

Paper number 30:
Title: Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation
Authors: Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri
Abstract: Artificial intelligence (AI) is expected to significantly enhance radio resource management (RRM) in sixth-generation (6G) networks. However, the lack of explainability in complex deep learning (DL) models poses a challenge for practical implementation. This paper proposes a novel explainable AI (XAI)- based framework for feature selection and model complexity reduction in a model-agnostic manner. Applied to a multi-agent deep reinforcement learning (MADRL) setting, our approach addresses the joint sub-band assignment and power allocation problem in cellular vehicle-to-everything (V2X) communications. We propose a novel two-stage systematic explainability framework leveraging feature relevance-oriented XAI to simplify the DRL agents. While the former stage generates a state feature importance ranking of the trained models using Shapley additive explanations (SHAP)-based importance scores, the latter stage exploits these importance-based rankings to simplify the state space of the agents by removing the least important features from the model input. Simulation results demonstrate that the XAI-assisted methodology achieves 97% of the original MADRL sum-rate performance while reducing optimal state features by 28%, average training time by 11%, and trainable weight parameters by 46% in a network with eight vehicular pairs.

Paper number 31:
Title: Instantaneous Core Loss -- Cycle-by-cycle Modeling of Power Magnetics in PWM DC-AC Converters
Authors: Binyu Cui, Jun Wang
Abstract: Nowadays, PWM excitation is one of the most common waveforms seen by magnetic components in power electronic converters. Core loss modeling approaches such as the improved Generalized Steinmetz Equation (iGSE) or the loss map based on the composite waveform hypothesis (CWH) generally process the PWM excitation piecewise, which has been proven effective for DC-DC converters. However, an additional challenge arises in DC-AC converters, i.e. the fundamental-frequency sinewave component induces the major loop loss on top of the piecewise high-frequency segments. This major loop loss cannot be modeled on a switching cycle basis by any existing methods. To address this gap, this paper proposes a novel fundamental concept, instantaneous core loss, which is observed empirically for the first time. Enabled by the reactive voltage cancellation method, the instantaneous core loss, which only contains the real power loss, can be measured as a function of time. Building on this concept, a modeling method is proposed to break down the major loop core loss, typically represented as an average value in the literature, into the time domain, enabling cycle-by-cycle modeling as a practical workflow for predicting core losses in PWM converters. Through experiments, the existence of the major loop loss is verified, and generic instantaneous core loss models are extracted for several magnetic components. An example workflow is proposed to extract the cycle-by-cycle core loss at the design stage of a PWM DC-AC converter. This work enhances the fundamental understanding of the core loss process in real time, contributing to the advancement of scientific knowledge.

Paper number 32:
Title: Survey of image processing settings used for mammography systems in the United Kingdom: how variable is it?
Authors: Alistair Mackenzie, John Loveland, Ruben van Engen
Abstract: The aim was to undertake a national survey of the setup of mammography imaging systems in the UK, we were particularly interested in image processing and software version. We created a program that can extract selected tags from the DICOM header. 28 medical physics departments used the program on processed images of the TORMAM phantom acquired since 2023 and this produced data for 497 systems. We received data for 7 different models of mammography systems. We found that currently in use each model had between 2 and 7 different versions of software for the acquisition workstation. Each of the systems had multiple versions of image processing settings, a preliminary investigation with TORMAM demonstrated large differences in the appearance of the image for the same X-ray model. The Fujifilm, GE and Siemens systems showed differences in the setup of the dose levels. In addition to these settings there were differences in the paddles used and grid type. Our snapshot of system set up showed that there is a potential for the images to appear differently according to the settings seen in the headers. These differences may affect the outcomes of AI and also human readers. Thus the introduction of AI must take these differences into consideration and the inevitably changes of settings in the future. There are responsibilities on AI suppliers, physics, mammographic equipment manufacturers, and breast-screening units to manage the use of AI and ensure the outcomes of breast screening are not adversely affected by the set-up of equipment.

Paper number 33:
Title: Learning-based A Posteriori Speech Presence Probability Estimation and Applications
Authors: Shuai Tao, Jesper Rindom Jensen, Yang Xiang, Himavanth Reddy, Qingzheng Zhang, Mads Græsbøll Christensen
Abstract: The a posteriori speech presence probability (SPP) is the fundamental component of noise power spectral density (PSD) estimation, which can contribute to speech enhancement and speech recognition systems. Most existing SPP estimators can estimate SPP accurately from the background noise. Nevertheless, numerous challenges persist, including the difficulty of accurately estimating SPP from non-stationary noise with statistics-based methods and the high latency associated with deep learning-based approaches. This paper presents an improved SPP estimation approach based on deep learning to achieve higher SPP estimation accuracy, especially in non-stationary noise conditions. To promote the information extraction performance of the DNN, the global information of the observed signal and the local information of the decoupled frequency bins from the observed signal are connected as hybrid global-local information. The global information is extracted by one encoder. Then, one decoder and two fully connected layers are used to estimate SPP from the information of residual connection. To evaluate the performance of our proposed SPP estimator, the noise PSD estimation and speech enhancement tasks are performed. In contrast to existing minimum mean-square error (MMSE)-based noise PSD estimation approaches, the noise PSD is estimated by the sub-optimal MMSE based on the current frame SPP estimate without smoothing. Directed by the noise PSD estimate, a standard speech enhancement framework, the log spectral amplitude estimator, is employed to extract clean speech from the observed signal. From the experimental results, we can confirm that our proposed SPP estimator can achieve high noise PSD estimation accuracy and speech enhancement performance while requiring low model complexity.

Paper number 34:
Title: Enhancing Medical Image Analysis through Geometric and Photometric transformations
Authors: Khadija Rais, Mohamed Amroune, Mohamed Yassine Haouam
Abstract: Medical image analysis suffers from a lack of labeled data due to several challenges including patient privacy and lack of experts. Although some AI models only perform well with large amounts of data, we will move to data augmentation where there is a solution to improve the performance of our models and increase the dataset size through traditional or advanced techniques. In this paper, we evaluate the effectiveness of data augmentation techniques on two different medical image datasets. In the first step, we applied some transformation techniques to the skin cancer dataset containing benign and malignant classes. Then, we trained the convolutional neural network (CNN) on the dataset before and after augmentation, which significantly improved test accuracy from 90.74% to 96.88% and decreased test loss from 0.7921 to 0.1468 after augmentation. In the second step, we used the Mixup technique by mixing two random images and their corresponding masks using the retina and blood vessels dataset, then we trained the U-net model and obtained the Dice coefficient which increased from 0 before augmentation to 0.4163 after augmentation. The result shows the effect of using data augmentation to increase the dataset size on the classification and segmentation performance.

Paper number 35:
Title: Variational U-Net with Local Alignment for Joint Tumor Extraction and Registration (VALOR-Net) of Breast MRI Data Acquired at Two Different Field Strengths
Authors: Muhammad Shahkar Khan, Haider Ali, Laura Villazan Garcia, Noor Badshah, Siegfried Trattnig, Florian Schwarzhans, Ramona Woitek, Olgica Zaric
Abstract: Background: Multiparametric breast MRI data might improve tumor diagnostics, characterization, and treatment planning. Accurate alignment and delineation of images acquired at different field strengths such as 3T and 7T, remain challenging research tasks. Purpose: To address alignment challenges and enable consistent tumor segmentation across different MRI field strengths. Study type: Retrospective. Subjects: Nine female subjects with breast tumors were involved: six histologically proven invasive ductal carcinomas (IDC) and three fibroadenomas. Field strength/sequence: Imaging was performed at 3T and 7T scanners using post-contrast T1-weighted three-dimensional time-resolved angiography with stochastic trajectories (TWIST) sequence. Assessments: The method's performance for joint image registration and tumor segmentation was evaluated using several quantitative metrics, including signal-to-noise ratio (PSNR), structural similarity index (SSIM), normalized cross-correlation (NCC), Dice coefficient, F1 score, and relative sum of squared differences (rel SSD). Statistical tests: The Pearson correlation coefficient was used to test the relationship between the registration and segmentation metrics. Results: When calculated for each subject individually, the PSNR was in a range from 27.5 to 34.5 dB, and the SSIM was from 82.6 to 92.8%. The model achieved an NCC from 96.4 to 99.3% and a Dice coefficient of 62.9 to 95.3%. The F1 score was between 55.4 and 93.2% and the rel SSD was in the range of 2.0 and 7.5%. The segmentation metrics Dice and F1 Score are highly correlated (0.995), while a moderate correlation between NCC and SSIM (0.681) was found for registration. Data conclusion: Initial results demonstrate that the proposed method may be feasible in providing joint tumor segmentation and registration of MRI data acquired at different field strengths.

Paper number 36:
Title: Safety in safe Bayesian optimization and its ramifications for control
Authors: Christian Fiedler, Johanna Menn, Sebastian Trimpe
Abstract: A recurring and important task in control engineering is parameter tuning under constraints, which conceptually amounts to optimization of a blackbox function accessible only through noisy evaluations. For example, in control practice parameters of a pre-designed controller are often tuned online in feedback with a plant, and only safe parameter values should be tried, avoiding for example instability. Recently, machine learning methods have been deployed for this important problem, in particular, Bayesian optimization (BO). To handle safety constraints, algorithms from safe BO have been utilized, especially SafeOpt-type algorithms, which enjoy considerable popularity in learning-based control, robotics, and adjacent fields. However, we identify two significant obstacles to practical safety. First, SafeOpt-type algorithms rely on quantitative uncertainty bounds, and most implementations replace these by theoretically unsupported heuristics. Second, the theoretically valid uncertainty bounds crucially depend on a quantity - the reproducing kernel Hilbert space norm of the target function - that at present is impossible to reliably bound using established prior engineering knowledge. By careful numerical experiments we show that these issues can indeed cause safety violations. To overcome these problems, we propose Lipschitz-only Safe Bayesian Optimization (LoSBO), a safe BO algorithm that relies only on a known Lipschitz bound for its safety. Furthermore, we propose a variant (LoS-GP-UCB) that avoids gridding of the search space and is therefore applicable even for moderately high-dimensional problems.

Paper number 37:
Title: The First Indoor Pathloss Radio Map Prediction Challenge
Authors: Stefanos Bakirtzis, Çağkan Yapar, Kehai Qiu, Ian Wassell, Jie Zhang
Abstract: To encourage further research and to facilitate fair comparisons in the development of deep learning-based radio propagation models, in the less explored case of directional radio signal emissions in indoor propagation environments, we have launched the ICASSP 2025 First Indoor Pathloss Radio Map Prediction Challenge. This overview paper describes the indoor path loss prediction problem, the datasets used, the Challenge tasks, and the evaluation methodology. Finally, the results of the Challenge and a summary of the submitted methods are presented.

Paper number 38:
Title: GenTL: A General Transfer Learning Model for Building Thermal Dynamics
Authors: Fabian Raisch, Thomas Krug, Christoph Goebel, Benjamin Tischler
Abstract: Transfer Learning (TL) is an emerging field in modeling building thermal dynamics. This method reduces the data required for a data-driven model of a target building by leveraging knowledge from a source building. Consequently, it enables the creation of data-efficient models that can be used for advanced control and fault detection & diagnosis. A major limitation of the TL approach is its inconsistent performance across different sources. Although accurate source-building selection for a target is crucial, it remains a persistent challenge. We present GenTL, a general transfer learning model for single-family houses in Central Europe. GenTL can be efficiently fine-tuned to a large variety of target buildings. It is pretrained on a Long Short-Term Memory (LSTM) network with data from 450 different buildings. The general transfer learning model eliminates the need for source-building selection by serving as a universal source for fine-tuning. Comparative analysis with conventional single-source to single-target TL demonstrates the efficacy and reliability of the general pretraining approach. Testing GenTL on 144 target buildings for fine-tuning reveals an average prediction error (RMSE) reduction of 42.1 % compared to fine-tuning single-source models.

Paper number 39:
Title: Parameterized Hardware Architecture for Frame Synchronization at all Noise Levels
Authors: Dimitris Nikolaidis
Abstract: Frame synchronization is the act of discerning the first bit of a valid data frame inside an incoming transmission. This is particularly important in high-noise environments where the communication channel significantly alters transmitted signals. Sync word frame synchronization is a subcategory of synchronization methods where sync words are detected through digital correlation. Despite its simplicity, this method has been overlooked in literature in favor of more sophisticated and mathematically more optimal solutions. In this article we employ binary sync-word correlation-based synchronization to achieve near perfect frame synchronization at any noise level. The proposed architecture leverages XNOR gates, adder and comparator tree structures to detect sync words that are placed in front of the frames through digital correlation. The tree structures are circuit elements that mimic binary trees in form and provide the summation (adder tree) or the maximum/minimum (comparator tree) of a set of binary numbers as output. Due to their minimalistic nature, synchronization can be implemented practically for very large sync word sizes (>500 bit) with multigigabit bit rates (>20 Gbps) and very high accuracy (10e-5 synchronization error when the bit error rate on the bitstream is close to 0.3) on commercial FPGAs. The architecture also delivers the payload of the frames to its output as an extra function.

Paper number 40:
Title: Maximum-Entropy-Rate Selection of Features for Classifying Changes in Knee and Ankle Dynamics During Running
Authors: Garry A. Einicke, Haider A. Sabti, David V. Thiel, Marta Fernandez
Abstract: This paper investigates deteriorations in knee and ankle dynamics during running. Changes in lower limb accelerations are analyzed by a wearable musculo-skeletal monitoring system. The system employs a machine learning technique to classify joint stiffness. A maximum-entropyrate method is developed to select the most relevant features. Experimental results demonstrate that distance travelled and energy expended can be estimated from observed changes in knee and ankle motions during 5 km runs.

Paper number 41:
Title: On Disentangled Training for Nonlinear Transform in Learned Image Compression
Authors: Han Li, Shaohui Li, Wenrui Dai, Maida Cao, Nuowen Kan, Chenglin Li, Junni Zou, Hongkai Xiong
Abstract: Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, i.e., feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance. Code will be released at \url{this https URL}

Paper number 42:
Title: Delay-Doppler Multiplexing With Global Filtering
Authors: Jialiang Zhu, Mohsen Bayat, Arman Farhang
Abstract: This paper proposes a novel modulation technique called globally filtered orthogonal time frequency space (GF-OTFS) which integrates single-carrier frequency division multiple access (SC-FDMA)-based delay-Doppler representation with universal filtered multi-carrier (UFMC) modulation. Our proposed technique first arranges the frequency-Doppler bins of an orthogonal time frequency space (OTFS) frame in adjacency using SC-FDMA and then applies universal filtering to the neighboring signals to mitigate inter-Doppler interference (IDI). By employing this approach, GF-OTFS achieves superior spectral containment and effectively mitigates interference caused by Doppler shifts in dynamic, time-varying channels. This paper also presents a detailed mathematical formulation of the proposed modulation technique. Furthermore, a comprehensive performance evaluation is conducted, comparing our GF-OTFS approach to state-of-the-art techniques, including Doppler-resilient UFMC (DR-UFMC) and receiver windowed OTFS (RW-OTFS). Key performance metrics, such as bit error rate (BER) and out-of-band (OOB) emissions, as well as the Doppler spread reduction are analyzed to assess the effectiveness of each approach. The results indicate that our proposed technique achieves comparable BER performance while significantly improving spectral containment.

Paper number 43:
Title: Towards Real-World Validation of a Physics-Based Ship Motion Prediction Model
Authors: Michail Mathioudakis, Christos Papandreou, Theodoros Stouraitis, Vicky Margari, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos, Kostas J. Spyrou
Abstract: The maritime industry aims towards a sustainable future, which requires significant improvements in operational efficiency. Current approaches focus on minimising fuel consumption and emissions through greater autonomy. Efficient and safe autonomous navigation requires high-fidelity ship motion models applicable to real-world conditions. Although physics-based ship motion models can predict ships' motion with sub-second resolution, their validation in real-world conditions is rarely found in the literature. This study presents a physics-based 3D dynamics motion model that is tailored to a container-ship, and compares its predictions against real-world voyages. The model integrates vessel motion over time and accounts for its hydrodynamic behavior under different environmental conditions. The model's predictions are evaluated against real vessel data both visually and using multiple distance measures. Both methodologies demonstrate that the model's predictions align closely with the real-world trajectories of the container-ship.

Paper number 44:
Title: Exploring Finetuned Audio-LLM on Heart Murmur Features
Authors: Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang
Abstract: Large language models (LLMs) for audio have excelled in recognizing and analyzing human speech, music, and environmental sounds. However, their potential for understanding other types of sounds, particularly biomedical sounds, remains largely underexplored despite significant scientific interest. In this study, we focus on diagnosing cardiovascular diseases using phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN) paradigms are restricted to heart murmur classification (healthy vs unhealthy) and do not predict other acoustic features of the murmur such as timing, grading, harshness, pitch, and quality, which are important in helping physicians diagnose the underlying heart conditions. We propose to finetune an audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG) dataset and evaluate its performance in classifying 11 expert-labeled murmur features. Additionally, we aim to achieve more noise-robust and generalizable system by exploring a preprocessing segmentation algorithm using an audio representation model, SSAMBA. Our results indicate that the LLM-based model outperforms state-of-the-art methods in 8 of the 11 features and performs comparably in the remaining 3. Moreover, the LLM successfully classifies long-tail murmur features with limited training data, a task that all previous methods have failed to classify. These findings underscore the potential of audio LLMs as assistants to human cardiologists in enhancing heart disease diagnosis.

Paper number 45:
Title: A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat Using Leader-Follower Strategy
Authors: Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao
Abstract: Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an evolving field in both aerospace and artificial intelligence. This paper aims to enhance adversarial performance through collaborative strategies. Previous approaches predominantly discretize the action space into predefined actions, limiting UAV maneuverability and complex strategy implementation. Others simplify the problem to 1v1 combat, neglecting the cooperative dynamics among multiple UAVs. To address the high-dimensional challenges inherent in six-degree-of-freedom space and improve cooperation, we propose a hierarchical framework utilizing the Leader-Follower Multi-Agent Proximal Policy Optimization (LFMAPPO) strategy. Specifically, the framework is structured into three levels. The top level conducts a macro-level assessment of the environment and guides execution policy. The middle level determines the angle of the desired action. The bottom level generates precise action commands for the high-dimensional action space. Moreover, we optimize the state-value functions by assigning distinct roles with the leader-follower strategy to train the top-level policy, followers estimate the leader's utility, promoting effective cooperation among agents. Additionally, the incorporation of a target selector, aligned with the UAVs' posture, assesses the threat level of targets. Finally, simulation experiments validate the effectiveness of our proposed method.

Paper number 46:
Title: On the reproducibility of discrete-event simulation studies in health research: an empirical study using open models
Authors: Amy Heather, Thomas Monks, Alison Harper, Navonil Mustafee, Andrew Mayne
Abstract: Reproducibility of computational research is critical for ensuring transparency, reliability and reusability. Challenges with computational reproducibility have been documented in several fields, but healthcare discrete-event simulation (DES) models have not been thoroughly examined in this context. This study assessed the computational reproducibility of eight published healthcare DES models (Python or R), selected to represent diverse contexts, complexities, and years of publication. Repositories and articles were also assessed against guidelines and reporting standards, offering insights into their relationship with reproducibility success. Reproducing results required up to 28 hours of troubleshooting per model, with 50% fully reproduced and 50% partially reproduced (12.5% to 94.1% of reported outcomes). Key barriers included the absence of open licences, discrepancies between reported and coded parameters, and missing code to produce model outputs, run scenarios, and generate tables and figures. Addressing these issues would often require relatively little effort from authors: adding an open licence and sharing all materials used to produce the article. Actionable recommendations are proposed to enhance reproducibility practices for simulation modellers and reviewers.

Paper number 47:
Title: Active RIS-Assisted URLLC NOMA-Based 5G Network with FBL under Jamming Attacks
Authors: Ghazal Asemian, Mohammadreza Amini, Burak Kantarci
Abstract: In this paper, we tackle the challenge of jamming attacks in Ultra-Reliable Low Latency Communication (URLLC) within Non-Orthogonal Multiple Access (NOMA)-based 5G networks under Finite Blocklength (FBL) conditions. We introduce an innovative approach that employs Reconfigurable Intelligent Surfaces (RIS) with active elements to enhance energy efficiency while ensuring reliability and meeting latency requirements. Our approach incorporates the traffic model, making it practical for real-world scenarios with dynamic traffic loads. We thoroughly analyze the impact of blocklength and packet arrival rate on network performance metrics and investigate the optimal amplitude value and number of RIS elements. Our results indicate that increasing the number of RIS elements from 4 to 400 can improve signal-to-jamming-plus-noise ratio (SJNR) by 13.64\%. Additionally, optimizing blocklength and packet arrival rate can achieve a 31.68% improvement in energy efficiency and reduced latency. These findings underscore the importance of optimized settings for effective jamming mitigation.

Paper number 48:
Title: Multimodal AI on Wound Images and Clinical Notes for Home Patient Referral
Authors: Reza Saadati Fard, Emmanuel Agu, Palawat Busaranuvong, Deepak Kumar, Shefalika Gautam, Bengisu Tulu, Diane Strong
Abstract: Chronic wounds affect 8.5 million Americans, particularly the elderly and patients with diabetes. These wounds can take up to nine months to heal, making regular care essential to ensure healing and prevent severe outcomes like limb amputations. Many patients receive care at home from visiting nurses with varying levels of wound expertise, leading to inconsistent care. Problematic, non-healing wounds should be referred to wound specialists, but referral decisions in non-clinical settings are often erroneous, delayed, or unnecessary. This paper introduces the Deep Multimodal Wound Assessment Tool (DM-WAT), a machine learning framework designed to assist visiting nurses in deciding whether to refer chronic wound patients. DM-WAT analyzes smartphone-captured wound images and clinical notes from Electronic Health Records (EHRs). It uses DeiT-Base-Distilled, a Vision Transformer (ViT), to extract visual features from images and DeBERTa-base to extract text features from clinical notes. DM-WAT combines visual and text features using an intermediate fusion approach. To address challenges posed by a small and imbalanced dataset, it integrates image and text augmentation with transfer learning to achieve high performance. In evaluations, DM-WAT achieved 77% with std 3% accuracy and a 70% with std 2% F1 score, outperforming prior approaches. Score-CAM and Captum interpretation algorithms provide insights into specific parts of image and text inputs that influence recommendations, enhancing interpretability and trust.

Paper number 49:
Title: Exploring GPT's Ability as a Judge in Music Understanding
Authors: Kun Fang, Ziyu Wang, Gus Xia, Ichiro Fujinaga
Abstract: Recent progress in text-based Large Language Models (LLMs) and their extended ability to process multi-modal sensory data have led us to explore their applicability in addressing music information retrieval (MIR) challenges. In this paper, we use a systematic prompt engineering approach for LLMs to solve MIR problems. We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors in three key MIR tasks: beat tracking, chord extraction, and key estimation. A concept augmentation method is proposed to evaluate LLMs' music reasoning consistency with the provided music concepts in the prompts. Our experiments tested the MIR capabilities of Generative Pre-trained Transformers (GPT). Results show that GPT has an error detection accuracy of 65.20%, 64.80%, and 59.72% in beat tracking, chord extraction, and key estimation tasks, respectively, all exceeding the random baseline. Moreover, we observe a positive correlation between GPT's error finding accuracy and the amount of concept information provided. The current findings based on symbolic music input provide a solid ground for future LLM-based MIR research.

Paper number 50:
Title: Threat-based Security Controls to Protect Industrial Control Systems
Authors: Maryam Karimi, Haritha Srinivasan
Abstract: This paper analyzes the reported threats to Industrial Control Systems (ICS)/Operational Technology (OT) and identifies common tactics, techniques, and procedures (TTP) used by threat actors. The paper then uses the MITRE ATT&CK framework to map the common TTPs and provide an understanding of the security controls needed to defend against the reported ICS threats. The paper also includes a review of ICS testbeds and ideas for future research using the identified controls.

Paper number 51:
Title: Gradient-Free Adversarial Purification with Diffusion Models
Authors: Xuelong Dai, Dong Wang, Duan Mingxing, Bin Xiao
Abstract: Adversarial training and adversarial purification are two effective and practical defense methods to enhance a model's robustness against adversarial attacks. However, adversarial training necessitates additional training, while adversarial purification suffers from low time efficiency. More critically, current defenses are designed under the perturbation-based adversarial threat model, which is ineffective against the recently proposed unrestricted adversarial attacks. In this paper, we propose an effective and efficient adversarial defense method that counters both perturbation-based and unrestricted adversarial attacks. Our defense is inspired by the observation that adversarial attacks are typically located near the decision boundary and are sensitive to pixel changes. To address this, we introduce adversarial anti-aliasing to mitigate adversarial modifications. Additionally, we propose adversarial super-resolution, which leverages prior knowledge from clean datasets to benignly recover images. These approaches do not require additional training and are computationally efficient without calculating gradients. Extensive experiments against both perturbation-based and unrestricted adversarial attacks demonstrate that our defense method outperforms state-of-the-art adversarial purification methods.

Paper number 52:
Title: A light-weight model to generate NDWI from Sentinel-1
Authors: Saleh Sakib Ahmed, Saifur Rahman Jony, Md. Toufikuzzaman, Saifullah Sayed, Rashed Uz Zzaman, Sara Nowreen, M. Sohel Rahman
Abstract: The use of Sentinel-2 images to compute Normalized Difference Water Index (NDWI) has many applications, including water body area detection. However, cloud cover poses significant challenges in this regard, which hampers the effectiveness of Sentinel-2 images in this context. In this paper, we present a deep learning model that can generate NDWI given Sentinel-1 images, thereby overcoming this cloud barrier. We show the effectiveness of our model, where it demonstrates a high accuracy of 0.9134 and an AUC of 0.8656 to predict the NDWI. Additionally, we observe promising results with an R2 score of 0.4984 (for regressing the NDWI values) and a Mean IoU of 0.4139 (for the underlying segmentation task). In conclusion, our model offers a first and robust solution for generating NDWI images directly from Sentinel-1 images and subsequent use for various applications even under challenging conditions such as cloud cover and nighttime.

Paper number 53:
Title: Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for Speech Enhancement
Authors: Meng-Ping Lin, Jen-Cheng Hou, Chia-Wei Chen, Shao-Yi Chien, Jun-Cheng Chen, Xugang Lu, Yu Tsao
Abstract: Speech Enhancement (SE) aims to improve the quality of noisy speech. It has been shown that additional visual cues can further improve performance. Given that speech communication involves audio, visual, and linguistic modalities, it is natural to expect another performance boost by incorporating linguistic information. However, bridging the modality gaps to efficiently incorporate linguistic information, along with audio and visual modalities during knowledge transfer, is a challenging task. In this paper, we propose a novel multi-modality learning framework for SE. In the model framework, a state-of-the-art diffusion Model backbone is utilized for Audio-Visual Speech Enhancement (AVSE) modeling where both audio and visual information are directly captured by microphones and video cameras. Based on this AVSE, the linguistic modality employs a PLM to transfer linguistic knowledge to the visual acoustic modality through a process termed Cross-Modal Knowledge Transfer (CMKT) during AVSE model training. After the model is trained, it is supposed that linguistic knowledge is encoded in the feature processing of the AVSE model by the CMKT, and the PLM will not be involved during inference stage. We carry out SE experiments to evaluate the proposed model framework. Experimental results demonstrate that our proposed AVSE system significantly enhances speech quality and reduces generative artifacts, such as phonetic confusion compared to the state-of-the-art. Moreover, our visualization results demonstrate that our Cross-Modal Knowledge Transfer method further improves the generated speech quality of our AVSE system. These findings not only suggest that Diffusion Model-based techniques hold promise for advancing the state-of-the-art in AVSE but also justify the effectiveness of incorporating linguistic information to improve the performance of Diffusion-based AVSE systems.

Paper number 54:
Title: A Quantitative Evaluation of Approximate Softmax Functions for Deep Neural Networks
Authors: Fabricio Elizondo-Fernández, Luis G. León-Vega, Cristina Meinhardt, Jorge Castro-Godínez
Abstract: The softmax function is used as an activation function placed in the output layer of a neural network. It allows extracting the probabilities of the output classes, while introduces a non-linearity to the model. In the field of low-end FPGAs, implementations of Deep Neural Networks (DNNs) require the exploration of optimisation techniques to improve computational efficiency and hardware resource consumption. This work explores approximate computing techniques to implement the softmax function, using Taylor and Padé approximations, and interpolation methods with Look-Up Tables (LUTs). The introduction of approximations aims to reduce the required execution time while reducing the precision of results produced by the softmax function. Each implementation is evaluated using Root Mean Square Error (RMSE) for accuracy assessment, and individual performance is verified by taking measurements of execution times. From our evaluation, quadratic interpolation with LUTs achieves the lowest error, but in terms of performance, Taylor and Padé approximations show better execution times, which highlights the existing design trade-off between numerical accuracy and power consumption.

Paper number 55:
Title: From Images to Point Clouds: An Efficient Solution for Cross-media Blind Quality Assessment without Annotated Training
Authors: Yipeng Liu, Qi Yang, Yujie Zhang, Yiling Xu, Le Yang, Zhu Li
Abstract: We present a novel quality assessment method which can predict the perceptual quality of point clouds from new scenes without available annotations by leveraging the rich prior knowledge in images, called the Distribution-Weighted Image-Transferred Point Cloud Quality Assessment (DWIT-PCQA). Recognizing the human visual system (HVS) as the decision-maker in quality assessment regardless of media types, we can emulate the evaluation criteria for human perception via neural networks and further transfer the capability of quality prediction from images to point clouds by leveraging the prior knowledge in the images. Specifically, domain adaptation (DA) can be leveraged to bridge the images and point clouds by aligning feature distributions of the two media in the same feature space. However, the different manifestations of distortions in images and point clouds make feature alignment a difficult task. To reduce the alignment difficulty and consider the different distortion distribution during alignment, we have derived formulas to decompose the optimization objective of the conventional DA into two suboptimization functions with distortion as a transition. Specifically, through network implementation, we propose the distortion-guided biased feature alignment which integrates existing/estimated distortion distribution into the adversarial DA framework, emphasizing common distortion patterns during feature alignment. Besides, we propose the quality-aware feature disentanglement to mitigate the destruction of the mapping from features to quality during alignment with biased distortions. Experimental results demonstrate that our proposed method exhibits reliable performance compared to general blind PCQA methods without needing point cloud annotations.

Paper number 56:
Title: Performance Analysis of Fluid Antenna Multiple Access Assisted Wireless Powered Communication Network
Authors: Xiao Lin, Yizhe Zhao, Halvin Yang, Jie Hu
Abstract: This paper investigates a novel fluid antenna multiple access (FAMA)-assisted wireless powered communication network (WPCN), in which a hybrid access point (HAP) equipped with multiple fixed position antennas (FPAs) provides integrated data and energy transfer (IDET) services towards low-power devices that are equipped with a single fluid antenna (FA), while the low-power devices use harvested energy to power their own uplink transmission. Using the block correlation channel model, both the downlink and uplink wireless data transfer (WDT) outage probabilities are analyzed under specific port selection strategies, including downlink signal-to-interference ratio-based port selection (DSPS) strategy, downlink energy harvesting power-based port selection (DEPS) strategy, uplink signal-to-noise ratio-based port selection (USPS) strategy, and uplink channel-based port selection (UCPS) strategy. A step function approximation (SFA) approach is also relied upon to derive closed-form expressions for the outage probabilities, while the lower bounds for uplink WDT outage probabilities are also formulated. Numerical results demonstrate the validity of our theoretical analysis, which also provide useful guidelines for the system design through the analytical framework.

Paper number 57:
Title: Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks
Authors: Ruijia Liu, Ancheng Hou, Xiao Yu, Xiang Yin
Abstract: Signal Temporal Logic (STL) is a powerful specification language for describing complex temporal behaviors of continuous signals, making it well-suited for high-level robotic task descriptions. However, generating executable plans for STL tasks is challenging, as it requires consideration of the coupling between the task specification and the system dynamics. Existing approaches either follow a model-based setting that explicitly requires knowledge of the system dynamics or adopt a task-oriented data-driven approach to learn plans for specific tasks. In this work, we investigate the problem of generating executable STL plans for systems whose dynamics are unknown a priori. We propose a new planning framework that uses only task-agnostic data during the offline training stage, enabling zero-shot generalization to new STL tasks. Our framework is hierarchical, involving: (i) decomposing the STL task into a set of progress and time constraints, (ii) searching for time-aware waypoints guided by task-agnostic data, and (iii) generating trajectories using a pre-trained safe diffusion model. Simulation results demonstrate the effectiveness of our method indeed in achieving zero-shot generalization to various STL tasks.

Paper number 58:
Title: Neural Vocoders as Speech Enhancers
Authors: Andong Li, Zhihang Sun, Fengyuan Hao, Xiaodong Li, Chengshi Zheng
Abstract: Speech enhancement (SE) and neural vocoding are traditionally viewed as separate tasks. In this work, we observe them under a common thread: the rank behavior of these processes. This observation prompts two key questions: \textit{Can a model designed for one task's rank degradation be adapted for the other?} and \textit{Is it possible to address both tasks using a unified model?} Our empirical findings demonstrate that existing speech enhancement models can be successfully trained to perform vocoding tasks, and a single model, when jointly trained, can effectively handle both tasks with performance comparable to separately trained models. These results suggest that speech enhancement and neural vocoding can be unified under a broader framework of speech restoration. Code: this https URL.

Paper number 59:
Title: DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition
Authors: Qijie Shao, Linhao Dong, Kun Wei, Sining Sun, Lei Xie
Abstract: Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked prediction, demonstrating remarkable performance in monolingual ASR. Previous studies have revealed that data2vec's shallow layers capture speaker and language information, middle layers encode phoneme and word features, while deep layers are responsible for reconstruction. Language and phoneme features are crucial for multilingual ASR. However, data2vec's masked representation generation relies on multi-layer averaging, inevitably coupling these features. To address this limitation, we propose a decoupling quantization based data2vec (DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two improved online K-means quantizers. Our core idea is using the K-means quantizer with specified cluster numbers to decouple language and phoneme information for masked prediction. Specifically, in the language quantization, considering that the number of languages is significantly different from other irrelevant features (e.g., speakers), we assign the cluster number to match the number of languages, explicitly decoupling shallow layers' language-related information from irrelevant features. This strategy is also applied to decoupling middle layers' phoneme and word features. In a self-supervised scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec achieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58% in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a weakly-supervised scenario incorporating language labels and high-resource language text labels, the relative reduction is 18.09% and 1.55%, respectively.

Paper number 60:
Title: Communication-Efficient Stochastic Distributed Learning
Authors: Xiaoxing Ren, Nicola Bastianello, Karl H. Johansson, Thomas Parisini
Abstract: We address distributed learning problems, both nonconvex and convex, over undirected networks. In particular, we design a novel algorithm based on the distributed Alternating Direction Method of Multipliers (ADMM) to address the challenges of high communication costs, and large datasets. Our design tackles these challenges i) by enabling the agents to perform multiple local training steps between each round of communications; and ii) by allowing the agents to employ stochastic gradients while carrying out local computations. We show that the proposed algorithm converges to a neighborhood of a stationary point, for nonconvex problems, and of an optimal point, for convex problems. We also propose a variant of the algorithm to incorporate variance reduction thus achieving exact convergence. We show that the resulting algorithm indeed converges to a stationary (or optimal) point, and moreover that local training accelerates convergence. We thoroughly compare the proposed algorithms with the state of the art, both theoretically and through numerical results.

Paper number 61:
Title: Diffusion-based Perceptual Neural Video Compression with Temporal Diffusion Information Reuse
Authors: Wenzhuo Ma, Zhenzhong Chen
Abstract: Recently, foundational diffusion models have attracted considerable attention in image compression tasks, whereas their application to video compression remains largely unexplored. In this article, we introduce DiffVC, a diffusion-based perceptual neural video compression framework that effectively integrates foundational diffusion model with the video conditional coding paradigm. This framework uses temporal context from previously decoded frame and the reconstructed latent representation of the current frame to guide the diffusion model in generating high-quality results. To accelerate the iterative inference process of diffusion model, we propose the Temporal Diffusion Information Reuse (TDIR) strategy, which significantly enhances inference efficiency with minimal performance loss by reusing the diffusion information from previous frames. Additionally, to address the challenges posed by distortion differences across various bitrates, we propose the Quantization Parameter-based Prompting (QPP) mechanism, which utilizes quantization parameters as prompts fed into the foundational diffusion model to explicitly modulate intermediate features, thereby enabling a robust variable bitrate diffusion-based neural compression framework. Experimental results demonstrate that our proposed solution delivers excellent performance in both perception metrics and visual quality.

Paper number 62:
Title: WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control
Authors: Claire Bizon Monroc, Ana Bušić, Donatien Dubuc, Jiamin Zhu
Abstract: The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (this http URL). For each simulator, $10$ wind layouts are provided, including $5$ real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on this http URL is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to this http URL.

Paper number 63:
Title: Musical ethnocentrism in Large Language Models
Authors: Anna Kruspe
Abstract: Large Language Models (LLMs) reflect the biases in their training data and, by extension, those of the people who created this training data. Detecting, analyzing, and mitigating such biases is becoming a focus of research. One type of bias that has been understudied so far are geocultural biases. Those can be caused by an imbalance in the representation of different geographic regions and cultures in the training data, but also by value judgments contained therein. In this paper, we make a first step towards analyzing musical biases in LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the first, we prompt LLMs to provide lists of the "Top 100" musical contributors of various categories and analyze their countries of origin. In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries. Our results indicate a strong preference of the LLMs for Western music cultures in both experiments.

Paper number 64:
Title: Centralized Versus Distributed Routing for Large-Scale Satellite Networks
Authors: Rudrapatna Vallabh Ramakanth, Eytan Modiano
Abstract: An important choice in the design of satellite networks is whether the routing decisions are made in a distributed manner onboard the satellite, or centrally on a ground-based controller. We study the tradeoff between centralized and distributed routing in large-scale satellite networks. In particular, we consider a centralized routing scheme that has access to global but delayed network state information and a distributed routing scheme that has access to local but real-time network state information. For both routing schemes, we analyze the throughput and delay performance of shortest-path algorithms in networks with and without buffers onboard the satellites. We show that distributed routing outperforms centralized routing when the rate of changes in the network link state is comparable to the inherent propagation and transmission delays. In particular, we show that in highly dynamic networks without buffers, the distributed scheme achieves higher throughput than a centralized scheme. In networks with buffers, the distributed scheme achieves lower delays with the same throughput.

Paper number 65:
Title: Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak
Authors: Erjia Xiao, Hao Cheng, Jing Shao, Jinhao Duan, Kaidi Xu, Le Yang, Jindong Gu, Renjing Xu
Abstract: Large Language Models (LLMs) demonstrate remarkable zero-shot performance across various natural language processing tasks. The integration of multimodal encoders extends their capabilities, enabling the development of Multimodal Large Language Models that process vision, audio, and text. However, these capabilities also raise significant security concerns, as these models can be manipulated to generate harmful or inappropriate content through jailbreak. While extensive research explores the impact of modality-specific input edits on text-based LLMs and Large Vision-Language Models in jailbreak, the effects of audio-specific edits on Large Audio-Language Models (LALMs) remain underexplored. Hence, this paper addresses this gap by investigating how audio-specific edits influence LALMs inference regarding jailbreak. We introduce the Audio Editing Toolbox (AET), which enables audio-modality edits such as tone adjustment, word emphasis, and noise injection, and the Edited Audio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also conduct extensive evaluations of state-of-the-art LALMs to assess their robustness under different audio edits. This work lays the groundwork for future explorations on audio-modality interactions in LALMs security.

Paper number 66:
Title: Temporal Logic Guided Safe Navigation for Autonomous Vehicles
Authors: Aditya Parameshwaran, Yue Wang
Abstract: Safety verification for autonomous vehicles (AVs) and ground robots is crucial for ensuring reliable operation given their uncertain environments. Formal language tools provide a robust and sound method to verify safety rules for such complex cyber-physical systems. In this paper, we propose a hybrid approach that combines the strengths of formal verification languages like Linear Temporal Logic (LTL) and Signal Temporal Logic (STL) to generate safe trajectories and optimal control inputs for autonomous vehicle navigation. We implement a symbolic path planning approach using LTL to generate a formally safe reference trajectory. A mixed integer linear programming (MILP) solver is then used on this reference trajectory to solve for the control inputs while satisfying the state, control and safety constraints described by STL. We test our proposed solution on two environments and compare the results with popular path planning algorithms. In contrast to conventional path planning algorithms, our formally safe solution excels in handling complex specification scenarios while ensuring both safety and comparable computation times.

Paper number 67:
Title: Sample-Based Piecewise Linear Power Flow Approximations Using Second-Order Sensitivities
Authors: Paprapee Buason, Sidhant Misra, Daniel K. Molzahn
Abstract: The inherent nonlinearity of the power flow equations poses significant challenges in accurately modeling power systems, particularly when employing linearized approximations. Although power flow linearizations provide computational efficiency, they can fail to fully capture nonlinear behavior across diverse operating conditions. To improve approximation accuracy, we propose conservative piecewise linear approximations (CPLA) of the power flow equations, which are designed to consistently over- or under-estimate the quantity of interest, ensuring conservative behavior in optimization. The flexibility provided by piecewise linear functions can yield improved accuracy relative to standard linear approximations. However, applying CPLA across all dimensions of the power flow equations could introduce significant computational complexity, especially for large-scale optimization problems. In this paper, we propose a strategy that selectively targets dimensions exhibiting significant nonlinearities. Using a second-order sensitivity analysis, we identify the directions where the power flow equations exhibit the most significant curvature and tailor the CPLAs to improve accuracy in these specific directions. This approach reduces the computational burden while maintaining high accuracy, making it particularly well-suited for mixed-integer programming problems involving the power flow equations.

Paper number 68:
Title: Everyone-Can-Sing: Zero-Shot Singing Voice Synthesis and Conversion with Speech Reference
Authors: Shuqi Dai, Yunyun Wang, Roger B. Dannenberg, Zeyu Jin
Abstract: We propose a unified framework for Singing Voice Synthesis (SVS) and Conversion (SVC), addressing the limitations of existing approaches in cross-domain SVS/SVC, poor output musicality, and scarcity of singing data. Our framework enables control over multiple aspects, including language content based on lyrics, performance attributes based on a musical score, singing style and vocal techniques based on a selector, and voice identity based on a speech sample. The proposed zero-shot learning paradigm consists of one SVS model and two SVC models, utilizing pre-trained content embeddings and a diffusion-based generator. The proposed framework is also trained on mixed datasets comprising both singing and speech audio, allowing singing voice cloning based on speech reference. Experiments show substantial improvements in timbre similarity and musicality over state-of-the-art baselines, providing insights into other low-data music tasks such as instrumental style transfer. Examples can be found at: this http URL.

Paper number 69:
Title: Quantum model reduction for continuous-time quantum filters
Authors: Tommaso Grigoletto, Clément Pellegrini, Francesco Ticozzi
Abstract: The use of quantum stochastic models is widespread in dynamical reduction, simulation of open systems, feedback control and adaptive estimation. In many applications only part of the information contained in the filter's state is actually needed to reconstruct the target observable quantities; thus, filters of smaller dimensions could be in principle implemented to perform the same this http URL this work, we propose a systematic method to find, when possible, reduced-order quantum filters that are capable of exactly reproducing the evolution of expectation values of interest. In contrast with existing reduction techniques, the reduced model we obtain is exact and in the form of a Belavkin filtering equation, ensuring physical this http URL is attained by leveraging tools from the theory of both minimal realization and non-commutative conditional expectations. The proposed procedure is tested on prototypical examples, laying the groundwork for applications in quantum trajectory simulation and quantum feedback control.

Paper number 70:
Title: What Does an Audio Deepfake Detector Focus on? A Study in the Time Domain
Authors: Petr Grinberg, Ankur Kumar, Surya Koppisetti, Gaurav Bharaj
Abstract: Adding explanations to audio deepfake detection (ADD) models will boost their real-world application by providing insight on the decision making process. In this paper, we propose a relevancy-based explainable AI (XAI) method to analyze the predictions of transformer-based ADD models. We compare against standard Grad-CAM and SHAP-based methods, using quantitative faithfulness metrics as well as a partial spoof test, to comprehensively analyze the relative importance of different temporal regions in an audio. We consider large datasets, unlike previous works where only limited utterances are studied, and find that the XAI methods differ in their explanations. The proposed relevancy-based XAI method performs the best overall on a variety of metrics. Further investigation on the relative importance of speech/non-speech, phonetic content, and voice onsets/offsets suggest that the XAI results obtained from analyzing limited utterances don't necessarily hold when evaluated on large datasets.

Paper number 71:
Title: Autonomous Polycrystalline Material Decomposition for Hyperspectral Neutron Tomography
Authors: Mohammad Samin Nur Chowdhury, Diyu Yang, Shimin Tang, Singanallur V. Venkatakrishnan, Hassina Z. Bilheux, Gregery T. Buzzard, Charles A. Bouman
Abstract: Hyperspectral neutron tomography is an effective method for analyzing crystalline material samples with complex compositions in a non-destructive manner. Since the counts in the hyperspectral neutron radiographs directly depend on the neutron cross-sections, materials may exhibit contrasting neutron responses across wavelengths. Therefore, it is possible to extract the unique signatures associated with each material and use them to separate the crystalline phases simultaneously. We introduce an autonomous material decomposition (AMD) algorithm to automatically characterize and localize polycrystalline structures using Bragg edges with contrasting neutron responses from hyperspectral data. The algorithm estimates the linear attenuation coefficient spectra from the measured radiographs and then uses these spectra to perform polycrystalline material decomposition and reconstructs 3D material volumes to localize materials in the spatial domain. Our results demonstrate that the method can accurately estimate both the linear attenuation coefficient spectra and associated reconstructions on both simulated and experimental neutron data.

Paper number 72:
Title: Guided Reconstruction with Conditioned Diffusion Models for Unsupervised Anomaly Detection in Brain MRIs
Authors: Finn Behrendt, Debayan Bhattacharya, Robin Mieling, Lennart Maack, Julia Krüger, Roland Opfer, Alexander Schlaefer
Abstract: The application of supervised models to clinical screening tasks is challenging due to the need for annotated data for each considered pathology. Unsupervised Anomaly Detection (UAD) is an alternative approach that aims to identify any anomaly as an outlier from a healthy training distribution. A prevalent strategy for UAD in brain MRI involves using generative models to learn the reconstruction of healthy brain anatomy for a given input image. As these models should fail to reconstruct unhealthy structures, the reconstruction errors indicate anomalies. However, a significant challenge is to balance the accurate reconstruction of healthy anatomy and the undesired replication of abnormal structures. While diffusion models have shown promising results with detailed and accurate reconstructions, they face challenges in preserving intensity characteristics, resulting in false positives. We propose conditioning the denoising process of diffusion models with additional information derived from a latent representation of the input image. We demonstrate that this conditioning allows for accurate and local adaptation to the general input intensity distribution while avoiding the replication of unhealthy structures. We compare the novel approach to different state-of-the-art methods and for different data sets. Our results show substantial improvements in the segmentation performance, with the Dice score improved by 11.9%, 20.0%, and 44.6%, for the BraTS, ATLAS and MSLUB data sets, respectively, while maintaining competitive performance on the WMH data set. Furthermore, our results indicate effective domain adaptation across different MRI acquisitions and simulated contrasts, an important attribute for general anomaly detection methods. The code for our work is available at this https URL

Paper number 73:
Title: Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models
Authors: Jing Xie, Fabio Bonassi, Riccardo Scattolini
Abstract: This paper explores the use of Control Affine Neural Nonlinear AutoRegressive eXogenous (CA-NNARX) models for nonlinear system identification and model-based control design. The idea behind this architecture is to match the known control-affine structure of the system to achieve improved performance. Coherently with recent literature of neural networks for data-driven control, we first analyze the stability properties of CA-NNARX models, devising sufficient conditions for their incremental Input-to-State Stability ($\delta$ISS) that can be enforced at the model training stage. The model's stability property is then leveraged to design a stable Internal Model Control (IMC) architecture. The proposed control scheme is tested on a real Quadruple Tank benchmark system to address the output reference tracking problem. The results achieved show that (i) the modeling accuracy of CA-NNARX is superior to the one of a standard NNARX model for given weight size and training epochs, (ii) the proposed IMC law provides performance comparable to the ones of a standard Model Predictive Controller (MPC) at a significantly lower computational burden, and (iii) the $\delta$ISS of the model is beneficial to the closed-loop performance.

Paper number 74:
Title: An Unsupervised Machine Learning to Optimize Hybrid Quantum Noise Clusters for Gaussian Quantum Channel
Authors: Mouli Chakraborty, Anshu Mukherjee, Ioannis Krikidis, Avishek Nag, Subhash Chandra
Abstract: This work focuses on optimizing the hybrid quantum noise model to improve the capacity of Gaussian quantum channels using Machine Learning (ML) generated clusters. The work specifically leverages Gaussian Mixture Model (GMM) and the Expectation-Maximization (EM) algorithm to model the complex noise characteristics of quantum channels. Hybrid quantum noise, which includes both quantum shot noise and classical Additive-White-Gaussian Noise (AWGN), is modeled as an infinite mixture of Gaussian distributions weighted by Poissonian parameters. The study proposes a method to reduce the number of clusters within this noise model, simplifying visualization and improving the accuracy of channel capacity estimations without compromising essential noise characteristics. Key contributions include the reduction of Gaussian clusters while maintaining error tolerances and using the EM algorithm to update quantum channel parameters, leading to more accurate channel capacity. The approach is validated through simulations, demonstrating that ML-enhanced quantum noise clustering significantly improves the channels performance in satellite-based quantum communication systems, specifically for Quantum Key Distribution (QKD). The work demonstrates that GMM and EM algorithms provide a practical solution for modeling quantum noise in real-time applications, advancing the optimization of quantum communication networks.

Paper number 75:
Title: Early Detection and Classification of Hidden Contingencies in Modern Power Systems: A Learning-based Stochastic Hybrid System Approach
Authors: Erfan Mehdipour Abadi, Hamid Varmazyari, Masoud H. Nazari
Abstract: This paper introduces a novel learning-based Stochastic Hybrid System (LSHS) approach for detecting and classifying various contingencies in modern power systems. Specifically, the proposed method is capable of identifying hidden contingencies that cannot be captured by existing sensing and monitoring systems, such as failures in protection systems or line outages in distribution networks. The LSHS approach detects contingencies by analyzing system outputs and behaviors. It then categorizes them based on their impact on the SHS model into physical, control network, and measurement contingencies. The stochastic hybrid system (SHS) model is further extended into an advanced closed-loop framework incorporating both system dynamics and observer-based state estimation error dynamics. Machine learning methods within the LSHS framework are employed for contingency classification and rapid detection. The practicality and effectiveness of the proposed methodology are validated through simulations on an enhanced IEEE-33 bus system. The results demonstrate that the LSHS framework significantly improves the accuracy and speed of contingency detection compared to state-of-the-art methods, offering a promising solution for enhancing power system contingency detection.

Paper number 76:
Title: Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors
Authors: Shoujin Huang, Guanxiong Luo, Yunlin Zhao, Yilong Liu, Yuwan Wang, Kexin Yang, Jingzhe Liu, Hua Guo, Min Wang, Lingyan Zhang, Mengye Lyu
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at this https URL.

Paper number 77:
Title: User-Driven Voice Generation and Editing through Latent Space Navigation
Authors: Yusheng Tian, Junbin Liu, Tan Lee
Abstract: This paper presents a user-driven approach for synthesizing specific target voices based on user feedback rather than reference recordings, which is particularly beneficial for speech-impaired individuals who want to recreate their lost voices but lack prior recordings. Our method leverages the neural analysis and synthesis framework to construct a latent speaker embedding space. Within this latent space, a human-in-the-loop search algorithm guides the voice generation process. Users participate in a series of straightforward listening-and-comparison tasks, providing feedback that iteratively refines the synthesized voice to match their desired target. Both computer simulations and real-world user studies demonstrate that the proposed approach can effectively approximate target voices. Moreover, by analyzing the mel-spectrogram generator's Jacobians, we identify a set of meaningful voice editing directions within the latent space. These directions enable users to further fine-tune specific attributes of the generated voice, including the pitch level, pitch range, volume, vocal tension, nasality, and tone color.

Paper number 78:
Title: IPMN Risk Assessment under Federated Learning Paradigm
Authors: Hongyi Pan, Ziliang Hong, Gorkem Durak, Elif Keles, Halil Ertugrul Aktas, Yavuz Taktak, Alpay Medetalibeyoglu, Zheyuan Zhang, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Frank Miller, Rajesh N. Keswani, Michael B. Wallace, Ziyue Xu, Ulas Bagci
Abstract: Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) is essential for identifying high-risk cases that require timely intervention. In this study, we develop a federated learning framework for multi-center IPMN classification utilizing a comprehensive pancreas MRI dataset. This dataset includes 652 T1-weighted and 655 T2-weighted MRI images, accompanied by corresponding IPMN risk scores from 7 leading medical institutions, making it the largest and most diverse dataset for IPMN classification to date. We assess the performance of DenseNet-121 in both centralized and federated settings for training on distributed data. Our results demonstrate that the federated learning approach achieves high classification accuracy comparable to centralized learning while ensuring data privacy across institutions. This work marks a significant advancement in collaborative IPMN classification, facilitating secure and high-accuracy model training across multiple centers.

Paper number 79:
Title: Fast Hyperspectral Reconstruction for Neutron Computed Tomography Using Subspace Extraction
Authors: Mohammad Samin Nur Chowdhury, Diyu Yang, Shimin Tang, Singanallur V. Venkatakrishnan, Andrew W. Needham, Hassina Z. Bilheux, Gregery T. Buzzard, Charles A. Bouman
Abstract: Hyperspectral neutron computed tomography enables 3D non-destructive imaging of the spectral characteristics of materials. In traditional hyperspectral reconstruction, the data for each neutron wavelength bin is reconstructed separately. This per-bin reconstruction is extremely time-consuming due to the typically large number of wavelength bins. Furthermore, these reconstructions may suffer from severe artifacts due to the low signal-to-noise ratio in each wavelength bin. We present a novel fast hyperspectral reconstruction algorithm for computationally efficient and accurate reconstruction of hyperspectral neutron data. Our algorithm uses a subspace extraction procedure that transforms hyperspectral data into low-dimensional data within an intermediate subspace. This step effectively reduces data dimensionality and spectral noise. High-quality reconstructions are then performed within this low-dimensional subspace. Finally, the algorithm expands the subspace reconstructions into hyperspectral reconstructions. We apply our algorithm to measured neutron data and demonstrate that it reduces computation and improves reconstruction quality compared to the conventional approach.

Paper number 80:
Title: RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data
Authors: Maxwell A. Xu, Jaya Narain, Gregory Darnell, Haraldur Hallgrimsson, Hyewon Jeong, Darren Forde, Richard Fineman, Karthik J. Raghuram, James M. Rehg, Shirley Ren
Abstract: We present RelCon, a novel self-supervised *Rel*ative *Con*trastive learning approach that uses a learnable distance measure in combination with a softened contrastive loss for training an motion foundation model from wearable sensors. The learnable distance measure captures motif similarity and domain-specific semantic information such as rotation invariance. The learned distance provides a measurement of semantic similarity between a pair of accelerometer time-series segments, which is used to measure the distance between an anchor and various other sampled candidate segments. The self-supervised model is trained on 1 billion segments from 87,376 participants from a large wearables dataset. The model achieves strong performance across multiple downstream tasks, encompassing both classification and regression. To our knowledge, we are the first to show the generalizability of a self-supervised learning model with motion data from wearables across distinct evaluation tasks.

Paper number 81:
Title: Regret Analysis: a control perspective
Authors: Travis E. Gibson, Sawal Acharya
Abstract: Online learning and model reference adaptive control have many interesting intersections. One area where they differ however is in how the algorithms are analyzed and what objective or metric is used to discriminate "good" algorithms from "bad" algorithms. In adaptive control there are usually two objectives: 1) prove that all time varying parameters/states of the system are bounded, and 2) that the instantaneous error between the adaptively controlled system and a reference system converges to zero over time (or at least a compact set). For online learning the performance of algorithms is often characterized by the regret the algorithm incurs. Regret is defined as the cumulative loss (cost) over time from the online algorithm minus the cumulative loss (cost) of the single optimal fixed parameter choice in hindsight. Another significant difference between the two areas of research is with regard to the assumptions made in order to obtain said results. Adaptive control makes assumptions about the input-output properties of the control problem and derives solutions for a fixed error model or optimization task. In the online learning literature results are derived for classes of loss functions (i.e. convex) while a priori assuming certain signals are bounded. In this work we discuss these differences in detail through the regret based analysis of gradient descent for convex functions and the control based analysis of a streaming regression problem. We close with a discussion about the newly defined paradigm of online adaptive control.

Paper number 82:
Title: Boosting Diffusion Guidance via Learning Degradation-Aware Models for Blind Super Resolution
Authors: Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu
Abstract: Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we present DADiff in this paper. DADiff incorporates degradation-aware models into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques: input perturbation and guidance scalar, to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks.

Paper number 83:
Title: Learning Hemodynamic Scalar Fields on Coronary Artery Meshes: A Benchmark of Geometric Deep Learning Models
Authors: Guido Nannini, Julian Suk, Patryk Rygiel, Simone Saitta, Luca Mariani, Riccardo Maragna, Andrea Baggiano, Gianluca Pontone, Jelmer M. Wolterink, Alberto Redaelli
Abstract: Coronary artery disease, caused by the narrowing of coronary vessels due to atherosclerosis, is the leading cause of death worldwide. The diagnostic gold standard, fractional flow reserve (FFR), measures the trans-stenotic pressure ratio during maximal vasodilation but is invasive and costly. This has driven the development of virtual FFR (vFFR) using computational fluid dynamics (CFD) to simulate coronary flow. Geometric deep learning algorithms have shown promise for learning features on meshes, including cardiovascular research applications. This study empirically analyzes various backends for predicting vFFR fields in coronary arteries as CFD surrogates, comparing six backends for learning hemodynamics on meshes using CFD solutions as ground truth. The study has two parts: i) Using 1,500 synthetic left coronary artery bifurcations, models were trained to predict pressure-related fields for vFFR reconstruction, comparing different learning variables. ii) Using 427 patient-specific CFD simulations, experiments were repeated focusing on the best-performing learning variable from the synthetic dataset. Most backends performed well on the synthetic dataset, especially when predicting pressure drop over the manifold. Transformer-based backends outperformed others when predicting pressure and vFFR fields and were the only models achieving strong performance on patient-specific data, excelling in both average per-point error and vFFR accuracy in stenotic lesions. These results suggest geometric deep learning backends can effectively replace CFD for simple geometries, while transformer-based networks are superior for complex, heterogeneous datasets. Pressure drop was identified as the optimal network output for learning pressure-related fields.

Paper number 84:
Title: BOOST: Microgrid Sizing using Ordinal Optimization
Authors: Mohamad Fares El Hajj Chehade, Sami Karaki
Abstract: The transition to sustainable energy systems has highlighted the critical need for efficient sizing of renewable energy resources in microgrids. In particular, designing photovoltaic (PV) and battery systems to meet residential loads is challenging due to trade-offs between cost, reliability, and environmental impact. While previous studies have employed dynamic programming and heuristic techniques for microgrid sizing, these approaches often fail to balance computational efficiency and accuracy. In this work, we propose BOOST, or Battery-solar Ordinal Optimization Sizing Technique, a novel framework for optimizing the sizing of PV and battery components in microgrids. Ordinal optimization enables computationally efficient evaluations of potential designs while preserving accuracy through robust ranking of solutions. To determine the optimal operation of the system at any given time, we introduce a mixed-integer linear programming (MILP) approach, which achieves lower costs than the commonly used dynamic programming methods. Our numerical experiments demonstrate that the proposed framework identifies optimal designs that achieve a levelized cost of energy (LCOE) as low as 8.84 cents/kWh, underscoring its potential for cost-effective microgrid design. The implications of our work are significant: BOOST provides a scalable and accurate methodology for integrating renewable energy into residential microgrids, addressing economic and environmental goals simultaneously.

Paper number 85:
Title: SE(3)-Based Trajectory Optimization and Target Tracking in UAV-Enabled ISAC Systems
Authors: Dongxiao Xu, Xinyang Li, Vlad C. Andrei, Moritz Wiese, Ullrich J. Moenich, Holger Boche
Abstract: This paper presents a novel approach to enhance sensing capabilities in UAV-enabled MIMO-OFDM ISAC systems by leveraging UAV mobility as a mono-static radar. By integrating uniform planar arrays (UPAs) and modeling the UAV dynamics in $SE(3)$, we address key challenges such as 3D space sensing and trajectory design. We propose a target tracking scheme using extended Kalman filtering (EKF) in $SE(3)$, along with trajectory optimization based on the conditional Posterior Cramer-Rao bound (CPCRB). Numerical results demonstrate the effectiveness of the proposed trajectory design in enhancing performance of target tracking and physical parameter estimation in UAV-enabled MIMO-OFDM ISAC systems.

Paper number 86:
Title: S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models
Authors: Tiezhi Wang, Nils Strodthoff
Abstract: Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components and achieve statistically significant performance improvements compared to state-of-the-art approaches on the extensive Sleep Heart Health Study dataset. We anticipate that the architectural insights gained from this study along with the refined methodology for architecture search demonstrated herein will not only prove valuable for future research in sleep staging but also hold relevance for other time series annotation tasks.

Paper number 87:
Title: Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain
Authors: Anastasios Papazafeiropoulos, Pandelis Kourtessis, Symeon Chatzinotas
Abstract: Although reconfigurable intelligent surface (RIS) is a promising technology for shaping the propagation environment, it consists of a single-layer structure within inherent limitations regarding the number of beam steering patterns. Based on the recently revolutionary technology, denoted as stacked intelligent metasurface (SIM), we propose its implementation not only on the base station (BS) side in a massive multiple-input multiple-output (mMIMO) setup but also in the intermediate space between the base station and the users to adjust the environment further as needed. For the sake of convenience, we call the former BS SIM (BSIM), and the latter channel SIM (CSIM). Hence, we achieve wave-based combining at the BS and wave-based configuration at the intermediate space. Specifically, we propose a channel estimation method with reduced overhead, being crucial for SIMassisted communications. Next, we derive the uplink sum spectral efficiency (SE) in closed form in terms of statistical channel state information (CSI). Notably, we optimize the phase shifts of both BSIM and CSIM simultaneously by using the projected gradient ascent method (PGAM). Compared to previous works on SIMs, we study the uplink transmission, a mMIMO setup, channel estimation in a single phase, a second SIM at the intermediate space, and simultaneous optimization of the two SIMs. Simulation results show the impact of various parameters on the sum SE, and demonstrate the superiority of our optimization approach compared to the alternating optimization (AO) method.

Paper number 88:
Title: Coded Beam Training for RIS Assisted Wireless Communications
Authors: Yuhao Chen, Linglong Dai
Abstract: Reconfigurable intelligent surface (RIS) is considered as one of the key technologies for future 6G communications. To fully unleash the performance of RIS, accurate channel state information (CSI) is crucial. Beam training is widely utilized to acquire the CSI. However, before aligning the beam correctly to establish stable connections, the signal-to-noise ratio (SNR) at UE is inevitably low, which reduces the beam training accuracy. To deal with this problem, we exploit the coded beam training framework for RIS systems, which leverages the error correction capability of channel coding to improve the beam training accuracy under low SNR. Specifically, we first extend the coded beam training framework to RIS systems by decoupling the base station-RIS channel and the RIS-user channel. For this framework, codewords that accurately steer to multiple angles is essential for fully unleashing the error correction capability. In order to realize effective codeword design in RIS systems, we then propose a new codeword design criterion, based on which we propose a relaxed Gerchberg-Saxton (GS) based codeword design scheme by considering the constant modulus constraints of RIS elements. In addition, considering the two dimensional structure of RIS, we further propose a dimension reduced encoder design scheme, which can not only guarentee a better beam shape, but also enable a stronger error correction capability. Simulation results reveal that the proposed scheme can realize effective and accurate beam training in low SNR scenarios.

Paper number 89:
Title: UNSURE: self-supervised learning with Unknown Noise level and Stein's Unbiased Risk Estimate
Authors: Julián Tachella, Mike Davies, Laurent Jacques
Abstract: Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references. Most existing methods cluster around two classes: i) Stein's Unbiased Risk Estimate (SURE) and similar approaches that assume full knowledge of the distribution, and ii) Noise2Self and similar cross-validation methods that require very mild knowledge about the noise distribution. The first class of methods tends to be impractical, as the noise level is often unknown in real-world applications, and the second class is often suboptimal compared to supervised learning. In this paper, we provide a theoretical framework that characterizes this expressivity-robustness trade-off and propose a new approach based on SURE, but unlike the standard SURE, does not require knowledge about the noise level. Throughout a series of experiments, we show that the proposed estimator outperforms other existing self-supervised methods on various imaging inverse problems.

Paper number 90:
Title: Numerically Robust Fixed-Point Smoothing Without State Augmentation
Authors: Nicholas Krämer
Abstract: Practical implementations of Gaussian smoothing algorithms have received a great deal of attention in the last 60 years. However, almost all work focuses on estimating complete time series (''fixed-interval smoothing'', $\mathcal{O}(K)$ memory) through variations of the Rauch--Tung--Striebel smoother, rarely on estimating the initial states (''fixed-point smoothing'', $\mathcal{O}(1)$ memory). Since fixed-point smoothing is a crucial component of algorithms for dynamical systems with unknown initial conditions, we close this gap by introducing a new formulation of a Gaussian fixed-point smoother. In contrast to prior approaches, our perspective admits a numerically robust Cholesky-based form (without downdates) and avoids state augmentation, which would needlessly inflate the state-space model and reduce the numerical practicality of any fixed-point smoother code. The experiments demonstrate how a JAX implementation of our algorithm matches the runtime of the fastest methods and the robustness of the most robust techniques while existing implementations must always sacrifice one for the other.

Paper number 91:
Title: What If We Had Used a Different App? Reliable Counterfactual KPI Analysis in Wireless Systems
Authors: Qiushuo Hou, Sangwoo Park, Matteo Zecchin, Yunlong Cai, Guanding Yu, Osvaldo Simeone
Abstract: In modern wireless network architectures, such as Open Radio Access Network (O-RAN), the operation of the radio access network (RAN) is managed by applications, or apps for short, deployed at intelligent controllers. These apps are selected from a given catalog based on current contextual information. For instance, a scheduling app may be selected on the basis of current traffic and network conditions. Once an app is chosen and run, it is no longer possible to directly test the key performance indicators (KPIs) that would have been obtained with another app. In other words, we can never simultaneously observe both the actual KPI, obtained by the selected app, and the counterfactual KPI, which would have been attained with another app, for the same network condition, making individual-level counterfactual KPIs analysis particularly challenging. This what-if analysis, however, would be valuable to monitor and optimize the network operation, e.g., to identify suboptimal app selection strategies. This paper addresses the problem of estimating the values of KPIs that would have been obtained if a different app had been implemented by the RAN. To this end, we propose a conformal-prediction-based counterfactual analysis method for wireless systems that provides reliable error bars for the estimated KPIs, despite the inherent covariate shift between logged and test data. Experimental results for medium access control-layer apps and for physical-layer apps demonstrate the merits of the proposed method.

Paper number 92:
Title: Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth Estimation
Authors: Dabbrata Das, Argho Deb Das, Farhan Sadaf
Abstract: Estimating depth from a single 2D image is a challenging task due to the lack of stereo or multi-view data, which are typically required for depth perception. In state-of-the-art architectures, the main challenge is to efficiently capture complex objects and fine-grained details, which are often difficult to predict. This paper introduces a novel deep learning-based approach using an enhanced encoder-decoder architecture, where the Inception-ResNet-v2 model serves as the encoder. This is the first instance of utilizing Inception-ResNet-v2 as an encoder for monocular depth estimation, demonstrating improved performance over previous models. It incorporates multi-scale feature extraction to enhance depth prediction accuracy across various object sizes and distances. We propose a composite loss function comprising depth loss, gradient edge loss, and Structural Similarity Index Measure (SSIM) loss, with fine-tuned weights to optimize the weighted sum, ensuring a balance across different aspects of depth estimation. Experimental results on the KITTI dataset show that our model achieves a significantly faster inference time of 0.019 seconds, outperforming vision transformers in efficiency while maintaining good accuracy. On the NYU Depth V2 dataset, the model establishes state-of-the-art performance, with an Absolute Relative Error (ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of 89.3% for $\delta$ < 1.25. These metrics demonstrate that our model can accurately and efficiently predict depth even in challenging scenarios, providing a practical solution for real-time applications.

Paper number 93:
Title: OCMDP: Observation-Constrained Markov Decision Process
Authors: Taiyi Wang, Jianheng Liu, Bryan Lee, Zhihao Wu, Yu Wu
Abstract: In many practical applications, decision-making processes must balance the costs of acquiring information with the benefits it provides. Traditional control systems often assume full observability, an unrealistic assumption when observations are expensive. We tackle the challenge of simultaneously learning observation and control strategies in such cost-sensitive environments by introducing the Observation-Constrained Markov Decision Process (OCMDP), where the policy influences the observability of the true state. To manage the complexity arising from the combined observation and control actions, we develop an iterative, model-free deep reinforcement learning algorithm that separates the sensing and control components of the policy. This decomposition enables efficient learning in the expanded action space by focusing on when and what to observe, as well as determining optimal control actions, without requiring knowledge of the environment's dynamics. We validate our approach on a simulated diagnostic task and a realistic healthcare environment using HeartPole. Given both scenarios, the experimental results demonstrate that our model achieves a substantial reduction in observation costs on average, significantly outperforming baseline methods by a notable margin in efficiency.

Paper number 94:
Title: Video-Guided Foley Sound Generation with Multimodal Controls
Authors: Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, Justin Salamon
Abstract: Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: this https URL

Paper number 95:
Title: Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM Dataset
Authors: Neil Shah, Shirish Karande, Vineet Gandhi
Abstract: Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning to simulate ground-truth speech from paired whispers. However, the simulated speech often lacks intelligibility and fails to generalize well across different speakers. To address this issue, we focus on learning phoneme-level alignments from paired whispers and text and employ a Text-to-Speech (TTS) system to simulate the ground-truth. To reduce dependence on whispers, we learn phoneme alignments directly from NAMs, though the quality is constrained by the available training data. To further mitigate reliance on NAM/whisper data for ground-truth simulation, we propose incorporating the lip modality to infer speech and introduce a novel diffusion-based method that leverages recent advancements in lip-to-speech technology. Additionally, we release the MultiNAM dataset with over 7.96 hours of paired NAM, whisper, video, and text data from two speakers and benchmark all methods on this dataset. Speech samples and the dataset are available at this https URL

Paper number 96:
Title: Deep-Relative-Trust-Based Diffusion for Decentralized Deep Learning
Authors: Muyun Li, Aaron Fainman, Stefan Vlaski
Abstract: Decentralized learning strategies allow a collection of agents to learn efficiently from local data sets without the need for central aggregation or orchestration. Current decentralized learning paradigms typically rely on an averaging mechanism to encourage agreement in the parameter space. We argue that in the context of deep neural networks, which are often over-parameterized, encouraging consensus of the neural network outputs, as opposed to their parameters can be more appropriate. This motivates the development of a new decentralized learning algorithm, termed DRT diffusion, based on deep relative trust (DRT), a recently introduced similarity measure for neural networks. We provide convergence analysis for the proposed strategy, and numerically establish its benefit to generalization, especially with sparse topologies, in an image classification task.

Paper number 97:
Title: DFingerNet: Noise-Adaptive Speech Enhancement for Hearing Aids
Authors: Iosif Tsangko, Andreas Triantafyllopoulos, Michael Müller, Hendrik Schröter, Björn W. Schuller
Abstract: The DeepFilterNet (DFN) architecture was recently proposed as a deep learning model suited for hearing aid devices. Despite its competitive performance on numerous benchmarks, it still follows a `one-size-fits-all' approach, which aims to train a single, monolithic architecture that generalises across different noises and environments. However, its limited size and computation budget can hamper its generalisability. Recent work has shown that in-context adaptation can improve performance by conditioning the denoising process on additional information extracted from background recordings to mitigate this. These recordings can be offloaded outside the hearing aid, thus improving performance while adding minimal computational overhead. We introduce these principles to the DFN model, thus proposing the DFingerNet (DFiN) model, which shows superior performance on various benchmarks inspired by the DNS Challenge.
    