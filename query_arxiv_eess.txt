
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A Scalable AI Driven, IoT Integrated Cognitive Digital Twin for Multi-Modal Neuro-Oncological Prognostics and Tumor Kinetics Prediction using Enhanced Vision Transformer and XAI
Authors: Saptarshi Banerjee, Himadri Nath Saha, Utsho Banerjee, Rajarshi Karmakar, Jon Turdiev
Abstract: Neuro-oncological prognostics are now vital in modern clinical neuroscience because brain tumors pose significant challenges in detection and management. To tackle this issue, we propose a cognitive digital twin framework that combines real-time EEG signals from a wearable skullcap with structural MRI data for dynamic and personalized tumor monitoring. At the heart of this framework is an Enhanced Vision Transformer (ViT++) that includes innovative components like Patch-Level Attention Regularization (PLAR) and an Adaptive Threshold Mechanism to improve tumor localization and understanding. A Bidirectional LSTM-based neural classifier analyzes EEG patterns over time to classify brain states such as seizure, interictal, and healthy. Grad-CAM-based heatmaps and a this http URL-powered 3D visualization module provide interactive anatomical insights. Furthermore, a tumor kinetics engine predicts volumetric growth by looking at changes in MRI trends and anomalies from EEG data. With impressive accuracy metrics of 94.6% precision, 93.2% recall, and a Dice score of 0.91, this framework sets a new standard for real-time, interpretable neurodiagnostics. It paves the way for future advancements in intelligent brain health monitoring.

Paper number 2:
Title: Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI Representations
Authors: Jakub Frac, Alexander Schmatz, Qiang Li, Guido Van Wingen, Shujian Yu
Abstract: Functional magnetic resonance imaging (fMRI) analysis faces significant challenges due to limited dataset sizes and domain variability between studies. Traditional self-supervised learning methods inspired by computer vision often rely on positive and negative sample pairs, which can be problematic for neuroimaging data where defining appropriate contrasts is non-trivial. We propose adapting a recently developed Hierarchical Functional Maximal Correlation Algorithm (HFMCA) to graph-structured fMRI data, providing a theoretically grounded approach that measures statistical dependence via density ratio decomposition in a reproducing kernel Hilbert space (RKHS),and applies HFMCA-based pretraining to learn robust and generalizable representations. Evaluations across five neuroimaging datasets demonstrate that our adapted method produces competitive embeddings for various classification tasks and enables effective knowledge transfer to unseen datasets. Codebase and supplementary material can be found here: this https URL

Paper number 3:
Title: Pricing Short-Circuit Current via a Primal-Dual Formulation for Preserving Integrality Constraints
Authors: Peng Wang, Luis Badesa
Abstract: Synchronous Generators (SGs) currently provide important levels of Short-Circuit Current (SCC), a critical ancillary service that ensures line protections trip during short-circuit faults. Given the ongoing replacement of SGs by power-electronics-based generation, which have a hard limit for current injection, it has become relevant to optimize the procurement of SCC provided by remaining SGs. Pricing this service is however challenging due to the integrality constraints in Unit Commitment (UC). Existing methods, e.g., dispatchable pricing, restricted pricing and marginal unit pricing, attempt to address this issue but exhibit limitations in handling binary variables, resulting in SCC prices that either fail to cover the operating costs of units or lack interpretability. To overcome these pitfalls, we propose a primal-dual formulation of the SCC-constrained dispatch that preserves the binary nature of UC while effectively computing shadow prices of SCC services. Using a modified IEEE 30-bus system, a comparison is carried out between the proposed approach and the state-of-the-art pricing schemes, highlighting the advantages of the primal-dual method in preserving UC integrality for SCC pricing.

Paper number 4:
Title: WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection
Authors: Xi Xuan, Xuechen Liu, Wenxin Zhang, Yi-Cheng Lin, Xiaojian Lin, Tomi Kinnunen
Abstract: Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at this https URL.

Paper number 5:
Title: Robust Sensor Placement for Poisson Arrivals with False Alarm Aware Spatiotemporal Sensing
Authors: Mingyu Kim, Pronoy Sarker, Seungmo Kim, Daniel J. Stilwell, Jorge Jimenez
Abstract: This paper studies sensor placement when detection performance varies stochastically due to environmental factors over space and time and false alarms are present, but a filter is used to attenuate the effect. We introduce a unified model that couples detection and false alarms through an availability function, which captures how false alarms reduce effective sensing and filtering responses to the disturbance. Building on this model, we give a sufficient condition under which filtering improves detection. In addition, we derive a coverage-based lower bound on the void probability. Furthermore, we prove robustness guarantees showing that performance remains stable when detection probabilities are learned from limited data. We validate the approach with numerical studies using AIS vessel-traffic data and synthetic maritime scenarios. Together, these results provide theory and practical guidance for deploying sensors in dynamic, uncertain environments.

Paper number 6:
Title: Koopman Control Factorization: Data-Driven Convex Controller Design for a Class of Nonlinear Systems
Authors: Taha Ondogan, Ran Jing, Andrew P. Sabelhaus, Roberto Tron
Abstract: Although Koopman operators provide a global linearization for autonomous dynamical systems, nonautonomous systems are not globally linear in the inputs. State (or output) feedback controller design therefore remains nonconvex in typical formulations, even with approximations via bilinear control-affine terms. We address this gap by introducing the Koopman Control Factorization, a novel parameterization of control-affine dynamical systems combined with a feedback controller defined as a linear combination of nonlinear measurements. With this choice, the Koopman operator of the closed-loop system is a bilinear combination of the coefficients in two matrices: one representing the system, and the other the controller. We propose a set of sufficient conditions such that the factorization holds. Then, we present an algorithm that calculates the feedback matrix via semi-definite programming, producing a Lyapunov-stable closed-loop system with convex optimization. We evaluate the proposed controllers on two canonical examples of control-affine nonlinear systems (inverted pendulums), and show that our factorization and controller successfully stabilize both under properly-chosen basis functions. This manuscript introduces a broadly generalizable control synthesis method for stabilization of nonlinear systems that is quick-to-compute, verifiably stable, data-driven, and does not rely on approximations.

Paper number 7:
Title: Digital Twins for Intelligent Intersections: A Literature Review
Authors: Alben Rome Bagabaldo, JÃ¼rgen Hackl
Abstract: Intelligent intersections play a pivotal role in urban mobility, demanding innovative solutions such as digital twins to enhance safety and efficiency. This literature review investigates the integration and application of digital twins for intelligent intersections, a critical area within smart urban traffic systems. The review systematically categorizes existing research into five key thematic areas: (i) Digital Twin Architectures and Frameworks; (ii) Data Processing and Simulation Techniques; (iii) Artificial Intelligence and Machine Learning for Adaptive Traffic Control; (iv) Safety and Protection of Vulnerable Road Users; and (v) Scaling from Localized Intersections to Citywide Traffic Networks. Each theme is explored comprehensively, highlighting significant advancements, current challenges, and critical insights. The findings reveal that multi-layered digital twin architectures incorporating real-time data fusion and AI-driven decision-making enhances traffic efficiency and safety. Advanced simulation techniques combined with sophisticated AI/ML algorithms demonstrate notable improvements in real-time responsiveness and predictive accuracy for traffic management. Additionally, the integration of digital twins has shown substantial promise in safeguarding vulnerable road users through proactive and adaptive safety strategies. Despite these advancements, key challenges persist, including interoperability of diverse data sources, scalability of digital twins for extensive traffic networks, and managing uncertainty within dynamic urban environments. Addressing these challenges will be essential for the future development and deployment of intelligent, adaptive, and sustainable intersection management systems.

Paper number 8:
Title: Safety-Critical Control with Bounded Inputs: A Closed-Form Solution for Backup Control Barrier Functions
Authors: David E. J. van Wijk, Ersin Das, Tamas G. Molnar, Aaron D. Ames, Joel W. Burdick
Abstract: Verifying the safety of controllers is critical for many applications, but is especially challenging for systems with bounded inputs. Backup control barrier functions (bCBFs) offer a structured approach to synthesizing safe controllers that are guaranteed to satisfy input bounds by leveraging the knowledge of a backup controller. While powerful, bCBFs require solving a high-dimensional quadratic program at run-time, which may be too costly for computationally-constrained systems such as aerospace vehicles. We propose an approach that optimally interpolates between a nominal controller and the backup controller, and we derive the solution to this optimization problem in closed form. We prove that this closed-form controller is guaranteed to be safe while obeying input bounds. We demonstrate the effectiveness of the approach on a double integrator and a nonlinear fixed-wing aircraft example.

Paper number 9:
Title: Operational Risks in Grid Integration of Large Data Center Loads: Characteristics, Stability Assessments, and Sensitivity Studies
Authors: Kyung-Bin Kwon, Sayak Mukherjee, Veronica Adetola
Abstract: This paper investigates the dynamic interactions between large-scale data centers and the power grid, focusing on reliability challenges arising from sudden fluctuations in demand. With the rapid growth of AI-driven workloads, such fluctuations, along with fast ramp patterns, are expected to exacerbate stressed grid conditions and system instabilities. We consider a few open-source AI data center consumption profiles from the MIT supercloud datasets, along with generating a few experimental HPC job-distribution-based inference profiles. Subsequently, we develop analytical methodologies for real-time assessment of grid stability, focusing on both transient and small-signal stability assessments. Energy-flow-like metrics for nonlinear transient stability, formulated by computing localized data center bus kinetic-like flows and coupling interactions with neighboring buses over varying time windows, help provide operators a real-time assessments of the regional grid stress in the data center hubs. On the other hand, small-signal stability metrics, constructed from analytical state matrices under variable operating conditions during a fast ramping period, enable snapshot-based assessments of data center load fluctuations, provide enhanced observability into evolving grid conditions. By quantifying the stability impacts of large data center clusters, studies conducted in the modified IEEE benchmark $68-$bus model support improved operator situational awareness to capture risks in reliable integration of large data center loads.

Paper number 10:
Title: Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming
Authors: Alexander James Fernandes, Ioannis Psaromiligkos
Abstract: A model-based deep learning (DL) architecture is proposed for reconfigurable intelligent surface (RIS)-assisted multi-user communications to reduce the overhead of transmitting phase shift information from the access point (AP) to the RIS controller. The phase shifts are computed at the AP, which has access to the channel state information, and then encoded into a compressed binary control message that is sent to the RIS controller for element configuration. To help reduce beamformer mismatches due to phase shift compression errors, the beamformer is updated using weighted minimum mean square error (WMMSE) based on the effective channel resulting from the actual (decompressed) RIS reflection coefficients. By unrolling the iterative WMMSE algorithm as part of the wireless communication informed DL architecture, joint phase shift compression and WMMSE beamforming can be trained end-to-end. Simulations show that accounting for phase shift compression errors during beamforming significantly improves the sum-rate performance, even when the number of control bits is lower than the number of RIS elements.

Paper number 11:
Title: A Predictive and Sampled-Data Barrier Method for Safe and Efficient Quadrotor Control
Authors: Ming Gao, Zhanglin Shangguan, Shuo Liu, Liang Wu, Bo Yang, Wei Xiao
Abstract: This paper proposes a cascaded control framework for quadrotor trajectory tracking with formal safety guarantees. First, we design a controller consisting of an outer-loop position model predictive control (MPC) and an inner-loop nonlinear attitude control, enabling decoupling of position safety and yaw orientation. Second, since quadrotor safety constraints often involve high relative degree, we adopt high order control barrier functions (HOCBFs) to guarantee safety. To employ HOCBFs in the MPC formulation that has formal guarantees, we extend HOCBFs to sampled-data HOCBF (SdHOCBFs) by introducing compensation terms, ensuring safety over the entire sampling interval. We show that embedding SdHOCBFs as control-affine constraints into the MPC formulation guarantees both safety and optimality while preserving convexity for real-time implementations. Finally, comprehensive simulations are conducted to demonstrate the safety guarantee and high efficiency of the proposed method compared to existing methods.

Paper number 12:
Title: AQA-TTRL: Self-Adaptation in Audio Question Answering with Test-Time Reinforcement Learning
Authors: Haoyu Zhang, Jiaxian Guo, Yusuke Iwasawa, Yutaka Matsuo
Abstract: Large Audio Language Models (LALMs) demonstrate impressive general audio understanding, but once deployed, they are static and fail to improve with new real-world audio data. As traditional supervised fine-tuning is costly, we introduce a novel framework for test-time audio understanding, AQA-TTRL, where an LALM evolves on-the-fly using only unlabeled test data. It first generates pseudo-labels from the prediction via majority voting, then optimizes the model via reinforcement learning. To handle the inherent noise in these self-generated labels, we introduce a confidence-based weighting method to adjust training signals. Furthermore, a multiple-attempt sampling operation mitigates advantage collapse and stabilizes training. On the MMAU (test-mini/test), MMAR, and MMSU benchmarks, AQA-TTRL achieves significant average improvements of 4.42% for the Qwen2.5-Omni 7B model and 11.04% for the 3B model. Notably, the adapted 3B model consistently outperforms the direct inference of the unadapted 7B model, highlighting the effectiveness of previously unexplored test-time adaptations in audio understanding.

Paper number 13:
Title: nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality Segmentation and Composition Analysis of Lumbar Paraspinal Muscles
Authors: Zhongyi Zhang, Julie A. Hides, Enrico De Martino, Abdul Joseph Fofanah, Gervase Tuxworth
Abstract: Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot segmentation of lumbar paraspinal muscles using only a single annotated slice per dataset, and to assess its statistical comparability with expert measurements across multi-sequence MRI and multi-protocol CT. Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762 participants across six datasets. Six slices (one per dataset) served as labeled examples, while the remaining 19,433 slices were used for testing. In this minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to generate pseudo-labels, which were pooled across datasets and refined through three sequential, independent nnU-Net models. Segmentation performance was evaluated using the Dice similarity coefficient (DSC), and automated measurements-including muscle volume, fat ratio, and CT attenuation-were assessed with two one-sided tests (TOST) and intraclass correlation coefficients (ICC). Results: nnsam2 outperformed vanilla SAM2, its medical variants, TotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96 on MR images and 0.92-0.93 on CT. Automated and expert measurements were statistically equivalent for muscle volume (MRI/CT), CT attenuation, and Dixon fat ratio (TOST, P < 0.05), with consistently high ICCs (0.86-1.00). Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for multi-modality LPM segmentation, producing muscle volume (MRI/CT), attenuation (CT), and fat ratio (Dixon MRI) measurements that were statistically comparable to expert references. Validated across multimodal, multicenter, and multinational cohorts, and released with open code and data, nnsam2 demonstrated high annotation efficiency, robust generalizability, and reproducibility.

Paper number 14:
Title: Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model
Authors: Md Rakibul Mowla, Sukhbinder Kumar, Ariane E. Rhone, Brian J. Dlouhy, Christopher K. Kovach
Abstract: Statistical significance testing of neural coherence is essential for distinguishing genuine cross-signal coupling from spurious correlations. A widely accepted approach uses surrogate-based inference, where null distributions are generated via time-shift or phase-randomization procedures. While effective, these methods are computationally expensive and yield discrete p-values that can be unstable near decision thresholds, limiting scalability to large EEG/iEEG datasets. We introduce and validate a parametric alternative based on a generalized linear model (GLM) applied to complex-valued time--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio test. Using real respiration belt traces as a driver and simulated neural signals contaminated with broadband Gaussian noise, we perform dense sweeps of ground-truth coherence and compare GLM-based inference against time-shift/phase-randomized surrogate testing under matched conditions. GLM achieved comparable or superior sensitivity while producing continuous, stable p-values and a substantial computational advantage. At 80% detection power, GLM detects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to an approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be nearly 200x faster than surrogate approaches. These results establish GLM-based inference on complex time--frequency coefficients as a robust, scalable alternative to surrogate testing, enabling efficient analysis of large EEG/iEEG datasets across channels, frequencies, and participants.

Paper number 15:
Title: Teaching Machines to Speak Using Articulatory Control
Authors: Akshay Anand, Chenxu Guo, Cheol Jun Cho, Jiachen Lian, Gopala Anumanchipalli
Abstract: Current speech production systems predominantly rely on large transformer models that operate as black boxes, providing little interpretability or grounding in the physical mechanisms of human speech. We address this limitation by proposing a new framework: speech generation through explicit articulatory control. This reframes speech as a motor control task similar to robotic manipulation. Our approach uses reinforcement learning to train a policy that directly controls the movements of vocal tract articulators, such as the tongue, lips, and jaw, to produce syllable-level speech. Specifically, we employ the Proximal Policy Optimization algorithm to learn optimal articulatory movements based on acoustic feedback provided by our audio perceiver, Sylber. The resulting articulatory trajectories are decoded into audio using SPARC, a pre-trained articulatory-to-speech decoder. We train this framework on six target syllables, and it demonstrates successful convergence, with similarity scores between the policy-generated audio and the target syllables exceeding 0.85. Accurate human transcription of the audio for syllables such as "please", "loot", and "cat" demonstrates the intelligibility of this framework.

Paper number 16:
Title: Learning Continuous Receive Apodization Weights via Implicit Neural Representation for Ultrafast ICE Ultrasound Imaging
Authors: RÃ©mi Delaunay, Christoph Hennersperger, Stefan WÃ¶rz
Abstract: Ultrafast intracardiac echocardiography (ICE) uses unfocused transmissions to capture cardiac motion at frame rates exceeding 1 kHz. While this enables real-time visualization of rapid dynamics, image quality is often degraded by diffraction artifacts, requiring many transmits to achieve satisfying resolution and contrast. To address this limitation, we propose an implicit neural representation (INR) framework to encode complex-valued receive apodization weights in a continuous manner, enabling high-quality ICE reconstructions from only three diverging wave (DW) transmits. Our method employs a multi-layer perceptron that maps pixel coordinates and transmit steering angles to complex-valued apodization weights for each receive channel. Experiments on a large in vivo porcine ICE imaging dataset show that the learned apodization suppresses clutter and enhances contrast, yielding reconstructions closely matching 26-angle compounded DW ground truths. Our study suggests that INRs could offer a powerful framework for ultrasound image enhancement.

Paper number 17:
Title: Sample-Efficient and Smooth Cross-Entropy Method Model Predictive Control Using Deterministic Samples
Authors: Markus Walker, Daniel Frisch, Uwe D. Hanebeck
Abstract: Cross-entropy method model predictive control (CEM--MPC) is a powerful gradient-free technique for nonlinear optimal control, but its performance is often limited by the reliance on random sampling. This conventional approach can lead to inefficient exploration of the solution space and non-smooth control inputs, requiring a large number of samples to achieve satisfactory results. To address these limitations, we propose deterministic sampling CEM (dsCEM), a novel framework that replaces the random sampling step with deterministic samples derived from localized cumulative distributions (LCDs). Our approach introduces modular schemes to generate and adapt these sample sets, incorporating temporal correlations to ensure smooth control trajectories. This method can be used as a drop-in replacement for the sampling step in existing CEM-based controllers. Experimental evaluations on two nonlinear control tasks demonstrate that dsCEM consistently outperforms state-of-the-art iCEM in terms of cumulative cost and control input smoothness, particularly in the critical low-sample regime.

Paper number 18:
Title: Investigation of perception inconsistency in speaker embedding for asynchronous voice anonymization
Authors: Rui Wang, Liping Chen, Kong Aik Lee, Zhengpeng Zha, Zhenhua Ling
Abstract: Given the speech generation framework that represents the speaker attribute with an embedding vector, asynchronous voice anonymization can be achieved by modifying the speaker embedding derived from the original speech. However, the inconsistency between machine and human perceptions of the speaker attribute within the speaker embedding remains unexplored, limiting its performance in asynchronous voice anonymization. To this end, this study investigates this inconsistency via modifications to speaker embedding in the speech generation process. Experiments conducted on the FACodec and Diff-HierVC speech generation models discover a subspace whose removal alters machine perception while preserving its human perception of the speaker attribute in the generated speech. With these findings, an asynchronous voice anonymization is developed, achieving 100% human perception preservation rate while obscuring the machine perception. Audio samples can be found in this https URL.

Paper number 19:
Title: Modulated INR with Prior Embeddings for Ultrasound Imaging Reconstruction
Authors: RÃ©mi Delaunay, Christoph Hennersperger, Stefan WÃ¶rz
Abstract: Ultrafast ultrasound imaging enables visualization of rapid physiological dynamics by acquiring data at exceptionally high frame rates. However, this speed often comes at the cost of spatial resolution and image quality due to unfocused wave transmissions and associated artifacts. In this work, we propose a novel modulated Implicit Neural Representation (INR) framework that leverages a coordinate-based neural network conditioned on latent embeddings extracted from time-delayed I/Q channel data for high-quality ultrasound image reconstruction. Our method integrates complex Gabor wavelet activation and a conditioner network to capture the oscillatory and phase-sensitive nature of I/Q ultrasound signals. We evaluate the framework on an in vivo intracardiac echocardiography (ICE) dataset and demonstrate that it outperforms the compared state-of-the-art methods. We believe these findings not only highlight the advantages of INR-based modeling for ultrasound image reconstruction, but also point to broader opportunities for applying INR frameworks across other medical imaging modalities.

Paper number 20:
Title: Neural Forward Filtering for Speaker-Image Separation
Authors: Jingqi Sun, Shulin He, Ruizhe Pang, Zhong-Qiu Wang
Abstract: We address monaural multi-speaker-image separation in reverberant conditions, aiming at separating mixed speakers but preserving the reverberation of each speaker. A straightforward approach for this task is to directly train end-to-end DNN systems to predict the reverberant speech of each speaker based on the input mixture. Although effective, this approach does not explicitly exploit the physical constraint that reverberant speech can be reproduced by convolving the direct-path signal with a linear filter. To address this, we propose CxNet, a two-DNN system with a neural forward filtering module in between. The first DNN is trained to jointly predict the direct-path signal and reverberant speech. Based on the direct-path estimate, the neural forward filtering module estimates the linear filter, and the estimated filter is then convolved with the direct-path estimate to obtain another estimate of reverberant speech, which is utilized as a discriminative feature to help the second DNN better estimate the reverberant speech. By explicitly modeling the linear filter, CxNet could leverage the physical constraint between the direct-path signal and reverberant speech to capture crucial information about reverberation tails. Evaluation results on the SMS-WSJ dataset show the effectiveness of the proposed algorithms.

Paper number 21:
Title: Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals
Authors: Pubudu L. Indrasiri, Bipasha Kashyap, Pubudu N. Pathirana
Abstract: Biomedical signals provide insights into various conditions affecting the human body. Beyond diagnostic capabilities, these signals offer a deeper understanding of how specific organs respond to an individual's emotions and feelings. For instance, ECG data can reveal changes in heart rate variability linked to emotional arousal, stress levels, and autonomic nervous system activity. This data offers a window into the physiological basis of our emotional states. Recent advancements in the field diverge from conventional approaches by leveraging the power of advanced transformer architectures, which surpass traditional machine learning and deep learning methods. We begin by assessing the effectiveness of the Vision Transformer (ViT), a forefront model in image classification, for identifying emotions in imaged ECGs. Following this, we present and evaluate an improved version of ViT, integrating both CNN and SE blocks, aiming to bolster performance on imaged ECGs associated with emotion detection. Our method unfolds in two critical phases: first, we apply advanced preprocessing techniques for signal purification and converting signals into interpretable images using continuous wavelet transform and power spectral density analysis; second, we unveil a performance-boosted vision transformer architecture, cleverly enhanced with convolutional neural network components, to adeptly tackle the challenges of emotion recognition. Our methodology's robustness and innovation were thoroughly tested using ECG data from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the YAAD dataset, our approach outperformed existing state-of-the-art methods in classifying seven unique emotional states, as well as in valence and arousal classification. Similarly, in the DREAMER dataset, our method excelled in distinguishing between valence, arousal and dominance, surpassing current leading techniques.

Paper number 22:
Title: Time-causal and time-recursive wavelets
Authors: Tony Lindeberg
Abstract: When to apply wavelet analysis to real-time temporal signals, where the future cannot be accessed, it is essential to base all the steps in the signal processing pipeline on computational mechanisms that are truly time-causal. This paper describes how a time-causal wavelet analysis can be performed based on concepts developed in the area of temporal scale-space theory, originating from a complete classification of temporal smoothing kernels that guarantee non-creation of new structures from finer to coarser temporal scale levels. By necessity, convolution with truncated exponential kernels in cascade constitutes the only permissable class of kernels, as well as their temporal derivatives as a natural complement to fulfil the admissibility conditions of wavelet representations. For a particular way of choosing the time constants in the resulting infinite convolution of truncated exponential kernels, to ensure temporal scale covariance and thus self-similarity over temporal scales, we describe how mother wavelets can be chosen as temporal derivatives of the resulting time-causal limit kernel. By developing connections between wavelet theory and scale-space theory, we characterize and quantify how the continuous scaling properties transfer to the discrete implementation, demonstrating how the proposed time-causal wavelet representation can reflect the duration of locally dominant temporal structures in the input signals. We propose that this notion of time-causal wavelet analysis could be a valuable tool for signal processing tasks, where streams of signals are to be processed in real time, specifically for signals that may contain local variations over a rich span of temporal scales, or more generally for analysing physical or biophysical temporal phenomena, where a fully time-causal analysis is called for to be physically realistic.

Paper number 23:
Title: Safe Landing on Small Celestial Bodies with Gravitational Uncertainty Using Disturbance Estimation and Control Barrier Functions
Authors: Felipe Arenas-Uribe, T. Michael Seigler, Jesse B. Hoagg
Abstract: Soft landing on small celestial bodies (SCBs) poses unique challenges, as uncertainties in gravitational models and poorly characterized, dynamic environments require a high level of autonomy. Existing control approaches lack formal guarantees for safety constraint satisfaction, necessary to ensure the safe execution of the maneuvers. This paper introduces a control that addresses this limitation by integrating trajectory tracking, disturbance estimation, and safety enforcement. An extended high-gain observer is employed to estimate disturbances resulting from gravitational model uncertainties. We then apply a feedback-linearizing and disturbance-canceling controller that achieves exponential tracking of reference trajectories. Finally, we use a control barrier function based minimum-intervention controller to enforce state and input constraints through out the maneuver execution. This control combines trajectory tracking of offline generated reference trajectories with formal guarantees of safety, which follows common guidance and control architectures for spacecraft and allows aggressive maneuvers to be executed without compromising safety. Numerical simulations using fuel-optimal trajectories demonstrate the effectiveness of the controller in achieving precise and safe soft-landing, highlighting its potential for autonomous SCB missions.

Paper number 24:
Title: Revisiting MFCCs: Evidence for Spectral-Prosodic Coupling
Authors: Vitor Magno de O. S. Bezerra, Gabriel F. A. Bastos, Jugurta MontalvÃ£o
Abstract: Mel-frequency cepstral coefficients (MFCCs) are an important feature in speech processing. A deeper understanding of their properties can contribute to the work that is being done with both classical and deep learning models. This study challenges the long-held assumption that MFCCs lack relevant temporal information by investigating their relationship with speech prosody. Using a null hypothesis significance testing framework, a systematic assessment is made about the statistical independence between MFCCs and the three prosodic features: energy, fundamental frequency (F0), and voicing. The results demonstrate that it is statistically implausible that the MFCCs are independent of any of these three prosodic features. This finding suggests that MFCCs inherently carry valuable prosodic information, which can inform the design of future models in speech analysis and recognition.

Paper number 25:
Title: Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions
Authors: Huang-Cheng Chou, Chi-Chun Lee
Abstract: Over the past two decades, speech emotion recognition (SER) has received growing attention. To train SER systems, researchers collect emotional speech databases annotated by crowdsourced or in-house raters who select emotions from predefined categories. However, disagreements among raters are common. Conventional methods treat these disagreements as noise, aggregating labels into a single consensus target. While this simplifies SER as a single-label task, it ignores the inherent subjectivity of human emotion perception. This dissertation challenges such assumptions and asks: (1) Should minority emotional ratings be discarded? (2) Should SER systems learn from only a few individuals' perceptions? (3) Should SER systems predict only one emotion per sample? Psychological studies show that emotion perception is subjective and ambiguous, with overlapping emotional boundaries. We propose new modeling and evaluation perspectives: (1) Retain all emotional ratings and represent them with soft-label distributions. Models trained on individual annotator ratings and jointly optimized with standard SER systems improve performance on consensus-labeled tests. (2) Redefine SER evaluation by including all emotional data and allowing co-occurring emotions (e.g., sad and angry). We propose an ``all-inclusive rule'' that aggregates all ratings to maximize diversity in label representation. Experiments on four English emotion databases show superior performance over majority and plurality labeling. (3) Construct a penalization matrix to discourage unlikely emotion combinations during training. Integrating it into loss functions further improves performance. Overall, embracing minority ratings, multiple annotators, and multi-emotion predictions yields more robust and human-aligned SER systems.

Paper number 26:
Title: Distributed Platoon Control Under Quantization: Stability Analysis and Privacy Preservation
Authors: Kaixiang Zhang, Zhaojian Li, Wei Lin
Abstract: Distributed control of connected and automated vehicles has attracted considerable interest for its potential to improve traffic efficiency and safety. However, such control schemes require sharing privacy-sensitive vehicle data, which introduces risks of information leakage and potential malicious activities. This paper investigates the stability and privacy-preserving properties of distributed platoon control under two types of quantizers: deterministic and probabilistic. For deterministic quantization, we show that the resulting control strategy ensures the system errors remain uniformly ultimately bounded. Moreover, in the absence of auxiliary information, an eavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the use of probabilistic quantization enables asymptotic convergence of the vehicle platoon in expectation with bounded variance. Importantly, probabilistic quantizers can satisfy differential privacy guarantees, thereby preserving privacy even when the eavesdropper possesses arbitrary auxiliary information. We further analyze the trade-off between control performance and privacy by formulating an optimization problem that characterizes the impact of the quantization step on both metrics. Numerical simulations are provided to illustrate the performance differences between the two quantization strategies.

Paper number 27:
Title: Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition
Authors: Jim Dai, Manxi Wu, Zhanhao Zhang
Abstract: Stochastic processing networks (SPNs) have broad applications in healthcare, transportation, and communication networks. The control of SPN is to dynamically assign servers in batches under uncertainty to optimize long-run performance. This problem is challenging as the policy dimension grows exponentially with the number of servers, making standard reinforcement learning and policy optimization methods intractable at scale. We propose an atomic action decomposition framework that addresses this scalability challenge by breaking joint assignments into sequential single-server assignments. This yields policies with constant dimension, independent of the number of servers. We study two classes of atomic policies, the step-dependent and step-independent atomic policies, and prove that both achieve the same optimal long-run average reward as the original joint policies. These results establish that computing the optimal SPN control can be made scalable without loss of optimality using the atomic framework. Our results offer theoretical justification for the strong empirical success of the atomic framework in large-scale applications reported in previous articles.

Paper number 28:
Title: Toward Model Matching for Remotely Controlled Differential Drive Robotic Vehicles
Authors: Nikolaos D. Kouvakas, Fotis N. Koumboulis, Konstantinos G. Tzierakis, John Sigalas, Anastasios Dimakakos
Abstract: The problem of regulation of the orientation angle of a remotely controlled differential-drive mobile robot with actuator dynamics and network-induced delays is studied. Using a preinstalled two-layer nonlinear control scheme that decouples linear and angular velocities and regulates heading, a third, delay-dependent layer that achieves exact model matching from the orientation angle command to the orientation angle is introduced. The proposed outer loop controller is a delay dependent dynamic measurable output-feedback controller with dynamic proper precompensator. Parameterization yields a simple characteristic quasi-polynomial with coefficients constrained to satisfy stability for all delays up to a computable bound. Computational experiments confirm accurate tracking, fast settling and bounded internal signals and control voltages. The approach offers an analytic design alternative to AI-based tuning for delayed robotic systems.

Paper number 29:
Title: Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2
Authors: Naveenkumar G Venkataswamy, Yu Liu, Soumyabrata Dey, Stephanie Schuckers, Masudul H Imtiaz
Abstract: Smartphone-based iris recognition in the visible spectrum (VIS) remains difficult due to illumination variability, pigmentation differences, and the absence of standardized capture controls. This work presents a compact end-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at acquisition and demonstrates that accurate VIS iris recognition is feasible on commodity devices. Using a custom Android application performing real-time framing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset of 752 compliant images from 47 subjects. A lightweight MobileNetV3-based multi-task segmentation network (LightIrisNet) is developed for efficient on-device processing, and a transformer matcher (IrisFormer) is adapted to the VIS domain. Under a standardized protocol and comparative benchmarking against prior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%), while IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on CUVIRIS. The acquisition app, trained models, and a public subset of the dataset are released to support reproducibility. These results confirm that standardized capture and VIS-adapted lightweight models enable accurate and practical iris recognition on smartphones.

Paper number 30:
Title: Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves
Authors: Shuixin Li, Jiecheng Chen, Qingtang Jiang, Lin Li
Abstract: To analyze signals with rapid frequency variations or transient components, the time-reassigned synchrosqueezing transform (TSST) and its variants have been recently proposed. Unlike the traditional synchrosqueezing transform, TSST squeezes the time-frequency (TF) coefficients along the group delay (GD) trajectories rather than the instantaneous frequency trajectories. Although TSST methods perform well in analyzing transient signals, they are fundamentally limited in processing multicomponent signals with intersecting GD curves. This limitation compromises the accuracy of both feature extraction and signal component recovery, thereby significantly reducing the interpretability of time-frequency representations (TFRs). This is particularly problematic in broadband signal processing systems, where the linearity of the phase response is critical and precise measurement of group delay dispersion (GDD) is essential. Motivated by the superior capability of frequency-domain signal modeling in characterizing rapidly frequency-varying signals, this paper proposes a novel three-dimensional time-frequency-group delay dispersion (TF-GDD) representation based on the frequency-domain chirplet transform. A subsequent time-reassigned synchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to achieve a sharper TF-GDD distribution and more accurate GD estimation. For mode retrieval, a novel frequency-domain group signal separation operation (FGSSO) is this http URL theoretical contributions include a derivation of the approximation error for the GD and GDD reference functions and an establishment of the error bounds for FGSSO-based mode retrieval. Experimental results demonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and retrieve modes--even for modes with intersecting GD trajectories.

Paper number 31:
Title: TokenChain: A Discrete Speech Chain via Semantic Token Modeling
Authors: Mingxuan Wang, Satoshi Nakamura
Abstract: Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.

Paper number 32:
Title: Multi-Segment Photonic Power Converters for Energy Harvesting and High-Speed Optical Wireless Communication
Authors: Othman Younus, Behnaz Majlesein, Richard Nacke, Isaac N. O. Osahon, Carmine Pellegrino, Sina Babadi, Iman Tavakkolnia, Henning Helmers, Harald Haas
Abstract: The demand for energy-efficient high-speed wireless communication, coupled with the rapid rise of IoT devices, requires systems that integrate power harvesting with optical data reception to eliminate the need for charging or battery replacements. Recent advances have explored the use of solar cells as optical receivers for high-speed data detection alongside power harvesting. \acs{GaAs}-based \acp{PPC} provide six times greater electron mobility than silicon- or cadmium telluride-based cells, enabling faster data detection and improved power efficiency. However, their bandwidth is constrained by junction capacitance, which increases with active area, creating a trade-off between power output and data rate. To address this, we propose and test multi-segment \acs{GaAs}-based \Acp{PPC} that serve as both energy harvesters and data detectors. By segmenting the active area into 2, 4, or 6 subcells, forming circular areas with diameters of 1, 1.5, or 2.08~mm, we reduce capacitance and boost bandwidth while preserving light collection. Fabricated on a semi-insulating \ac{GaAs} substrate with etched trenches for electrical isolation, the series-connected subcells optimize absorption and minimize parasitic effects. The \Acp{PPC} were used for an eye-safe 1.5~m optical wireless link, employing \ac{OFDM} with adaptive bit and power loading. The system achieved a world record data rate of 3.8~Gbps, which is four times higher than prior works. The system converts 39.7\% of optical power from a beam of 2.3~mW, although the segmentation increases the sensitivity of the alignment. These findings provide new solutions for off-grid backhaul for future communication networks, such as 6th generation (6G) cellular.

Paper number 33:
Title: Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices
Authors: Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee
Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly half a day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

Paper number 34:
Title: Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models
Authors: Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha
Abstract: Current methods for automated assessment of cognitive-linguistic impairment via picture description often neglect the visual narrative path - the sequence and locations of elements a speaker described in the picture. Analyses of spatio-semantic features capture this path using content information units (CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and pairwise ranking loss, for automated CIU extraction and ordering from the Cookie Theft picture description. Evaluated by 5-fold cross-validation, it achieves 93% median precision, 96% median recall in CIU detection, and 24% sequence error rates. The proposed method extracts features that exhibit strong Pearson correlations with ground truth, surpassing the dictionary-based baseline in external validation. These features also perform comparably to those derived from manual annotations in evaluating group differences via ANCOVA. The pipeline is shown to effectively characterize visual narrative paths for cognitive impairment assessment, with the implementation and models open-sourced to public.

Paper number 35:
Title: SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography
Authors: Zahra Maleki, Amirhossein Akbari, Amirhossein Binesh, Babak Khalaj
Abstract: Remote photoplethysmography (rPPG) is an innovative method for monitoring heart rate and vital signs by using a simple camera to record a person, as long as any part of their skin is visible. This low-cost, contactless approach helps in remote patient monitoring, emotion analysis, smart vehicle utilization, and more. Over the years, various techniques have been proposed to improve the accuracy of this technology, especially given its sensitivity to lighting and movement. In the unsupervised pipeline, it is necessary to first select skin regions from the video to extract the rPPG signal from the skin color changes. We introduce a novel skin segmentation technique that prioritizes skin regions to enhance the quality of the extracted signal. It can detect areas of skin all over the body, making it more resistant to movement, while removing areas such as the mouth, eyes, and hair that may cause interference. Our model is evaluated on publicly available datasets, and we also present a new dataset, called SYNC-rPPG, to better represent real-world conditions. The results indicate that our model demonstrates a prior ability to capture heartbeats in challenging conditions, such as talking and head rotation, and maintain the mean absolute error (MAE) between predicted and actual heart rates, while other methods fail to do so. In addition, we demonstrate high accuracy in detecting a diverse range of skin tones, making this technique a promising option for real-world applications.

Paper number 36:
Title: A System Level Approach to LQR Control of the Diffusion Equation
Authors: Addie McCurdy, Andrew Gusty, Emily Jensen
Abstract: The continuous-time, infinite horizon LQR problem for the diffusion equation over the unit circle with fully distributed actuation is considered. It is well-known that the solution to this problem can be obtained from the solution to an operator-valued algebraic Riccati equation. Here, it is demonstrated that this solution can be equivalently obtained by solving an $H_2$ control problem through a closed-loop design procedure that is analogous to the "System Level Synthesis" methodology previously developed for systems over a discrete spatial domain and/or over a finite time horizon. The presented extension to the continuous spatial domain and continuous and infinite-horizon time setting admits analytical solutions that may complement computational approaches for discrete or finite-horizon settings. It is further illustrated that spatio-temporal constraints on the closed-loop responses can be incorporated into this new formulation in a convex manner.

Paper number 37:
Title: AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control
Authors: Shao-Yi Yu, Jen-Wei Wang, Maya Horii, Vikas Garg, Tarek Zohdi
Abstract: Mobile robots, such as ground vehicles and quadrotors, are becoming increasingly important in various fields, from logistics to agriculture, where they automate processes in environments that are difficult to access for humans. However, to perform effectively in uncertain environments using model-based controllers, these systems require dynamics models capable of responding to environmental variations, especially when direct access to environmental information is limited. To enable such adaptivity and facilitate integration with model predictive control, we propose an adaptive dynamics model which bypasses the need for direct environmental knowledge by inferring operational environments from state-action history. The dynamics model is based on neural ordinary equations, and a two-phase training procedure is used to learn latent environment representations. We demonstrate the effectiveness of our approach through goal-reaching and path-tracking tasks on three robotic platforms of increasing complexity: a 2D differential wheeled robot with changing wheel contact conditions, a 3D quadrotor in variational wind fields, and the Sphero BOLT robot under two contact conditions for real-world deployment. Empirical results corroborate that our method can handle temporally and spatially varying environmental changes in both simulation and real-world systems.

Paper number 38:
Title: Optimization via a Control-Centric Framework
Authors: Liraz Mudrik, Isaac Kaminer, Sean Kragelund, Abram H. Clark
Abstract: Optimization plays a central role in intelligent systems and cyber-physical technologies, where the speed and reliability of convergence directly impact performance. In control theory, optimization-centric methods are standard: controllers are designed by repeatedly solving optimization problems, as in linear quadratic regulation, $H_\infty$ control, and model predictive control. In contrast, this paper develops a control-centric framework for optimization itself, where algorithms are constructed directly from Lyapunov stability principles rather than being proposed first and analyzed afterward. A key element is the stationarity vector, which encodes first-order optimality conditions and enables Lyapunov-based convergence analysis. By pairing a Lyapunov function with a selectable decay law, we obtain continuous-time dynamics with guaranteed exponential, finite-time, fixed-time, or prescribed-time convergence. Within this framework, we introduce three feedback realizations of increasing restrictiveness: the Hessian-gradient, Newton, and gradient dynamics. Each shapes the decay of the stationarity vector to achieve the desired rate. These constructions unify unconstrained optimization, extend naturally to constrained problems via Lyapunov-consistent primal-dual dynamics, and broaden the results for minimax and generalized Nash equilibrium seeking problems beyond exponential stability. The framework provides systematic design tools for optimization algorithms in control and game-theoretic problems.

Paper number 39:
Title: Sci-Phi: A Large Language Model Spatial Audio Descriptor
Authors: Xilin Jiang, Hannes Gamper, Sebastian Braun
Abstract: Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: this https URL

Paper number 40:
Title: GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps
Authors: Yan Rui Tan, Wenqi Liu, Wai Lun Leong, John Guan Zhong Tan, Wayne Wen Huei Yong, Fan Shi, Rodney Swee Huat Teo
Abstract: Artificial Potential Field (APF) methods are widely used for reactive flocking control, but they often suffer from challenges such as deadlocks and local minima, especially in the presence of obstacles. Existing solutions to address these issues are typically passive, leading to slow and inefficient collective navigation. As a result, many APF approaches have only been validated in obstacle-free environments or simplified, pseudo 3D simulations. This paper presents GO-Flock, a hybrid flocking framework that integrates planning with reactive APF-based control. GO-Flock consists of an upstream Perception Module, which processes depth maps to extract waypoints and virtual agents for obstacle avoidance, and a downstream Collective Navigation Module, which applies a novel APF strategy to achieve effective flocking behavior in cluttered environments. We evaluate GO-Flock against passive APF-based approaches to demonstrate their respective merits, such as their flocking behavior and the ability to overcome local minima. Finally, we validate GO-Flock through obstacle-filled environment and also hardware-in-the-loop experiments where we successfully flocked a team of nine drones, six physical and three virtual, in a forest environment.

Paper number 41:
Title: Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks
Authors: Yao Zhang, Yuchen Song, Shengnan Li, Yan Shi, Shikui Shen, Xiongyan Tang, Min Zhang, Danshi Wang
Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.

Paper number 42:
Title: Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions
Authors: Wanli Ni, Hui Tian, Shuai Wang, Chengyang Li, Lei Sun, Zhaohui Yang
Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for enabling collaborative intelligence in industrial Internet of Things (IoT) systems, particularly in smart factories where data privacy, communication efficiency, and device heterogeneity are critical concerns. In this article, we present a comprehensive study of FedSL frameworks tailored for resource-constrained robots in industrial scenarios. We compare synchronous, asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of workflow, scalability, adaptability, and limitations under dynamic industrial conditions. Furthermore, we systematically categorize token fusion strategies into three paradigms: input-level (pre-fusion), intermediate-level (intra-fusion), and output-level (post-fusion), and summarize their respective strengths in industrial applications. We also provide adaptive optimization techniques to enhance the efficiency and feasibility of FedSL implementation, including model compression, split layer selection, computing frequency allocation, and wireless resource management. Simulation results validate the performance of these frameworks under industrial detection scenarios. Finally, we outline open issues and research directions of FedSL in future smart manufacturing systems.

Paper number 43:
Title: Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music
Authors: Aleksandr Lukoianov, Anssi Klapuri
Abstract: Whereas chord transcription has received considerable attention during the past couple of decades, far less work has been devoted to transcribing and encoding the rhythmic patterns that occur in a song. The topic is especially relevant for instruments such as the rhythm guitar, which is typically played by strumming rhythmic patterns that repeat and vary over time. However, in many cases one cannot objectively define a single "right" rhythmic pattern for a given song section. To create a dataset with well-defined ground-truth labels, we asked expert musicians to transcribe the rhythmic patterns in 410 popular songs and record cover versions where the guitar tracks followed those transcriptions. To transcribe the strums and their corresponding rhythmic patterns, we propose a three-step framework. Firstly, we perform approximate stem separation to extract the guitar part from the polyphonic mixture. Secondly, we detect individual strums within the separated guitar audio, using a pre-trained foundation model (MERT) as a backbone. Finally, we carry out a pattern-decoding process in which the transcribed sequence of guitar strums is represented by patterns drawn from an expert-curated vocabulary. We show that it is possible to transcribe the rhythmic patterns of the guitar track in polyphonic music with quite high accuracy, producing a representation that is human-readable and includes automatically detected bar lines and time signature markers. We perform ablation studies and error analysis and propose a set of evaluation metrics to assess the accuracy and readability of the predicted rhythmic pattern sequence.

Paper number 44:
Title: Human-in-the-loop Optimisation in Robot-assisted Gait Training
Authors: Andreas Christou, Andreas Sochopoulos, Elliot Lister, Sethu Vijayakumar
Abstract: Wearable robots offer a promising solution for quantitatively monitoring gait and providing systematic, adaptive assistance to promote patient independence and improve gait. However, due to significant interpersonal and intrapersonal variability in walking patterns, it is important to design robot controllers that can adapt to the unique characteristics of each individual. This paper investigates the potential of human-in-the-loop optimisation (HILO) to deliver personalised assistance in gait training. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) was employed to continuously optimise an assist-as-needed controller of a lower-limb exoskeleton. Six healthy individuals participated over a two-day experiment. Our results suggest that while the CMA-ES appears to converge to a unique set of stiffnesses for each individual, no measurable impact on the subjects' performance was observed during the validation trials. These findings highlight the impact of human-robot co-adaptation and human behaviour variability, whose effect may be greater than potential benefits of personalising rule-based assistive controllers. Our work contributes to understanding the limitations of current personalisation approaches in exoskeleton-assisted gait rehabilitation and identifies key challenges for effective implementation of human-in-the-loop optimisation in this domain.

Paper number 45:
Title: StereoSync: Spatially-Aware Stereo Audio Generation from Video
Authors: Christian Marinoni, Riccardo Fosco Gramaccioni, Kazuki Shimada, Takashi Shibuya, Yuki Mitsufuji, Danilo Comminiello
Abstract: Although audio generation has been widely studied over recent years, video-aligned audio generation still remains a relatively unexplored frontier. To address this gap, we introduce StereoSync, a novel and efficient model designed to generate audio that is both temporally synchronized with a reference video and spatially aligned with its visual context. Moreover, StereoSync also achieves efficiency by leveraging pretrained foundation models, reducing the need for extensive training while maintaining high-quality synthesis. Unlike existing methods that primarily focus on temporal synchronization, StereoSync introduces a significant advancement by incorporating spatial awareness into video-aligned audio generation. Indeed, given an input video, our approach extracts spatial cues from depth maps and bounding boxes, using them as cross-attention conditioning in a diffusion-based audio generation model. Such an approach allows StereoSync to go beyond simple synchronization, producing stereo audio that dynamically adapts to the spatial structure and movement of a video scene. We evaluate StereoSync on Walking The Maps, a curated dataset comprising videos from video games that feature animated characters walking through diverse environments. Experimental results demonstrate the ability of StereoSync to achieve both temporal and spatial alignment, advancing the state of the art in video-to-audio generation and resulting in a significantly more immersive and realistic audio experience.

Paper number 46:
Title: FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders
Authors: Riccardo Fosco Gramaccioni, Christian Marinoni, Eleonora Grassucci, Giordano Cicchetti, Aurelio Uncini, Danilo Comminiello
Abstract: In this work, we present FoleyGRAM, a novel approach to video-to-audio generation that emphasizes semantic conditioning through the use of aligned multimodal encoders. Building on prior advancements in video-to-audio generation, FoleyGRAM leverages the Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities, enabling precise semantic control over the audio generation process. The core of FoleyGRAM is a diffusion-based audio synthesis model conditioned on GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness and temporal alignment with the corresponding input video. We evaluate FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio models. Our experiments demonstrate that aligning multimodal encoders using GRAM enhances the system's ability to semantically align generated audio with video content, advancing the state of the art in video-to-audio synthesis.

Paper number 47:
Title: Segment-Factorized Full-Song Generation on Symbolic Piano Music
Authors: Ping-Yi Chen, Chih-Pin Tan, Yi-Hsuan Yang
Abstract: We propose the Segmented Full-Song Model (SFS) for symbolic full-song generation. The model accepts a user-provided song structure and an optional short seed segment that anchors the main idea around which the song is developed. By factorizing a song into segments and generating each one through selective attention to related segments, the model achieves higher quality and efficiency compared to prior work. To demonstrate its suitability for human-AI interaction, we further wrap SFS into a web application that enables users to iteratively co-create music on a piano roll with customizable structures and flexible ordering.

Paper number 48:
Title: A Dynamic Mode Decomposition Approach to Morphological Component Analysis
Authors: Owen T. Huber, Raghu G. Raj, Tianyu Chen, Zacharie I. Idriss
Abstract: This paper introduces a novel methodology of adapting the representation of videos based on the dynamics of their scene content variation. In particular, we demonstrate how the clustering of dynamic mode decomposition eigenvalues can be leveraged to learn an adaptive video representation for separating structurally distinct morphologies of a video. We extend the morphological component analysis (MCA) algorithm, which uses multiple predefined incoherent dictionaries and a sparsity prior to separate distinct sources in signals, by introducing our novel eigenspace clustering technique to obtain data-driven MCA dictionaries, which we call dynamic morphological component analysis (DMCA). After deriving our novel algorithm, we offer a motivational example of DMCA applied to a still image, then demonstrate DMCA's effectiveness in denoising applications on videos from the Adobe 240fps dataset. Afterwards, we provide an example of DMCA enhancing the signal-to-noise ratio of a faint target summed with a sea state, and conclude the paper by applying DMCA to separate a bicycle from wind clutter in inverse synthetic aperture radar images.

Paper number 49:
Title: ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning
Authors: Tao Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
Abstract: Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser's ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model's training cost and complexity.

Paper number 50:
Title: Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP
Authors: Aueaphum Aueawatthanaphisut, Nyi Wunna Tun
Abstract: The comparative evaluation between classical and quantum reinforcement learning (QRL) paradigms was conducted to investigate their convergence behavior, robustness under observational noise, and computational efficiency in a benchmark control environment. The study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast, the VQC exhibited limited learning capability, with an average return of 14.6 +/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise robustness analysis further revealed that the MLP policy deteriorated gracefully under Gaussian perturbations, while the VQC displayed higher sensitivity at equivalent noise levels. Despite the lower asymptotic performance, the VQC exhibited significantly lower parameter count and marginally increased training time, highlighting its potential scalability for low-resource quantum processors. The results suggest that while classical neural policies remain dominant in current control benchmarks, quantum-enhanced architectures could offer promising efficiency advantages once hardware noise and expressivity limitations are mitigated.

Paper number 51:
Title: Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method
Authors: Lulu Gong, Shreya Saxena
Abstract: Mixtures of linear dynamical systems (MoLDS) provide a path to model time-series data that exhibit diverse temporal dynamics across trajectories. However, its application remains challenging in complex and noisy settings, limiting its effectiveness for neural data analysis. Tensor-based moment methods can provide global identifiability guarantees for MoLDS, but their performance degrades under noise and complexity. Commonly used expectation-maximization (EM) methods offer flexibility in fitting latent models but are highly sensitive to initialization and prone to poor local minima. Here, we propose a tensor-based method that provides identifiability guarantees for learning MoLDS, which is followed by EM updates to combine the strengths of both approaches. The novelty in our approach lies in the construction of moment tensors using the input-output data to recover globally consistent estimates of mixture weights and system parameters. These estimates can then be refined through a Kalman EM algorithm, with closed-form updates for all LDS parameters. We validate our framework on synthetic benchmarks and real-world datasets. On synthetic data, the proposed Tensor-EM method achieves more reliable recovery and improved robustness compared to either pure tensor or randomly initialized EM methods. We then analyze neural recordings from the primate somatosensory cortex while a non-human primate performs reaches in different directions. Our method successfully models and clusters different conditions as separate subsystems, consistent with supervised single-LDS fits for each condition. Finally, we apply this approach to another neural dataset where monkeys perform a sequential reaching task. These results demonstrate that MoLDS provides an effective framework for modeling complex neural data, and that Tensor-EM is a reliable approach to MoLDS learning for these applications.

Paper number 52:
Title: Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing
Authors: Kurt Butler, Guanchao Feng, Petar Djuric
Abstract: Feature attributions are post-training analysis methods that assess how various input features of a machine learning model contribute to an output prediction. Their interpretation is straightforward when features act independently, but becomes less direct when the predictive model involves interactions such as multiplicative relationships or joint feature contributions. In this work, we propose a general theory of higher-order feature attribution, which we develop on the foundation of Integrated Gradients (IG). This work extends existing frameworks in the literature on explainable AI. When using IG as the method of feature attribution, we discover natural connections to statistics and topological signal processing. We provide several theoretical results that establish the theory, and we validate our theory on a few examples.

Paper number 53:
Title: Differentiable Model Predictive Control on the GPU
Authors: Emre Adabag, Marcus Greiff, John Subosits, Thomas Lew
Abstract: Differentiable model predictive control (MPC) offers a powerful framework for combining learning and control. However, its adoption has been limited by the inherently sequential nature of traditional optimization algorithms, which are challenging to parallelize on modern computing hardware like GPUs. In this work, we tackle this bottleneck by introducing a GPU-accelerated differentiable optimization tool for MPC. This solver leverages sequential quadratic programming and a custom preconditioned conjugate gradient (PCG) routine with tridiagonal preconditioning to exploit the problem's structure and enable efficient parallelization. We demonstrate substantial speedups over CPU- and GPU-based baselines, significantly improving upon state-of-the-art training times on benchmark reinforcement learning and imitation learning tasks. Finally, we showcase the method on the challenging task of reinforcement learning for driving at the limits of handling, where it enables robust drifting of a Toyota Supra through water puddles.

Paper number 54:
Title: Conformalized Gaussian processes for online uncertainty quantification over graphs
Authors: Jinwen Xu, Qin Lu, Georgios B. Giannakis
Abstract: Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP.

Paper number 55:
Title: Latent Speech-Text Transformer
Authors: Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le
Abstract: Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.

Paper number 56:
Title: Modulation Discovery with Differentiable Digital Signal Processing
Authors: Christopher Mitcheltree, Hao Hao Tan, Joshua D. Reiss
Abstract: Modulations are a critical part of sound design and music production, enabling the creation of complex and evolving audio. Modern synthesizers provide envelopes, low frequency oscillators (LFOs), and more parameter automation tools that allow users to modulate the output with ease. However, determining the modulation signals used to create a sound is difficult, and existing sound-matching / parameter estimation systems are often uninterpretable black boxes or predict high-dimensional framewise parameter values without considering the shape, structure, and routing of the underlying modulation curves. We propose a neural sound-matching approach that leverages modulation extraction, constrained control signal parameterizations, and differentiable digital signal processing (DDSP) to discover the modulations present in a sound. We demonstrate the effectiveness of our approach on highly modulated synthetic and real audio samples, its applicability to different DDSP synth architectures, and investigate the trade-off it incurs between interpretability and sound-matching accuracy. We make our code and audio samples available and provide the trained DDSP synths in a VST plugin.

Paper number 57:
Title: High-pass filtered fidelity-imposed network edit (HP-FINE) for robust quantitative susceptibility mapping from high-pass filtered phase
Authors: Jinwei Zhang, Alexey Dimov, Chao Li, Hang Zhang, Thanh D. Nguyen, Pascal Spincemaille, Yi Wang
Abstract: Purpose: To improve the generalization ability of deep learning based predictions of quantitative susceptibility mapping (QSM) from high-pass filtered phase (HPFP) data. Methods: A network fine-tuning step called HP-FINE is proposed, which is based on the high-pass filtering forward model with low-frequency preservation regularization. Several comparisons were conducted: 1. HP-FINE with and without low-frequency regularization, 2. three 3D network architectures (Unet, Progressive Unet, and Big Unet), 3. two types of network output (recovered field and susceptibility), and 4. pre-training with and without the filtering augmentation. HPFP datasets with diverse high-pass filters, another acquisition voxel size, and prospective acquisition were used to assess the accuracy of QSM predictions. In the retrospective datasets, quantitative metrics (PSNR, SSIM, RMSE and HFEN) were used for evaluation. In the prospective dataset, statistics of ROI linear regression and Bland-Altman analysis were used for evaluation. Results: In the retrospective datasets, adding low-frequency regularization in HP-FINE substantially improved prediction accuracy compared to the pre-trained results, especially when combined with the filtering augmentation and recovered field output. In the prospective datasets, HP-FINE with low-frequency regularization and recovered field output demonstrated the preservation of ROI values, a result that was not achieved when using susceptibility as the output. Furthermore, Progressive Unet pre-trained with a combination of multiple losses outperformed both Unet and Progressive Unet pre-trained with a single loss in terms of preserving ROI values.

Paper number 58:
Title: RimSet: Quantitatively Identifying and Characterizing Chronic Active Multiple Sclerosis Lesion on Quantitative Susceptibility Maps
Authors: Jinwei Zhang, Thanh D. Nguyen, Renjiu Hu, Susan A. Gauthier, Yi Wang, Hang Zhang
Abstract: Background: Rim+ lesions in multiple sclerosis (MS), detectable via Quantitative Susceptibility Mapping (QSM), correlate with increased disability. Existing literature lacks quantitative analysis of these lesions. We introduce RimSet for quantitative identification and characterization of rim+ lesions on QSM. Methods: RimSet combines RimSeg, an unsupervised segmentation method using level-set methodology, and radiomic measurements with Local Binary Pattern texture descriptors. We validated RimSet using simulated QSM images and an in vivo dataset of 172 MS subjects with 177 rim+ and 3986 rim-lesions. Results: RimSeg achieved a 78.7% Dice score against the ground truth, with challenges in partial rim lesions. RimSet detected rim+ lesions with a partial ROC AUC of 0.808 and PR AUC of 0.737, surpassing existing methods. QSMRim-Net showed the lowest mean square error (0.85) and high correlation (0.91; 95% CI: 0.88, 0.93) with expert annotations at the subject level.

Paper number 59:
Title: SAMCIRT: A Simultaneous Reconstruction and Affine Motion Compensation Technique for Four Dimensional Computed Tomography (4DCT)
Authors: Anh-Tuan Nguyen, Jens Renders, Khoi-Nguyen Nguyen, Tat-Dat To, Domenico Iuso, Yves Maris
Abstract: The majority of the recent iterative approaches in 4DCT not only rely on nested iterations, thereby increasing computational complexity and constraining potential acceleration, but also fail to provide a theoretical proof of convergence for their proposed iterative schemes. On the other hand, the latest MATLAB and Python image processing toolboxes lack the implementation of analytic adjoints of affine motion operators for 3D object volumes, which does not allow gradient methods using exact derivatives towards affine motion parameters. In this work, we propose the Simultaneous Affine Motion-Compensated Image Reconstruction Technique (SAMCIRT)- an efficient iterative reconstruction scheme that combines image reconstruction and affine motion estimation in a single update step, based on the analytic adjoints of the motion operators then exact partial derivatives with respect to both the reconstruction and the affine motion parameters. Moreover, we prove the separated Lipschitz continuity of the objective function and its associated functions, including the gradient, which supports the convergence of our proposed iterative scheme, despite the non-convexity of the objective function with respect to the affine motion parameters. Results from simulation and real experiments show that our method outperforms the state-of-the-art CT reconstruction with affine motion correction methods in computational feasibility and projection distance. In particular, this allows accurate reconstruction for a real, nonstationary diamond, showing a novel application of 4DCT.

Paper number 60:
Title: Miniature UAV-Aided Cooperative THz Networks with Reconfigurable Energy Harvesting Holographic Surfaces
Authors: Yifei Song, Jalal Jalali, Yanyu Qin, Mostafa Darabi, Filip Lemic, Natasha Devroye
Abstract: This paper focuses on enhancing the energy efficiency (EE) of a cooperative network that features a miniature unmanned aerial vehicle (UAV) operating at terahertz (THz) frequencies and equipped with holographic surfaces to improve network performance. Unlike traditional reconfigurable intelligent surfaces (RIS), which serve as passive relays for signal reflection, this work introduces a novel concept: energy harvesting (EH) using reconfigurable holographic surfaces (RHS). These surfaces provide more powerful and focused energy delivery during wireless power transfer than RIS and are mounted on the miniature UAV. In this system, a source node enables the UAV to simultaneously receive both information and energy signals, with the harvested energy powering data transmission to a specific destination. The EE optimization problem involves adjusting non-orthogonal multiple access (NOMA) power coefficients and the UAV's flight path while accounting for the unique characteristics of the THz channel. The problem is solved in two stages to maximize EE and meet a target transmission rate. The UAV trajectory is optimized using a successive convex approximation (SCA) method, followed by the adjustment of NOMA power coefficients through a quadratic transform technique. Simulation results demonstrate the effectiveness of the proposed algorithm, showing significant improvements over baseline methods.

Paper number 61:
Title: Machine Learning Based Probe Skew Correction for High-frequency BH Loop Measurements
Authors: Yakun Wang, Song Liu, Jun Wang, Binyu Cui, Jingrong Yang
Abstract: Experimental characterization of magnetic components has grown to be increasingly important to understand and model their behaviours in high-frequency PWM converters. The BH loop measurement is the only available approach to separate the core loss as an electrical method, which, however, is susceptive to the probe phase skew. As an alternative to the regular de-skew approaches based on hardware, this work proposes a novel machine-learning-based method to identify and correct the probe skew, which builds on the newly discovered correlation between the skew and the shape/trajectory of the measured BH loop. A special technique is proposed to artificially generate skewed images from measured waveforms as augmented training sets. A machine learning pipeline is developed with the Convolutional Neural Network (CNN) to treat the problem as an image-based prediction task. The trained model has demonstrated a high accuracy and generalizability in identifying the skew value from a BH loop unseen by the model, which enables the compensation of the skew to yield the corrected core loss value and BH loop.

Paper number 62:
Title: Modern Base Station Architecture: Enabling Passive Beamforming with Beyond Diagonal RISs
Authors: Mahmoud Raeisi, Hui Chen, Henk Wymeersch, Ertugrul Basar
Abstract: Beamforming plays a crucial role in millimeter wave (mmWave) communication systems to mitigate the severe attenuation inherent to this spectrum. However, the use of large active antenna arrays in conventional architectures often results in high implementation costs and excessive power consumption, limiting their practicality. As an alternative, deploying large arrays at transceivers using passive devices, such as reconfigurable intelligent surfaces (RISs), offers a more cost-effective and energy-efficient solution. In this paper, we investigate a promising base station (BS) architecture that integrates a beyond diagonal RIS (BD-RIS) within the BS to enable passive beamforming. By utilizing Takagi's decomposition and leveraging the effective beamforming vector, the RIS profile can be designed to enable passive beamforming directed toward the target. Through the beamforming analysis, we reveal that BD-RIS provides robust beamforming performance across various system configurations, whereas the traditional diagonal RIS (D-RIS) exhibits instability with increasing RIS size and decreasing BS-RIS separation-two critical factors in optimizing RIS-assisted systems. Comprehensive computer simulation results across various aspects validate the superiority of the proposed BS-integrated BD-RIS over conventional D-RIS architectures, showcasing performance comparable to active analog beamforming antenna arrays.

Paper number 63:
Title: A Graph-Based Framework for Interpretable Whole Slide Image Analysis
Authors: Alexander Weers, Alexander H. Berger, Laurin Lux, Peter SchÃ¼ffler, Daniel Rueckert, Johannes C. Paetzold
Abstract: The histopathological analysis of whole-slide images (WSIs) is fundamental to cancer diagnosis but is a time-consuming and expert-driven process. While deep learning methods show promising results, dominant patch-based methods artificially fragment tissue, ignore biological boundaries, and produce black-box predictions. We overcome these limitations with a novel framework that transforms gigapixel WSIs into biologically-informed graph representations and is interpretable by design. Our approach builds graph nodes from tissue regions that respect natural structures, not arbitrary grids. We introduce an adaptive graph coarsening technique, guided by learned embeddings, to efficiently merge homogeneous regions while preserving diagnostically critical details in heterogeneous areas. Each node is enriched with a compact, interpretable feature set capturing clinically-motivated priors. A graph attention network then performs diagnosis on this compact representation. We demonstrate strong performance on challenging cancer staging and survival prediction tasks. Crucially, our resource-efficient model ($>$13x fewer parameters and $>$300x less data) achieves results competitive with a massive foundation model, while offering full interpretability through feature attribution. Our code is publicly available at this https URL.

Paper number 64:
Title: Submillimeter-Accurate 3D Lumbar Spine Reconstruction from Biplanar X-Ray Images: Incorporating a Multi-Task Network and Landmark-Weighted Loss
Authors: Wanxin Yu, Zhemin Zhu, Cong Wang, Yihang Bao, Chunjie Xia, Rongshan Cheng, Yan Yu, Tsung-Yuan Tsai
Abstract: To meet the clinical demand for accurate 3D lumbar spine assessment in a weight-bearing position, this study presents a novel, fully automatic framework for high-precision 3D reconstruction from biplanar X-ray images, overcoming the limitations of existing methods. The core of this method involves a novel multi-task deep learning network that simultaneously performs lumbar decomposition and landmark detection on the original biplanar radiographs. The decomposition effectively eliminates interference from surrounding tissues, simplifying subsequent image registration, while the landmark detection provides an initial pose estimation for the Statistical Shape Model (SSM), enhancing the efficiency and robustness of the registration process. Building on this, we introduce a landmark-weighted 2D-3D registration strategy. By assigning higher weights to complex posterior structures like the transverse and spinous processes during optimization, this strategy significantly enhances the reconstruction accuracy of the posterior arch. Our method was validated against a gold standard derived from registering CT segmentations to the biplanar X-rays. It sets a new benchmark by achieving sub-millimeter accuracy and completes the full reconstruction and measurement workflow in under 20 seconds, establishing a state-of-the-art combination of precision and speed. This fast and low-dose pipeline provides a powerful automated tool for diagnosing lumbar conditions such as spondylolisthesis and scoliosis in their functional, weight-bearing state.

Paper number 65:
Title: Probabilistic Simulation of Aircraft Descent via a Hybrid Physics-Informed Machine Learning Approach
Authors: Amy Hodgkin, Nick Pepper, Marc Thomas
Abstract: This paper presents a method for generating probabilistic descent trajectories in simulations of real-world airspace. A dataset of 116,066 trajectories harvested from Mode S radar returns in UK airspace was used to train and test the model. Thirteen aircraft types with varying performance characteristics were investigated. It was found that the error in the mean prediction of time to reach the bottom of descent for the proposed method was less than that of the the Base of Aircraft Data (BADA) model by a factor of 10. Furthermore, the method was capable of generating a range of trajectories that were similar to the held out test dataset when analysed in distribution. The proposed method is hybrid, with aircraft drag and calibrated airspeed functions generated probabilistically to parameterise the BADA equations, ensuring the physical plausibility of generated trajectories.

Paper number 66:
Title: Optimal Duration of Reserve Capacity Ancillary Services for Distributed Energy Resources
Authors: Lorenzo Zapparoli, Blazhe Gjorgiev, Giovanni Sansavini
Abstract: The increasing integration of distributed energy resources (DERs) into power systems presents opportunities and challenges for ancillary services (AS) provision. Technical requirements of existing AS (i.e., duration, reliability, ramp rate, and lead time) have been designed for traditional generating units, making their provision by DER aggregates particularly challenging. This paper proposes a method to design the duration of reserve capacity AS products considering the operational constraints of DERs and the temporal dynamics of system imbalances. The optimal product duration is determined by maximizing product availability and aligning the supply profile with the system's balancing needs. We apply the methodology to a realistic Swiss low-voltage network with a diverse DER portfolio. The results reveal that (i) shorter product durations maximize average availability and (ii) long product durations improve the alignment with system balancing needs. This paper offers valuable insights for system operators to design AS products tailored for DER participation.

Paper number 67:
Title: Equivariant Filter for Relative Attitude and Target's Angular Velocity Estimation
Authors: Gil Serrano, Bruno J. Guerreiro, Pedro LourenÃ§o, Rita Cunha
Abstract: Accurate estimation of the relative attitude and angular velocity between two rigid bodies is fundamental in aerospace applications such as spacecraft rendezvous and docking. In these scenarios, a chaser vehicle must determine the orientation and angular velocity of a target object using onboard sensors. This work addresses the challenge of designing an Equivariant Filter (EqF) that can reliably estimate both the relative attitude and the target angular velocity using noisy observations of two known, non-collinear vectors fixed in the target frame. To derive the EqF, a symmetry for the system is proposed and an equivariant lift onto the symmetry group is calculated. Observability and convergence properties are analyzed. Simulations demonstrate the filter's performance, with Monte Carlo runs yielding statistically significant results. The impact of low-rate measurements is also examined and a strategy to mitigate this effect is proposed. Experimental results, using fiducial markers and both conventional and event cameras for measurement acquisition, further validate the approach, confirming its effectiveness in a realistic setting.

Paper number 68:
Title: A Fairness-Aware Strategy for B5G Physical-layer Security Leveraging Reconfigurable Intelligent Surfaces
Authors: Alex Pierron, Michel Barbeau, Luca De Cicco, Jose Rubio-Hernan, Joaquin Garcia-Alfaro
Abstract: Reconfigurable Intelligent Surfaces are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and lead to improvements in areas with low coverage properties. Combined with Reinforcement Learning techniques, they have the potential to be conduct as well physical-layer security hardening. Yet, and in addition to security improvements, it is crucial to consider the concept of fair communication. Reconfigurable Intelligent Surfaces must ensure that User Equipment units receive their signals with adequate strength, without other units being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining both an efficient and fair duplex Reconfigurable Intelligent Surface-Reinforcement Learning system for multiple legitimate User Equipment units without reducing the level of achieved physical-layer security hardening. In terms of contributions, we uncover a fairness imbalance of a previous physical-layer security hardening solution, validate our findings and report experimental work via simulation results. We also provide an alternative reward strategy to solve the uncovered problems and release both code and datasets to foster further research in the topics of this paper.

Paper number 69:
Title: Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging
Authors: Atif Bilal Asad, Achyut Paudel, Safal Kshetri, Chenchen Kang, Salik Ram Khanal, Nataliya Shcherbatyuk, Pierre Davadant, R. Paul Schreiner, Santosh Kalauni, Manoj Karkee, Markus Keller
Abstract: Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting plant growth and subsequent products such as wine and juice. Because soil N has high spatial and temporal variability, it is desirable to accurately estimate the N concentration of grapevine leaves and manage fertilization at the individual plant level to optimally meet plant needs. In this study, we used in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of four different grapevine cultivars collected from distinct vineyards and over two growth stages during two growing seasons to develop models for predicting N concentration at the leaf-level and canopy-level. After image processing, two feature selection methods were employed to identify the optimal set of spectral bands that were responsive to leaf N concentrations. The selected spectral bands were used to train and test two different Machine Learning (ML) models, Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The comparison of selected bands for both leaf-level and canopy-level datasets showed that most of the spectral regions identified by the feature selection methods were across both methods and the dataset types (leaf- and canopy-level datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm, and 900-950nm. These findings indicated the robustness of these spectral regions for predicting nitrogen content. The results for N prediction demonstrated that the ML model achieved an R square of 0.49 for canopy-level data and an R square of 0.57 for leaf-level data, despite using different sets of selected spectral bands for each analysis level. The study demonstrated the potential of using in-field hyperspectral imaging and the use of spectral data in integrated feature selection and ML techniques to monitor N status in vineyards.

Paper number 70:
Title: Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System
Authors: Hashim Ali, Surya Subramani, Lekha Bollinani, Nithin Sai Adupa, Sali El-Loh, Hafiz Malik
Abstract: The SAFE Challenge evaluates synthetic speech detection across three tasks: unmodified audio, processed audio with compression artifacts, and laundered audio designed to evade detection. We systematically explore self-supervised learning (SSL) front-ends, training data compositions, and audio length configurations for robust deepfake detection. Our AASIST-based approach incorporates WavLM large frontend with RawBoost augmentation, trained on a multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS. Through extensive experimentation with different SSL front-ends, three training data versions, and two audio lengths, we achieved second place in both Task 1 (unmodified audio detection) and Task 3 (laundered audio detection), demonstrating strong generalization and robustness.

Paper number 71:
Title: Quickest Change Detection with Cost-Constrained Experiment Design
Authors: Patrick Vincent N. Lubenia, Taposh Banerjee
Abstract: In the classical quickest change detection problem, an observer performs a single experiment to monitor a stochastic process. The goal in the classical problem is to detect a change in the statistical properties of the process, with the minimum possible delay, subject to a constraint on the rate of false alarms. This paper considers the case where, at each observation time, the decision-maker must choose between multiple experiments with varying information qualities and costs. The change can be detected using any of the experiments. The goal here is to detect the change with the minimum delay, subject to constraints on the rate of false alarms and the fraction of time each experiment is performed before the time of change. The constraint on the fraction of time can be used to control the overall cost of using the system of experiments. An algorithm called the two-experiment cumulative sum (2E-CUSUM) algorithm is first proposed to solve the problem when there are only two experiments. The algorithm for the case of multiple experiments, starting with three experiments, is then designed iteratively using the 2E-CUSUM algorithm. Two key ideas used in the design are the scaling of undershoots and the truncation of tests. The multiple-experiment algorithm can be designed to satisfy the constraints and can achieve the delay performance of the experiment with the highest quality within a constant. The important concept of data efficiency, where the observer has the choice of not performing any experiment, is explored as well.

Paper number 72:
Title: Neural Network-based Co-design of Output-Feedback Control Barrier Function and Observer
Authors: Vaishnavi Jagabathula, Ahan Basu, Pushpak Jagtap
Abstract: Control Barrier Functions (CBFs) provide a powerful framework for ensuring safety in dynamical systems. However, their application typically relies on full state information, which is often violated in real-world scenarios due to the availability of partial state information. In this work, we propose a neural network-based framework for the co-design of a safety controller, observer, and CBF for partially observed continuous-time systems. By formulating barrier conditions over an augmented state space, our approach ensures safety without requiring bounded estimation errors or handcrafted barrier functions. All components are jointly trained by formulating appropriate loss functions, and we introduce a validity condition to provide formal safety guarantees beyond the training data. Finally, we demonstrate the effectiveness of the proposed approach through several case studies.

Paper number 73:
Title: Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment
Authors: Fahad Mostafa, Kannon Hossain, Hafiz Khan
Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.

Paper number 74:
Title: Adapting Large Language Models to Mitigate Skin Tone Biases in Clinical Dermatology Tasks: A Mixed-Methods Study
Authors: Kiran Nijjer, Ryan Bui, Derek Jiu, Adnan Ahmed, Peter Wang, Kevin Zhu, Lilly Zhu
Abstract: SkinGPT-4, a large vision-language model, leverages annotated skin disease images to augment clinical workflows in underserved communities. However, its training dataset predominantly represents lighter skin tones, limiting diagnostic accuracy for darker tones. Here, we evaluated performance biases in SkinGPT-4 across skin tones on common skin diseases, including eczema, allergic-contact dermatitis, and psoriasis using the open-sourced SCIN dataset. We leveraged the SkinGPT-4 backbone to develop finetuned models for custom skin disease classification tasks and explored bias mitigation strategies. Clinical evaluation by board-certified dermatologists on six relevant skin diseases from 300 SCIN cases assessed images for diagnostic accuracy, informativity, physician utility, and patient utility. Model fairness metrics, including demographic parity and equalized odds, were calculated across skin tones. SkinGPT-4 achieved an average demographic parity of 0.10 across Fitzpatrick types, with notable differences of 0.10-0.15 between lightest and darkest tones across evaluation metrics. Model hallucinations in artifacts and anatomy occurred at a rate of 17.8. Our customized models achieved average F1, precision, and AUROC of 0.75, 0.78, and 0.78 across visually similar disease pairs. Fairness analysis showed an average demographic parity of 0.75, with a maximum disparity of 0.21 across skin tones. The best model achieved parity scores of 0.83, 0.83, 0.76, 0.89, 0.90, and 0.90 for Fitzpatrick I-VI, indicating robust fairness. Large language models such as SkinGPT-4 showed weaker performance on darker tones. Model biases exist across evaluation criteria, and hallucinations may affect diagnostic efficacy. These findings demonstrate the efficacy of training accurate, fair models using existing backbones for custom skin disease classification.

Paper number 75:
Title: CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation
Authors: Aref Farhadipour, Shiran Liu, Masoumeh Chapariniya, Valeriia Vyshnevetska, Srikanth Madikeri, Teodora Vukovic, Volker Dellwo
Abstract: The CL-UZH team submitted one system each for the fixed and open conditions of the NIST SRE 2024 challenge. For the closed-set condition, results for the audio-only trials were achieved using the X-vector system developed with Kaldi. For the audio-visual results we used only models developed for the visual modality. Two sets of results were submitted for the open-set and closed-set conditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2 datasets. An Xvector-based model was trained from scratch using the CTS superset dataset for the closed set. In addition to the submission of the results of the SRE24 evaluation to the competition website, we talked about the performance of the proposed systems on the SRE24 evaluation in this report.

Paper number 76:
Title: Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations
Authors: Hanyang He, John Harlim, Daning Huang, Yan Li
Abstract: Model predictive control (MPC)-based energy management systems (EMS) are essential for ensuring optimal, secure, and stable operation in microgrids with high penetrations of distributed energy resources. However, due to the high computational cost for the decision-making, the conventional MPC-based EMS typically adopts a simplified integrated-bus power balance model. While this simplification is effective for small networks, large-scale systems require a more detailed branch flow model to account for the increased impact of grid power losses and security constraints. This work proposes an efficient and reliable MPC-based EMS that incorporates power-loss effects and grid-security constraints. %, while adaptively shaping the battery power profile in response to online renewable inputs, achieving reduced operational costs. It enhances system reliability, reduces operational costs, and shows strong potential for online implementation due to its reduced computational effort. Specifically, a second-order cone program (SOCP) branch flow relaxation is integrated into the constraint set, yielding a convex formulation that guarantees globally optimal solutions with high computational efficiency. Owing to the radial topology of the microgrid, this relaxation is practically tight, ensuring equivalence to the original problem. Building on this foundation, an online demand response (DR) module is designed to further reduce the operation cost through peak shaving. To the best of our knowledge, no prior MPC-EMS framework has simultaneously modeled losses and security constraints while coordinating flexible loads within a unified architecture. The developed framework enables secure operation with effective peak shaving and reduced total cost. The effectiveness of the proposed method is validated on 10-bus, 18-bus, and 33-bus systems.

Paper number 77:
Title: Electrical System Architecture for Aviation Electrification
Authors: Anoy Saha, Mona Ghassemi
Abstract: The electrification of aircraft is reshaping the foundations of aerospace design by positioning electrical systems at the center of propulsion, control, and onboard functionality. This chapter provides an overview of electrical system architectures for electric and hybrid electric aircraft, highlighting both established principles and emerging design strategies. The discussion begins with the motivations for electrification, including reducing environmental impact, improving operational efficiency, and replacing complex pneumatic and hydraulic subsystems with lighter and more reliable electrical alternatives. Aircraft electrical architectures are classified into four major categories: conventional, more electric, all electric, and hybrid electric. A range of system topologies is examined, including direct current (DC), alternating current (AC), hybrid, and distributed configurations. Each is considered in terms of its effectiveness in delivering power, enabling redundancy, supporting fault isolation, and managing thermal performance. Real world examples are presented to demonstrate practical applications, with case studies drawn from the Boeing 787 Dreamliner, the Eviation Alice commuter aircraft, and NASA X57 Maxwell demonstrator. These examples illustrate the ongoing transition from incremental subsystem electrification toward fully integrated architectures that promise higher efficiency and greater sustainability.

Paper number 78:
Title: MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling
Authors: Bi-Cheng Yan, Ming-Kang Tsai, Berlin Chen
Abstract: Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.

Paper number 79:
Title: My First Five Years of Faculty Career at the University of Delaware
Authors: Xiang-Gen Xia
Abstract: In this short article, I would like to briefly summarize my research in the first 5 years in my university academia life in USA. I think that my research results obtained in these 5 years are the best in my career, at least which I like the most by myself. I wish that my experience in my junior academia career could be of some help to young researchers.

Paper number 80:
Title: An Investigation of Incorporating Mamba for Speech Enhancement
Authors: Rong Chao, Wen-Huang Cheng, Moreno La Quatra, Sabato Marco Siniscalchi, Chao-Han Huck Yang, Szu-Wei Fu, Yu Tsao
Abstract: This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.

Paper number 81:
Title: Unified Cross-Modal Medical Image Synthesis with Hierarchical Mixture of Product-of-Experts
Authors: Reuben Dorent, Nazim Haouchine, Alexandra Golby, Sarah Frisken, Tina Kapur, William Wells
Abstract: We propose a deep mixture of multimodal hierarchical variational auto-encoders called MMHVAE that synthesizes missing images from observed images in different modalities. MMHVAE's design focuses on tackling four challenges: (i) creating a complex latent representation of multimodal data to generate high-resolution images; (ii) encouraging the variational distributions to estimate the missing information needed for cross-modal image synthesis; (iii) learning to fuse multimodal information in the context of missing data; (iv) leveraging dataset-level information to handle incomplete data sets at training time. Extensive experiments are performed on the challenging problem of pre-operative brain multi-parametric magnetic resonance and intra-operative ultrasound imaging.

Paper number 82:
Title: Image-Based Visual Servoing for Enhanced Cooperation of Dual-Arm Manipulation
Authors: Zizhe Zhang, Yuan Yang, Wenqiang Zuo, Guangming Song, Aiguo Song, Yang Shi
Abstract: The cooperation of a pair of robot manipulators is required to manipulate a target object without any fixtures. The conventional control methods coordinate the end-effector pose of each manipulator with that of the other using their kinematics and joint coordinate measurements. Yet, the manipulators' inaccurate kinematics and joint coordinate measurements can cause significant pose synchronization errors in practice. This paper thus proposes an image-based visual servoing approach for enhancing the cooperation of a dual-arm manipulation system. On top of the classical control, the visual servoing controller lets each manipulator use its carried camera to measure the image features of the other's marker and adapt its end-effector pose with the counterpart on the move. Because visual measurements are robust to kinematic errors, the proposed control can reduce the end-effector pose synchronization errors and the fluctuations of the interaction forces of the pair of manipulators on the move. Theoretical analyses have rigorously proven the stability of the closed-loop system. Comparative experiments on real robots have substantiated the effectiveness of the proposed control.

Paper number 83:
Title: DiffCom: Decoupled Sparse Priors Guided Diffusion Compression for Point Clouds
Authors: Xiaoge Zhang, Zijie Wu, Mehwish Nasim, Mingtao Feng, Saeed Anwar, Ajmal Mian
Abstract: Lossy compression relies on an autoencoder to transform a point cloud into latent points for storage, leaving the inherent redundancy of latent representations unexplored. To reduce redundancy in latent points, we propose a diffusion-based framework guided by sparse priors that achieves high reconstruction quality, especially at low bitrates. Our approach features an efficient dual-density data flow that relaxes size constraints on latent points. It hybridizes a probabilistic conditional diffusion model to encapsulate essential details for reconstruction within sparse priors, which are decoupled hierarchically into intra- and inter-point priors. Specifically, our DiffCom encodes the original point cloud into latent points and decoupled sparse priors through separate encoders. To dynamically attend to geometric and semantic cues from the priors at each encoding and decoding layer, we employ an attention-guided latent denoiser conditioned on the decoupled priors. Additionally, we integrate the local distribution into the arithmetic encoder and decoder to enhance local context modeling of the sparse points. The original point cloud is reconstructed through a point decoder. Compared to state-of-the-art methods, our approach achieves a superior rate-distortion trade-off, as evidenced by extensive evaluations on the ShapeNet dataset and standard test datasets from the MPEG PCC Group.

Paper number 84:
Title: Can We Ignore Labels In Out of Distribution Detection?
Authors: Hong Yang, Qi Yu, Travis Desell
Abstract: Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real-world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. 'label blindness', II) we define a new OOD task - Adjacent OOD detection - that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods.

Paper number 85:
Title: Offline changepoint localization using a matrix of conformal p-values
Authors: Sanjit Dandapanthula, Aaditya Ramdas
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable MCP algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We prove a novel conformal Neyman-Pearson lemma, motivating practical classifier-based choices for our conformal score function. Finally, we exemplify the MCP algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images, text, and accelerometer data.

Paper number 86:
Title: Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding for Diffusion-Based Speech Enhancement
Authors: Hao Shi, Xugang Lu, Kazuki Shimada, Tatsuya Kawahara
Abstract: Diffusion-based speech enhancement (SE) models need to incorporate correct prior knowledge as reliable conditions to generate accurate predictions. However, providing reliable conditions using noisy features is challenging. One solution is to use features enhanced by deterministic methods as conditions. However, the information distortion and loss caused by deterministic methods might affect the diffusion process. In this paper, we first investigate the effects of using different deterministic SE models as conditions for diffusion. We validate two conditions depending on whether the noisy feature was used as part of the condition: one using only the deterministic feature (deterministic-only), and the other using both deterministic and noisy features (deterministic-noisy). Preliminary investigation found that using deterministic enhanced conditions improves hearing experiences on real data, while the choice between using deterministic-only or deterministic-noisy conditions depends on the deterministic models. Based on these findings, we propose a dual-streaming encoding Repair-Diffusion Model for SE (DERDM-SE) to more effectively utilize both conditions. Moreover, we found that fine-grained deterministic models have greater potential in objective evaluation metrics, while UNet-based deterministic models provide more stable diffusion performance. Therefore, in the DERDM-SE, we propose a deterministic model that combines coarse- and fine-grained processing. Experimental results on CHiME4 show that the proposed models effectively leverage deterministic models to achieve better SE evaluation scores, along with more stable performance compared to other diffusion-based SE models.

Paper number 87:
Title: Large-Scale Training Data Attribution for Music Generative Models via Unlearning
Authors: Woosung Choi, Junghyun Koo, Kin Wai Cheuk, Joan SerrÃ , Marco A. MartÃ­nez-RamÃ­rez, Yukara Ikemiya, Naoki Murata, Yuhta Takida, Wei-Hsiang Liao, Yuki Mitsufuji
Abstract: This paper explores the use of unlearning methods for training data attribution (TDA) in music generative models trained on large-scale datasets. TDA aims to identify which specific training data points contributed the most to the generation of a particular output from a specific model. This is crucial in the context of AI-generated music, where proper recognition and credit for original artists are generally overlooked. By enabling white-box attribution, our work supports a fairer system for acknowledging artistic contributions and addresses pressing concerns related to AI ethics and copyright. We apply unlearning-based attribution to a text-to-music diffusion model trained on a large-scale dataset and investigate its feasibility and behavior in this setting. To validate the method, we perform a grid search over different hyperparameter configurations and quantitatively evaluate the consistency of the unlearning approach. We then compare attribution patterns from unlearning with non-counterfactual approaches. Our findings suggest that unlearning-based approaches can be effectively adapted to music generative models, introducing large-scale TDA to this domain and paving the way for more ethical and accountable AI systems for music creation.

Paper number 88:
Title: Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance
Authors: Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji
Abstract: We propose a step-by-step video-to-audio (V2A) generation method for finer controllability over the generation process and more realistic audio synthesis. Inspired by traditional Foley workflows, our approach aims to comprehensively capture all sound events induced by a video through the incremental generation of missing sound events. To avoid the need for costly multi-reference video-audio datasets, each generation step is formulated as a negatively guided V2A process that discourages duplication of existing sounds. The guidance model is trained by finetuning a pre-trained V2A model on audio pairs from adjacent segments of the same video, allowing training with standard single-reference audiovisual datasets that are easily accessible. Objective and subjective evaluations demonstrate that our method enhances the separability of generated sounds at each step and improves the overall quality of the final composite audio, outperforming existing baselines.

Paper number 89:
Title: Electromagnetic Inverse Scattering from a Single Transmitter
Authors: Yizhe Cheng, Chunxun Tian, Haoru Wang, Wentao Zhu, Xiaoxuan Ma, Yizhou Wang
Abstract: Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in applications such as medical imaging, where the goal is to reconstruct the relative permittivity from scattered electromagnetic field. This inverse process is inherently ill-posed and highly nonlinear, making it particularly challenging, especially under sparse transmitter setups, e.g., with only one transmitter. A recent machine learning-based approach, Img-Interiors, shows promising results by leveraging continuous implicit functions. However, it requires time-consuming case-specific optimization and fails under sparse transmitter setups. To address these limitations, we revisit EISP from a data-driven perspective. The scarcity of transmitters leads to an insufficient amount of measured data, which fails to capture adequate physical information for stable inversion. Built on this insight, we propose a fully end-to-end and data-driven framework that predicts the relative permittivity of scatterers from measured fields, leveraging data distribution priors to compensate for the lack of physical information. This design enables data-driven training and feed-forward prediction of relative permittivity while maintaining strong robustness to transmitter sparsity. Extensive experiments show that our method outperforms state-of-the-art approaches in reconstruction accuracy and robustness. Notably, it achieves high-quality results even with a single transmitter, a setting where previous methods consistently fail. This work offers a fundamentally new perspective on electromagnetic inverse scattering and represents a major step toward cost-effective practical solutions for electromagnetic imaging.

Paper number 90:
Title: Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies
Authors: Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sepehr Karimi, Sina Rashidi, Ali Zolnour, Maryam Dadkhah, Yasaman Haghbin, Hossein AzadMaleki, Maryam Zolnoori
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.

Paper number 91:
Title: Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection
Authors: Rami Zewail
Abstract: In an attempt to address the need for skilled clinicians in heart sound interpretation, recent research efforts on automating cardiac auscultation have explored deep learning approaches. The majority of these approaches have been based on supervised learning that is always challenged in occasions where training data is limited. More recently, there has been a growing interest in potentials of pre-trained self-supervised audio foundation models for biomedical end tasks. Despite exhibiting promising results, these foundational models are typically computationally intensive. Within the context of automatic cardiac auscultation, this study explores a lightweight alternative to these general-purpose audio foundation models by introducing the Scattering Transformer, a novel, training-free transformer architecture for heart murmur detection. The proposed method leverages standard wavelet scattering networks by introducing contextual dependencies in a transformer-like architecture without any backpropagation. We evaluate our approach on the public CirCor DigiScope dataset, directly comparing it against leading general-purpose foundational models. The Scattering Transformer achieves a Weighted Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697, demonstrating performance highly competitive with contemporary state of the art methods. This study establishes the Scattering Transformer as a viable and promising alternative in resource-constrained setups.

Paper number 92:
Title: Synthetic Audio Forensics Evaluation (SAFE) Challenge
Authors: Kirill Trapeznikov, Paul Cummer, Pranay Pherwani, Jai Aslam, Michael S. Davinroy, Peter Bautista, Laura Cassani, Matthew Stamm, Jill Crisman
Abstract: The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at \href{this https URL}{this https URL}.
    