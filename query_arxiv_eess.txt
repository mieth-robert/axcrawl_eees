
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: CoRPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models
Authors: Amy Rafferty, Rishi Ramaesh, Ajitha Rajan
Abstract: Deep learning models for medical image classification tasks are becoming widely implemented in AI-assisted diagnostic tools, aiming to enhance diagnostic accuracy, reduce clinician workloads, and improve patient outcomes. However, their vulnerability to adversarial attacks poses significant risks to patient safety. Current attack methodologies use general techniques such as model querying or pixel value perturbations to generate adversarial examples designed to fool a model. These approaches may not adequately address the unique characteristics of clinical errors stemming from missed or incorrectly identified clinical features. We propose the Concept-based Report Perturbation Attack (CoRPA), a clinically-focused black-box adversarial attack framework tailored to the medical imaging domain. CoRPA leverages clinical concepts to generate adversarial radiological reports and images that closely mirror realistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA using the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our evaluation reveals that deep learning models exhibiting strong resilience to conventional adversarial attacks are significantly less robust when subjected to CoRPA's clinically-focused perturbations. This underscores the importance of addressing domain-specific vulnerabilities in medical AI systems. By introducing a specialized adversarial attack framework, this study provides a foundation for developing robust, real-world-ready AI models in healthcare, ensuring their safe and reliable deployment in high-stakes clinical environments.

Paper number 2:
Title: Gait Disorder Assessment Based on a Large-Scale Clinical Trial: WiFi vs. Video vs. Doctor's Visual Inspection
Authors: Alireza Parsay, Mert Torun, Philip R. Delio, Yasamin Mostofi
Abstract: Neurological gait disorders can affect a large population, with devastating consequences on the quality of life. In this paper, we are interested in fundamentally understanding the potential of emerging sensing modalities (e.g., WiFi) for neurological gait disorder assessment. Towards this goal, we conduct a one-year-long clinical trial in collaboration with the Neurology Associates of Santa Barbara, the results of which are detailed in this paper. More specifically, our medical campaign encompasses 114 real subjects and a wide spectrum of disorders (e.g., Parkinson's, Neuropathy, Post Stroke, Dementia, and Arthritis). We then develop the first WiFi-based gait disorder sensing system of its kind, distinguished by its scope of validation with a large and diverse patient cohort. To ensure generalizability, we mainly leverage publicly-accessible online videos of gait disorders for training, and develop a video-to-RF pipeline to convert them to synthetic RF data. We then extensively test the system in a neurology center (i.e., the Neurology Associates of Santa Barbara). In addition, we provide a 1-1 comparison with a vision-based system, by developing a vision-based gait disorder assessment system under the same exact conditions (e.g., same subjects, etc), a comparison crucial for developing smart health systems and the first of its kind to our knowledge. We finally contrast both systems with the accuracy of neurologists when basing evaluation solely on the visual inspection of the gait, by designing a large survey and distributing it to an extensive network of neurologists, thus offering the first such apples-to-apples comparison of these three sensing modalities. Overall, the findings can shape the integration of new sensing modalities into medical practice, and pave the way toward equitable healthcare.

Paper number 3:
Title: Multi-Class Segmentation of Aortic Branches and Zones in Computed Tomography Angiography: The AortaSeg24 Challenge
Authors: Muhammad Imran, Jonathan R. Krebs, Vishal Balaji Sivaraman, Teng Zhang, Amarjeet Kumar, Walker R. Ueland, Michael J. Fassler, Jinlong Huang, Xiao Sun, Lisheng Wang, Pengcheng Shi, Maximilian Rokuss, Michael Baumgartner, Yannick Kirchhof, Klaus H. Maier-Hein, Fabian Isensee, Shuolin Liu, Bing Han, Bong Thanh Nguyen, Dong-jin Shin, Park Ji-Woo, Mathew Choi, Kwang-Hyun Uhm, Sung-Jea Ko, Chanwoong Lee, Jaehee Chun, Jin Sung Kim, Minghui Zhang, Hanxiao Zhang, Xin You, Yun Gu, Zhaohong Pan, Xuan Liu, Xiaokun Liang, Markus Tiefenthaler, Enrique Almar-Munoz, Matthias Schwab, Mikhail Kotyushev, Rostislav Epifanov, Marek Wodzinski, Henning Muller, Abdul Qayyum, Moona Mazher, Steven A. Niederer, Zhiwei Wang, Kaixiang Yang, Jintao Ren, Stine Sofia Korreman, Yuchong Gao, Hongye Zeng, Haoyu Zheng, Rui Zheng, Jinghua Yue, Fugen Zhou, Bo Liu, Alexander Cosman, Muxuan Liang, Chang Zhao, Gilbert R. Upchurch Jr., Jun Ma, Yuyin Zhou, Michol A. Cooper, Wei Shao
Abstract: Multi-class segmentation of the aorta in computed tomography angiography (CTA) scans is essential for diagnosing and planning complex endovascular treatments for patients with aortic dissections. However, existing methods reduce aortic segmentation to a binary problem, limiting their ability to measure diameters across different branches and zones. Furthermore, no open-source dataset is currently available to support the development of multi-class aortic segmentation methods. To address this gap, we organized the AortaSeg24 MICCAI Challenge, introducing the first dataset of 100 CTA volumes annotated for 23 clinically relevant aortic branches and zones. This dataset was designed to facilitate both model development and validation. The challenge attracted 121 teams worldwide, with participants leveraging state-of-the-art frameworks such as nnU-Net and exploring novel techniques, including cascaded models, data augmentation strategies, and custom loss functions. We evaluated the submitted algorithms using the Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD), highlighting the approaches adopted by the top five performing teams. This paper presents the challenge design, dataset details, evaluation metrics, and an in-depth analysis of the top-performing algorithms. The annotated dataset, evaluation code, and implementations of the leading methods are publicly available to support further research. All resources can be accessed at this https URL.

Paper number 4:
Title: Distillation and Pruning for Scalable Self-Supervised Representation-Based Speech Quality Assessment
Authors: Benjamin Stahl, Hannes Gamper
Abstract: In this paper, we investigate distillation and pruning methods to reduce model size for non-intrusive speech quality assessment based on self-supervised representations. Our experiments build on XLS-R-SQA, a speech quality assessment model using wav2vec 2.0 XLS-R embeddings. We retrain this model on a large compilation of mean opinion score datasets, encompassing over 100,000 labeled clips. For distillation, using this model as a teacher, we generate pseudo-labels on unlabeled degraded speech signals and train student models of varying sizes. For pruning, we use a data-driven strategy. While data-driven pruning performs better at larger model sizes, distillation on unlabeled data is more effective for smaller model sizes. Distillation can halve the gap between the baseline's correlation with ground-truth MOS labels and that of the XLS-R-based teacher model, while reducing model size by two orders of magnitude compared to the teacher model.

Paper number 5:
Title: Low Dimensional Koopman Generalized Eigenfunctions Representation: An Approach to Address Koopman High-Dimensionality Problem
Authors: Simone Martini, Margareta Stefanovic, Kimon P. Valavanis
Abstract: This Paper introduces a methodology to achieve a lower dimensional Koopman quasi linear representation of nonlinear dynamics using Koopman generalized eigenfunctions. The methodology is presented for the analytically derived Koopman formulation of rigid body dynamics but can be generalized to any data-driven or analytically derived generalized eigenfunction set. The presented approach aim at achieving a representation for which the number of Koopman observables matches the number of input leading to an exact linearization solution instead of resorting to the least square approximation method. The methodology is tested by designing a linear quadratic (LQ) flight controller of a quadrotor unmanned aerial vehicle (UAV). Hardware in the loop simulations validate the applicability of this approach to real-time implementation in presence of noise and sensor delays.

Paper number 6:
Title: High-Dimensional Sequential Change Detection
Authors: Robert Malinas, Dogyoon Song, Benjamin D. Robinson, Alfred O. Hero III
Abstract: We address the problem of detecting a change in the distribution of a high-dimensional multivariate normal time series. Assuming that the post-change parameters are unknown and estimated using a window of historical data, we extend the framework of quickest change detection (QCD) to the highdimensional setting in which the number of variables increases proportionally with the size of the window used to estimate the post-change parameters. Our analysis reveals that an information theoretic quantity, which we call the Normalized High- Dimensional Kullback-Leibler divergence (NHDKL), governs the high-dimensional asymptotic performance of QCD procedures. Specifically, we show that the detection delay is asymptotically inversely proportional to the difference between the NHDKL of the true post-change versus pre-change distributions and the NHDKL of the true versus estimated post-change distributions. In cases of perfect estimation, where the latter NHDKL is zero, the delay is inversely proportional to the NHDKL between the post-change and pre-change distributions alone. Thus, our analysis is a direct generalization of the traditional fixed-dimension, large-sample asymptotic framework, where the standard KL divergence is asymptotically inversely proportional to detection delay. Finally, we identify parameter estimators that asymptotically minimize the NHDKL between the true versus estimated post-change distributions, resulting in a QCD method that is guaranteed to outperform standard approaches based on fixed-dimension asymptotics.

Paper number 7:
Title: A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation
Authors: Canxuan Gang
Abstract: Segmentation of 3D medical images is a critical task for accurate diagnosis and treatment planning. Convolutional neural networks (CNNs) have dominated the field, achieving significant success in 3D medical image segmentation. However, CNNs struggle with capturing long-range dependencies and global context, limiting their performance, particularly for fine and complex structures. Recent transformer-based models, such as TransUNet and nnFormer, have demonstrated promise in addressing these limitations, though they still rely on hybrid CNN-transformer architectures. This paper introduces a novel, fully convolutional-free model based on transformer architecture and self-attention mechanisms for 3D medical image segmentation. Our approach focuses on improving multi-semantic segmentation accuracy and addressing domain adaptation challenges between thick and thin slice CT images. We propose a joint loss function that facilitates effective segmentation of thin slices based on thick slice annotations, overcoming limitations in dataset availability. Furthermore, we present a benchmark dataset for multi-semantic segmentation on thin slices, addressing a gap in current medical imaging research. Our experiments demonstrate the superiority of the proposed model over traditional and hybrid architectures, offering new insights into the future of convolution-free medical image segmentation.

Paper number 8:
Title: Aero-engines Anomaly Detection using an Unsupervised Fisher Autoencoder
Authors: Saba Sanami, Amir G. Aghdam
Abstract: Reliable aero-engine anomaly detection is crucial for ensuring aircraft safety and operational efficiency. This research explores the application of the Fisher autoencoder as an unsupervised deep learning method for detecting anomalies in aero-engine multivariate sensor data, using a Gaussian mixture as the prior distribution of the latent space. The proposed method aims to minimize the Fisher divergence between the true and the modeled data distribution in order to train an autoencoder that can capture the normal patterns of aero-engine behavior. The Fisher divergence is robust to model uncertainty, meaning it can handle noisy or incomplete data. The Fisher autoencoder also has well-defined latent space regions, which makes it more generalizable and regularized for various types of aero-engines as well as facilitates diagnostic purposes. The proposed approach improves the accuracy of anomaly detection and reduces false alarms. Simulations using the CMAPSS dataset demonstrate the model's efficacy in achieving timely anomaly detection, even in the case of an unbalanced dataset.

Paper number 9:
Title: Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning
Authors: Manh Luong, Khai Nguyen, Dinh Phung, Gholamreza Haffari, Lizhen Qu
Abstract: Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of $\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy.

Paper number 10:
Title: Diverse Image Generation with Diffusion Models and Cross Class Label Learning for Polyp Classification
Authors: Vanshali Sharma, Debesh Jha, M.K. Bhuyan, Pradip K. Das, Ulas Bagci
Abstract: Pathologic diagnosis is a critical phase in deciding the optimal treatment procedure for dealing with colorectal cancer (CRC). Colonic polyps, precursors to CRC, can pathologically be classified into two major types: adenomatous and hyperplastic. For precise classification and early diagnosis of such polyps, the medical procedure of colonoscopy has been widely adopted paired with various imaging techniques, including narrow band imaging and white light imaging. However, the existing classification techniques mainly rely on a single imaging modality and show limited performance due to data scarcity. Recently, generative artificial intelligence has been gaining prominence in overcoming such issues. Additionally, various generation-controlling mechanisms using text prompts and images have been introduced to obtain visually appealing and desired outcomes. However, such mechanisms require class labels to make the model respond efficiently to the provided control input. In the colonoscopy domain, such controlling mechanisms are rarely explored; specifically, the text prompt is a completely uninvestigated area. Moreover, the unavailability of expensive class-wise labels for diverse sets of images limits such explorations. Therefore, we develop a novel model, PathoPolyp-Diff, that generates text-controlled synthetic images with diverse characteristics in terms of pathology, imaging modalities, and quality. We introduce cross-class label learning to make the model learn features from other classes, reducing the burdensome task of data annotation. The experimental results report an improvement of up to 7.91% in balanced accuracy using a publicly available dataset. Moreover, cross-class label learning achieves a statistically significant improvement of up to 18.33% in balanced accuracy during video-level analysis. The code is available at this https URL.

Paper number 11:
Title: Unsupervised Self-Prior Embedding Neural Representation for Iterative Sparse-View CT Reconstruction
Authors: Xuanyu Tian, Lixuan Chen, Qing Wu, Chenhe Du, Jingjing Shi, Hongjiang Wei, Yuyao Zhang
Abstract: Emerging unsupervised implicit neural representation (INR) methods, such as NeRP, NeAT, and SCOPE, have shown great potential to address sparse-view computed tomography (SVCT) inverse problems. Although these INR-based methods perform well in relatively dense SVCT reconstructions, they struggle to achieve comparable performance to supervised methods in sparser SVCT scenarios. They are prone to being affected by noise, limiting their applicability in real clinical settings. Additionally, current methods have not fully explored the use of image domain priors for solving SVCsT inverse problems. In this work, we demonstrate that imperfect reconstruction results can provide effective image domain priors for INRs to enhance performance. To leverage this, we introduce Self-prior embedding neural representation (Spener), a novel unsupervised method for SVCT reconstruction that integrates iterative reconstruction algorithms. During each iteration, Spener extracts local image prior features from the previous iteration and embeds them to constrain the solution space. Experimental results on multiple CT datasets show that our unsupervised Spener method achieves performance comparable to supervised state-of-the-art (SOTA) methods on in-domain data while outperforming them on out-of-domain datasets. Moreover, Spener significantly improves the performance of INR-based methods in handling SVCT with noisy sinograms. Our code is available at this https URL.

Paper number 12:
Title: Graph Neural Network Enabled Pinching Antennas
Authors: Xinke Xie, Yang Lu, Zhiguo Ding
Abstract: The pinching-antenna system is a novel flexible-antenna technology, which has the capabilities not only to combat large-scale path loss, but also to reconfigure the antenna array in a flexible manner. The key idea of pinching antennas is to apply small dielectric particles on a waveguide of arbitrary length, so that they can be positioned close to users to avoid significant large-scale path loss. This paper investigates the graph neural network (GNN) enabled transmit design for the joint optimization of antenna placement and power allocation in pinching-antenna systems. We formulate the downlink communication system equipped with pinching antennas as a bipartite graph, and propose a graph attention network (GAT) based model, termed bipartite GAT (BGAT), to solve an energy efficiency (EE) maximization problem. With the tailored readout processes, the BGAT guarantees a feasible solution, which also facilitates the unsupervised training. Numerical results demonstrate the effectiveness of pinching antennas in enhancing the system EE as well as the proposed BGAT in terms of optimality, scalability and computational efficiency.

Paper number 13:
Title: Distributionally Robust Model Predictive Control with Mixture of Gaussian Processes
Authors: Jingyi Wu, Chao Ning
Abstract: Despite the success of Gaussian process based Model Predictive Control (MPC) in robotic control, its applicability scope is greatly hindered by multimodal disturbances that are prevalent in real-world settings. Here we propose a novel Mixture of Gaussian Processes based Distributionally Robust MPC (MoGP-DR-MPC) framework for linear time invariant systems subject to potentially multimodal state-dependent disturbances. This framework utilizes MoGP to automatically determine the number of modes from disturbance data. Using the mean and variance information provided by each mode-specific predictive distribution, it constructs a data-driven state-dependent ambiguity set, which allows for flexible and fine-grained disturbance modeling. Based on this ambiguity set, we impose Distributionally Robust Conditional Value-at Risk (DR-CVaR) constraints to effectively achieve distributional robustness against errors in the predictive distributions. To address the computational challenge posed by these constraints in the resulting MPC problem, we equivalently reformulate the DR-CVaR constraints into tractable second-order cone constraints. Furthermore, we provide theoretical guarantees on the recursive feasibility and stability of the proposed framework. The enhanced control performance of MoGP-DR-MPC is validated through both numerical experiments and simulations on a quadrotor system, demonstrating notable reductions in closed-loop cost by 17% and 4% respectively compared against Gaussian process based MPC.

Paper number 14:
Title: Prescribed-Time Newton Extremum Seeking using Delays and Time-Periodic Gains
Authors: Nicolas Espitia, Miroslav Krstic, Jorge I. Poveda
Abstract: We study prescribed-time extremum seeking (ES) for scalar maps in the presence of time delay. The problem has been solved by Yilmaz and Krstic using chirpy probing and time-varying singular gains. To alleviate the gain singularity, we present an alternative approach, employing delays with bounded time-periodic gains, for achieving prescribed-time convergence to the extremum. Our results are not extensions or refinements but a new methodological direction, even in the absence of the delay on the map. The main result we present compensates the map's delay and uses perturbation-based and the Newton (rather than gradient) approaches. The simultaneous presence of perturbation period, and two delays -- a map delay and a seeking feedback delay -- whose values are different (feedback delay must be longer than map delay), makes for an intricate situation in the design and analysis. ES can settle arbitrarily soon after four times the map delay. In the absence of a map delay, the settling time is arbitrarily short, with feedback delay chosen as one quarter of the prescribed settling time, i.e., the search settles after four times any positive feedback delay. In addition to removing the gain singularity of the Yilmaz-Krstic singular-gain prescribed-time ES, we go beyond that method's limitation to operating only up to the terminal time. With the help of averaging theorems in infinite dimension, we conduct a prescribed-time convergence analysis on a suitable perturbation-averaged \textit{target} ES system, which contains the time-periodic gains of the map and feedback delays. Since the notion of ``dead-beat'' Lyapunov stabilization by time-periodic delayed feedback originates from Hale and Verduyn-Lunel (analysis, 1993) and Karafyllis (feedback design, 2006), we refer to our approach to prescribed-time ES as the ``Karafyllis, Hale, Verduyn-Lunel" (KHV) PT-ES approach.

Paper number 15:
Title: Data-Driven Neural Certificate Synthesis
Authors: Luke Rickard, Alessandro Abate, Kostas Margellos
Abstract: We investigate the problem of verifying different properties of discrete time dynamical systems, namely, reachability, safety and reach-while-avoid. To achieve this, we adopt a data driven perspective and using past systems' trajectories as data, we aim at learning a specific function termed \emph{certificate} for each property we wish to verify. The certificate construction problem is treated as a safety informed neural network training process, where we use a neural network to learn the parameterization of each certificate, while the loss function we seek to minimize is designed to encompass conditions on the certificate to be learned that encode the satisfaction of the associated property. Besides learning a certificate, we quantify probabilistically its generalization properties, namely, how likely it is for a certificate to be valid (and hence for the associated property to be satisfied) when it comes to a new system trajectory not included in the training data set. We view this problem under the realm of probably approximately correct (PAC) learning under the notion of compression, and use recent advancements of the so-called scenario approach to obtain scalable generalization bounds on the learned certificates. To achieve this, we design a novel algorithm that minimizes the loss function and hence constructs a certificate, and at the same time determines a quantity termed compression, which is instrumental in obtaining meaningful probabilistic guarantees. This process is novel per se and provides a constructive mechanism for compression set calculation, thus opening the road for its use to more general non-convex optimization problems. We verify the efficacy of our methodology on several numerical case studies, and compare it (both theoretically and numerically) with closely related results on data-driven property verification.

Paper number 16:
Title: Channel Estimation for RIS-Aided MU-MIMO mmWave Systems with Practical Hybrid Architecture
Authors: Liuchang Zhuo, Cunhua Pan, Hong Ren, Ruisong Weng, Shi Jin, A. Lee Swindlehurst, Jiangzhou Wang
Abstract: This paper proposes a correlation-based three-stage channel estimation strategy with low pilot overhead for reconfigurable intelligent surface (RIS)-aided millimeter wave (mmWave) multi-user (MU) MIMO systems, in which both users and base station (BS) are equipped with a hybrid RF architecture. In Stage I, all users jointly transmit pilots and recover the uncompressed received signals to estimate the angle of arrival (AoA) at the BS using the discrete Fourier transform (DFT). Based on the observation that the overall cascaded MIMO channel can be decomposed into multiple sub-channels, the cascaded channel for a typical user is estimated in Stage II. Specifically, using the invariance of angles and the linear correlation of gains related to different cascaded subchannels, we use compressive sensing (CS), least squares (LS), and a one-dimensional search to estimate the Angles of Departure (AoDs), based on which the overall cascaded channel is obtained. In Stage III, the remaining users independently transmit pilots to estimate their individual cascaded channel with the same approach as in Stage II, which exploits the equivalent common RIS-BS channel obtained in Stage II to reduce the pilot overhead. In addition, the hybrid combining matrix and the RIS phase shift matrix are designed to reduce the noise power, thereby further improving the estimation performance. Simulation results demonstrate that the proposed algorithm can achieve high estimation accuracy especially when the number of antennas at the users is small, and reduce pilot overhead by more than five times compared with the existing benchmark approach.

Paper number 17:
Title: Efficient Sampling Allocation Strategies for General Graph-Filter-Based Signal Recovery
Authors: Lital Dabush, Tirza Routtenberg
Abstract: Sensor placement plays a crucial role in graph signal recovery in underdetermined systems. In this paper, we present the graph-filtered regularized maximum likelihood (GFR-ML) estimator of graph signals, which integrates general graph filtering with regularization to enhance signal recovery performance under a limited number of sensors. Then, we investigate task-based sampling allocation aimed at minimizing the mean squared error (MSE) of the GFR-ML estimator by wisely choosing sensor placement. Since this MSE depends on the unknown graph signals to be estimated, we propose four cost functions for the optimization of the sampling allocation: the biased Cram$\acute{\text{e}}$r-Rao bound (bCRB), the worst-case MSE (WC-MSE), the Bayesian MSE (BMSE), and the worst-case BMSE (WC-BMSE), where the last two assume a Gaussian prior. We investigate the properties of these cost functions and develop two algorithms for their practical implementation: 1) the straightforward greedy algorithm; and 2) the alternating projection gradient descent (PGD) algorithm that reduces the computational complexity. Simulation results on synthetic and real-world datasets of the IEEE 118-bus power system and the Minnesota road network demonstrate that the proposed sampling allocation methods reduce the MSE by up to $50\%$ compared to the common sampling methods A-design, E-design, and LR-design in the tested scenarios. Thus, the proposed methods improve the estimation performance and reduce the required number of measurements in graph signal processing (GSP)-based signal recovery in the case of underdetermined systems.

Paper number 18:
Title: NOMANet: A Graph Neural Network Enabled Power Allocation Scheme for NOMA
Authors: Yipu Hou, Yang Lu, Wei Chen, Bo Ai, Dusit Niyato, Zhiguo Ding
Abstract: This paper proposes a graph neural network (GNN) enabled power allocation scheme for non-orthogonal multiple access (NOMA) networks. In particular, a downlink scenario with one base station serving multiple users over several subchannels is considered, where the number of subchannels is less than the number of users, and thus, some users have to share a subchannel via NOMA. Our goal is to maximize the system energy efficiency subject to the rate requirement of each user and the overall budget. We propose a deep learning based approach termed NOMA net (NOMANet) to address the considered problem. Particularly, NOMANet is GNN-based, which maps channel state information to the desired power allocation scheme for all subchannels. The multi-head attention and the residual/dense connection are adopted to enhance the feature extraction. The output of NOMANet is guaranteed to be feasible via the customized activation function and the penalty method. Numerical results show that NOMANet trained unsupervised achieves performance close to that of the successive convex approximation method but with a faster inference speed by about $700$ times. Besides, NOMANet is featured by its scalability to both users and subchannels.

Paper number 19:
Title: Design of optimal repetitive control based on EID estimator with adaptive periodic event-triggered mechanism for linear systems subjected to exogenous disturbances
Authors: Mohammed Soliman, Abdul-Wahid A. Saif
Abstract: The periodic signal tracking and the unknown disturbance rejection under limited communication resources are main important issues in many physical systems and practical applications. The control of such systems has some challenges such as time-varying delay, unknown external disturbances, structure uncertainty, and the heavy communication burden on the sensors and controller. These challenges affect the system performance and may destabilize the system. Hence, in this article, an improved scheme has been designed to overcome these challenges to achieve a good control performance based on optimization technique, and to guarantee the closed-loop system stability. The proposed scheme can be described as: modified repetitive control (MRC) with equivalent-input-disturbance (EID) estimator based on adaptive periodic event-triggered mechanism (APETM). The scheme that has been created is intended for linear systems that experience external disturbances which are not known, and must operate within constraints on communication resources. MRC based on EID has been developed with the goal of achieving periodic reference tracking and enhancing the ability to effectively reject both periodic and aperiodic unknown disturbances. In addition, utilizing APETM to reduce data transmission, computational burden and to save communication resources. Additionally, an optimization method is employed to fine-tune the parameters of the controller, enabling adjustments to the control and learning actions. Overall architecture of the system, incorporating the APETM-MRC with the utilization of an EID estimator and optimal techniques, can be described as a time-varying delay system. Proposed schemes were demonstrated to be effective, feasible, and robust through simulated application.

Paper number 20:
Title: Evaluating the Techno-Economic Viability of a Solar PV-Wind Turbine Hybrid System with Battery Storage for an Electric Vehicle Charging Station in Khobar, Saudi Arabia
Authors: Ahmed S. AbdElrazek, Mohamed Soliman, Muhammad Khalid
Abstract: The main aim of this investigation is to replicate and enhance a sustainable hybrid energy structure that combines solar photovoltaic, wind turbines, battery storage. The study employs the Homer simulation model to evaluate the scaling, cost, and control strategy of this hybrid power system. This work primarily focuses on determining the most efficient design for a renewable energy generation system architecture for a significant electric vehicle charging station. The hybrid power system is designed to meet an AC base load of 2424.25 kWh/day with peak consumption of 444 kW. The simulation results indicate that the optimized components and the cost of energy are at an optimal level and the optimal design in terms of renewable energy penetration.

Paper number 21:
Title: Less is More for Synthetic Speech Detection in the Wild
Authors: Ashi Garg, Zexin Cai, Henry Li Xinyuan, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews
Abstract: Driven by advances in self-supervised learning for speech, state-of-the-art synthetic speech detectors have achieved low error rates on popular benchmarks such as ASVspoof. However, prior benchmarks do not address the wide range of real-world variability in speech. Are reported error rates realistic in real-world conditions? To assess detector failure modes and robustness under controlled distribution shifts, we introduce ShiftySpeech, a benchmark with more than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12 vocoders, and 3 languages. We found that all distribution shifts degraded model performance, and contrary to prior findings, training on more vocoders, speakers, or with data augmentation did not guarantee better generalization. In fact, we found that training on less diverse data resulted in better generalization, and that a detector fit using samples from a single carefully selected vocoder and a single speaker achieved state-of-the-art results on the challenging In-the-Wild benchmark.

Paper number 22:
Title: Consistent sampling of Paley-Wiener functions on graphons
Authors: Hartmut Führ, Mahya Ghandehari
Abstract: We study sampling methods for Paley-Wiener functions on graphons, thereby adapting and generalizing methods initially developed for graphs to the graphon setting. We then derive conditions under which such a sampling estimate is consistent with graphon convergence.

Paper number 23:
Title: Graph Neural Networks for Efficient AC Power Flow Prediction in Power Grids
Authors: Seyedamirhossein Talebi, Kaixiong Zhou
Abstract: This paper proposes a novel approach using Graph Neural Networks (GNNs) to solve the AC Power Flow problem in power grids. AC OPF is essential for minimizing generation costs while meeting the operational constraints of the grid. Traditional solvers struggle with scalability, especially in large systems with renewable energy sources. Our approach models the power grid as a graph, where buses are nodes and transmission lines are edges. We explore different GNN architectures, including GCN, GAT, SAGEConv, and GraphConv to predict AC power flow solutions efficiently. Our experiments on IEEE test systems show that GNNs can accurately predict power flow solutions and scale to larger systems, outperforming traditional solvers in terms of computation time. This work highlights the potential of GNNs for real-time power grid management, with future plans to apply the model to even larger grid systems.

Paper number 24:
Title: 4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis
Authors: An Zhao, Moucheng Xu, Ahmed H. Shahin, Wim Wuyts, Mark G. Jones, Joseph Jacob, Daniel C. Alexander
Abstract: Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.

Paper number 25:
Title: Towards Autonomous Experimentation: Bayesian Optimization over Problem Formulation Space for Accelerated Alloy Development
Authors: Danial Khatamsaz, Joseph Wagner, Brent Vela, Raymundo Arroyave, Douglas L. Allaire
Abstract: Accelerated discovery in materials science demands autonomous systems capable of dynamically formulating and solving design problems. In this work, we introduce a novel framework that leverages Bayesian optimization over a problem formulation space to identify optimal design formulations in line with decision-maker preferences. By mapping various design scenarios to a multi attribute utility function, our approach enables the system to balance conflicting objectives such as ductility, yield strength, density, and solidification range without requiring an exact problem definition at the outset. We demonstrate the efficacy of our method through an in silico case study on a Mo-Nb-Ti-V-W alloy system targeted for gas turbine engine blade applications. The framework converges on a sweet spot that satisfies critical performance thresholds, illustrating that integrating problem formulation discovery into the autonomous design loop can significantly streamline the experimental process. Future work will incorporate human feedback to further enhance the adaptability of the system in real-world experimental settings.

Paper number 26:
Title: Target Speaker Lipreading by Audio-Visual Self-Distillation Pretraining and Speaker Adaptation
Authors: Jing-Xuan Zhang, Tingzhi Mao, Longjiang Guo, Jin Li, Lichen Zhang
Abstract: Lipreading is an important technique for facilitating human-computer interaction in noisy environments. Our previously developed self-supervised learning method, AV2vec, which leverages multimodal self-distillation, has demonstrated promising performance in speaker-independent lipreading on the English LRS3 dataset. However, AV2vec faces challenges such as high training costs and a potential scarcity of audio-visual data for lipreading in languages other than English, such as Chinese. Additionally, most studies concentrate on speakerindependent lipreading models, which struggle to account for the substantial variation in speaking styles across di?erent speakers. To address these issues, we propose a comprehensive approach. First, we investigate cross-lingual transfer learning, adapting a pre-trained AV2vec model from a source language and optimizing it for the lipreading task in a target language. Second, we enhance the accuracy of lipreading for specific target speakers through a speaker adaptation strategy, which is not extensively explored in previous research. Third, after analyzing the complementary performance of lipreading with lip region-of-interest (ROI) and face inputs, we introduce a model ensembling strategy that integrates both, signi?cantly boosting model performance. Our method achieved a character error rate (CER) of 77.3% on the evaluation set of the ChatCLR dataset, which is lower than the top result from the 2024 Chat-scenario Chinese Lipreading Challenge.

Paper number 27:
Title: Non-invasive electromyographic speech neuroprosthesis: a geometric perspective
Authors: Harshavardhana T. Gowda, Ferdous Rahimi, Lee M. Miller
Abstract: In this article, we present a high-bandwidth egocentric neuromuscular speech interface for translating silently voiced speech articulations into textand audio. Specifically, we collect electromyogram (EMG) signals from multiple articulatorysites on the face and neck as individuals articulate speech in an alaryngeal manner to perform EMG-to-text or EMG-to-audio translation. Such an interface is useful for restoring audible speech in individuals who have lost the ability to speak intelligibly due to laryngectomy, neuromuscular disease, stroke, or trauma-induced damage (e.g., radiotherapy toxicity) to speech articulators. Previous works have focused on training text or speech synthesis models using EMG collected during audible speech articulations or by transferring audio targets from EMG collected during audible articulation to EMG collected during silent articulation. However, such paradigms are not suited for individuals who have already lost the ability to audibly articulate speech. We are the first to present an alignment-free EMG-to-text and EMG-to-audio conversion using only EMG collected during silently articulated speech in an open-sourced manner. On a limited vocabulary corpora, our approach achieves almost 2.4x improvement in word error rate with a model that is 25x smaller by leveraging the inherent geometry of EMG.

Paper number 28:
Title: Audio-Visual Representation Learning via Knowledge Distillation from Speech Foundation Models
Authors: Jing-Xuan Zhang, Genshun Wan, Jianqing Gao, Zhen-Hua Ling
Abstract: Audio-visual representation learning is crucial for advancing multimodal speech processing tasks, such as lipreading and audio-visual speech recognition. Recently, speech foundation models (SFMs) have shown remarkable generalization capabilities across various speech-related tasks. Building on this progress, we propose an audio-visual representation learning model that leverages cross-modal knowledge distillation from SFMs. In our method, SFMs serve as teachers, from which multi-layer hidden representations are extracted using clean audio inputs. We also introduce a multi-teacher ensemble method to distill the student, which receives audio-visual data as inputs. A novel representational knowledge distillation loss is employed to train the student during pretraining, which is also applied during finetuning to further enhance the performance on downstream tasks. Our experiments utilized both a self-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results demonstrated that our proposed method achieved superior or at least comparable performance to previous state-of-the-art baselines across automatic speech recognition, visual speech recognition, and audio-visual speech recognition tasks. Additionally, comprehensive ablation studies and the visualization of learned representations were conducted to evaluate the effectiveness of our proposed method.

Paper number 29:
Title: Cooperative Optimization of Grid-Edge Cyber and Physical Resources for Resilient Power System Operation
Authors: Xiang Huo, Shining Sun, Khandaker Akramul Haque, Leen Al Homoud, Ana E. Goulart, Katherine R. Davis
Abstract: The cooperative operation of grid-edge power and energy resources is crucial to improving the resilience of power systems during contingencies. However, given the complex cyber-physical nature of power grids, it is hard to respond timely with limited costs for deploying additional cyber and/or phyiscal resources, such as during a high-impact low-frequency cyber-physical event. Therefore, the paper examines the design of cooperative cyber-physical resource optimization solutions to control grid-tied cyber and physical resources. First, the operation of a cyber-physical power system is formulated into a constrained optimization problem, including the cyber and physical objectives and constraints. Then, a bi-level solution is provided to obtain optimal cyber and physical actions, including the reconfiguration of cyber topology (e.g., activation of communication links) in the cyber layer and the control of physical resources (e.g., energy storage systems) in the physical layer. The developed method improves grid resilience during cyberattacks and can provide guidance on the control of coupled physical side resources. Numerical simulation on a modified IEEE 14-bus system demonstrates the effectiveness of the proposed approach.

Paper number 30:
Title: Positioning-Aided Channel Estimation for Multi-LEO Satellite Downlink Communications
Authors: Yuchen Zhang, Pinjun Zheng, Jie Ma, Henk Wymeersch, Tareq Y. Al-Naffouri
Abstract: We investigate a multi-low Earth orbit (LEO) satellite system that simultaneously provides positioning and communication services to terrestrial user terminals. To address the challenges of channel estimation in LEO satellite systems, we propose a novel two-timescale positioning-aided channel estimation framework, exploiting the distinct variation rates of position-related parameters and channel gains inherent in LEO satellite channels. Using the misspecified Cramer-Rao bound (MCRB) theory, we systematically analyze positioning performance under practical imperfections, such as inter-satellite clock bias and carrier frequency offset. Furthermore, we theoretically demonstrate how position information derived from downlink positioning can enhance uplink channel estimation accuracy, even in the presence of positioning errors, through an MCRB-based analysis. To overcome the constraints of limited link budgets and communication rates associated with single-satellite-based communication, we develop a distributed beamforming strategy for downlink communication. This strategy allows LEO satellites to independently optimize their beamformers using local channel state information, eliminating the need for centralized processing while preserving the advantages of multi-satellite cooperative communication. Theoretical analyses and numerical results confirm the effectiveness of the proposed framework in achieving high-precision downlink positioning under practical imperfections, facilitating uplink channel estimation, and enabling efficient downlink communication.

Paper number 31:
Title: Image-Based Alzheimer's Disease Detection Using Pretrained Convolutional Neural Network Models
Authors: Nasser A Alsadhan
Abstract: Alzheimer's disease is an untreatable, progressive brain disorder that slowly robs people of their memory, thinking abilities, and ultimately their capacity to complete even the most basic tasks. Among older adults, it is the most frequent cause of dementia. Although there is presently no treatment for Alzheimer's disease, scientific trials are ongoing to discover drugs to combat the condition. Treatments to slow the signs of dementia are also available. Many researchers throughout the world became interested in developing computer-aided diagnosis systems to aid in the early identification of this deadly disease and assure an accurate diagnosis. In particular, image based approaches have been coupled with machine learning techniques to address the challenges of Alzheimer's disease detection. This study proposes a computer aided diagnosis system to detect Alzheimer's disease from biomarkers captured using neuroimaging techniques. The proposed approach relies on deep learning techniques to extract the relevant visual features from the image collection to accurately predict the Alzheimer's class value. In the experiments, standard datasets and pre-trained deep learning models were investigated. Moreover, standard performance measures were used to assess the models' performances. The obtained results proved that VGG16-based models outperform the state of the art performance.

Paper number 32:
Title: Machine learning-based hybrid dynamic modeling and economic predictive control of carbon capture process for ship decarbonization
Authors: Xuewen Zhang, Kuniadi Wandy Huang, Dat-Nguyen Vo, Minghao Han, Benjamin Decardi-Nelson, Xunyuan Yin
Abstract: Implementing carbon capture technology on-board ships holds promise as a solution to facilitate the reduction of carbon intensity in international shipping, as mandated by the International Maritime Organization. In this work, we address the energy-efficient operation of shipboard carbon capture processes by proposing a hybrid modeling-based economic predictive control scheme. Specifically, we consider a comprehensive shipboard carbon capture process that encompasses the ship engine system and the shipboard post-combustion carbon capture plant. To accurately and robustly characterize the dynamic behaviors of this shipboard plant, we develop a hybrid dynamic process model that integrates available imperfect physical knowledge with neural networks trained using process operation data. An economic model predictive control approach is proposed based on the hybrid model to ensure carbon capture efficiency while minimizing energy consumption required for the carbon capture process operation. \textcolor{blue}{The cross-entropy method is employed to efficiently solve the complex non-convex optimization problem associated with the proposed hybrid model-based economic model predictive control method.} Extensive simulations, analyses, and comparisons are conducted to verify the effectiveness and illustrate the superiority of the proposed framework.

Paper number 33:
Title: Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models
Authors: Shiva Kumar C, Jitendra Kumar Dhiman, Nagaraj Adiga, Shatrughan Singh
Abstract: Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impact of combining KD loss with alternative pruning techniques, including Low-Rank Factorization (LRF) and l0 regularization, on a conformer-based pre-trained network under the paradigm of Self-Supervised Learning (SSL). We also propose a strategy to jointly prune and train an RNN-T-based ASR model, demonstrating that this approach yields superior performance compared to pruning a pre-trained network first and then using it for ASR training. This approach led to a significant reduction in word error rate: l0 and KD combination achieves the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER) improvement over the baseline, while LRF and KD combination yields the best results for streaming ASR, improving RWER by 13.4%.

Paper number 34:
Title: On the use of Performer and Agent Attention for Spoken Language Identification
Authors: Jitendra Kumar dhiman, Jainag Ambati
Abstract: One of the methods for language Identification (LID) involves deriving speech representation from pre-trained models using self-supervised learning, followed by fine-tuning the model for the LID task. State-of-the-art approaches for LID use an attention-based statistical pooling layer to facilitate the aggregation of contextual information across time frames of the embedding vectors extracted from the pre-trained model. In this paper, we delve into exploring recently proposed attention mechanisms, namely performer and agent-attention, in conjunction with the statistical pooling layer. The LID experiments are performed on three datasets: VoxPopuli, FLEURS, and VoxLingua. We compare their performance against vanilla self-attention. Our findings suggest that performer-attention outperforms self-attention and agent-attention exhibits comparable or occasionally superior performance to self-attention, while also being computationally less expensive.

Paper number 35:
Title: A Grid-Forming HVDC Series Tapping Converter Using Extended Techniques of Flex-LCC
Authors: Qianhao Sun, Ruofan Li, Jichen Wang, Mingchao Xia, Qifang Chen, Meiqi Fan, Gen Li, Xuebo Qiao
Abstract: This paper discusses an extension technology for the previously proposed Flexible Line-Commutated Converter (Flex LCC) [1]. The proposed extension involves modifying the arm internal-electromotive-force control, redesigning the main-circuit parameters, and integrating a low-power coordination strategy. As a result, the Flex-LCC transforms from a grid-forming (GFM) voltage source converter (VSC) based on series-connected LCC and FBMMC into a novel GFM HVDC series tapping converter, referred to as the Extended Flex-LCC (EFLCC). The EFLCC provides dc characteristics resembling those of current source converters (CSCs) and ac characteristics resembling those of GFM VSCs. This makes it easier to integrate relatively small renewable energy sources (RESs) that operate in islanded or weak-grid supported conditions with an existing LCC-HVDC. Meanwhile, the EFLCC distinguishes itself by requiring fewer full-controlled switches and less energy storage, resulting in lower losses and costs compared to the FBMMC HVDC series tap solution. In particular, the reduced capacity requirement and the wide allowable range of valve-side ac voltages in the FBMMC part facilitate the matching of current-carrying capacities between full-controlled switches and thyristors. The application scenario, system-level analysis, implementation, converter-level operation, and comparison of the EFLCC are presented in detail in this paper. The theoretical analysis is confirmed by experimental and simulation results.

Paper number 36:
Title: Exploiting the Hidden Capacity of MMC Through Accurate Quantification of Modulation Indices
Authors: Qianhao Sun, Jingwei Meng, Ruofan Li, Mingchao Xia, Qifang Chen, Jiejie Zhou, Meiqi Fan, Peiqian Guo
Abstract: The modular multilevel converter (MMC) has become increasingly important in voltage-source converter-based high-voltage direct current (VSC-HVDC) systems. Direct and indirect modulation are widely used as mainstream modulation techniques in MMCs. However, due to the challenge of quantitatively evaluating the operation of different modulation schemes, the academic and industrial communities still hold differing opinions on their performance. To address this controversy, this paper employs the state-of-the-art computational methods and quantitative metrics to compare the performance among different modulation schemes. The findings indicate that direct modulation offers superior modulation potential for MMCs, highlighting its higher ac voltage output capability and broader linear PQ operation region. Conversely, indirect modulation is disadvantaged in linear modulation, which indicates inferior output voltage capability. Furthermore, this paper delves into the conditions whereby direct and indirect modulation techniques become equivalent in steady-state. The study findings suggest that the modulation capability of direct modulation is the same as that of indirect modulation in steady-state when additional controls, including closed-loop capacitor voltage control and circulating current suppression control (CCSC), are simultaneously active. Simulation and experiments verify the correctness and validity.

Paper number 37:
Title: Rapid Detection of High-impedance Arc Faults in Medium Voltage Electrical Distribution Systems
Authors: Kriti Thakur, Divyanshi Dwivedi, K. Victor Sam Moses Babu, Alivelu Manga Parimi, Prasanta K. Panigrahi, Pradeep Kumar Yemula, Pratyush Chakraborty, Mayukha Pal
Abstract: High-impedance arc faults in AC power systems have the potential to lead to catastrophic accidents. However, significant challenges exist in identifying these faults because of the much weaker characteristics and variety when grounded with different surfaces. Addressing a noteworthy gap in prior research, which largely focused on arc fault detection in low-voltage systems. A novel approach has been applied that offers rapid arc fault detection for medium voltage distribution lines. In contrast to existing black-box feature-based approaches, Hankel alternative view of the Koopman (HAVOK) analysis developed from nonlinear dynamics has been applied which offers not only interpretable features but also opens up new application options in the area of arc fault detection. The method displays a much faster detection speed within 0.45 ms making it appropriate for real-time applications. It demonstrates the ability to detect arc faults across various scenarios, boosting its practical importance for stakeholders in safety-critical industries.

Paper number 38:
Title: Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo
Authors: Idan Achituve, Hai Victor Habi, Amir Rosenfeld, Arnon Netzer, Idit Diamant, Ethan Fetaya
Abstract: In image processing, solving inverse problems is the task of finding plausible reconstructions of an image that was corrupted by some (usually known) degradation model. Commonly, this process is done using a generative image model that can guide the reconstruction towards solutions that appear natural. The success of diffusion models over the last few years has made them a leading candidate for this task. However, the sequential nature of diffusion models makes this conditional sampling process challenging. Furthermore, since diffusion models are often defined in the latent space of an autoencoder, the encoder-decoder transformations introduce additional difficulties. Here, we suggest a novel sampling method based on sequential Monte Carlo (SMC) in the latent space of diffusion models. We use the forward process of the diffusion model to add additional auxiliary observations and then perform an SMC sampling as part of the backward process. Empirical evaluations on ImageNet and FFHQ show the benefits of our approach over competing methods on various inverse problem tasks.

Paper number 39:
Title: Beam Training for Pinching-Antenna Systems (PASS)
Authors: Suyu Lv, Yuanwei Liu, Zhiguo Ding
Abstract: This article investigates the beam training design problems for pinching-antenna systems (PASS), where single-waveguide-single-user (SWSU), single-waveguide-multi-user (SWMU) and multi-waveguide-multi-user (MWMU) scenarios are considered. For SWSU-PASS, we design a scalable codebook, based on which we propose a three-stage beam training (3SBT) scheme. Specifically, 1) firstly, the 3SBT scheme utilizes one activated pinching antenna to obtain a coarse one-dimensional location at the first stage; 2) secondly, it achieves further phase matching with an increased number of activated antennas at the second stage; 3) finally, it realizes precise beam alignment through an exhaustive search at the third stage. For SWMU-PASS, based on the scalable codebook design, we propose an improved 3SBT scheme to support non-orthogonal multiple access (NOMA) transmission. For MWMU-PASS, we first present a generalized expression of the received signal based on the partially-connected hybrid beamforming structure. Furthermore, we introduce an increased-dimensional scalable codebook design, based on which an increased-dimensional 3SBT scheme is proposed. Numerical results reveal that: i) the proposed beam training scheme can significantly reduce the training overhead compared to the two-dimensional exhaustive search, while maintaining reasonable rate performance; ii) compared to fixed-location pinching antennas and conventional array antennas, the proposed dynamic pinching antennas yield better flexibility and improved performance.

Paper number 40:
Title: A Generative Framework for Bidirectional Image-Report Understanding in Chest Radiography
Authors: Nicholas Evans, Stephen Baker, Miles Reed
Abstract: The rapid advancements in large language models (LLMs) have unlocked their potential for multimodal tasks, where text and visual data are processed jointly. However, applying LLMs to medical imaging, particularly for chest X-rays (CXR), poses significant challenges due to the need for precise visual-textual alignment and the preservation of critical diagnostic details. In this paper, we propose Multi-Stage Adaptive Vision-Language Tuning (MAViLT), a novel framework designed to enhance multimodal reasoning and generation for CXR understanding. MAViLT incorporates a clinical gradient-weighted tokenization process and a hierarchical fine-tuning strategy, enabling it to generate accurate radiology reports, synthesize realistic CXRs from text, and answer vision-based clinical questions. We evaluate MAViLT on two benchmark datasets, MIMIC-CXR and Indiana University CXR, achieving state-of-the-art results across all tasks. Human evaluations further validate the clinical relevance and utility of MAViLT, making it a robust tool for real-world medical applications. This work demonstrates the feasibility of leveraging LLMs for multimodal medical imaging while addressing key challenges in vision-language integration.

Paper number 41:
Title: Comprehensive Review of Deep Unfolding Techniques for Next-Generation Wireless Communication Systems
Authors: Sukanya Deka, Kuntal Deka, Nhan Thanh Nguyen, Sanjeev Sharma, Vimal Bhatia, Nandana Rajatheva
Abstract: The application of machine learning in wireless communications has been extensively explored, with deep unfolding emerging as a powerful model-based technique. Deep unfolding enhances interpretability by transforming complex iterative algorithms into structured layers of deep neural networks (DNNs). This approach seamlessly integrates domain knowledge with deep learning (DL), leveraging the strengths of both methods to simplify complex signal processing tasks in communication systems. To provide a solid foundation, we first present a brief overview of DL and deep unfolding. We then explore the applications of deep unfolding in key areas, including signal detection, channel estimation, beamforming design, decoding for error-correcting codes, sensing and communication, power allocation, and security. Each section focuses on a specific task, highlighting its significance in emerging 6G technologies and reviewing recent advancements in deep unfolding-based solutions. Finally, we discuss the challenges associated with developing deep unfolding techniques and propose potential improvements to enhance their applicability across diverse wireless communication scenarios.

Paper number 42:
Title: A Comprehensive Energy Management Application Method considering Smart Home Occupant Behavior using IoT and Real Big Data
Authors: S. Saba Rafiei, Mahdi S. Naderi, Mehrdad Abedi
Abstract: One of the most far-reaching use cases of the internet of things is in smart grid and smart home operation. The smart home concept allows residents to control, monitor, and manage their energy consumption with minimum loss and self-involvement. Since each household's lifestyle and energy consumption is unique, the management system needs background knowledge about residents' energy consumption behavioral patterns for more accurate planning. To obtain this information, data related to residents' consumption records must be processed. This research has attempted to provide an optimal decentralized management system consisting of interoperable sections to forecast, optimize, schedule, and implement load management in a smart home. Comparing different prediction models using 4 years of 1-min interval real data of a smart home with photovoltaic generation (PV) and electric vehicle (EV), forecasting non-controllable loads and taking a deterministic approach in different scenarios, the system uses mixed integer linear programming (MILP) to provide load scheduling with the objective of an optimal total energy cost reduction with minimum changes in the household's desired consumption compared to the initial state. The results have shown that the proposed system has reliable performance due to the high precision of the forecast and has led to increased energy efficiency, reduced energy cost (up to 62. 05\%), reduced peak-to-average ratio (PAR) (up to 44. 19\%) and reduced standard deviation (SD) (up to 19. 70\%) in net consumption.

Paper number 43:
Title: Multi-modal Data Fusion and Deep Ensemble Learning for Accurate Crop Yield Prediction
Authors: Akshay Dagadu Yewle, Laman Mirzayeva, Oktay Karakuş
Abstract: This study introduces RicEns-Net, a novel Deep Ensemble model designed to predict crop yields by integrating diverse data sources through multimodal data fusion techniques. The research focuses specifically on the use of synthetic aperture radar (SAR), optical remote sensing data from Sentinel 1, 2, and 3 satellites, and meteorological measurements such as surface temperature and rainfall. The initial field data for the study were acquired through Ernst & Young's (EY) Open Science Challenge 2023. The primary objective is to enhance the precision of crop yield prediction by developing a machine-learning framework capable of handling complex environmental data. A comprehensive data engineering process was employed to select the most informative features from over 100 potential predictors, reducing the set to 15 features from 5 distinct modalities. This step mitigates the ``curse of dimensionality" and enhances model performance. The RicEns-Net architecture combines multiple machine learning algorithms in a deep ensemble framework, integrating the strengths of each technique to improve predictive accuracy. Experimental results demonstrate that RicEns-Net achieves a mean absolute error (MAE) of 341 kg/Ha (roughly corresponds to 5-6\% of the lowest average yield in the region), significantly exceeding the performance of previous state-of-the-art models, including those developed during the EY challenge.

Paper number 44:
Title: Geolocation with Large LEO Constellations: Insights from Fisher Information
Authors: Don-Roberts Emenonye, Harpreet S. Dhillon, R. Michael Buehrer
Abstract: Interest in the use of the low earth orbit (LEO) in space - from $160 \text{ km}$ to $2000 \text{ km}$ - has skyrocketed; this is evident by the fact that National Aeronautics and Space Administration (NASA) has partnered with various commercial platforms like Axiom Space, Blue Origin, SpaceX, Sierra Space, Starlab Space, ThinkOrbital, and Vast Space to deploy satellites. %and platforms like Northrop Grumman and Boeing to transport cargo and crew. The most apparent advantage of satellites in LEO over satellites in Geostationary (GEO) and medium earth orbit (MEO) is their closeness to the earth; hence, signals from LEOs encounter lower propagation losses and reduced propagation delay, opening up the possibility of using these LEO satellites for localization. This article reviews the existing signal processing algorithms for localization using LEO satellites, introduces the basics of estimation theory, connects estimation theory to model identifiability with Fisher Information Matrix (FIM), and with the FIM, provides conditions that allow for $9$D localization of a terrestrial receiver using signals from multiple LEOs (unsynchronized in time and frequency) across multiple time slots and multiple receive antennas. We also compare the structure of the information available in LEO satellites with the structure of the information available in the Global Positioning System (GPS).

Paper number 45:
Title: Intelligent Reconfigurable Optical Wireless Ether
Authors: Hongwei Cui, Soung Chang Liew
Abstract: Optical wireless communication (OWC) uses light for wireless data transmission, potentially providing faster and more secure communication than traditional radio-frequency-based techniques like Wi-Fi. However, light's high directionality and its limited penetration ability restrict the signal coverage. To address this limitation, we propose an artificial "optical wireless ether" (OWE) fabric. OWE acts as a reconfigurable electromagnetic (EM) wave-propagating medium, intelligently enhancing the strength of light signals and redirecting their propagation to cover a broader area. Our proposed ether fabric comprises simple optical signal amplification units, called ether amplifiers (EAs), strategically placed in the environment, e.g., on ceilings. The EAs amplify and propagate signals at the analog level and are agnostic to the signal format: Signals propagate wirelessly between the EAs, losing strength due to attenuation during transmission but regaining it as they pass through the EAs. The key challenge in OWE design lies in the fact that, while increasing EA gains can extend signal coverage, it can also create positive feedback loops, resulting in self-interference and amplifier saturation, which distort the signals -- the key challenge in OWE design. This paper presents a systematic theoretical analysis to prevent amplifier saturation while optimizing the performance of OWE in both single-basic-service-set (single-BSS) and multiple-BSS scenarios. Optimization objectives could include signal-to-noise ratio, resource allocation fairness, and mutual interference. Furthermore, we conducted simulations and experiments to corroborate our theories. To our knowledge, ours is the first experimental demonstration of the feasibility of an artificial ether fabric for extending and guiding light propagation, laying a solid groundwork for future development and exploration of OWE.

Paper number 46:
Title: Analysis and Optimization of Robustness in Multiplex Flow Networks Against Cascading Failures
Authors: Orkun İrsoy, Osman Yağan
Abstract: Networked systems are susceptible to cascading failures, where the failure of an initial set of nodes propagates through the network, often leading to system-wide failures. In this work, we propose a multiplex flow network model to study robustness against cascading failures triggered by random failures. The model is inspired by systems where nodes carry or support multiple types of flows, and failures result in the redistribution of flows within the same layer rather than between layers. To represent different types of interdependencies between the layers of the multiplex network, we define two cases of failure conditions: layer-independent overload and layer-influenced overload. We provide recursive equations and their solutions to calculate the steady-state fraction of surviving nodes, validate them through a set of simulation experiments, and discuss optimal load-capacity allocation strategies. Our results demonstrate that allocating the total excess capacity to each layer proportional to the mean effective load in the layer and distributing that excess capacity equally among the nodes within the layer ensures maximum robustness. The proposed framework for different failure conditions allows us to analyze the two overload conditions presented and can be extended to explore more complex interdependent relationships.

Paper number 47:
Title: A Data-Efficient Pan-Tumor Foundation Model for Oncology CT Interpretation
Authors: Wenhui Lei, Hanyu Chen, Zitian Zhang, Luyang Luo, Qiong Xiao, Yannian Gu, Peng Gao, Yankai Jiang, Ci Wang, Guangtao Wu, Tongjia Xu, Yingjie Zhang, Xiaofan Zhang, Pranav Rajpurkar, Shaoting Zhang, Zhenning Wang
Abstract: Artificial intelligence-assisted imaging analysis has made substantial strides in tumor diagnosis and management. Here we present PASTA, a pan-tumor CT foundation model that achieves state-of-the-art performance on 45 of 46 representative oncology tasks -- including lesion segmentation, tumor detection in plain CT, tumor staging, survival prediction, structured report generation, and cross-modality transfer learning, significantly outperforming the second-best models on 35 tasks. This remarkable advancement is driven by our development of PASTA-Gen, an innovative synthetic tumor generation framework that produces a comprehensive dataset of 30,000 CT scans with pixel-level annotated lesions and paired structured reports, encompassing malignancies across ten organs and five benign lesion types. By leveraging this rich, high-quality synthetic data, we overcome a longstanding bottleneck in the development of CT foundation models -- specifically, the scarcity of publicly available, high-quality annotated datasets due to privacy constraints and the substantial labor required for scaling precise data annotation. Encouragingly, PASTA demonstrates exceptional data efficiency with promising practical value, markedly improving performance on various tasks with only a small amount of real-world data. The open release of both the synthetic dataset and PASTA foundation model effectively addresses the challenge of data scarcity, thereby advancing oncological research and clinical translation.

Paper number 48:
Title: Learning the Frequency Dynamics of the Power System Using Higher-order Dynamic Mode Decomposition
Authors: Xiao Li, Xinyi Wen, Benjamin Schafer
Abstract: The increasing penetration of renewable energy sources, characterised by low inertia and intermittent disturbances, presents substantial challenges to power system stability. As critical indicators of system stability, frequency dynamics and associated oscillatory phenomena have attracted significant research attention. While existing studies predominantly employ linearized models, our findings demonstrate that linear approximations exhibit considerable errors when predicting frequency oscillation dynamics across multiple time scales, thus necessitating the incorporation of nonlinear characteristics. This paper proposes a data-driven approach based on higher-order dynamical mode decomposition (HODMD) for learning frequency dynamics. The proposed method offers distinct advantages over alternative nonlinear methods, including no prior knowledge required, adaptability to high-dimensional systems, and robust performance. Furthermore, HODMD demonstrates superior capability in capturing system-wide spatio-temporal modes, successfully identifying modal behaviour that remains undetectable through standard Dynamic Mode Decomposition techniques. The efficacy of the proposed methodology is validated through comprehensive case studies on both IEEE 14-bus and WECC systems.

Paper number 49:
Title: Weighted-Sum Energy Efficiency Maximization in User-Centric Uplink Cell-Free Massive MIMO
Authors: Donghwi Kim, Liesbet Van der Perre, Wan Choi
Abstract: This paper introduces the weighted-sum energy efficiency (WSEE) as an advanced performance metric designed to represent the uplink energy efficiency (EE) of individual user equipment (UE) in a user-centric Cell-Free massive MIMO (CF-mMIMO) system more accurately. In this realistic user-centric CF-mMIMO context, each UE may exhibit distinct characteristics, such as maximum transmit power limits or specific minimum data rate requirements. By computing the EE of each UE independently and adjusting the weights accordingly, the system can accommodate these unique attributes, thus promoting energy-efficient operation. The uplink WSEE is formulated as a multiple-ratio fractional programming (FP) problem, representing a weighted sum of the EE of individual UEs, which depends on each UE's transmit power and the combining vector at the CPU. To effectively maximize WSEE, we present optimization algorithms that utilize the Dinkelbach transform and the quadratic transform (QT). Applying the QT twice consecutively yields significant performance gains in terms of WSEE. This framework establishes a foundation for developing operational strategies tailored to specific system requirements.

Paper number 50:
Title: AVSim - Realistic Simulation Framework for Airborne and Vector-Borne Disease Dynamics
Authors: Pandula Thennakoon, Mario De Silva, M. Mahesha Viduranga, Sashini Liyanage, Roshan Godaliyadda, Mervyn Parakrama Ekanayake, Vijitha Herath, Anuruddhika Rathnayake, Ganga Thilakarathne, Janaka Ekanayake, Samath Dharmarathne
Abstract: The COVID-19 pandemic underscored the critical need for rapid epidemic trend identification and effective intervention strategies to mitigate disease progression and its socio-economic impact. Concurrent with emerging threats, endemic diseases like dengue continue to strain healthcare systems, particularly in populous, economically challenged nations. This paper introduces AVSim (Airborne and Vectorborne Simulator), an agent-based model designed to provide granular insights for optimizing resource allocation within existing healthcare management frameworks. AVSim leverages realistic human mobility and behavioral patterns to simulate disease propagation within a detailed, scalable environment encompassing homes, schools, hospitals, and commercial venues. Human movement is modeled based on occupational and behavioral patterns, including age-specific activities. The simulator incorporates age- and environment-specific disease outcomes, host-host and host-vector interactions, and multiple disease stages, including mild, severe, and critical phases. Immunity, quarantine, and hospitalization are also modeled. Furthermore, AVSim supports tracing the path of disease spread, providing micro-level insights into transmission dynamics. Implemented in Python, AVSim offers flexibility and extensibility, enabling users to create highly customized scenarios for airborne and vector-borne disease modeling. Case studies demonstrating AVSim's application to COVID-19 and dengue illustrate its potential for generating actionable epidemic insights, thereby enhancing public health planning and response.

Paper number 51:
Title: Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System
Authors: Yueqing Wang, Yikun Mei, Zhen Gao, Ziwei Wan, Boyu Ning, De Mi, Sami Muhaidat
Abstract: The spatial diversity and multiplexing advantages of massive multi-input-multi-output (mMIMO) can significantly improve the capacity of massive non-orthogonal multiple access (NOMA) in machine type communications. However, state-of-the-art grant-free massive NOMA schemes for mMIMO systems require accurate estimation of random access channels to perform activity detection and the following coherent data demodulation, which suffers from excessive pilot overhead and access latency. To address this, we propose a pre-equalization aided grant-free massive access scheme for mMIMO systems, where an iterative detection scheme is conceived. Specifically, the base station (BS) firstly activates one of its antennas (i.e., beacon antenna) to broadcast a beacon signal, which facilitates the user equipment (UEs) to perform downlink channel estimation and pre-equalize the uplink random access signal with respect to the channels associated with the beacon antenna. During the uplink transmission stage, the BS detects UEs' activity and data by using the proposed iterative detection algorithm, which consists of three modules: coarse data detection (DD), data-aided channel estimation (CE), and fine DD. In the proposed algorithm, the joint activity and DD is firstly performed based on the signals received by the beacon antenna. Subsequently, the DD is further refined by iteratively performing data-aided CE module and fine DD module using signals received by all BS antennas. Our simulation results demonstrate that the proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive NOMA schemes with the same access latency.

Paper number 52:
Title: UAV-Enabled IoT Networks: A SWIPT Energy Harvesting Architecture with Relay Support for Disaster Response
Authors: Hossein Mohammadi Firouzjaei, Javad Zeraatkar Moghaddam, Mehrdad Ardebilipour
Abstract: Due to the wide application of unmanned aerial vehicles (UAVs) as relays to establish Disaster Response Networks (DRNs), an effective model of energy harvesting (EH) and energy consumption for the UAV-aided Disaster Response Network (DRN) is rising to be a challenging issue. This is mainly manifest in Internet of Things (IoT) scenarios where multiple users are looking to communicate with the UAV. In this paper, the possibility of connecting an UAV with several users is investigated where the UAV as a relay receives data from a DRN and delivers to another network considering two IoT scenarios. The first scenario represents a conventional method with limited UAV energy where low communication rates and inadequate service coverage for all users are challenges. But in the second scenario, a Simultaneous Wireless Information and Power Transmission (SWIPT) technique is used to serve users. Considering potential limitations in transmission energy of users within disaster networks, the SWIPT technique is applied to maximize energy acquisition by the UAV, leading to improve the efficiency of the investigated scenario. Finally, the required energy of the UAV to serve the largest number of users in the shortest possible time is clarified. Furthermore, by Considering the relationship between energy and UAV flight time and defining the UAV flight time optimization problem, optimal network parameters are obtained. Simulation results show the effectiveness of the proposed scenario.

Paper number 53:
Title: A Novel Energy-aware Design for a D2D Underlaid UAV-Aided Cognitive Radio Network
Authors: Hossein Mohammadi Firouzjaei, Javad Zeraatkar Moghaddam, Mehrdad Ardebilipour
Abstract: This paper investigates the effectiveness of a free disaster response network (free-DRN) and an energy harvesting enabled DRN (EH-enabled DRN) enhanced by cognitive radio (CR) technology and unmanned aerial vehicles (UAVs). In the EH-enabled DRN scenario, where device-to-device (D2D) communication harvests energy from cellular users, significant improvements are demonstrated in terms of energy efficiency (EE) and communication rate compared to the free-DRN approach. The impact of user density and UAV specifications on network performance is analyzed, addressing the challenge of optimizing the duration of energy harvesting for both cellular users and D2D devices in the EH-enabled DRN scenario. Additionally, simulation results reveal an optimal UAV height that ensures efficient network operation for varying densities of D2D devices. Overall, numerical and simulation findings highlight the superior performance of the EH-enabled DRN approach, showcasing the positive effects of enabling D2D links and improved EE. Notably, reducing energy harvesting duration and cellular user density can further enhance the EE of the DRN by up to 3dB.

Paper number 54:
Title: Delay Optimization of a Federated Learning-based UAV-aided IoT network
Authors: Hossein Mohammadi Firouzjaei, Javad Zeraatkar Moghaddam, Mehrdad Ardebilipour
Abstract: This paper explores the integration of power splitting(PS) simultaneous wireless information and power transfer (SWIPT) architecture and federated learning (FL) in Internet of Things (IoT) networks. The use of SWIPT allows power-constrained devices to simultaneously harvest energy and transmit data, addressing the energy limitations faced by IoT devices. The proposed scenario involves an Unmanned Arial Vehicle (UAV) serving as the base station (BS) and edge server, aggregating weight updates from IoT devices and unicasting aggregated updates to each device. The results demonstrate the feasibility of FL in IoT scenarios, ensuring communication efficiency without depleting device batteries.

Paper number 55:
Title: Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?
Authors: Qingshan Hou, Yukun Zhou, Jocelyn Hui Lin Goh, Ke Zou, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Thaddaeus Lo, Xiaofeng Lei, Siegfried K. Wagner, Mark A. Chia, Dawei Yang, Hongyang Jiang, AnRan Ran, Rui Santos, Gabor Mark Somfai, Juan Helen Zhou, Haoyu Chen, Qingyu Chen, Carol Yim-Lui Cheung, Pearse A. Keane, Yih Chung Tham
Abstract: The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.

Paper number 56:
Title: A Novel Phenomenological Model of Equalization-enhanced Phase Noise
Authors: Benedikt Geiger, Fred Buchali, Vahid Aref, Laurent Schmalen
Abstract: We show that equalization-enhanced phase noise manifests as a time-varying, frequency-dependent phase error, which can be modeled and reversed by a time-varying all-pass finite impulse response filter.

Paper number 57:
Title: Toolbox for Developing Physics Informed Neural Networks for Power Systems Components
Authors: Ioannis Karampinis, Petros Ellinas, Ignasi Ventura Nadal, Rahul Nellikkath, Spyros Chatzivasileiadis
Abstract: This paper puts forward the vision of creating a library of neural-network-based models for power system simulations. Traditional numerical solvers struggle with the growing complexity of modern power systems, necessitating faster and more scalable alternatives. Physics-Informed Neural Networks (PINNs) offer promise to solve fast the ordinary differential equations (ODEs) governing power system dynamics. This is vital for the reliability, cost optimization, and real-time decision-making in the electricity grid. Despite their potential, standardized frameworks to train PINNs remain scarce. This poses a barrier for the broader adoption and reproducibility of PINNs; it also does not allow the streamlined creation of a PINN-based model library. This paper addresses these gaps. It introduces a Python-based toolbox for developing PINNs tailored to power system components, available on GitHub https://github. com/radiakos/PowerPINN. Using this framework, we capture the dynamic characteristics of a 9th-order system, which is probably the most complex power system component trained with a PINN to date, demonstrating the toolbox capabilities, limitations, and potential improvements. The toolbox is open and free to use by anyone interested in creating PINN-based models for power system components.

Paper number 58:
Title: Stochastic MPC with Online-optimized Policies and Closed-loop Guarantees
Authors: Marcell Bartos, Alexandre Didier, Jerome Sieber, Johannes Köhler, Melanie N. Zeilinger
Abstract: This paper proposes a stochastic model predictive control method for linear systems affected by additive Gaussian disturbances. Closed-loop satisfaction of probabilistic constraints and recursive feasibility of the underlying convex optimization problem is guaranteed. Optimization over feedback policies online increases performance and reduces conservatism compared to fixed-feedback approaches. The central mechanism is a finitely determined maximal admissible set for probabilistic constraints, together with the reconditioning of the predicted probabilistic constraints on the current knowledge at every time step. The proposed method's reduced conservatism and improved performance in terms of the achieved closed-loop cost is demonstrated in a numerical example.

Paper number 59:
Title: Retrieving Filter Spectra in CNN for Explainable Sleep Stage Classification
Authors: Stephan Goerttler, Yucheng Wang, Emadeldeen Eldele, Fei He, Min Wu
Abstract: Despite significant advances in deep learning-based sleep stage classification, the clinical adoption of automatic classification models remains slow. One key challenge is the lack of explainability, as many models function as black boxes with millions of parameters. In response, recent work has increasingly focussed on enhancing model explainability. This study contributes to these efforts by globally explaining spectral processing of individual EEG channels. Specifically, we introduce a method to retrieve the filter spectrum of low-level convolutional feature extraction and compare it with the classification-relevant spectral information in the data. We evaluate our approach on the MSA-CNN model using the ISRUC-S3 and Sleep-EDF-20 datasets. Our findings show that spectral processing plays a significant role in the lower frequency bands. In addition, comparing the correlation between filter spectrum and data-based spectral information with univariate performance indicates that the model naturally prioritises the most informative channels in a multimodal setting. We specify how these insights can be leveraged to enhance model performance. The code for the filter spectrum retrieval and its analysis is available at this https URL.

Paper number 60:
Title: Recent Advances in Discrete Speech Tokens: A Review
Authors: Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
Abstract: The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.

Paper number 61:
Title: Three-Dimensional MRI Reconstruction with Gaussian Representations: Tackling the Undersampling Problem
Authors: Tengya Peng, Ruyi Zha, Zhen Li, Xiaofeng Liu, Qing Zou
Abstract: Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in the field of computer vision, but remains unexplored in the field of magnetic resonance imaging (MRI). This study explores its potential for the reconstruction of isotropic resolution 3D MRI from undersampled k-space data. We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs 3D Gaussian distributions as an explicit representation for MR volumes. Experimental evaluations indicate that this method can effectively reconstruct voxelized MR images, achieving a quality on par with that of well-established 3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR scheme operates under a self-supervised framework, obviating the need for extensive training datasets or prior model training. This approach introduces significant innovations to the domain, notably the adaptation of 3DGS to MRI reconstruction and the novel application of the existing 3DGS methodology to decompose MR signals, which are presented in a complex-valued format.

Paper number 62:
Title: Inventory Consensus Control in Supply Chain Networks using Dissipativity-Based Control and Topology Co-Design
Authors: Shirantha Welikala, Hai Lin, Panos J. Antsaklis
Abstract: Recent global and local phenomena have exposed vulnerabilities in critical supply chain networks (SCNs), drawing significant attention from researchers across various fields. Typically, SCNs are viewed as static entities regularly optimized to maintain their optimal operation. However, the dynamic nature of SCNs and their associated uncertainties have motivated researchers to treat SCNs as dynamic networked systems requiring robust control techniques. In this paper, we address the SCN inventory consensus problem, which aims to synchronize multiple parallel supply chains, enhancing coordination and robustness of the overall SCN. To achieve this, we take a novel approach exploiting dissipativity theory. In particular, we propose a dissipativity-based co-design strategy for distributed consensus controllers and communication topology in SCNs. It requires only the dissipativity information of the individual supply chains and involves solving a set of convex optimization problems, thus contributing to scalability, compositionality, and computational efficiency. Moreover, it optimizes the robustness of the SCN to various associated uncertainties, mitigating both bullwhip and ripple effects. We demonstrate our contributions using numerical examples, mainly by comparing the consensus performance with respect to standard steady-state control, feedback control, and consensus control strategies.

Paper number 63:
Title: A Review of Conceptualizations of Safety and Risk in Current Automated Driving Regulation
Authors: Marcus Nolte, Leon Johann Brettin, Hans Steege, Nayel Salem, Marvin Loba, Robert Graubohm, Markus Maurer
Abstract: "Safety" and "Risk" are key concepts for the design and development of automated vehicles. For the market introduction or large-scale field tests, both concepts are not only relevant for engineers developing the vehicles, but for all stakeholders (e.g., regulators, lawyers, or the general public) who have stakes in the technology. In the communication between stakeholder groups, common notions of these abstract concepts are key for efficient communication and setting mutual expectations. In the European market, automated vehicles require Europe-wide type approval or at least operating permits in the individual states. For this, a central means of communication between regulators and engineers are regulatory documents. Flawed terminology regarding the safety expectations for automated vehicles can unnecessarily complicate relations between regulators and manufacturers, and thus hinder the introduction of the technology. In this paper, we review relevant documents at the UN- and EU-level, for the UK, and Germany regarding their (implied) notions of safety and risk. We contrast the regulatory notions with established and more recently developing notions of safety and risk in the field of automated driving. Based on the analysis, we provide recommendations on how explicit definitions of safety and risk in regulatory documents can support rather than hinder the market introduction of automated vehicles.

Paper number 64:
Title: Towards Closing the Gap between Model-Based Systems Engineering and Automated Vehicle Assurance: Tailoring Generic Methods by Integrating Domain Knowledge
Authors: Marcus Nolte, Markus Maurer
Abstract: Designing, assuring and releasing safe automated vehicles is a highly interdisciplinary process. As complex systems, automated driving systems will inevitably be subject to emergent properties, i. e., the properties of the overall system will be more than just a sum of the properties of its integrated elements. Safety is one example of such emergent properties. In this regard, it must be ensured that effects of emergence do not render an overall system that is composed of safety-approved sub systems unsafe. The key challenges in this regard are twofold: Regarding the interdisciplinary character of the development and assurance processes, all relevant stakeholders must speak a common language and have a common understanding of the key concepts that influence system safety. Additionally, the individual properties of system elements should remain traceable to the system level. Model-Based Systems Engineering (MBSE) provides an interdisciplinary mindset, as well as methods and processes to manage emergent system properties over the entire system lifecycle. By this, MBSE provides tools that can assist the assurance process for automated vehicles. However, concepts from the domain of MBSE have a reputation for not being directly accessible for domain experts who are no experts in the field of Systems Engineering. This paper highlights challenges when applying MBSE methods to the design and development of automated driving systems. It will present an approach to create and apply domain-specific SysML profiles, which can be a first step for enhancing communication between different stake-holders in the development and safety assurance processes.

Paper number 65:
Title: Estimation of Food Intake Quantity Using Inertial Signals from Smartwatches
Authors: Ioannis Levi, Konstantinos Kyritsis, Vasileios Papapanagiotou, Georgios Tsakiridis, Anastasios Delopoulos
Abstract: Accurate monitoring of eating behavior is crucial for managing obesity and eating disorders such as bulimia nervosa. At the same time, existing methods rely on multiple and/or specialized sensors, greatly harming adherence and ultimately, the quality and continuity of data. This paper introduces a novel approach for estimating the weight of a bite, from a commercial smartwatch. Our publicly-available dataset contains smartwatch inertial data from ten participants, with manually annotated start and end times of each bite along with their corresponding weights from a smart scale, under semi-controlled conditions. The proposed method combines extracted behavioral features such as the time required to load the utensil with food, with statistical features of inertial signals, that serve as input to a Support Vector Regression model to estimate bite weights. Under a leave-one-subject-out cross-validation scheme, our approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To contextualize this performance, we introduce the improvement metric, that measures the relative MAE difference compared to a baseline model. Our method demonstrates a 17.41% improvement, while the adapted state-of-the art method shows a -28.89% performance against that same baseline. The results presented in this work establish the feasibility of extracting meaningful bite weight estimates from commercial smartwatch inertial sensors alone, laying the groundwork for future accessible, non-invasive dietary monitoring systems.

Paper number 66:
Title: Solving Optimal Power Flow on a Data-Budget: Feature Selection on Smart Meter Data
Authors: Vassilis Kekatos, Ridley Annin, Manish K. Singh, Junjie Qin
Abstract: How much data is needed to optimally schedule distributed energy resources (DERs)? Does the distribution system operator (DSO) have to precisely know load demands and solar injections at each bus of the feeder to solve an optimal power flow (OPF)? This work exploits redundancies in OPF's structure and data to avoid communicating such data deluge, and explores the trade-off between data compression and grid's performance. We propose an OPF data distillation framework involving two steps. The DSO first collects OPF data from only a subset of nodes. The DSO subsequently reconstructs the complete OPF data from the partial ones, and feeds them into the OPF solver. Selecting and reconstructing OPF data may be performed to either maximize the fidelity of reconstructed OPF data, or maximize the fidelity of OPF solutions corresponding to reconstructed data. Under the first objective, OPF data distillation is posed as a sparsity-regularized convex problem. Under the second objective, it is posed as a sparsity-regularized bilevel program. Both problems are solved using accelerated proximal gradient (PGD) algorithms. Numerical tests corroborate that the bilevel formulation enhances fidelity and feasibility of reconstructed OPF solutions, and that OPF solutions can be approximated reasonably well even from partial data.

Paper number 67:
Title: Optical RIS-enabled Multiple Access Communications
Authors: Georgios D. Chondrogiannis, Athanasios P. Chrysologou, Alexandros-Apostolos A. Boulogeorgos, Nestor D. Chatzidiamantis, Harald Haas
Abstract: In this paper, we identify optical reconfigurable intelligent surfaces (ORISs) as key enablers of next-generation free-space optical (FSO) multiple access systems. By leveraging their beam steering and beam splitting capabilities, ORISs are able to effectively address line-of-sight (LoS) constraints, while enabling multi-user connectivity. We consider an ORIS-assisted non-orthogonal multiple access (NOMA) system model consisting of a single transmitter (Tx) and two receivers (Rxs). We derive novel analytical expressions to characterize the statistical particularities of the Tx-ORIS-Rx communication channel. Building upon the aforementioned expressions, we investigate the outage performance of the Rxs by deriving exact analytical expressions for the outage probability (OP) of each Rx. To provide deeper insights into the impact of various system parameters and physical conditions on the outage performance of each Rx, we conduct a high signal-to-noise ratio (SNR) analysis, that returns asymptotic expressions for the Rxs OPs at the high-SNR regime. Monte Carlo simulations validate the analysis, demonstrate the effectiveness of ORIS-enabled NOMA under a variety of configurations and physical scenarios, and showcase its superiority over its orthogonal-based counterpart.

Paper number 68:
Title: Multi-Objective Mobile Damped Wave Algorithm (MOMDWA): A Novel Approach For Quantum System Control
Authors: Juntao Yu, Jiaquan Yu, Dedai Wei, Xinye Sha, Shengwei Fu, Miuyu Qiu, Yurun Jin, Kaichen Ouyang
Abstract: In this paper, we introduce a novel multi-objective optimization algorithm, the Multi-Objective Mobile Damped Wave Algorithm (MOMDWA), specifically designed to address complex quantum control problems. Our approach extends the capabilities of the original Mobile Damped Wave Algorithm (MDWA) by incorporating multiple objectives, enabling a more comprehensive optimization process. We applied MOMDWA to three quantum control scenarios, focusing on optimizing the balance between control fidelity, energy consumption, and control smoothness. The results demonstrate that MOMDWA significantly enhances quantum control efficiency and robustness, achieving high fidelity while minimizing energy use and ensuring smooth control pulses. This advancement offers a valuable tool for quantum computing and other domains requiring precise, multi-objective control.

Paper number 69:
Title: Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers
Authors: Adam Stooke, Rohit Prabhavalkar, Khe Chai Sim, Pedro Moreno Mengibar
Abstract: Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the "Aligner-Encoder". To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention -- it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform "self-transduction".

Paper number 70:
Title: Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance
Authors: Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Mikyas T. Desta, Roy Fejgin, Rafael Valle, Jason Li
Abstract: While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.

Paper number 71:
Title: Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems
Authors: Fernando Llorente, Daniel Waxman, Petar M. Djurić
Abstract: Flexible and scalable decentralized learning solutions are fundamentally important in the application of multi-agent systems. While several recent approaches introduce (ensembles of) kernel machines in the distributed setting, Bayesian solutions are much more limited. We introduce a fully decentralized, asymptotically exact solution to computing the random feature approximation of Gaussian processes. We further address the choice of hyperparameters by introducing an ensembling scheme for Bayesian multiple kernel learning based on online Bayesian model averaging. The resulting algorithm is tested against Bayesian and frequentist methods on simulated and real-world datasets.

Paper number 72:
Title: Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment
Authors: Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder
Abstract: This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.

Paper number 73:
Title: Motion Planning of Nonholonomic Cooperative Mobile Manipulators
Authors: Keshab Patra, Arpita Sinha, Anirban Guha
Abstract: We propose a real-time implementable motion planning technique for cooperative object transportation by nonholonomic mobile manipulator robots (MMRs) in an environment with static and dynamic obstacles. The proposed motion planning technique works in two steps. A novel visibility vertices-based path planning algorithm computes a global piece-wise linear path between the start and the goal location in the presence of static obstacles offline. It defines the static obstacle free space around the path with a set of convex polygons for the online motion planner. We employ a Nonliner Model Predictive Control (NMPC) based online motion planning technique for nonholonomic MMRs that jointly plans for the mobile base and the manipulators arm. It efficiently utilizes the locomotion capability of the mobile base and the manipulation capability of the arm. The motion planner plans feasible motion for the MMRs and generates trajectory for object transportation considering the kinodynamic constraints and the static and dynamic obstacles. The efficiency of our approach is validated by numerical simulation and hardware experiments in varied environments.

Paper number 74:
Title: Data-Driven Distributionally Robust Mixed-Integer Control through Lifted Control Policy
Authors: Xutao Ma, Chao Ning, Wenli Du, Yang Shi
Abstract: This paper investigates the finite-horizon distributionally robust mixed-integer control (DRMIC) of uncertain linear systems. However, deriving an optimal causal feedback control policy to this DRMIC problem is computationally formidable for most ambiguity sets. To address the computational challenge, we propose a novel distributionally robust lifted control policy (DR-LCP) method to derive a high-quality approximate solution to this DRMIC problem for a rich class of Wasserstein metric-based ambiguity sets, including the Wasserstein ambiguity set and its variants. In theory, we analyze the asymptotic performance and establish a tight non-asymptotic bound of the proposed method. In numerical experiments, the proposed DR-LCP method empirically demonstrates superior performance compared with existing methods in the literature.

Paper number 75:
Title: Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned Flow Matching Model
Authors: Jialong Zuo, Shengpeng Ji, Minghui Fang, Ziyue Jiang, Xize Cheng, Qian Yang, Wenrui Liu, Guangyan Zhang, Zehai Tu, Yiwen Guo, Zhou Zhao
Abstract: This paper introduces PFlow-VC, a conditional flow matching voice conversion model that leverages fine-grained discrete pitch tokens and target speaker prompt information for expressive voice conversion (VC). Previous VC works primarily focus on speaker conversion, with further exploration needed in enhancing expressiveness (such as prosody and emotion) for timbre conversion. Unlike previous methods, we adopt a simple and efficient approach to enhance the style expressiveness of voice conversion models. Specifically, we pretrain a self-supervised pitch VQVAE model to discretize speaker-irrelevant pitch information and leverage a masked pitch-conditioned flow matching model for Mel-spectrogram synthesis, which provides in-context pitch modeling capabilities for the speaker conversion model, effectively improving the voice style transfer capacity. Additionally, we improve timbre similarity by combining global timbre embeddings with time-varying timbre tokens. Experiments on unseen LibriTTS test-clean and emotional speech dataset ESD show the superiority of the PFlow-VC model in both timbre conversion and style transfer. Audio samples are available on the demo page this https URL.

Paper number 76:
Title: Modeling of Core Loss Based on Machine Learning and Deep Learning
Authors: Junqi He, Yifeng Wei, Daiguang Jin
Abstract: This article proposes a Mix Neural Network (MNN) based on CNN-FCNN for predicting magnetic loss of different materials. In traditional magnetic core loss models, empirical equations usually need to be regressed under the same external conditions. When the magnetic core material is different, it needs to be classified and discussed. If external factors increase, multiple models need to be proposed for classification and discussion, making the modeling process extremely cumbersome. And traditional empirical equations still has the problem of low accuracy, although various correction equations have been introduced later, the accuracy has always been unsatisfactory. By introducing machine learning and deep learning, it is possible to simultaneously solve prediction problems with low accuracy of empirical equations and complex conditions. Based on the MagNet database, through the training of the newly proposed MNN, it is found that a single model is sufficient to make predictions for at least four different materials under varying temperatures, frequencies, and waveforms, with accuracy far exceeding that of traditional models. At the same time, we also used three other machine learning and deep learning models (Random Forest, XGBoost, MLP-LSTM) for training, all of which had much higher accuracy than traditional models. On the basis of the predicted results, a hybrid model combining MNN and XGBoost was proposed, which predicted through weighting and found that the accuracy could continue to improve. This provides a solution for modeling magnetic core loss under different materials and operating modes.

Paper number 77:
Title: Lie-algebra Adaptive Tracking Control for Rigid Body Dynamics
Authors: Jiawei Tang, Shilei Li, Ling Shi
Abstract: Adaptive tracking control for rigid body dynamics is of critical importance in control and robotics, particularly for addressing uncertainties or variations in system model parameters. However, most existing adaptive control methods are designed for systems with states in vector spaces, often neglecting the manifold constraints inherent to robotic systems. In this work, we propose a novel Lie-algebra-based adaptive control method that leverages the intrinsic relationship between the special Euclidean group and its associated Lie algebra. By transforming the state space from the group manifold to a vector space, we derive a linear error dynamics model that decouples model parameters from the system state. This formulation enables the development of an adaptive optimal control method that is both geometrically consistent and computationally efficient. Extensive simulations demonstrate the effectiveness and efficiency of the proposed method. We have made our source code publicly available to the community to support further research and collaboration.

Paper number 78:
Title: IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System
Authors: Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang
Abstract: Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning this http URL, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at this https URL.

Paper number 79:
Title: Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage Guarantees
Authors: Ali Baheri, Marzieh Amiri Shahbazi
Abstract: We propose a multi-scale extension of conformal prediction, an approach that constructs prediction sets with finite-sample coverage guarantees under minimal statistical assumptions. Classic conformal prediction relies on a single notion of conformity, overlooking the multi-level structures that arise in applications such as image analysis, hierarchical data exploration, and multi-resolution time series modeling. In contrast, the proposed framework defines a distinct conformity function at each relevant scale or resolution, producing multiple conformal predictors whose prediction sets are then intersected to form the final multi-scale output. We establish theoretical results confirming that the multi-scale prediction set retains the marginal coverage guarantees of the original conformal framework and can, in fact, yield smaller or more precise sets in practice. By distributing the total miscoverage probability across scales in proportion to their informative power, the method further refines the set sizes. We also show that dependence between scales can lead to conservative coverage, ensuring that the actual coverage exceeds the nominal level. Numerical experiments in a synthetic classification setting demonstrate that multi-scale conformal prediction achieves or surpasses the nominal coverage level while generating smaller prediction sets compared to single-scale conformal methods.

Paper number 80:
Title: TrackDiffuser: Nearly Model-Free Bayesian Filtering with Diffusion Model
Authors: Yangguang He, Wenhao Li, Minzhe Li, Juan Zhang, Xiangfeng Wang, Bo Jin
Abstract: State estimation remains a fundamental challenge across numerous domains, from autonomous driving, aircraft tracking to quantum system control. Although Bayesian filtering has been the cornerstone solution, its classical model-based paradigm faces two major limitations: it struggles with inaccurate state space model (SSM) and requires extensive prior knowledge of noise characteristics. We present TrackDiffuser, a generative framework addressing both challenges by reformulating Bayesian filtering as a conditional diffusion model. Our approach implicitly learns system dynamics from data to mitigate the effects of inaccurate SSM, while simultaneously circumventing the need for explicit measurement models and noise priors by establishing a direct relationship between measurements and states. Through an implicit predict-and-update mechanism, TrackDiffuser preserves the interpretability advantage of traditional model-based filtering methods. Extensive experiments demonstrate that our framework substantially outperforms both classical and contemporary hybrid methods, especially in challenging non-linear scenarios involving non-Gaussian noises. Notably, TrackDiffuser exhibits remarkable robustness to SSM inaccuracies, offering a practical solution for real-world state estimation problems where perfect models and prior knowledge are unavailable.

Paper number 81:
Title: Gender Bias in Instruction-Guided Speech Synthesis Models
Authors: Chun-Yi Kuan, Hung-yi Lee
Abstract: Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like "Act like a nurse". We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts. Our experimental results reveal the model's tendency to exhibit gender bias for certain occupations. Moreover, models of different sizes show varying degrees of this bias across these occupations.

Paper number 82:
Title: Online Controller Synthesis for Robot Collision Avoidance: A Case Study
Authors: Yuheng Fan, Wang Lin
Abstract: The inherent uncertainty of dynamic environments poses significant challenges for modeling robot behavior, particularly in tasks such as collision avoidance. This paper presents an online controller synthesis framework tailored for robots equipped with deep learning-based perception components, with a focus on addressing distribution shifts. Our approach integrates periodic monitoring and repair mechanisms for the deep neural network perception component, followed by uncertainty reassessment. These uncertainty evaluations are injected into a parametric discrete-time markov chain, enabling the synthesis of robust controllers via probabilistic model checking. To ensure high system availability during the repair process, we propose a dual-component configuration that seamlessly transitions between operational states. Through a case study on robot collision avoidance, we demonstrate the efficacy of our method, showcasing substantial performance improvements over baseline approaches. This work provides a comprehensive and scalable solution for enhancing the safety and reliability of autonomous systems operating in uncertain environments.

Paper number 83:
Title: On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers
Authors: Miroslav Štrupl, Oleg Szehr, Francesco Faccio, Dylan R. Ashley, Rupesh Kumar Srivastava, Jürgen Schmidhuber
Abstract: This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.

Paper number 84:
Title: Variational integrators for optimal control of foldable drones
Authors: L. Colombo, J. Giribet, D. Martín de Diego
Abstract: Numerical methods that preserves geometric invariants of the system such as energy, momentum and symplectic form, are called geometric integrators. These include variational integrators as an important subclass of geometric integrators. The general idea for those variational integrators is to discretize Hamilton's principle rather than the equations of motion and as a consequence these methods preserves some of the invariants of the original system (symplecticity, symmetry, good behavior of energy,...). In this paper, we construct variational integrators for control-dependent Lagrangian systems on Lie groups. These integrators are derived via a discrete-time variational principle for discrete-time control-dependent reduced Lagrangians. We employ the variational integrator into optimal control problems for path planning of foldable unmanned aerial vehicles (UAVs). Simulation are shown to validate the performance of the geometric integrator.

Paper number 85:
Title: Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks
Authors: Zijiang Yan, Jianhua Pei, Hongda Wu, Hina Tabassum, Ping Wang
Abstract: This paper proposes a novel framework for real-time adaptive-bitrate video streaming by integrating latent diffusion models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional constant bitrate streaming (CBS) and adaptive bitrate streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While it keeps B-frames and P-frames as adjustment metadata to ensure efficient video reconstruction at the user side, the proposed framework is complemented with the most state-of-the-art denoising and video frame interpolation (VFI) techniques. These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.

Paper number 86:
Title: Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization
Authors: Naoki Saito, David Weber
Abstract: We propose a new method to extract discriminant and explainable features from a particular machine learning model, i.e., a combination of the scattering transform and the multiclass logistic regression. Although this model is well-known for its ability to learn various signal classes with high classification rate, it remains elusive to understand why it can generate such successful classification, mainly due to the nonlinearity of the scattering transform. In order to uncover the meaning of the scattering transform coefficients selected by the multiclass logistic regression (with the Lasso penalty), we adopt zeroth-order optimization algorithms to search an input pattern that maximizes the class probability of a class of interest given the learned model. In order to do so, it turns out that imposing sparsity and smoothness of input patterns is important. We demonstrate the effectiveness of our proposed method using a couple of synthetic time-series classification problems.

Paper number 87:
Title: UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control
Authors: Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi
Abstract: Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob's $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at this https URL.

Paper number 88:
Title: Large Language Model-based Nonnegative Matrix Factorization For Cardiorespiratory Sound Separation
Authors: Yasaman Torabi, Shahram Shirani, James P. Reilly
Abstract: This study represents the first integration of large language models (LLMs) with non-negative matrix factorization (NMF), marking a novel advancement in the source separation field. The LLM is employed in two unique ways: enhancing the separation results by providing detailed insights for disease prediction and operating in a feedback loop to optimize a fundamental frequency penalty added to the NMF cost function. We tested the algorithm on two datasets: 100 synthesized mixtures of real measurements, and 210 recordings of heart and lung sounds from a clinical manikin including both individual and mixed sounds, captured using a digital stethoscope. The approach consistently outperformed existing methods, demonstrating its potential to significantly enhance medical sound analysis for disease diagnostics.

Paper number 89:
Title: Multi-Agent Reinforcement Learning in Wireless Distributed Networks for 6G
Authors: Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Enyu Shi, Bokai Xu, Chau Yuen, Dusit Niyato, Mérouane Debbah, Shi Jin, Bo Ai, Xuemin (Sherman)Shen
Abstract: The introduction of intelligent interconnectivity between the physical and human worlds has attracted great attention for future sixth-generation (6G) networks, emphasizing massive capacity, ultra-low latency, and unparalleled reliability. Wireless distributed networks and multi-agent reinforcement learning (MARL), both of which have evolved from centralized paradigms, are two promising solutions for the great attention. Given their distinct capabilities, such as decentralization and collaborative mechanisms, integrating these two paradigms holds great promise for unleashing the full power of 6G, attracting significant research and development attention. This paper provides a comprehensive study on MARL-assisted wireless distributed networks for 6G. In particular, we introduce the basic mathematical background and evolution of wireless distributed networks and MARL, as well as demonstrate their interrelationships. Subsequently, we analyze different structures of wireless distributed networks from the perspectives of homogeneous and heterogeneous. Furthermore, we introduce the basic concepts of MARL and discuss two typical categories, including model-based and model-free. We then present critical challenges faced by MARL-assisted wireless distributed networks, providing important guidance and insights for actual implementation. We also explore an interplay between MARL-assisted wireless distributed networks and emerging techniques, such as information bottleneck and mirror learning, delivering in-depth analyses and application scenarios. Finally, we outline several compelling research directions for future MARL-assisted wireless distributed networks.

Paper number 90:
Title: DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains
Authors: Seunghyun Lee, I Made Aswin Nahrendra, Dongkyu Lee, Byeongho Yu, Minho Oh, Hyun Myung
Abstract: Recent advances in quadrupedal robots have demonstrated impressive agility and the ability to traverse diverse terrains. However, hardware issues, such as motor overheating or joint locking, may occur during long-distance walking or traversing through rough terrains leading to locomotion failures. Although several studies have proposed fault-tolerant control methods for quadrupedal robots, there are still challenges in traversing unstructured terrains. In this paper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that enables a quadrupedal robot to traverse complex environments even under joint failure conditions. DreamFLEX integrates an explicit failure estimation and modulation network that jointly estimates the robot's joint fault vector and utilizes this information to adapt the locomotion pattern to faulty conditions in real-time, enabling quadrupedal robots to maintain stability and performance in rough terrains. Experimental results demonstrate that DreamFLEX outperforms existing methods in both simulation and real-world scenarios, effectively managing hardware failures while maintaining robust locomotion performance.

Paper number 91:
Title: Stacked Intelligent Metasurface Enabled Near-Field Multiuser Beamfocusing in the Wave Domain
Authors: Xing Jia, Jiancheng An, Hao Liu, Lu Gan, Marco Di Renzo, Mérouane Debbah, Chau Yuen
Abstract: Intelligent surfaces represent a breakthrough technology capable of customizing the wireless channel cost-effectively. However, the existing works generally focus on planar wavefront, neglecting near-field spherical wavefront characteristics caused by large array aperture and high operation frequencies in the terahertz (THz). Additionally, the single-layer reconfigurable intelligent surface (RIS) lacks the signal processing ability to mitigate the computational complexity at the base station (BS). To address this issue, we introduce a novel stacked intelligent metasurfaces (SIM) comprised of an array of programmable metasurface layers. The SIM aims to substitute conventional digital baseband architecture to execute computing tasks with ultra-low processing delay, albeit with a reduced number of radio-frequency (RF) chains and low-resolution digital-to-analog converters. In this paper, we present a SIM-aided multiuser multiple-input single-output (MU-MISO) near-field system, where the SIM is integrated into the BS to perform beamfocusing in the wave domain and customize an end-to-end channel with minimized inter-user interference. Finally, the numerical results demonstrate that near-field communication achieves superior spatial gain over the far-field, and the SIM effectively suppresses inter-user interference as the wireless signals propagate through it.

Paper number 92:
Title: Room-scale magnetoquasistatic wireless power transfer using a cavity-based multimode resonator
Authors: Takuya Sasatani, Alanson P. Sample, Yoshihiro Kawahara
Abstract: Magnetoquasistatic wireless power transfer can be used to charge and power electronic devices such as smartphones and small home appliances. However, existing coil-based transmitters, which are composed of wire conductors, have a limited range. Here we show that multimode quasistatic cavity resonance can provide room-scale wireless power transfer. The approach uses multidirectional, widely distributed currents on conductive surfaces that are placed around the target volume. It generates multiple, mutually unique, three-dimensional magnetic field patterns, where each pattern is attributed to different eigenmodes of a single room-scale resonator. Using these modes together, a power delivery efficiency exceeding 37.1% can be achieved throughout a 3 m * 3 m * 2 m test room. With this approach, power exceeding 50 W could potentially be delivered to mobile receivers in accordance with safety guidelines.

Paper number 93:
Title: Suppressing Leakage Magnetic Field in Wireless Power Transfer using Halbach Array-Based Resonators
Authors: Yuichi Honjo, Cedric Caremel, Yoshihiro Kawahara, Takuya Sasatani
Abstract: Wireless power transfer has the potential to seamlessly power electronic systems, such as electric vehicles, industrial robots, and mobile devices. However, the leakage magnetic field is a critical bottleneck that limits the transferable power level, and heavy ferromagnetic shields are needed for transferring large amounts of power. In this paper, we propose a ferrite-less coil design that generates an asymmetric magnetic field pattern focused on one side of the resonator, which effectively reduces the leakage magnetic field. The key to enabling the asymmetric field pattern is a coil winding strategy inspired by the Halbach array, a permanent magnet arrangement, which is then tailored for wireless power using an evolutionary strategy algorithm. Numerical analyses and simulations demonstrated that the proposed coil structure delivers the same amount of power as spiral coils, while achieving an 86.6% reduction in magnetic field intensity at a plane located 75 mm away from the resonator pair and a power efficiency of 96.0%. We verified our approach by measuring the power efficiency and magnetic field intensity of a test wireless power system operating at 6.78 MHz. These findings indicate that our approach can efficiently deliver over 50 times more power without increasing magnetic field exposure, making it a promising solution for high-power wireless power transfer applications.

Paper number 94:
Title: Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local Detection and Static-to-Dynamic Planning
Authors: Hao Chen, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada
Abstract: Robotic grasping is facing a variety of real-world uncertainties caused by non-static object states, unknown object properties, and cluttered object arrangements. The difficulty of grasping increases with the presence of more uncertainties, where commonly used learning-based approaches struggle to perform consistently across varying conditions. In this study, we integrate the idea of similarity matching to tackle the challenge of grasping novel objects that are simultaneously in motion and densely cluttered using a single RGBD camera, where multiple uncertainties coexist. We achieve this by shifting visual detection from global to local states and operating grasp planning from static to dynamic scenes. Notably, we introduce optimization methods to enhance planning efficiency for this time-sensitive task. Our proposed system can adapt to various object types, arrangements and movement speeds without the need for extensive training, as demonstrated by real-world experiments.

Paper number 95:
Title: Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding
Authors: Xingjian Diao, Chunhui Zhang, Weiyi Wu, Zhongyu Ouyang, Peijun Qing, Ming Cheng, Soroush Vosoughi, Jiang Gui
Abstract: Multimodal foundation models (MFMs) have demonstrated significant success in tasks such as visual captioning, question answering, and image-text retrieval. However, these models face inherent limitations due to their finite internal capacity, which restricts their ability to process extended temporal sequences, a crucial requirement for comprehensive video and audio analysis. To overcome these challenges, we introduce a specialized cognitive module, temporal working memory (TWM), which aims to enhance the temporal modeling capabilities of MFMs. It selectively retains task-relevant information across temporal dimensions, ensuring that critical details are preserved throughout the processing of video and audio content. The TWM uses a query-guided attention approach to focus on the most informative multimodal segments within temporal sequences. By retaining only the most relevant content, TWM optimizes the use of the model's limited capacity, enhancing its temporal modeling ability. This plug-and-play module can be easily integrated into existing MFMs. With our TWM, nine state-of-the-art models exhibit significant performance improvements across tasks such as video captioning, question answering, and video-text retrieval. By enhancing temporal modeling, TWM extends the capability of MFMs to handle complex, time-sensitive data effectively. Our code is available at this https URL.

Paper number 96:
Title: An adaptive filter bank based neural network approach for time delay estimation and speech enhancement
Authors: Lu Ma
Abstract: Time delay estimation (TDE) plays a key role in acoustic echo cancellation (AEC) using adaptive filter method. Considerable residual echo will be left if estimation error arises. Here, in this paper, we proposed an adaptive filter bank based neural network approach where the delay is estimated by a bank of adaptive filters with overlapped time scope, and all the energy of filter weights are concatenated and feed to a classification network. The index with maximal probability is chosen as the estimated delay. Based on this TDE, an AEC scheme is designed using a neural network for residual echo and noise suppression, and the optimally-modified log-spectral amplitude (OMLSA) algorithm is adopted to make it robust. Also, a robust automatic gain control (AGC) scheme with spectrum smoothing method is designed to amplify speech segments. Performance evaluations reveal that higher performance can be achieved for our scheme.

Paper number 97:
Title: Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text Recognition
Authors: Chenyu Liu, Jinshui Hu, Baocai Yin, Jia Pan, Bing Yin, Jun Du, Qingfeng Liu
Abstract: Online Handwritten Text Recognition (OLHTR) has gained considerable attention for its diverse range of applications. Current approaches usually treat OLHTR as a sequence recognition task, employing either a single trajectory or image encoder, or multi-stream encoders, combined with a CTC or attention-based recognition decoder. However, these approaches face several drawbacks: 1) single encoders typically focus on either local trajectories or visual regions, lacking the ability to dynamically capture relevant global features in challenging cases; 2) multi-stream encoders, while more comprehensive, suffer from complex structures and increased inference costs. To tackle this, we propose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that learns multimodal features during training while maintaining a single-stream inference process. Col-OLHTR consists of a trajectory encoder, a Point-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The P2SA module is designed to learn image-level spatial features through trajectory-encoded features and 2D rotary position embeddings. During training, an additional image-stream encoder-decoder is collaboratively trained to provide supervision for P2SA features. At inference, the extra streams are discarded, and only the P2SA module is used and merged before the decoder, simplifying the process while preserving high performance. Extensive experimental results on several OLHTR benchmarks demonstrate the state-of-the-art (SOTA) performance, proving the effectiveness and robustness of our design.

Paper number 98:
Title: Towards Bio-inspired Heuristically Accelerated Reinforcement Learning for Adaptive Underwater Multi-Agents Behaviour
Authors: Antoine Vivien, Thomas Chaffre, Matthew Stephenson, Eva Artusi, Paulo Santos, Benoit Clement, Karl Sammut
Abstract: This paper describes the problem of coordination of an autonomous Multi-Agent System which aims to solve the coverage planning problem in a complex environment. The considered applications are the detection and identification of objects of interest while covering an area. These tasks, which are highly relevant for space applications, are also of interest among various domains including the underwater context, which is the focus of this study. In this context, coverage planning is traditionally modelled as a Markov Decision Process where a coordinated MAS, a swarm of heterogeneous autonomous underwater vehicles, is required to survey an area and search for objects. This MDP is associated with several challenges: environment uncertainties, communication constraints, and an ensemble of hazards, including time-varying and unpredictable changes in the underwater environment. MARL algorithms can solve highly non-linear problems using deep neural networks and display great scalability against an increased number of agents. Nevertheless, most of the current results in the underwater domain are limited to simulation due to the high learning time of MARL algorithms. For this reason, a novel strategy is introduced to accelerate this convergence rate by incorporating biologically inspired heuristics to guide the policy during training. The PSO method, which is inspired by the behaviour of a group of animals, is selected as a heuristic. It allows the policy to explore the highest quality regions of the action and state spaces, from the beginning of the training, optimizing the exploration/exploitation trade-off. The resulting agent requires fewer interactions to reach optimal performance. The method is applied to the MSAC algorithm and evaluated for a 2D covering area mission in a continuous control environment.

Paper number 99:
Title: Token-Domain Multiple Access: Exploiting Semantic Orthogonality for Collision Mitigation
Authors: Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Deniz Gündüz
Abstract: Token communications is an emerging generative semantic communication concept that reduces transmission rates by using context and transformer-based token processing, with tokens serving as universal semantic units. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as ToDMA, where a large number of devices share a tokenizer and a modulation codebook for source and channel coding, respectively. Specifically, the source signal is tokenized into sequences, with each token modulated into a codeword. Codewords from multiple devices are transmitted simultaneously, resulting in overlap at the receiver. The receiver detects the transmitted tokens, assigns them to their respective sources, and mitigates token collisions by leveraging context and semantic orthogonality across the devices' messages. Simulations demonstrate that the proposed ToDMA framework outperforms context-unaware orthogonal and non-orthogonal communication methods in image transmission tasks, achieving lower latency and better image quality.

Paper number 100:
Title: Reward-Based Collision-Free Algorithm for Trajectory Planning of Autonomous Robots
Authors: Jose D. Hoyos, Tianyu Zhou, Zehui Lu, Shaoshuai Mou
Abstract: This paper introduces a new mission planning algorithm for autonomous robots that enables the reward-based selection of an optimal waypoint sequence from a predefined set. The algorithm computes a feasible trajectory and corresponding control inputs for a robot to navigate between waypoints while avoiding obstacles, maximizing the total reward, and adhering to constraints on state, input and its derivatives, mission time window, and maximum distance. This also solves a generalized prize-collecting traveling salesman problem. The proposed algorithm employs a new genetic algorithm that evolves solution candidates toward the optimal solution based on a fitness function and crossover. During fitness evaluation, a penalty method enforces constraints, and the differential flatness property with clothoid curves efficiently penalizes infeasible trajectories. The Euler spiral method showed promising results for trajectory parameterization compared to minimum snap and jerk polynomials. Due to the discrete exploration space, crossover is performed using a dynamic time-warping-based method and extended convex combination with projection. A mutation step enhances exploration. Results demonstrate the algorithm's ability to find the optimal waypoint sequence, fulfill constraints, avoid infeasible waypoints, and prioritize high-reward ones. Simulations and experiments with a ground vehicle, quadrotor, and quadruped are presented, complemented by benchmarking and a time-complexity analysis.

Paper number 101:
Title: Automatic Identification of Samples in Hip-Hop Music via Multi-Loss Training and an Artificial Dataset
Authors: Huw Cheston, Jan Van Balen, Simon Durand
Abstract: Sampling, the practice of reusing recorded music or sounds from another source in a new work, is common in popular music genres like hip-hop and rap. Numerous services have emerged that allow users to identify connections between samples and the songs that incorporate them, with the goal of enhancing music discovery. Designing a system that can perform the same task automatically is challenging, as samples are commonly altered with audio effects like pitch- and time-stretching and may only be seconds long. Progress on this task has been minimal and is further blocked by the limited availability of training data. Here, we show that a convolutional neural network trained on an artificial dataset can identify real-world samples in commercial hip-hop music. We extract vocal, harmonic, and percussive elements from several databases of non-commercial music recordings using audio source separation, and train the model to fingerprint a subset of these elements in transformed versions of the original audio. We optimize the model using a joint classification and metric learning loss and show that it achieves 13% greater precision on real-world instances of sampling than a fingerprinting system using acoustic landmarks, and that it can recognize samples that have been both pitch shifted and time stretched. We also show that, for half of the commercial music recordings we tested, our model is capable of locating the position of a sample to within five seconds.

Paper number 102:
Title: Predictive Red Teaming: Breaking Policies Without Breaking Robots
Authors: Anirudha Majumdar, Mohit Sharma, Dmitry Kalashnikov, Sumeet Singh, Pierre Sermanet, Vikas Sindhwani
Abstract: Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x.

Paper number 103:
Title: Joint parameter and state estimation for regularized time-discrete multibody dynamics
Authors: Hannes Marklund, Martin Servin, Mats G Larson
Abstract: We develop a method for offline parameter estimation of discrete multibody dynamics with regularized and frictional kinematic constraints. This setting leads to unobserved degrees of freedom, which we handle using joint state and parameter estimation. Our method finds the states and parameters as the solution to a nonlinear least squares optimization problem based on the inverse dynamics and the observation error. The solution is found using a Levenberg-Marquardt algorithm with derivatives from automatic differentiation and custom differentiation rules for the complementary conditions that appear due to dry frictional constraints. We reduce the number of method parameters to the choice of the time-step, regularization coefficients, and a parameter that controls the relative weighting of inverse dynamics and observation errors. We evaluate the method using synthetic and real measured data, focusing on performance and sensitivity to method parameters. In particular, we optimize over a 13-dimensional parameter space, including inertial, frictional, tilt, and motor parameters, using data from a real Furuta pendulum. Results show fast convergence, in the order of seconds, and good agreement for different time-series of recorded data over multiple method parameter choices. However, very stiff constraints may cause difficulties in solving the optimization problem. We conclude that our method can be very fast and has method parameters that are robust and easy to set in the tested scenarios.

Paper number 104:
Title: Multi-Scale Feature Fusion with Image-Driven Spatial Integration for Left Atrium Segmentation from Cardiac MRI Images
Authors: Bipasha Kundu, Zixin Yang, Richard Simon, Cristian Linte
Abstract: Accurate segmentation of the left atrium (LA) from late gadolinium-enhanced magnetic resonance imaging plays a vital role in visualizing diseased atrial structures, enabling the diagnosis and management of cardiovascular diseases. It is particularly essential for planning treatment with ablation therapy, a key intervention for atrial fibrillation (AF). However, manual segmentation is time-intensive and prone to inter-observer variability, underscoring the need for automated solutions. Class-agnostic foundation models like DINOv2 have demonstrated remarkable feature extraction capabilities in vision tasks. However, their lack of domain specificity and task-specific adaptation can reduce spatial resolution during feature extraction, impacting the capture of fine anatomical detail in medical imaging. To address this limitation, we propose a segmentation framework that integrates DINOv2 as an encoder with a UNet-style decoder, incorporating multi-scale feature fusion and input image integration to enhance segmentation accuracy. The learnable weighting mechanism dynamically prioritizes hierarchical features from different encoder blocks of the foundation model, optimizing feature selection for task relevance. Additionally, the input image is reintroduced during the decoding stage to preserve high-resolution spatial details, addressing limitations of downsampling in the encoder. We validate our approach on the LAScarQS 2022 dataset and demonstrate improved performance with a 92.3% Dice and 84.1% IoU score for giant architecture compared to the nnUNet baseline model. These findings emphasize the efficacy of our approach in advancing the field of automated left atrium segmentation from cardiac MRI.

Paper number 105:
Title: Enhancing healthcare infrastructure resilience through agent-based simulation methods
Authors: David Carramiñana, Ana M. Bernardos, Juan A. Besada, José R. Casar
Abstract: Critical infrastructures face demanding challenges due to natural and human-generated threats, such as pandemics, workforce shortages or cyber-attacks, which might severely compromise service quality. To improve system resilience, decision-makers would need intelligent tools for quick and efficient resource allocation. This article explores an agent-based simulation model that intends to capture a part of the complexity of critical infrastructure systems, particularly considering the interdependencies of healthcare systems with information and telecommunication systems. Such a model enables to implement a simulation-based optimization approach in which the exposure of critical systems to risks is evaluated, while comparing the mitigation effects of multiple tactical and strategical decision alternatives to enhance their resilience. The proposed model is designed to be parameterizable, to enable adapting it to risk scenarios with different severity, and it facilitates the compilation of relevant performance indicators enabling monitoring at both agent level and system level. To validate the agent-based model, a literature-supported methodology has been used to perform cross-validation, sensitivity analysis and test the usefulness of the proposed model through a use case. The use case analyzes the impact of a concurrent pandemic and a cyber-attack on a hospital and compares different resiliency-enhancing countermeasures using contingency tables. Overall, the use case illustrates the feasibility and versatility of the proposed approach to enhance resiliency.

Paper number 106:
Title: Koopman-Equivariant Gaussian Processes
Authors: Petar Bevanda, Max Beier, Armin Lederer, Alexandre Capone, Stefan Sosnowski, Sandra Hirche
Abstract: Credible forecasting and representation learning of dynamical systems are of ever-increasing importance for reliable decision-making. To that end, we propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and representational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance -- which we refer to as \textit{Koopman equivariance} -- we obtain a GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems.

Paper number 107:
Title: Heisenberg-limited calibration of entangling gates with robust phase estimation
Authors: Kenneth Rudinger, J. P. Marceaux, Akel Hashim, David I. Santiago, Irfan Siddiqi, Kevin C. Young
Abstract: The calibration of high-quality two-qubit entangling gates is an essential component in engineering large-scale, fault-tolerant quantum computers. However, many standard calibration techniques are based on randomized circuits that are only quadratically sensitive to calibration errors. As a result, these approaches are inefficient, requiring many experimental shots to achieve acceptable performance. In this work, we demonstrate that robust phase estimation can enable high-precision, Heisenberg-limited estimates of coherent errors in multi-qubit gates. Equipped with an efficient estimator, the calibration problem may be reduced to a simple optimization loop that minimizes the estimated coherent error. We experimentally demonstrate our calibration protocols by improving the operation of a two-qubit controlled-Z gate on a superconducting processor, and we validate the improved performance with gate set tomography. Our methods are applicable to gates in other quantum hardware platforms such as ion traps and neutral atoms, and on other multi-qubit gates, such as CNOT or iSWAP.

Paper number 108:
Title: Learning Musical Representations for Music Performance Question Answering
Authors: Xingjian Diao, Chunhui Zhang, Tingxuan Wu, Ming Cheng, Zhongyu Ouyang, Weiyi Wu, Jiang Gui
Abstract: Music performances are representative scenarios for audio-visual modeling. Unlike common scenarios with sparse audio, music performances continuously involve dense audio signals throughout. While existing multimodal learning methods on the audio-video QA demonstrate impressive capabilities in general scenarios, they are incapable of dealing with fundamental problems within the music performances: they underexplore the interaction between the multimodal signals in performance and fail to consider the distinctive characteristics of instruments and music. Therefore, existing methods tend to answer questions regarding musical performances inaccurately. To bridge the above research gaps, (i) given the intricate multimodal interconnectivity inherent to music data, our primary backbone is designed to incorporate multimodal interactions within the context of music; (ii) to enable the model to learn music characteristics, we annotate and release rhythmic and music sources in the current music datasets; (iii) for time-aware audio-visual modeling, we align the model's music predictions with the temporal dimension. Our experiments show state-of-the-art effects on the Music AVQA datasets. Our code is available at this https URL.

Paper number 109:
Title: Rough Stochastic Pontryagin Maximum Principle and an Indirect Shooting Method
Authors: Thomas Lew
Abstract: We derive first-order Pontryagin optimality conditions for stochastic optimal control with deterministic controls for systems modeled by rough differential equations (RDE) driven by Gaussian rough paths. This Pontryagin Maximum Principle (PMP) applies to systems following stochastic differential equations (SDE) driven by Brownian motion, yet it does not rely on forward-backward SDEs and involves the same Hamiltonian as the deterministic PMP. The proof consists of first deriving various integrable error bounds for solutions to nonlinear and linear RDEs by leveraging recent results on Gaussian rough paths. The PMP then follows using standard techniques based on needle-like variations. As an application, we propose the first indirect shooting method for nonlinear stochastic optimal control and show that it converges 10x faster than a direct method on a stabilization task.

Paper number 110:
Title: Parameter-Dependent Control Lyapunov Functions for Stabilizing Nonlinear Parameter-Varying Systems
Authors: Pan Zhao
Abstract: This paper introduces the concept of parameter-dependent (PD) control Lyapunov functions (CLFs) for gainscheduled stabilization of nonlinear parameter-varying (NPV) systems under input constraints. It shows that given a PD-CLF, a locally Lipschitz control law can be constructed by solving a robust quadratic program. For polynomial control-affine NPV systems, it provides convex conditions, based on the sum of squares (SOS) programming, to jointly synthesize both a PD-CLF and a PD controller, aimed at maximizing the PD region of stabilization. Input constraints can be straightforwardly incorporated into the synthesis procedure. Unlike traditional linear parameter-varying (LPV) methods that rely on linearization or over-approximation to get an LPV model, the proposed framework fully captures the nonlinearities of the system dynamics. Simulation results validate the efficacy of the method, showcasing its potential for stabilizing NPV systems under input constraints.

Paper number 111:
Title: Label-free segmentation from cardiac ultrasound using self-supervised learning
Authors: Danielle L. Ferreira, Zaynaf Salaymang, Rima Arnaout
Abstract: Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compared to clinical measurements. A subset of test echocardiograms (n=553) had corresponding cardiac MRIs, where MRI is the gold standard. Correlation between pipeline and MRI measurements was similar to that between clinical echocardiogram and MRI. Finally, the pipeline accurately segments the left ventricle with an average Dice score of 0.89 (95% CI [0.89]) in the external, manually labeled dataset. Our results demonstrate a manual-label free, clinically valid, and highly scalable method for segmentation from ultrasound, a noisy but globally important imaging modality.

Paper number 112:
Title: Platelet Inventory Management with Approximate Dynamic Programming
Authors: Hossein Abouee-Mehrizi, Mahdi Mirjalili, Vahid Sarhangian
Abstract: We study a stochastic perishable inventory control problem with endogenous (decision-dependent) uncertainty in shelf-life of units. Our primary motivation is determining ordering policies for blood platelets. Determining optimal ordering quantities is a challenging task due to the short maximum shelf-life of platelets (3-5 days after testing) and high uncertainty in daily demand. We formulate the problem as an infinite-horizon discounted Markov Decision Process (MDP). The model captures salient features observed in our data from a network of Canadian hospitals and allows for fixed ordering costs. We show that with uncertainty in shelf-life, the value function of the MDP is non-convex and key structural properties valid under deterministic shelf-life no longer hold. Hence, we propose an Approximate Dynamic Programming (ADP) algorithm to find approximate policies. We approximate the value function using a linear combination of basis functions and tune the parameters using a simulation-based policy iteration algorithm. We evaluate the performance of the proposed policy using extensive numerical experiments in parameter regimes relevant to the platelet inventory management problem. We further leverage the ADP algorithm to evaluate the impact of ignoring shelf-life uncertainty. Finally, we evaluate the out-of-sample performance of the ADP algorithm in a case study using real data and compare it to the historical hospital performance and other benchmarks. The ADP policy can be computed online in a few minutes and results in more than 50% lower expiry and shortage rates compared to the historical performance. In addition, it performs better or as well as an exact policy that ignores uncertainty in shelf-life and becomes hard to compute for larger instance of the problem.

Paper number 113:
Title: Quantized distributed Nash equilibrium seeking under DoS attacks
Authors: Shuai Feng, Maojiao Ye, Lihua Xie, Shengyuan Xu
Abstract: This paper studies distributed Nash equilibrium (NE) seeking under Denial-of-Service (DoS) attacks and quantization. The players can only exchange information with their own direct neighbors. The transmitted information is subject to quantization and packet losses induced by malicious DoS attacks. We propose a quantized distributed NE seeking strategy based on the approach of dynamic quantized consensus. To solve the quantizer saturation problem caused by DoS attacks, the quantization mechanism is equipped to have zooming-in and holding capabilities, in which the holding capability is consistent with the results in quantized consensus under DoS. A sufficient condition on the number of quantizer levels is provided, under which the quantizers are free from saturation under DoS attacks. The proposed distributed quantized NE seeking strategy is shown to have the so-called maximum resilience to DoS attacks. Namely, if the bound characterizing the maximum resilience is violated, an attacker can deny all the transmissions and hence distributed NE seeking is impossible.

Paper number 114:
Title: Multispectral Indices for Wildfire Management
Authors: Afonso Oliveira, João P. Matos-Carvalho, Filipe Moutinho, Nuno Fachada
Abstract: The increasing frequency and severity of wildfires requires advanced methods for effective surveillance and management. Traditional ground-based observation techniques often struggle to adapt to rapidly changing fire behavior and environmental conditions. This paper examines the application of multispectral aerial and satellite imagery in wildfire management, emphasizing the identification and analysis of key factors influencing wildfire behavior, such as combustible vegetation and water features. Through a comprehensive review of current literature and the presentation of two practical case studies, we assess various multispectral indices and evaluate their effectiveness in extracting critical environmental attributes essential for wildfire prevention and management. Our case studies highlight several indices as particularly effective for segmentation and extraction: NVDI for vegetation, MNDWI for water features, and MSR for artificial structures. These indices significantly enhance wildfire data processing, thereby supporting improved monitoring and response strategies.

Paper number 115:
Title: Mixed Graph Signal Analysis of Joint Image Denoising / Interpolation
Authors: Niruhan Viswarupan, Gene Cheung, Fengbo Lan, Michael Brown
Abstract: A noise-corrupted image often requires interpolation. Given a linear denoiser and a linear interpolator, when should the operations be independently executed in separate steps, and when should they be combined and jointly optimized? We study joint denoising / interpolation of images from a mixed graph filtering perspective: we model denoising using an undirected graph, and interpolation using a directed graph. We first prove that, under mild conditions, a linear denoiser is a solution graph filter to a maximum a posteriori (MAP) problem regularized using an undirected graph smoothness prior, while a linear interpolator is a solution to a MAP problem regularized using a directed graph smoothness prior. Next, we study two variants of the joint interpolation / denoising problem: a graph-based denoiser followed by an interpolator has an optimal separable solution, while an interpolator followed by a denoiser has an optimal non-separable solution. Experiments show that our joint denoising / interpolation method outperformed separate approaches noticeably.

Paper number 116:
Title: Real-time Estimation of DoS Duration and Frequency for Security Control
Authors: Yifan Sun, Jianquan Lu, Daniel W. C. Ho, Lulu Li
Abstract: In this paper, we develop a new denial-of-service (DoS) estimator, enabling defenders to identify duration and frequency parameters of any DoS attacker, except for three edge cases, exclusively using real-time data. The key advantage of the estimator lies in its capability to facilitate security control in a wide range of practical scenarios, even when the attacker's information is previously unknown. We demonstrate the advantage and application of our new estimator in the context of two classical control scenarios, namely consensus of multi-agent systems and impulsive stabilization of nonlinear systems, for illustration.

Paper number 117:
Title: 4D-ONIX: A deep learning approach for reconstructing 3D movies from sparse X-ray projections
Authors: Yuhe Zhang, Zisheng Yao, Robert Klöfkorn, Tobias Ritschel, Pablo Villanueva-Perez
Abstract: The X-ray flux provided by X-ray free-electron lasers and storage rings offers new spatiotemporal possibilities to study in-situ and operando dynamics, even using single pulses of such facilities. X-ray Multi-Projection Imaging (XMPI) is a novel technique that enables volumetric information using single pulses of such facilities and avoids centrifugal forces induced by state-of-the-art time-resolved 3D methods such as time-resolved tomography. As a result, XMPI offers the potential to acquire 3D movies (4D) at least three orders of magnitude faster than current methods. However, it is exceptionally challenging to reconstruct 4D from highly sparse projections as acquired by XMPI with current algorithms. Here, we present 4D-ONIX, a Deep Learning (DL)-based approach that learns to reconstruct 3D movies (4D) from an extremely limited number of projections. It combines the computational physical model of X-ray interaction with matter and state-of-the-art DL methods. We demonstrate the potential of 4D-ONIX to generate high-quality 4D by generalizing over multiple experiments with only two to three projections per timestamp for binary droplet collisions and additive manufacturing. We envision that 4D-ONIX will become an enabling tool for 4D analysis, offering new spatiotemporal resolutions to study processes not possible before.

Paper number 118:
Title: Infinite-horizon optimal scheduling for feedback control
Authors: Siyi Wang, Sandra Hirche
Abstract: Emerging cyber-physical systems impel the development of communication protocols that optimize resource utilization. This article investigates infinite-horizon optimal scheduling for resource-aware networked control systems by addressing the rate-regulation tradeoff. Consider a scenario where the sensor and the controller communicate via a networked channel, the transmission scheduling problem is formulated as a Markov decision process on unbounded general state space controlled by scheduling decisions. The value of information (VoI) serves as a metric to assess the importance of sensory data for transmission. We derive the optimal scheduling law for feedback control based on VoI and show that it is deterministic and stationary, with an explicit expression obtained via value iteration. The closed-loop system under the designed scheduling law is shown to be stochastically stable. By analyzing the dynamic behavior of the iteration process, we show that the VoI function and the optimal scheduling law exhibit symmetry. Furthermore, when the system matrix is diagonalizable, the VoI function is monotone and quasi-convex. Consequently, the optimal scheduling law is shown to exhibit a threshold structure and takes a quadratic form, with the threshold region explicitly characterized. Finally, the numerical simulation illustrates the theoretical result of the VoI-based scheduling.

Paper number 119:
Title: CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image Compression
Authors: Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code is available at this https URL.

Paper number 120:
Title: Endmember Extraction from Hyperspectral Images Using Self-Dictionary Approach with Linear Programming
Authors: Tomohiko Mizutani
Abstract: Hyperspectral imaging technology has a wide range of applications, including forest management, mineral resource exploration, and Earth surface monitoring. A key step in utilizing this technology is endmember extraction, which aims to identify the spectral signatures of materials in observed scenes. Theoretical studies suggest that self-dictionary methods using linear programming (LP), known as Hottopixx methods, are effective in extracting endmembers. However, their practical application is hindered by high computational costs, as they require solving LP problems whose size grows quadratically with the number of pixels in the image. As a result, their actual effectiveness remains unclear. To address this issue, we propose an enhanced implementation of Hottopixx designed to reduce computational time and improve endmember extraction performance. We demonstrate its effectiveness through experiments. The results suggest that our implementation enables the application of Hottopixx for endmember extraction from real hyperspectral images and allows us to achieve reasonably high accuracy in estimating endmember signatures.

Paper number 121:
Title: Weakly-Supervised PET Anomaly Detection using Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center, Multi-Cancer, and Multi-Tracer Study
Authors: Shadab Ahamed, Arman Rahmim
Abstract: Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current un-/weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks trained only on healthy data; however GAN-based networks are more challenging to train due to issues with simultaneous optimization of two competing networks, mode collapse, etc. In this paper, we present the weakly-supervised Implicitly guided COuNterfactual diffusion model for Detecting Anomalies in PET images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases containing both local and public datasets. The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates "unhealthy-to-healthy" domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUVmax thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: this https URL.

Paper number 122:
Title: Magnetic Resonance Image Processing Transformer for General Accelerated Image Reconstruction
Authors: Guoyao Shen, Mengyu Li, Stephan Anderson, Chad W. Farris, Xin Zhang
Abstract: Recent advancements in deep learning have enabled the development of generalizable models that achieve state-of-the-art performance across various imaging tasks. Vision Transformer (ViT)-based architectures, in particular, have demonstrated strong feature extraction capabilities when pre-trained on large-scale datasets. In this work, we introduce the Magnetic Resonance Image Processing Transformer (MR-IPT), a ViT-based framework designed to enhance the generalizability and robustness of accelerated MRI reconstruction. Unlike conventional deep learning models that require separate training for different acceleration factors, MR-IPT is pre-trained on a large-scale dataset encompassing multiple undersampling patterns and acceleration settings, enabling a unified reconstruction framework. By leveraging a shared transformer backbone, MR-IPT effectively learns universal feature representations, allowing it to generalize across diverse reconstruction tasks. Extensive experiments demonstrate that MR-IPT outperforms both CNN-based and existing transformer-based methods, achieving superior reconstruction quality across varying acceleration factors and sampling masks. Moreover, MR-IPT exhibits strong robustness, maintaining high performance even under unseen acquisition setups, highlighting its potential as a scalable and efficient solution for accelerated MRI. Our findings suggest that transformer-based general models can significantly advance MRI reconstruction, offering improved adaptability and stability compared to traditional deep learning approaches.

Paper number 123:
Title: Competitive Perimeter Defense in Tree Environments
Authors: Richard L. Frost, Shaunak D. Bopardikar
Abstract: We consider a perimeter defense problem in a rooted full tree graph environment in which a single defending vehicle seeks to defend a set of specified vertices, termed as the perimeter from mobile intruders that enter the environment through the tree's leaves. We adopt the technique of competitive analysis to characterize the performance of an online algorithm for the defending vehicle. We first derive fundamental limits on the performance of any online algorithm relative to that of an optimal offline algorithm. Specifically, we give three fundamental conditions for finite, 2, and 3/2 competitive ratios in terms of the environment parameters. We then design and analyze three classes of online algorithms that have provably finite competitiveness under varying environmental parameter regimes. Finally, we give a numerical visualization of these regimes to better show the comparative strengths and weaknesses of each algorithm.

Paper number 124:
Title: Joint 9D Receiver Localization and Ephemeris Correction with LEO and $5$G Base Stations
Authors: Don-Roberts Emenonye, Harpreet S. Dhillon, R. Michael Buehrer
Abstract: In this paper, we use the Fisher information matrix (FIM) to analyze the interaction between low-earth orbit (LEO) satellites and $5$G base stations in providing $9$D receiver localization and correcting LEO ephemeris. First, we give a channel model that captures all the information in the LEO-receiver, LEO-BS, and BS-receiver links. Subsequently, we use FIM to capture the amount of information about the channel parameters in these links. Then, we transform these FIM for channel parameters to the FIM for the $9$D ($3$D position, $3$D orientation, and $3$D velocity estimation) receiver localization parameters and the LEO position and velocity offset. Closed-form expressions for the entries in the FIM for these location parameters are presented. Our results on identifiability utilizing the FIM for the location parameters indicate: i) with one LEO, we need three BSs and three time slots to both estimate the $9$D location parameters and correct the LEO position and velocity, ii) with two LEO, we need three BSs and three time slots to both estimate the $9$D location parameters and correct the LEO position and velocity, and iii) with three LEO, we need three BSs and four-time slots to both estimate the $9$D location parameters and correct the LEO position and velocity. Another key insight is that through the Cramer Rao lower bound we show that with a single LEO, three time slots, and three BSs, the receiver positioning error, velocity estimation error, orientation error, LEO position offset estimation error, and LEO velocity offset estimation error are $0.1 \text{ cm}$, $1 \text{ mm/s}$, $10^{-3} \text{ rad}$, $0.01 \text{ m}$, and $1 \text{ m/s}$, respectively.

Paper number 125:
Title: CISCA and CytoDArk0: a Cell Instance Segmentation and Classification method for histo(patho)logical image Analyses and a new, open, Nissl-stained dataset for brain cytoarchitecture studies
Authors: Valentina Vadori, Jean-Marie Graïc, Antonella Peruffo, Giulia Vadori, Livio Finos, Enrico Grisan
Abstract: Delineating and classifying individual cells in microscopy tissue images is inherently challenging yet remains essential for advancements in medical and neuroscientific research. In this work, we propose a new deep learning framework, CISCA, for automatic cell instance segmentation and classification in histological slices. At the core of CISCA is a network architecture featuring a lightweight U-Net with three heads in the decoder. The first head classifies pixels into boundaries between neighboring cells, cell bodies, and background, while the second head regresses four distance maps along four directions. The outputs from the first and second heads are integrated through a tailored post-processing step, which ultimately produces the segmentation of individual cells. The third head enables the simultaneous classification of cells into relevant classes, if required. We demonstrate the effectiveness of our method using four datasets, including CoNIC, PanNuke, and MoNuSeg, which are publicly available H&Estained datasets that cover diverse tissue types and magnifications. In addition, we introduce CytoDArk0, the first annotated dataset of Nissl-stained histological images of the mammalian brain, containing nearly 40k annotated neurons and glia cells, aimed at facilitating advancements in digital neuropathology and brain cytoarchitecture studies. We evaluate CISCA against other state-of-the-art methods, demonstrating its versatility, robustness, and accuracy in segmenting and classifying cells across diverse tissue types, magnifications, and staining techniques. This makes CISCA well-suited for detailed analyses of cell morphology and efficient cell counting in both digital pathology workflows and brain cytoarchitecture research.

Paper number 126:
Title: Wavelet GPT: Wavelet Inspired Large Language Models
Authors: Prateek Verma
Abstract: Large Language Models (LLMs) have ushered in a new wave of artificial intelligence advancements impacting every scientific field and discipline. We live in a world where most of the data around us, e.g., text, audio, and music, has a multi-scale structure. This paper infuses LLMs with a traditional signal processing idea, namely wavelets, during pre-training to take advantage of the structure. Without adding \textbf{any extra parameters} to a GPT-style LLM architecture in an academic setup, we achieve the same pre-training performance almost twice as fast in text, audio, and images. This is done by imposing a structure on intermediate embeddings. When trained for the same number of training steps, we achieve significant gains in performance, which is comparable to pre-training a larger neural architecture. Further, we show this extends to the Long Range Arena benchmark and several input representations such as characters, BPE tokens, bytes, waveform, math expression, and image pixels. Our architecture allows every next token prediction access to intermediate embeddings at different temporal resolutions in every decoder block. We hope this will pave the way for incorporating multi-rate signal processing into pre-training.

Paper number 127:
Title: An alternative Approach in Voice Extraction
Authors: The Hieu Pham, Phuong Thanh Tran Nguyen, Xuan Tho Nguyen, Tan Dat Nguyen, Duc Dung Nguyen
Abstract: The research on audio clue-based target speaker extraction (TSE) has mostly focused on modeling the mixture and reference speech, achieving high performance in English due to the availability of large datasets. However, less attention has been given to the consistent properties of human speech across languages. To bridge this gap, we introduce an alternative model which addresses the challenge of transferring TSE models from one language to another without fine-tuning. In this work, we proposed a gating mechanism that is able to modify specific frequencies based on the speaker's acoustic features. The model achieves an SI-SDR of 17.3544 on clean English speech and 13.2032 on clean speech mixed with Wham! noise, outperforming all other models in its ability to adapt to different languages.

Paper number 128:
Title: Arrhythmia Classification Using Graph Neural Networks Based on Correlation Matrix
Authors: Seungwoo Han
Abstract: With the advancements in graph neural network, there has been increasing interest in applying this network to ECG signal analysis. In this study, we generated an adjacency matrix using correlation matrix of extracted features and applied a graph neural network to classify arrhythmias. The proposed model was compared with existing approaches from the literature. The results demonstrated that precision and recall for all arrhythmia classes exceeded 50%, suggesting that this method can be considered an approach for arrhythmia classification.

Paper number 129:
Title: Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding
Authors: Bohan Li, Hankun Wang, Situo Zhang, Yiwei Guo, Kai Yu
Abstract: The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.

Paper number 130:
Title: TACO: Training-free Sound Prompted Segmentation via Semantically Constrained Audio-visual CO-factorization
Authors: Hugo Malard, Michel Olvera, Stephane Lathuiliere, Slim Essid
Abstract: Large-scale pre-trained audio and image models demonstrate an unprecedented degree of generalization, making them suitable for a wide range of applications. Here, we tackle the specific task of sound-prompted segmentation, aiming to segment image regions corresponding to objects heard in an audio signal. Most existing approaches tackle this problem by fine-tuning pre-trained models or by training additional modules specifically for the task. We adopt a different strategy: we introduce a training-free approach that leverages Non-negative Matrix Factorization (NMF) to co-factorize audio and visual features from pre-trained models so as to reveal shared interpretable concepts. These concepts are passed on to an open-vocabulary segmentation model for precise segmentation maps. By using frozen pre-trained models, our method achieves high generalization and establishes state-of-the-art performance in unsupervised sound-prompted segmentation, significantly surpassing previous unsupervised methods.

Paper number 131:
Title: Adaptive Economic Model Predictive Control: Performance Guarantees for Nonlinear Systems
Authors: Maximilian Degner, Raffaele Soloperto, Melanie N. Zeilinger, John Lygeros, Johannes Köhler
Abstract: We consider the problem of optimizing the economic performance of nonlinear constrained systems subject to uncertain time-varying parameters and bounded disturbances. In particular, we propose an adaptive economic model predictive control (MPC) framework that: (i) directly minimizes transient economic costs, (ii) addresses parametric uncertainty through online model adaptation, (iii) determines optimal setpoints online, and (iv) ensures robustness by using a tube-based approach. The proposed design ensures recursive feasibility, robust constraint satisfaction, and a transient performance bound. In case the disturbances have a finite energy and the parameter variations have a finite path length, the asymptotic average performance is (approximately) not worse than the performance obtained when operating at the best reachable steady-state. We highlight performance benefits in a numerical example involving a chemical reactor with unknown time-invariant and time-varying parameters.

Paper number 132:
Title: Canine EEG Helps Human: Cross-Species and Cross-Modality Epileptic Seizure Detection via Multi-Space Alignment
Authors: Z. Wang, S. Li, Dongrui Wu
Abstract: Epilepsy significantly impacts global health, affecting about 65 million people worldwide, along with various animal species. The diagnostic processes of epilepsy are often hindered by the transient and unpredictable nature of seizures. Here we propose a multi-space alignment approach based on cross-species and cross-modality electroencephalogram (EEG) data to enhance the detection capabilities and understanding of epileptic seizures. By employing deep learning techniques, including domain adaptation and knowledge distillation, our framework aligns cross-species and cross-modality EEG signals to enhance the detection capability beyond traditional within-species and with-modality models. Experiments on multiple surface and intracranial EEG datasets of humans and canines demonstrated substantial improvements in the detection accuracy, achieving over 90% AUC scores for cross-species and cross-modality seizure detection with extremely limited labeled data from the target species/modality. To our knowledge, this is the first study that demonstrates the effectiveness of integrating heterogeneous data from different species and modalities to improve EEG-based seizure detection performance. The approach may also be generalizable to different brain-computer interface paradigms, and suggests the possibility to combine data from different species/modalities to increase the amount of training data for large EEG models.

Paper number 133:
Title: ELFATT: Efficient Linear Fast Attention for Vision Transformers
Authors: Chong Wu, Maolin Che, Renjie Xu, Zhuoheng Ran, Hong Yan
Abstract: The attention mechanism is the key to the success of transformers in different machine learning tasks. However, the quadratic complexity with respect to the sequence length of the vanilla softmax-based attention mechanism becomes the major bottleneck for the application of long sequence tasks, such as vision tasks. Although various efficient linear attention mechanisms have been proposed, they need to sacrifice performance to achieve high efficiency. What's more, memory-efficient methods, such as FlashAttention-1-3, still have quadratic computation complexity which can be further improved. In this paper, we propose a novel efficient linear fast attention (ELFATT) mechanism to achieve low memory input/output operations, linear computational complexity, and high performance at the same time. ELFATT offers 4-7x speedups over the vanilla softmax-based attention mechanism in high-resolution vision tasks without losing performance. ELFATT is FlashAttention friendly. Using FlashAttention-2 acceleration, ELFATT still offers 2-3x speedups over the vanilla softmax-based attention mechanism on high-resolution vision tasks without losing performance. Even on edge GPUs, ELFATT still offers 1.6x to 2.0x speedups compared to state-of-the-art attention mechanisms in various power modes from 5W to 60W. Furthermore, ELFATT can be used to enhance and accelerate diffusion tasks directly without training.

Paper number 134:
Title: Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by Level of Detail (LOD) Theory
Authors: BG Tong
Abstract: Objective: This study explores a novel deep learning approach for EEG analysis and perceptual state guidance, inspired by Level of Detail (LOD) theory. The goal is to improve perceptual state identification accuracy and advance personalized psychological therapy. Methods: Portable EEG devices and music rhythm signals were used for data collection. LOD theory was applied to dynamically adjust EEG signal processing, extracting core perceptual features. A Unity-based software system integrated EEG data with audio materials. The deep learning model combined a CNN for feature extraction and classification, and a DQN for reinforcement learning to optimize rhythm adjustments. Results: The CNN achieved 94.05% accuracy in perceptual state classification. The DQN guided subjects to target states with a 92.45% success rate, averaging 13.2 rhythm cycles. However, only 50% of users reported psychological alignment with the target state, indicating room for improvement. Discussion: The results validate the potential of LOD-based EEG biofeedback. Limitations include dataset source, label subjectivity, and reward function optimization. Future work will expand to diverse subjects, incorporate varied musical elements, and refine reward functions for better generalization and personalization.

Paper number 135:
Title: Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using Score Neural Networks
Authors: Hai Victor Habi, Hagit Messer, Yoram Bresler
Abstract: The Bayesian Cramér-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cramér-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise.

Paper number 136:
Title: Bilinear Subspace Variational Bayesian Inference for Joint Scattering Environment Sensing and Data Recovery in ISAC Systems
Authors: An Liu, Wenkang Xu, Wei Xu, Giuseppe Caire
Abstract: This paper considers a joint scattering environment sensing and data recovery problem in an uplink integrated sensing and communication (ISAC) system. To facilitate joint scatterers localization and multi-user (MU) channel estimation, we introduce a three-dimensional (3D) location-domain sparse channel model to capture the joint sparsity of the MU channel (i.e., different user channels share partially overlapped scatterers). Then the joint problem is formulated as a bilinear structured sparse recovery problem with a dynamic position grid and imperfect parameters (such as time offset and user position errors). We propose an expectation maximization based turbo bilinear subspace variational Bayesian inference (EM-Turbo-BiSVBI) algorithm to solve the problem effectively, where the E-step performs Bayesian estimation of the the location-domain sparse MU channel by exploiting the joint sparsity, and the M-step refines the dynamic position grid and learns the imperfect factors via gradient update. Two methods are introduced to greatly reduce the complexity with almost no sacrifice on the performance and convergence speed: 1) a subspace constrained bilinear variational Bayesian inference (VBI) method is proposed to avoid any high-dimensional matrix inverse; 2) the multiple signal classification (MUSIC) and subspace constrained VBI methods are combined to obtain a coarse estimation result to reduce the search range. Simulations verify the advantages of the proposed scheme over baseline schemes.

Paper number 137:
Title: Compressed Image Generation with Denoising Diffusion Codebook Models
Authors: Guy Ohayon, Hila Manor, Tomer Michaeli, Michael Elad
Abstract: We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered.

Paper number 138:
Title: Antenna Position Optimization for Movable Antenna-Empowered Near-Field Sensing
Authors: Yushen Wang, Weidong Mei, Xin Wei, Boyu Ning, Zhi Chen
Abstract: Movable antennas (MAs) show great promise for enhancing the sensing capabilities of future sixth-generation (6G) networks. With the growing prevalence of near-field propagation at ultra-high frequencies, this paper focuses on the application of MAs for near-field sensing to jointly estimate the angle and distance information of a target. First, to gain essential insights into MA-enhanced near-field sensing, we investigate two simplified cases with only the spatial angle-of-arrival (AoA) or distance estimation, respectively, assuming that the other information is already known. We derive the worst-case Cramer-Rao bounds (CRBs) on the mean square errors (MSEs) of the AoA estimation and the distance estimation via the multiple signal classification (MUSIC) algorithm in these two cases. Then, we jointly optimize the positions of the MAs within a linear array to minimize these CRBs and derive their closed-form solutions, which yield an identical array geometry to MA-aided far-field sensing. Furthermore, we proceed to the more challenging case with the joint AoA and distance estimation and derive the worst-case CRB under the two-dimensional (2D) MUSIC algorithm. The corresponding CRB minimization problem is efficiently solved by adopting a discrete sampling-based approach. Numerical results demonstrate that the proposed MA-enhanced near-field sensing significantly outperforms conventional sensing with fixed-position antennas (FPAs). Moreover, the joint angle and distance estimation results in a different array geometry from that in the individual estimation of angle or distance.

Paper number 139:
Title: A Lyapunov theory demonstrating a fundamental limit on the speed of systems consolidation
Authors: Alireza Alemi, Emre R. F. Aksay, Mark S. Goldman
Abstract: The nervous system reorganizes memories from an early site to a late site, a commonly observed feature of learning and memory systems known as systems consolidation. Previous work has suggested learning rules by which consolidation may occur. Here, we provide conditions under which such rules are guaranteed to lead to stable convergence of learning and consolidation. We use the theory of Lyapunov functions, which enforces stability by requiring learning rules to decrease an energy-like (Lyapunov) function. We present the theory in the context of a simple circuit architecture motivated by classic models of learning in systems consolidation mediated by the cerebellum. Stability is only guaranteed if the learning rate in the late stage is not faster than the learning rate in the early stage. Further, the slower the learning rate at the late stage, the larger the perturbation the system can tolerate with a guarantee of stability. We provide intuition for this result by mapping the consolidation model to a damped driven oscillator system, and showing that the ratio of early- to late-stage learning rates in the consolidation model can be directly identified with the (square of the) oscillator's damping ratio. This work suggests the power of the Lyapunov approach to provide constraints on nervous system function.

Paper number 140:
Title: FAST functional connectivity implicates P300 connectivity in working memory deficits in Alzheimer's disease
Authors: Om Roy, Yashar Moshfeghi, Agustin Ibanez, Francisco Lopera, Mario A Parra, Keith M Smith
Abstract: Measuring transient functional connectivity is an important challenge in Electroencephalogram (EEG) research. Here, the rich potential for insightful, discriminative information of brain activity offered by high temporal resolution is confounded by the inherent noise of the medium and the spurious nature of correlations computed over short temporal windows. We propose a novel methodology to overcome these problems called Filter Average Short-Term (FAST) functional connectivity. First, long-term, stable, functional connectivity is averaged across an entire study cohort for a given pair of Visual Short Term Memory (VSTM) tasks. The resulting average connectivity matrix, containing information on the strongest general connections for the tasks, is used as a filter to analyse the transient high temporal resolution functional connectivity of individual subjects. In simulations, we show that this method accurately discriminates differences in noisy Event-Related Potentials (ERPs) between two conditions where standard connectivity and other comparable methods fail. We then apply this to analyse activity related to visual short-term memory binding deficits in two cohorts of familial and sporadic Alzheimer's disease. Reproducible significant differences were found in the binding task with no significant difference in the shape task in the P300 ERP range. This allows new sensitive measurements of transient functional connectivity, which can be implemented to obtain results of clinical significance.

Paper number 141:
Title: Adaptive Nonlinear Model Predictive Control for a Real-World Labyrinth Game
Authors: Johannes Gaber, Thomas Bi, Raffaello D'Andrea
Abstract: We present a nonlinear non-convex model predictive control approach to solving a real-world labyrinth game. We introduce adaptive nonlinear constraints, representing the non-convex obstacles within the labyrinth. Our method splits the computation-heavy optimization problem into two layers; first, a high-level model predictive controller which incorporates the full problem formulation and finds pseudo-global optimal trajectories at a low frequency. Secondly, a low-level model predictive controller that receives a reduced, computationally optimized version of the optimization problem to follow the given high-level path in real-time. Further, a map of the labyrinth surface irregularities is learned. Our controller is able to handle the major disturbances and model inaccuracies encountered on the labyrinth and outperforms other classical control methods.

Paper number 142:
Title: MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection
Authors: Zhuoxiao Chen, Junjie Meng, Mahsa Baktashmotlagh, Yonggang Zhang, Zi Huang, Yadan Luo
Abstract: LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging cross-corruption scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: this https URL.

Paper number 143:
Title: uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes
Authors: Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad Abdul-Mageed
Abstract: Recent work on distilling Whisper's knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally, the distillation process requires a large amount of data thereby limiting its applicability in low-resource settings. To address this, we propose a distillation framework that does not require any labeled data. Through experimentation, we show that our best-distilled models outperform the teacher model by 5-7 WER points and are on par with or outperform similar supervised data filtering setups. When scaling the data, our models significantly outperform all zero-shot and supervised models. Our models are also 25-50% more compute- and memory-efficient while maintaining performance equal to or better than that of the teacher model. For more details about our models, dataset, and other resources, please visit our GitHub page: this https URL.

Paper number 144:
Title: Hybrid STAR-RIS Enabled Integrated Sensing and Communication
Authors: Zehra Yigit, Ertugrul Basar
Abstract: Integrated sensing and communication (ISAC) is recognized as one of the key enabling technologies for sixth-generation (6G) wireless communication networks, facilitating diverse emerging applications and services in an energy and cost-efficient manner. This paper proposes a multi-user multi-target ISAC system to enable full-space coverage for communication and sensing tasks. The proposed system employs a hybrid simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) comprising active transmissive and passive reflective elements. In the proposed scheme, the passive reflective elements support communication and sensing links for local communication users and sensing targets situated within the same physical region as the base station (BS), while low-power active transmissive elements are deployed to improve sensing performance and overcome high path attenuation due to multi-hop transmission for distant communication users and sensing targets situated far from of the coverage area of the BS. Moreover, to optimize the transmissive/reflective coefficients of the hybrid STAR-RIS, a semi-definite relaxation (SDR)-based algorithm is proposed. Furthermore, to evaluate communication and sensing performance, signal-to-interference-noise ratio (SINR) and Cramer-Rao bound (CRB) metrics have been derived and investigated via conducting extensive computer simulations.

Paper number 145:
Title: Multi-Scale Cell Decomposition for Path Planning using Restrictive Routing Potential Fields
Authors: Josue N. Rivera, Dengfeng Sun
Abstract: In burgeoning domains such as urban goods distribution, the advent of aerial transportation necessitates the development of routing solutions that prioritize safe navigation. This paper introduces Larp, a novel path planning and navigation framework that leverages the concept of repulsive potential fields as continuous cost maps to forge safe routes. The algorithm achieves it by segmenting the potential field into a hierarchy of cells, each with a designated risk zone determined by the proximity of obstacles. The meshing allows the airspace to be partitioned based on an area's potential for restriction violations, enabling navigation that is aware of these risks. While the primary impetus behind Larp is to enhance the safety of aerial pathways for Unmanned Aerial Vehicles (UAVs) in urban air mobility, its utility extends to a wide array of routing scenarios. Comparative analyses with both established and contemporary potential field-based methods reveal Larp's proficiency in maintaining a safe distance from restrictions and its adeptness in circumventing local minima. Additionally, large-scale aerial path planning of Austin, TX demonstrates Larp's capability to be implemented at a large scale.

Paper number 146:
Title: Certifiably Robust Policies for Uncertain Parametric Environments
Authors: Yannik Schnitzer, Alessandro Abate, David Parker
Abstract: We present a data-driven approach for producing policies that are provably robust across unknown stochastic environments. Existing approaches can learn models of a single environment as an interval Markov decision processes (IMDP) and produce a robust policy with a probably approximately correct (PAC) guarantee on its performance. However these are unable to reason about the impact of environmental parameters underlying the uncertainty. We propose a framework based on parametric Markov decision processes (MDPs) with unknown distributions over parameters. We learn and analyse IMDPs for a set of unknown sample environments induced by parameters. The key challenge is then to produce meaningful performance guarantees that combine the two layers of uncertainty: (1) multiple environments induced by parameters with an unknown distribution; (2) unknown induced environments which are approximated by IMDPs. We present a novel approach based on scenario optimisation that yields a single PAC guarantee quantifying the risk level for which a specified performance level can be assured in unseen environments, plus a means to trade-off risk and performance. We implement and evaluate our framework using multiple robust policy generation methods on a range of benchmarks. We show that our approach produces tight bounds on a policy's performance with high confidence.

Paper number 147:
Title: Ergodic-risk Criterion for Stochastically Stabilizing Policy Optimization
Authors: Shahriar Talebi, Na Li
Abstract: This paper introduces ergodic-risk criteria, which capture long-term cumulative risks associated with controlled Markov chains through probabilistic limit theorems--in contrast to existing methods that require assumptions of either finite hitting time, finite state/action space, or exponentiation necessitating light-tailed distributions. Using tailored Functional Central Limit Theorems (FCLT), we demonstrate that the time-correlated terms in the ergodic-risk] criteria converge under uniform ergodicity and establish conditions for the convergence of these criteria in non-stationary general-state Markov chains involving heavy-tailed distributions. For quadratic risk functionals on stochastic linear system, in addition to internal stability, this requires the (possibly heavy-tailed) process noise to have only a finite fourth moment. After quantifying cumulative uncertainties in risk functionals that account for extreme deviations, these ergodic-risk criteria are then incorporated into policy optimizations, thereby extending the standard average optimal synthesis to a risk-sensitive framework. Finally, by establishing the strong duality of the constrained policy optimization, we propose a primal-dual algorithm that optimizes average performance while ensuring that certain risks associated with these ergodic-risk criteria are constrained. Our risk-sensitive framework offers a theoretically guaranteed policy iteration for the long-term risk-sensitive control of processes involving heavy-tailed noise, which is shown to be effective through several simulations.

Paper number 148:
Title: High-Resolution Speech Restoration with Latent Diffusion Model
Authors: Tushar Dhyani, Florian Lux, Michele Mancusi, Giorgio Fabbro, Fritz Hohl, Ngoc Thang Vu
Abstract: Traditional speech enhancement methods often oversimplify the task of restoration by focusing on a single type of distortion. Generative models that handle multiple distortions frequently struggle with phone reconstruction and high-frequency harmonics, leading to breathing and gasping artifacts that reduce the intelligibility of reconstructed speech. These models are also computationally demanding, and many solutions are restricted to producing outputs in the wide-band frequency range, which limits their suitability for professional applications. To address these challenges, we propose Hi-ResLDM, a novel generative model based on latent diffusion designed to remove multiple distortions and restore speech recordings to studio quality, sampled at 48kHz. We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and Conditional Flow Matching (CFM) components, demonstrating superior performance in regenerating high-frequency-band details. Hi-ResLDM not only excels in non-instrusive metrics but is also consistently preferred in human evaluation and performs competitively on intrusive evaluations, making it ideal for high-resolution speech restoration.

Paper number 149:
Title: LC-Protonets: Multi-Label Few-Shot Learning for World Music Audio Tagging
Authors: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos
Abstract: We introduce Label-Combination Prototypical Networks (LC-Protonets) to address the problem of multi-label few-shot classification, where a model must generalize to new classes based on only a few available examples. Extending Prototypical Networks, LC-Protonets generate one prototype per label combination, derived from the power set of labels present in the limited training items, rather than one prototype per label. Our method is applied to automatic audio tagging across diverse music datasets, covering various cultures and including both modern and traditional music, and is evaluated against existing approaches in the literature. The results demonstrate a significant performance improvement in almost all domains and training setups when using LC-Protonets for multi-label classification. In addition to training a few-shot learning model from scratch, we explore the use of a pre-trained model, obtained via supervised learning, to embed items in the feature space. Fine-tuning improves the generalization ability of all methods, yet LC-Protonets achieve high-level performance even without fine-tuning, in contrast to the comparative approaches. We finally analyze the scalability of the proposed method, providing detailed quantitative metrics from our experiments. The implementation and experimental setup are made publicly available, offering a benchmark for future research.

Paper number 150:
Title: Towards Defining an Efficient and Expandable File Format for AI-Generated Contents
Authors: Yixin Gao, Runsen Feng, Xin Li, Weiping Li, Zhibo Chen
Abstract: Recently, AI-generated content (AIGC) has gained significant traction due to its powerful creation capability. However, the storage and transmission of large amounts of high-quality AIGC images inevitably pose new challenges for recent file formats. To overcome this, we define a new file format for AIGC images, named AIGIF, enabling ultra-low bitrate coding of AIGC images. Unlike compressing AIGC images intuitively with pixel-wise space as existing file formats, AIGIF instead compresses the generation syntax. This raises a crucial question: Which generation syntax elements, e.g., text prompt, device configuration, etc, are necessary for compression/transmission? To answer this question, we systematically investigate the effects of three essential factors: platform, generative model, and data configuration. We experimentally find that a well-designed composable bitstream structure incorporating the above three factors can achieve an impressive compression ratio of even up to 1/10,000 while still ensuring high fidelity. We also introduce an expandable syntax in AIGIF to support the extension of the most advanced generation models to be developed in the future.

Paper number 151:
Title: CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning
Authors: Sjoerd Groot, Qinyu Chen, Jan C. van Gemert, Chang Gao
Abstract: This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: this https URL

Paper number 152:
Title: Marine spatial planning techniques with a case study on wave-powered offshore aquaculture farms
Authors: Gabriel Ewig, Arezoo Hasankhani, Eugene Won, Maha Haji
Abstract: As emerging marine technologies lead to the development of new infrastructure across the ocean, they enter an environment that existing ecosystems and industries already rely on. Although necessary to provide sustainable sources of energy and food, careful planning will be important to make informed decisions and avoid conflicts. This paper examines several techniques used for marine spatial planning, an approach for analyzing and planning the use of marine resources. Using open source software including QGIS and Python, the potential for developing wave-powered offshore aquaculture farms using the RM3 wave energy converter along the Northeast coast of the United States is assessed and several feasible sites are identified. The optimal site, located at 43.7°N, 68.9°W along the coast of Maine, has a total cost for a 5-pen farm of \$56.8M, annual fish yield of 676 tonnes, and a levelized cost of fish of \$9.23 per kilogram. Overall trends indicate that the cost greatly decreases with distance to shore due to the greater availability of wave energy and that conflicts and environmental constraints significantly limit the number of feasible sites in this region.

Paper number 153:
Title: ImmerseDiffusion: A Generative Spatial Audio Latent Diffusion Model
Authors: Mojtaba Heydari, Mehrez Souden, Bruno Conejo, Joshua Atkins
Abstract: We introduce ImmerseDiffusion, an end-to-end generative audio model that produces 3D immersive soundscapes conditioned on the spatial, temporal, and environmental conditions of sound objects. ImmerseDiffusion is trained to generate first-order ambisonics (FOA) audio, which is a conventional spatial audio format comprising four channels that can be rendered to multichannel spatial output. The proposed generative system is composed of a spatial audio codec that maps FOA audio to latent components, a latent diffusion model trained based on various user input types, namely, text prompts, spatial, temporal and environmental acoustic parameters, and optionally a spatial audio and text encoder trained in a Contrastive Language and Audio Pretraining (CLAP) style. We propose metrics to evaluate the quality and spatial adherence of the generated spatial audio. Finally, we assess the model performance in terms of generation quality and spatial conformance, comparing the two proposed modes: ``descriptive", which uses spatial text prompts) and ``parametric", which uses non-spatial text prompts and spatial parameters. Our evaluations demonstrate promising results that are consistent with the user conditions and reflect reliable spatial fidelity.

Paper number 154:
Title: Enhancing Learned Image Compression via Cross Window-based Attention
Authors: Priyanka Mudgal, Feng Liu
Abstract: In recent years, learned image compression methods have demonstrated superior rate-distortion performance compared to traditional image compression methods. Recent methods utilize convolutional neural networks (CNN), variational autoencoders (VAE), invertible neural networks (INN), and transformers. Despite their significant contributions, a main drawback of these models is their poor performance in capturing local redundancy. Therefore, to leverage global features along with local redundancy, we propose a CNN-based solution integrated with a feature encoding module. The feature encoding module encodes important features before feeding them to the CNN and then utilizes cross-scale window-based attention, which further captures local redundancy. Cross-scale window-based attention is inspired by the attention mechanism in transformers and effectively enlarges the receptive field. Both the feature encoding module and the cross-scale window-based attention module in our architecture are flexible and can be incorporated into any other network architecture. We evaluate our method on the Kodak and CLIC datasets and demonstrate that our approach is effective and on par with state-of-the-art methods.

Paper number 155:
Title: Commissioning An All-Sky Infrared Camera Array for Detection Of Airborne Objects
Authors: Laura Dominé, Ankit Biswas, Richard Cloete, Alex Delacroix, Andriy Fedorenko, Lucas Jacaruso, Ezra Kelderman, Eric Keto, Sarah Little, Abraham Loeb, Eric Masson, Mike Prior, Forrest Schultz, Matthew Szenher, Wes Watters, Abby White
Abstract: To date there is little publicly available scientific data on Unidentified Aerial Phenomena (UAP) whose properties and kinematics purportedly reside outside the performance envelope of known phenomena. To address this deficiency, the Galileo Project is designing, building, and commissioning a multi-modal ground-based observatory to continuously monitor the sky and conduct a rigorous long-term aerial census of all aerial phenomena, including natural and human-made. One of the key instruments is an all-sky infrared camera array using eight uncooled long-wave infrared FLIR Boson 640 cameras. Their calibration includes a novel extrinsic calibration method using airplane positions from Automatic Dependent Surveillance-Broadcast (ADS-B) data. We establish a first baseline for the system performance over five months of field operation, using a real-world dataset derived from ADS-B data, synthetic 3-D trajectories, and a hand-labelled real-world dataset. We report acceptance rates (e.g. viewable airplanes that are recorded) and detection efficiencies (e.g. recorded airplanes which are successfully detected) for a variety of weather conditions, range and aircraft size. We reconstruct $\sim$500,000 trajectories of aerial objects from this commissioning period. A toy outlier search focused on large sinuosity of the 2-D reconstructed trajectories flags about 16% of trajectories as outliers. After manual review, 144 trajectories remain ambiguous: they are likely mundane objects but cannot be elucidated at this stage of development without distance and kinematics estimation or other sensor modalities. Our observed count of ambiguous outliers combined with systematic uncertainties yields an upper limit of 18,271 outliers count for the five-month interval at a 95% confidence level. This likelihood-based method to evaluate significance is applicable to all of our future outlier searches.

Paper number 156:
Title: Scalable Wavelength Arbitration for Microring-based DWDM Transceivers
Authors: Sunjin Choi, Vladimir Stojanović
Abstract: This paper introduces the concept of autonomous microring arbitration, or wavelength arbitration, to address the challenge of multi-microring initialization in microring-based Dense-Wavelength-Division-Multiplexed (DWDM) transceivers. This arbitration is inherently policy-driven, defining critical system characteristics such as the spectral ordering of microrings. Furthermore, to facilitate large-scale deployment, the arbitration algorithms must operate independently of specific wavelength information and be resilient to system variability. Addressing these complexities requires a holistic approach that encompasses the entire system, from device-level variabilities to the transceiver electrical-to-optical interface - this system-wide perspective is the focus of this paper. To support efficient analysis, we develop a hierarchical framework incorporating an ideal, wavelength-aware arbitration model to examine arbitration failures at both the policy and algorithmic levels. The effectiveness of this approach is demonstrated in two ways: by analyzing the robustness of each policy in relation to device variabilities, and by developing an algorithm that achieves near-perfect alignment with the ideal model, offering superior robustness compared to the traditional sequential tuning method. The simulator code used in this paper is available at this https URL.

Paper number 157:
Title: NBM: an Open Dataset for the Acoustic Monitoring of Nocturnal Migratory Birds in Europe
Authors: Louis Airale, Adrien Pajot, Juliette Linossier
Abstract: The persisting threats on migratory bird populations highlights the urgent need for effective monitoring techniques that could assist in their conservation. Among these, passive acoustic monitoring is an essential tool, particularly for nocturnal migratory species that are difficult to track otherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a collection of 13,359 annotated vocalizations from 117 species of the Western Palearctic. The dataset includes precise time and frequency annotations, gathered by dozens of bird enthusiasts across France, enabling novel downstream acoustic analysis. In particular, we demonstrate that a two-stage object detection model, tailored for the processing of audio data, can be trained on our dataset to retrieve localized bounding box coordinates around each signal of interest in a spectrogram. This object detection approach, which is largely overlooked in the bird sound recognition literature, allows important applications by potentially differentiating individual birds within audio windows. Further, we show that the accuracy of our recognition model on the 45 main species of the dataset competes with state-of-the-art systems trained on much larger datasets. This highlights the interest of fostering similar open-science initiatives to acquire costly but valuable fine-grained annotations of audio files. All data and code are made openly available.

Paper number 158:
Title: Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison
Authors: Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow
Abstract: Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.

Paper number 159:
Title: Generalized Linear Models with 1-Bit Measurements: Asymptotics of the Maximum Likelihood Estimator
Authors: Jaimin Shah, Martina Cardone, Cynthia Rush, Alex Dytso
Abstract: This work establishes regularity conditions for consistency and asymptotic normality of the multiple parameter maximum likelihood estimator(MLE) from censored data, where the censoring mechanism is in the form of $1$-bit measurements. The underlying distribution of the uncensored data is assumed to belong to the exponential family, with natural parameters expressed as a linear combination of the predictors, known as generalized linear model (GLM). As part of the analysis, the Fisher information matrix is also derived for both censored and uncensored data, which helps to quantify the impact of censoring and assess the performance of the MLE. The choice of GLM allows one to consider a variety of practical examples where 1-bit estimation is of interest. In particular, it is shown how the derived results can be used to analyze two practically relevant scenarios: the Gaussian model with both unknown mean and variance, and the Poisson model with an unknown mean.

Paper number 160:
Title: Unsupervised Learning in Echo State Networks for Input Reconstruction
Authors: Taiki Yamada, Yuichi Katori, Kantaro Fujiwara
Abstract: Conventional echo state networks (ESNs) require supervised learning to train the readout layer, using the desired outputs as training data. In this study, we focus on input reconstruction (IR), which refers to training the readout layer to reproduce the input time series in its output. We reformulate the learning algorithm of the ESN readout layer to perform IR using unsupervised learning (UL). By conducting theoretical analysis and numerical experiments, we demonstrate that IR in ESNs can be effectively implemented under realistic conditions without explicitly using the desired outputs as training data; in this way, UL is enabled. Furthermore, we demonstrate that applications relying on IR, such as dynamical system replication and noise filtering, can be reformulated within the UL framework. Our findings establish a theoretically sound and universally applicable IR formulation, along with its related tasks in ESNs. This work paves the way for novel predictions and highlights unresolved theoretical challenges in ESNs, particularly in the context of time-series processing methods and computational models of the brain.

Paper number 161:
Title: Performance Analysis of Fluid Antenna Multiple Access Assisted Wireless Powered Communication Network
Authors: Xiao Lin, Yizhe Zhao, Halvin Yang, Jie Hu
Abstract: This paper investigates a novel fluid antenna multiple access (FAMA)-assisted wireless powered communication network (WPCN), in which a hybrid access point (HAP) equipped with multiple fixed position antennas (FPAs) provides integrated data and energy transfer (IDET) services towards low-power devices that are equipped with a single fluid antenna (FA), while the low-power devices use harvested energy to power their own uplink transmission. Using the block correlation channel model, both the downlink and uplink wireless data transfer (WDT) outage probabilities are analyzed under specific port selection strategies, including downlink signal-to-interference ratio-based port selection (DSPS) strategy, downlink energy harvesting power-based port selection (DEPS) strategy, uplink signal-to-noise ratio-based port selection (USPS) strategy, and uplink channel-based port selection (UCPS) strategy. A step function approximation (SFA) approach is also relied upon to derive closed-form expressions for the outage probabilities, while the lower bounds for uplink WDT outage probabilities are also formulated. Numerical results demonstrate the validity of our theoretical analysis, which also provide useful guidelines for the system design through the analytical framework.

Paper number 162:
Title: Exploring Audio Editing Features as User-Centric Privacy Defenses Against Large Language Model(LLM) Based Emotion Inference Attacks
Authors: Mohd. Farhan Israk Soumik, W.K.M. Mithsara, Abdur R. Shahid, Ahmed Imteaj
Abstract: The rapid proliferation of speech-enabled technologies, including virtual assistants, video conferencing platforms, and wearable devices, has raised significant privacy concerns, particularly regarding the inference of sensitive emotional information from audio data. Existing privacy-preserving methods often compromise usability and security, limiting their adoption in practical scenarios. This paper introduces a novel, user-centric approach that leverages familiar audio editing techniques, specifically pitch and tempo manipulation, to protect emotional privacy without sacrificing usability. By analyzing popular audio editing applications on Android and iOS platforms, we identified these features as both widely available and usable. We rigorously evaluated their effectiveness against a threat model, considering adversarial attacks from diverse sources, including Deep Neural Networks (DNNs), Large Language Models (LLMs), and and reversibility testing. Our experiments, conducted on three distinct datasets, demonstrate that pitch and tempo manipulation effectively obfuscates emotional data. Additionally, we explore the design principles for lightweight, on-device implementation to ensure broad applicability across various devices and platforms.

Paper number 163:
Title: Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency Approach for ECG Classification
Authors: Sam Jeong, Hae Yong Kim
Abstract: Machine learning has transformed the classification of biomedical signals such as electrocardiograms (ECGs). Advances in deep learning, particularly convolutional neural networks (CNNs), enable automatic feature extraction, raising the question: Can combining time- and frequency-domain attributes enhance classification accuracy? To explore this, we evaluated three ECG classification tasks: (1) arrhythmia classification, (2) identity recognition, and (3) apnea detection. We initially tested three methods: (i) 2-D spectrogram-based frequency-time classification (SPECT), (ii) time-domain classification using a 1-D CNN (CNN1D), and (iii) frequency-domain classification using a Fourier transform-based CNN (FFT1D). Performance was validated using K-fold cross-validation. Among these, CNN1D (time only) performed best, followed by SPECT (time-frequency) and FFT1D (frequency only). Surprisingly, SPECT, which integrates time- and frequency-domain features, performed worse than CNN1D, suggesting a need for a more effective time and frequency fusion approach. To address this, we tested the recently proposed Fourier Analysis Network (FAN), which combines time- and frequency-domain features. However, FAN performed comparably to CNN1D, excelling in some tasks while underperforming in others. To enhance this approach, we developed the Convolutional Fourier Analysis Network (CFAN), which integrates FAN with CNN. CFAN outperformed all previous methods across all classification tasks. These findings underscore the advantages of combining time- and frequency-domain features, demonstrating CFAN's potential as a powerful and versatile solution for ECG classification and broader biomedical signal analysis

Paper number 164:
Title: ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills
Authors: Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, Linxi "Jim" Fan, Yuke Zhu, Changliu Liu, Guanya Shi
Abstract: Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.

Paper number 165:
Title: LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection
Authors: Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang
Abstract: Electroencephalogram (EEG) provides a non-invasive, highly accessible, and cost-effective solution for Alzheimer's Disease (AD) detection. However, existing methods, whether based on manual feature extraction or deep learning, face two major challenges: the lack of large-scale datasets for robust feature learning and evaluation, and poor detection performance due to inter-subject variations. To address these challenges, we curate an EEG-AD corpus containing 813 subjects, which forms the world's largest EEG-AD dataset to the best of our knowledge. Using this unique dataset, we propose LEAD, the first large foundation model for EEG-based AD detection. Our method encompasses an entire pipeline, from data selection and preprocessing to self-supervised contrastive pretraining, fine-tuning, and key setups such as subject-independent evaluation and majority voting for subject-level detection. We pre-train the model on 11 EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised pre-training design includes sample-level and subject-level contrasting to extract useful general EEG features. Fine-tuning is performed on 5 channel-aligned datasets together. The backbone encoder incorporates temporal and channel embeddings to capture features across both temporal and spatial dimensions. Our method demonstrates outstanding AD detection performance, achieving up to a 9.86% increase in F1 score at the sample-level and up to a 9.31% at the subject-level compared to state-of-the-art methods. The results of our model strongly confirm the effectiveness of contrastive pre-training and channel-aligned unified fine-tuning for addressing inter-subject variation. The source code is at this https URL.

Paper number 166:
Title: Calibrated Unsupervised Anomaly Detection in Multivariate Time-series using Reinforcement Learning
Authors: Saba Sanami, Amir G. Aghdam
Abstract: This paper investigates unsupervised anomaly detection in multivariate time-series data using reinforcement learning (RL) in the latent space of an autoencoder. A significant challenge is the limited availability of anomalous data, often leading to misclassifying anomalies as normal events, thus raising false negatives. RL can help overcome this limitation by promoting exploration and balancing exploitation during training, effectively preventing overfitting. Wavelet analysis is also utilized to enhance anomaly detection, enabling time-series data decomposition into both time and frequency domains. This approach captures anomalies at multiple resolutions, with wavelet coefficients extracted to detect both sudden and subtle shifts in the data, thereby refining the anomaly detection process. We calibrate the decision boundary by generating synthetic anomalies and embedding a supervised framework within the model. This supervised element aids the unsupervised learning process by fine-tuning the decision boundary and increasing the model's capacity to distinguish between normal and anomalous patterns effectively.

Paper number 167:
Title: Enhancing Hallucination Detection through Noise Injection
Authors: Litian Liu, Reza Pourreza, Sunny Panchal, Apratim Bhattacharyya, Yao Qin, Roland Memisevic
Abstract: Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations. Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs. Recent research has linked hallucinations to model uncertainty, suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from a model. While drawing from the distribution over tokens defined by the model is a natural way to obtain samples, in this work, we argue that it is sub-optimal for the purpose of detecting hallucinations. We show that detection can be improved significantly by taking into account model uncertainty in the Bayesian sense. To this end, we propose a very simple and efficient approach that perturbs an appropriate subset of model parameters, or equivalently hidden unit activations, during sampling. We demonstrate its effectiveness across a wide range of datasets and model architectures.

Paper number 168:
Title: UniForm: A Unified Diffusion Transformer for Audio-Video Generation
Authors: Lei Zhao, Linfeng Feng, Dongxu Ge, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li
Abstract: As a natural multimodal content, audible video delivers an immersive sensory experience. Consequently, audio-video generation systems have substantial potential. However, existing diffusion-based studies mainly employ relatively independent modules for generating each modality, which lack exploration of shared-weight generative modules. This approach may under-use the intrinsic correlations between audio and visual modalities, potentially resulting in sub-optimal generation quality. To address this, we propose UniForm, a unified diffusion transformer designed to enhance cross-modal consistency. By concatenating auditory and visual information, UniForm learns to generate audio and video simultaneously within a unified latent space, facilitating the creation of high-quality and well-aligned audio-visual pairs. Extensive experiments demonstrate the superior performance of our method in joint audio-video generation, audio-guided video generation, and video-guided audio generation tasks. Our demos are available at this https URL.

Paper number 169:
Title: XAttnMark: Learning Robust Audio Watermarking with Cross-Attention
Authors: Yixin Liu, Lie Lu, Jihui Jin, Lichao Sun, Andrea Fanelli
Abstract: The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces Cross-Attention Robust Audio Watermark (XAttnMark), which bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. The project webpage is available at this https URL.
    