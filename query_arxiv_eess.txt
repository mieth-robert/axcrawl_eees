
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Model Predictive Black Start for Dynamic Formation of DER-Led Microgrids with Inrush Current Impacts
Authors: Cong Bai, Salish Maharjan, Zhaoyu Wang
Abstract: Black start (BS) of the distribution system (DS) with high penetration of distributed energy resources (DERs) requires advanced control frameworks to ensure secure and efficient restoration. This paper proposes a model predictive black start (MPBS) framework incorporating an inrush current feasibility module to dynamically generate real-time feasible and optimal restoration sequences. Short-term forecasts of DER output and transmission grid (TG) availability are utilized to construct adaptive cranking paths. The inrush current feasibility module analytically estimates the transient inrush current caused by energizing no-load distribution transformers (DTs). To mitigate excessive inrush current and avoid potential misoperations of protection devices, an emergency operation-inspired voltage control strategy and a switch blocking mechanism are developed. The proposed inrush model is validated against electromagnetic transient (EMT) simulations in PowerFactory with estimation accuracies exceeding 90 %. Case studies on a modified IEEE 123-node feeder demonstrate that the MPBS framework prevents misoperations of fuses and reclosers, reduces unnecessary DER energy consumption, and enhances load restoration efficiency during DER-led BS processes.

Paper number 2:
Title: Deep Bilinear Koopman Model for Real-Time Vehicle Control in Frenet Frame
Authors: Mohammad Abtahi, Farhang Motallebi Araghi, Navid Mojahed, Shima Nazari
Abstract: Accurate modeling and control of autonomous vehicles remain a fundamental challenge due to the nonlinear and coupled nature of vehicle dynamics. While Koopman operator theory offers a framework for deploying powerful linear control techniques, learning a finite-dimensional invariant subspace for high-fidelity modeling continues to be an open problem. This paper presents a deep Koopman approach for modeling and control of vehicle dynamics within the curvilinear Frenet frame. The proposed framework uses a deep neural network architecture to simultaneously learn the Koopman operator and its associated invariant subspace from the data. Input-state bilinear interactions are captured by the algorithm while preserving convexity, which makes it suitable for real-time model predictive control (MPC) application. A multi-step prediction loss is utilized during training to ensure long-horizon prediction capability. To further enhance real-time trajectory tracking performance, the model is integrated with a cumulative error regulator (CER) module, which compensates for model mismatch by mitigating accumulated prediction errors. Closed-loop performance is evaluated through hardware-in-the-loop (HIL) experiments using a CarSim RT model as the target plant, with real-time validation conducted on a dSPACE SCALEXIO system. The proposed controller achieved significant reductions in tracking error relative to baseline controllers, confirming its suitability for real-time implementation in embedded autonomous vehicle systems.

Paper number 3:
Title: Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS
Authors: Sandesh Rao Mattu, Nishant Mehrotra, Robert Calderbank
Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a delay-Doppler (DD) domain modulation scheme in which the signal processing is carried out in the DD domain. The channel when viewed in the DD domain is predictable. However, even with Zak-OTFS, pilots need to be sent periodically, albeit at a lower rate. In this paper, we propose a differential communication scheme for Zak-OTFS systems that alleviates the need for periodic pilot transmission. Towards this, we analytically show that the detected data can be used as a pilot and that the channel estimate obtained from the detected data can enable further detection enabling the "differential" aspect of the communication. Specifically, we leverage the prediction capability of the DD channel in Zak-OTFS to use the channel estimate (obtained from detected data symbols treated as pilots) in the previous instant to detect data in the next instant and propagate this forward. The advantages are two fold. First, it allows the data symbols to enjoy higher energy since the energy that would otherwise be required for pilot symbols can also be allocated to data symbols. Second, it allows for full spectral efficiency compared to point or embedded pilots. Comparison with the full spectral efficiency achieving spread pilot scheme shows that the proposed method achieves better bit-error rate at lower complexity.

Paper number 4:
Title: Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models
Authors: Orchid Chetia Phukan, Mohd Mujtaba Akhtar, Girish, Arun Balaji Buduru
Abstract: In this work, we address EmoFake Detection (EFD). We hypothesize that multilingual speech foundation models (SFMs) will be particularly effective for EFD due to their pre-training across diverse languages, enabling a nuanced understanding of variations in pitch, tone, and intensity. To validate this, we conduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs. Our results shows the superiority of multilingual SFMs for same language (in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we also propose, THAMA for fusion of foundation models (FMs) motivated by related research where combining FMs have shown improved performance. THAMA leverages the complementary conjunction of tucker decomposition and hadamard product for effective fusion. With THAMA, synergized with cooperative multilingual SFMs achieves topmost performance across in-domain and out-domain settings, outperforming individual FMs, baseline fusion techniques, and prior SOTA methods.

Paper number 5:
Title: Pathology-Guided Virtual Staining Metric for Evaluation and Training
Authors: Qiankai Wang, James E.D. Tweel, Parsin Haji Reza, Anita Layton
Abstract: Virtual staining has emerged as a powerful alternative to traditional histopathological staining techniques, enabling rapid, reagent-free image transformations. However, existing evaluation methods predominantly rely on full-reference image quality assessment (FR-IQA) metrics such as structural similarity, which are originally designed for natural images and often fail to capture pathology-relevant features. Expert pathology reviews have also been used, but they are inherently subjective and time-consuming. In this study, we introduce PaPIS (Pathology-Aware Perceptual Image Similarity), a novel FR-IQA metric specifically tailored for virtual staining evaluation. PaPIS leverages deep learning-based features trained on cell morphology segmentation and incorporates Retinex-inspired feature decomposition to better reflect histological perceptual quality. Comparative experiments demonstrate that PaPIS more accurately aligns with pathology-relevant visual cues and distinguishes subtle cellular structures that traditional and existing perceptual metrics tend to overlook. Furthermore, integrating PaPIS as a guiding loss function in a virtual staining model leads to improved histological fidelity. This work highlights the critical need for pathology-aware evaluation frameworks to advance the development and clinical readiness of virtual staining technologies.

Paper number 6:
Title: Achieving Robust Channel Estimation Neural Networks by Designed Training Data
Authors: Dianxin Luan, John Thompson
Abstract: Channel estimation is crucial in cognitive communications, as it enables intelligent spectrum sensing and adaptive transmission by providing accurate information about the current channel state. However, in many papers neural networks are frequently tested by training and testing on one example channel or similar channels. This is because data-driven methods often degrade on new data which they are not trained on, as they cannot extrapolate their training knowledge. This is despite the fact physical channels are often assumed to be time-variant. However, due to the low latency requirements and limited computing resources, neural networks may not have enough time and computing resources to execute online training to fine-tune the parameters. This motivates us to design offline-trained neural networks that can perform robustly over wireless channels, but without any actual channel information being known at design time. In this paper, we propose design criteria to generate synthetic training datasets for neural networks, which guarantee that after training the resulting networks achieve a certain mean squared error (MSE) on new and previously unseen channels. Therefore, neural network solutions require no prior channel information or parameters update for real-world implementations. Based on the proposed design criteria, we further propose a benchmark design which ensures intelligent operation for different channel profiles. To demonstrate general applicability, we use neural networks with different levels of complexity to show that the generalization achieved appears to be independent of neural network architecture. From simulations, neural networks achieve robust generalization to wireless channels with both fixed channel profiles and variable delay spreads.

Paper number 7:
Title: A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis
Authors: Mohammed Guhdar, Ramadhan J. Mstafa, Abdulhakeem O. Mohammed
Abstract: The increasing need for accurate and unified analysis of diverse biological signals, such as ECG and EEG, is paramount for comprehensive patient assessment, especially in synchronous monitoring. Despite advances in multi-sensor fusion, a critical gap remains in developing unified architectures that effectively process and extract features from fundamentally different physiological signals. Another challenge is the inherent class imbalance in many biomedical datasets, often causing biased performance in traditional methods. This study addresses these issues by proposing a novel and unified deep learning framework that achieves state-of-the-art performance across different signal types. Our method integrates a ResNet-based CNN with an attention mechanism, enhanced by a novel data augmentation strategy: time-domain concatenation of multiple augmented variants of each signal to generate richer representations. Unlike prior work, we scientifically increase signal complexity to achieve future-reaching capabilities, which resulted in the best predictions compared to the state of the art. Preprocessing steps included wavelet denoising, baseline removal, and standardization. Class imbalance was effectively managed through the combined use of this advanced data augmentation and the Focal Loss function. Regularization techniques were applied during training to ensure generalization. We rigorously evaluated the proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%, and 100%, respectively, demonstrating robustness across diverse signal types and clinical contexts. Finally, the architecture requires ~130 MB of memory and processes each sample in ~10 ms, suggesting suitability for deployment on low-end or wearable devices.

Paper number 8:
Title: InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion
Authors: Ananya Raghu, Anisha Raghu, Alice S. Tang, Yannis M. Paulus, Tyson N. Kim, Tomiko T. Oskotsky
Abstract: Background/Objectives: Age-related macular degeneration, glaucoma, diabetic retinopathy (DR), diabetic macular edema, and pathological myopia affect hundreds of millions of people worldwide. Early screening for these diseases is essential, yet access to medical care remains limited in low- and middle-income countries as well as in resource-limited settings. We develop InSight, an AI-based app that combines patient metadata with fundus images for accurate diagnosis of five common eye diseases to improve accessibility of screenings. Methods: InSight features a three-stage pipeline: real-time image quality assessment, disease diagnosis model, and a DR grading model to assess severity. Our disease diagnosis model incorporates three key innovations: (a) Multimodal fusion technique (MetaFusion) combining clinical metadata and images; (b) Pretraining method leveraging supervised and self-supervised loss functions; and (c) Multitask model to simultaneously predict 5 diseases. We make use of BRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets, both of which also contain clinical metadata for model training/evaluation. Results: Trained on a dataset of BRSET and mBRSET images, the image quality checker achieves near-100% accuracy in filtering out low-quality fundus images. The multimodal pretrained disease diagnosis model outperforms models using only images by 6% in balanced accuracy for BRSET and 4% for mBRSET. Conclusions: The InSight pipeline demonstrates robustness across varied image conditions and has high diagnostic accuracy across all five diseases, generalizing to both smartphone and lab captured images. The multitask model contributes to the lightweight nature of the pipeline, making it five times computationally efficient compared to having five individual models corresponding to each disease.

Paper number 9:
Title: TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets
Authors: Rajesh Sureddi, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik
Abstract: Image Quality Assessment (IQA) models aim to predict perceptual image quality in alignment with human judgments. No-Reference (NR) IQA remains particularly challenging due to the absence of a reference image. While deep learning has significantly advanced this field, a major hurdle in developing NR-IQA models is the limited availability of subjectively labeled data. Most existing deep learning-based NR-IQA approaches rely on pre-training on large-scale datasets before fine-tuning for IQA tasks. To further advance progress in this area, we propose a novel approach that constructs a custom dataset using a limited number of reference content images and introduces a no-reference IQA model that incorporates both content and quality features for perceptual quality prediction. Specifically, we train a quality-aware model using contrastive triplet-based learning, enabling efficient training with fewer samples while achieving strong generalization performance across publicly available datasets. Our repository is available at this https URL.

Paper number 10:
Title: Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images
Authors: Zahra TehraniNasab, Amar Kumar, Tal Arbel
Abstract: Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - this https URL.

Paper number 11:
Title: Joint Price and Power MPC for Peak Power Reduction at Workplace EV Charging Stations
Authors: Thibaud Cambronne, Samuel Bobick, Wente Zeng, Scott Moura
Abstract: Demand charge often constitutes a significant portion of electricity costs for commercial electric vehicle charging station operators. This paper explores control methods to reduce peak power consumption at workplace EV charging stations in a joint price and power optimization framework. We optimize a menu of price options to incentivize users to select controllable charging service. Using this framework, we propose several solutions to achieve a reduction in both demand charge and overall operator costs. Through a Monte Carlo simulation, we find that model predictive control using a time series forecast can significantly reduce station operator costs.

Paper number 12:
Title: Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers
Authors: Sanghyun Kim, Jiwon Seo
Abstract: In urban environments, global navigation satellite system (GNSS) positioning is often compromised by signal blockages and multipath effects caused by buildings, leading to significant positioning errors. To address this issue, this study proposes a robust enhancement of zonotope shadow matching (ZSM)-based positioning by employing a conservative satellite selection strategy using unanimous voting across multiple machine learning classifiers. Three distinct models - random forest (RF), gradient boosting decision tree (GBDT), and support vector machine (SVM) - were trained to perform line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global positioning system (GPS) signal features. A satellite is selected for positioning only when all classifiers unanimously agree on its classification and their associated confidence scores exceed a threshold. Experiments with real-world GPS data collected in dense urban areas demonstrate that the proposed method significantly improves the positioning success rate and the receiver containment rate, even with imperfect LOS/NLOS classification. Although a slight increase in the position bound was observed due to the reduced number of satellites used, overall positioning reliability was substantially enhanced, indicating the effectiveness of the proposed approach in urban GNSS environments.

Paper number 13:
Title: A Stackelberg Game of Demand Response from the Aggregator's Perspective
Authors: Seangleng Khe, Parin Chaipunya, Athikom Bangviwat
Abstract: In this paper, we investigate on the modeling of demand response activities between the single aggregator and multiple participating consumers. The model incorporates the bilevel structure that naturally occurs in the information structure and decision sequence, where the aggregator assumes the role of a leader and the participating consumers play the role of followers. The proposed model is demonstrated to be effective in load control, helping the aggregator to meet the target reduction while the consumers pay cheaper electricity bill.

Paper number 14:
Title: On the Properties of Optimal-Decay Control Barrier Functions
Authors: Pio Ong, Max H. Cohen, Tamas G. Molnar, Aaron D. Ames
Abstract: Control barrier functions provide a powerful means for synthesizing safety filters that ensure safety framed as forward set invariance. Key to CBFs' effectiveness is the simple inequality on the system dynamics: $\dot{h} \geq - \alpha(h)$. Yet determining the class $\mathcal{K}^e$ function $\alpha$ is a user defined choice that can have a dramatic effect on the resulting system behavior. This paper formalizes the process of choosing $\alpha$ using optimal-decay control barrier functions (OD-CBFs). These modify the traditional CBF inequality to: $\dot{h} \geq - \omega \alpha(h)$, where $\omega \geq 0$ is automatically determined by the safety filter. A comprehensive characterization of this framework is elaborated, including tractable conditions on OD-CBF validity, control invariance of the underlying sets in the state space, forward invariance conditions for safe sets, and discussion on optimization-based safe controllers in terms of their feasibility, Lipschitz continuity, and closed-form expressions. The framework also extends existing higher-order CBF techniques, addressing safety constraints with vanishing relative degrees. The proposed method is demonstrated on a satellite control problem in simulation.

Paper number 15:
Title: Invariance Guarantees using Continuously Parametrized Control Barrier Functions
Authors: Inkyu Jang, H. Jin Kim
Abstract: Constructing a control invariant set with an appropriate shape that fits within a given state constraint is a fundamental problem in safety-critical control but is known to be difficult, especially for large or complex spaces. This paper introduces a safe control framework of utilizing PCBF: continuously parametrized control barrier functions (CBFs). In PCBF, each choice of parameter corresponds to a control invariant set of relatively simple shape. Invariance-preserving control is done by dynamically selecting a parameter whose corresponding invariant set lies within the safety bound. This eliminates the need for synthesizing a single complex CBF that matches the entire free space. It also enables easier adaptation to diverse environments. By assigning a differentiable dynamics on the parameter space, we derive a lightweight feedback controller based on quadratic programming (QP), namely PCBF-QP. We also discuss on how to build a valid PCBF for a class of systems and how to constrain the parameter so that the invariant set does not exceed the safety bound. The concept is also extended to cover continuously parametrized high-order CBFs, which is called high-order PCBF. Finally, simulation experiments are conducted to validate the proposed approach.

Paper number 16:
Title: Latency-Optimal File Assignment in Geo-Distributed Storage with Preferential Demands
Authors: Srivathsa Acharya, P. Vijay Kumar, Viveck R. Cadambe
Abstract: We consider the problem of data storage in a geographically distributed (or geo-distributed) network of servers (or nodes) where inter-node communication incurs certain round-trip delays. Every node serves a set of users who can request any file in the network. If the requested file is not available at the node, it communicates with other nodes to obtain the file, thus causing the user to experience latency in obtaining the file. The files can be placed uncoded, where each node stores exact copies of the files, or in coded fashion, where certain linear combination of files are placed at each node. We aim to obtain an optimal file placement on the nodes with respect to minimizing the worst-case latency at each node, as well as the system-average latency. The prior literature considered the case of equiprobable file demands at the nodes. In this paper, we investigate the generic case of non-uniform file-demand probabilities at each node. The scheme presented here is optimal within the family of uncoded schemes. It is obtained first by modeling the worst-case latency constraint as a vertex coloring problem, and then converting the system-average latency optimization to a problem of balanced-assignment.

Paper number 17:
Title: DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization
Authors: Huakang Chen, Yuepeng Jiang, Guobin Ma, Chunbo Hao, Shuai Wang, Jixun Yao, Ziqian Ning, Meng Meng, Jian Luan, Lei Xie
Abstract: Songs, as a central form of musical art, exemplify the richness of human intelligence and creativity. While recent advances in generative modeling have enabled notable progress in long-form song generation, current systems for full-length song synthesis still face major challenges, including data imbalance, insufficient controllability, and inconsistent musical quality. DiffRhythm, a pioneering diffusion-based model, advanced the field by generating full-length songs with expressive vocals and accompaniment. However, its performance was constrained by an unbalanced model training dataset and limited controllability over musical style, resulting in noticeable quality disparities and restricted creative flexibility. To address these limitations, we propose DiffRhythm+, an enhanced diffusion-based framework for controllable and flexible full-length song generation. DiffRhythm+ leverages a substantially expanded and balanced training dataset to mitigate issues such as repetition and omission of lyrics, while also fostering the emergence of richer musical skills and expressiveness. The framework introduces a multi-modal style conditioning strategy, enabling users to precisely specify musical styles through both descriptive text and reference audio, thereby significantly enhancing creative control and diversity. We further introduce direct performance optimization aligned with user preferences, guiding the model toward consistently preferred outputs across evaluation metrics. Extensive experiments demonstrate that DiffRhythm+ achieves significant improvements in naturalness, arrangement complexity, and listener satisfaction over previous systems.

Paper number 18:
Title: Guaranteeing and Explaining Stability across Heterogeneous Load Balancing using Calculus Network Dynamics
Authors: Mengbang Zou, Yun Tang, Adolfo Perrusquía, Weisi Guo
Abstract: Load balancing between base stations (BSs) allows BS capacity to be efficiently utilised and avoid outages. Currently, data-driven mechanisms strive to balance inter-BS load and reduce unnecessary handovers. The challenge is that over a large number of BSs, networks observe an oscillatory effect of load evolution that causes high inter-BS messaging. Without a calculus function that integrates network topology to describe the evolution of load states, current data-driven algorithms cannot explain the oscillation phenomenon observed in load states, nor can they provide theoretical guarantees on the stability of the ideal synchronised state. Whilst we know load state oscillation is coupled with the load balancing process algorithms and the topology structure of inter-BS boundary relations, we do not have a theoretical framework to prove this and a pathway to improving load balancing algorithms. Here, we abstract generic and heterogeneous data-driven algorithms into a calculus dynamics space, so that we can establish the synchronization conditions for networked load balancing dynamics with any network topology. By incorporating what is known as "non-conservative error" and the eigenvalue spectrum of the networked dynamics, we can adjust the inter-BS load balancing mechanisms to achieve high efficiency and convergence guarantee, or to mitigate the oscillation when the synchronisation condition cannot be satisfied.

Paper number 19:
Title: Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO
Authors: Xi Ding, Luca Kunz, E. Jorswieck
Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While prior works have explored JSAC optimization using methods such as successive convex approximation (SCA) and semidefinite relaxation (SDR), many of these approaches either lack global optimality or require additional rank-reduction steps. In contrast, we propose an SDR-based optimization framework that guarantees globally optimal solutions without post-processing. To benchmark its performance, we introduce a standalone BF strategy that dedicates each access point (AP) exclusively to either communication or sensing. The proposed formulation builds upon a general multi-user system model, enabling future extensions beyond the single-user setting. Overall, our framework offers a globally optimal and computationally efficient BF design, providing valuable insights for the development of next-generation wireless networks.

Paper number 20:
Title: Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion
Authors: Caixia Dong, Duwei Dai, Xinyi Han, Fan Liu, Xu Yang, Zongfang Li, Songhua Xu
Abstract: Accurate coronary artery segmentation is critical for computeraided diagnosis of coronary artery disease (CAD), yet it remains challenging due to the small size, complex morphology, and low contrast with surrounding tissues. To address these challenges, we propose a novel segmentation framework that leverages the power of vision foundation models (VFMs) through a parallel encoding architecture. Specifically, a vision transformer (ViT) encoder within the VFM captures global structural features, enhanced by the activation of the final two ViT blocks and the integration of an attention-guided enhancement (AGE) module, while a convolutional neural network (CNN) encoder extracts local details. These complementary features are adaptively fused using a cross-branch variational fusion (CVF) module, which models latent distributions and applies variational attention to assign modality-specific weights. Additionally, we introduce an evidential-learning uncertainty refinement (EUR) module, which quantifies uncertainty using evidence theory and refines uncertain regions by incorporating multi-scale feature aggregation and attention mechanisms, further enhancing segmentation accuracy. Extensive evaluations on one in-house and two public datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods, achieving superior performance in accurate coronary artery segmentation and showcasing strong generalization across multiple datasets. The code is available at this https URL.

Paper number 21:
Title: UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets
Authors: Zhichao Sheng, Shilin Zhou, Chen Gong, Zhenghua Li
Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric multimedia applications, enabling machines to comprehend spoken language in scenarios such as meetings, interviews, and customer service interactions. SLU encompasses multiple tasks, including Automatic Speech Recognition (ASR), spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA). However, existing methods often rely on separate model architectures for individual tasks such as spoken NER and SA, which increases system complexity, limits cross-task interaction, and fails to fully exploit heterogeneous datasets available across tasks. To address these limitations, we propose UniSLU, a unified framework that jointly models multiple SLU tasks within a single architecture. Specifically, we propose a unified representation for diverse SLU tasks, enabling full utilization of heterogeneous datasets across multiple tasks. Built upon this representation, we propose a unified generative method that jointly models ASR, spoken NER, and SA tasks, enhancing task interactions and enabling seamless integration with large language models to harness their powerful generative capabilities. Extensive experiments on public SLU datasets demonstrate the effectiveness of our approach, achieving superior SLU performance compared to several benchmark methods, making it well-suited for real-world speech-based multimedia scenarios. We will release all code and models at github to facilitate future research.

Paper number 22:
Title: Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset
Authors: Nerma Kadric, Amila Akagic, Medina Kapo
Abstract: Pigmented skin lesions represent localized areas of increased melanin and can indicate serious conditions like melanoma, a major contributor to skin cancer mortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced to advance research in biomedical imaging and includes DermaMNIST, a dataset for classifying pigmented lesions based on the HAM10000 dataset. This study assesses ResNet-50 and EfficientNetV2L models for multi-class classification using DermaMNIST, employing transfer learning and various layer configurations. One configuration achieves results that match or surpass existing methods. This study suggests that convolutional neural networks (CNNs) can drive progress in biomedical image analysis, significantly enhancing diagnostic accuracy.

Paper number 23:
Title: AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning
Authors: Daning Zhang, Ying Wei
Abstract: Separating target speech from mixed signals containing flexible speaker quantities presents a challenging task. While existing methods demonstrate strong separation performance and noise robustness, they predominantly assume prior knowledge of speaker counts in mixtures. The limited research addressing unknown speaker quantity scenarios exhibits significantly constrained generalization capabilities in real acoustic environments. To overcome these challenges, this paper proposes AVFSNet -- an audio-visual speech separation model integrating multi-scale encoding and parallel architecture -- jointly optimized for speaker counting and multi-speaker separation tasks. The model independently separates each speaker in parallel while enhancing environmental noise adaptability through visual information integration. Comprehensive experimental evaluations demonstrate that AVFSNet achieves state-of-the-art results across multiple evaluation metrics and delivers outstanding performance on diverse datasets.

Paper number 24:
Title: Learning-Based Cost-Aware Defense of Parallel Server Systems against Malicious Attacks
Authors: Yuzhen Zhan, Li Jin
Abstract: We consider the cyber-physical security of parallel server systems, which is relevant for a variety of engineering applications such as networking, manufacturing, and transportation. These systems rely on feedback control and may thus be vulnerable to malicious attacks such as denial-of-service, data falsification, and instruction manipulations. In this paper, we develop a learning algorithm that computes a defensive strategy to balance technological cost for defensive actions and performance degradation due to cyber attacks as mentioned above. We consider a zero-sum Markov security game. We develop an approximate minimax-Q learning algorithm that efficiently computes the equilibrium of the game, and thus a cost-aware defensive strategy. The algorithm uses interpretable linear function approximation tailored to the system structure. We show that, under mild assumptions, the algorithm converges with probability one to an approximate Markov perfect equilibrium. We first use a Lyapunov method to address the unbounded temporal-difference error due to the unbounded state space. We then use an ordinary differential equation-based argument to establish convergence. Simulation results demonstrate that our algorithm converges about 50 times faster than a representative neural network-based method, with an insignificant optimality gap between 4\%--8\%, depending on the complexity of the linear approximator and the number of parallel servers.

Paper number 25:
Title: From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation
Authors: Jinseo An, Min Jin Lee, Kyu Won Shim, Helen Hong
Abstract: Accurate segmentation of orbital bones in facial computed tomography (CT) images is essential for the creation of customized implants for reconstruction of defected orbital bones, particularly challenging due to the ambiguous boundaries and thin structures such as the orbital medial wall and orbital floor. In these ambiguous regions, existing segmentation approaches often output disconnected or under-segmented results. We propose a novel framework that corrects segmentation results by leveraging consensus from multiple diffusion model outputs. Our approach employs a conditional Bernoulli diffusion model trained on diverse annotation patterns per image to generate multiple plausible segmentations, followed by a consensus-driven correction that incorporates position proximity, consensus level, and gradient direction similarity to correct challenging regions. Experimental results demonstrate that our method outperforms existing methods, significantly improving recall in ambiguous regions while preserving the continuity of thin structures. Furthermore, our method automates the manual process of segmentation result correction and can be applied to image-guided surgical planning and surgery.

Paper number 26:
Title: Fractional-order controller tuning via minimization of integral of time-weighted absolute error without multiple closed-loop tests
Authors: Ansei Yonezawa, Heisei Yonezawa, Shuichi Yahagi, Itsuro Kajiwara, Shinya Kijimoto
Abstract: This study presents a non-iterative tuning technique for a linear fractional-order (FO) controller, based on the integral of the time-weighted absolute error (ITAE) criterion. Minimizing the ITAE is a traditional approach for tuning FO controllers. This technique reduces the over/undershoot and suppresses the steady-state error. In contrast to conventional approaches of ITAE-based controller tuning, the proposed approach does not require multiple closed-loop experiments or model-based simulations to evaluate the ITAE. The one-shot input/output data is collected from the controlled plant. A fictitious reference signal is defined on the basis of the collected input and output signal, which enables us to evaluate the closed-loop response provided by the arbitrary controller parameters. To avoid repeated experiments that are necessary in the conventional approach, we reformulate the ITAE minimization problem using the fictitious reference signal. The desired FO controller parameters minimizing the ITAE are obtained by solving the optimization problem that is based on the fictitious reference signal. The validity of the proposed approach is demonstrated by a numerical study. The avoidance of repeated experiments significantly reduces the development cost of linear FO controllers, thereby facilitating their practical application.

Paper number 27:
Title: Vertical Vibration Reduction of Maglev Vehicles using Nonlinear MPC
Authors: Mario Hermle, Arnim Kargl, Peter Eberhard
Abstract: This work presents a novel Nonlinear Model Predictive Control (NMPC) strategy for high-speed Maglev vehicles that explicitly incorporates mechanical suspension dynamics into the control model. Unlike conventional approaches, which often neglect the interaction between levitation magnet and car body motion, the proposed method enables predictive vibration mitigation by modeling both electromagnetic forces and suspension behavior. This integrated approach significantly improves passenger comfort and ride quality by reducing vertical oscillations caused by track irregularities. Moreover, it allows for a more effective tuning of the trade-off between precise air gap tracking and ride comfort. Simulations based on a detailed multibody model of the Transrapid demonstrate that the method outperforms existing controllers in vibration suppression, making it a promising solution for future high-speed Maglev applications.

Paper number 28:
Title: Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation
Authors: Guangyao Liu, Tianqi Mao, Yanqun Tang, Jingjing Zhao, Zhenyu Xiao
Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier technique utilizing chirp signals, has been envisioned as an effective solution for high-mobility communication scenarios. In this paper, we develop a multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM, which aims to further improve the spectral and energy efficiencies of AFDM. Specifically, multiple constellation alphabets are selected for different chirp-based subcarriers (chirps). Aside from classical amplitude/phase modulation, additional information bits can be conveyed by the dynamic patterns of both constellation mode selection and chirp activation, without extra energy consumption. Furthermore, we discuss the mode selection strategy and derive an asymptotically tight upper bound on the bit error rate (BER) of the proposed scheme under maximum-likelihood detection. Simulation results are provided to demonstrate the superior performance of MM-AFDM-IM compared to conventional benchmark schemes.

Paper number 29:
Title: Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized Intersection: Deployment, Data Collection, and Preliminary Analysis
Authors: Saswat Priyadarshi Nayak, Guoyuan Wu, Kanok Boriboonsomsin, Matthew Barth
Abstract: Traffic Movement Count (TMC) at intersections is crucial for optimizing signal timings, assessing the performance of existing traffic control measures, and proposing efficient lane configurations to minimize delays, reduce congestion, and promote safety. Traditionally, methods such as manual counting, loop detectors, pneumatic road tubes, and camera-based recognition have been used for TMC estimation. Although generally reliable, camera-based TMC estimation is prone to inaccuracies under poor lighting conditions during harsh weather and nighttime. In contrast, Light Detection and Ranging (LiDAR) technology is gaining popularity in recent times due to reduced costs and its expanding use in 3D object detection, tracking, and related applications. This paper presents the authors' endeavor to develop, deploy and evaluate a dual-LiDAR system at an intersection in the city of Rialto, California, for TMC estimation. The 3D bounding box detections from the two LiDARs are used to classify vehicle counts based on traffic directions, vehicle movements, and vehicle classes. This work discusses the estimated TMC results and provides insights into the observed trends and irregularities. Potential improvements are also discussed that could enhance not only TMC estimation, but also trajectory forecasting and intent prediction at intersections.

Paper number 30:
Title: Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects
Authors: Morteza Alijani, Wout Joseph, David Plets
Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for next-generation indoor positioning systems (IPS), particularly within the scope of sixth-generation (6G) wireless networks. Its attractiveness stems from leveraging existing lighting infrastructures equipped with light-emitting diodes (LEDs), enabling cost-efficient deployments and achieving high-precision positioning accuracy in the centimeter-todecimeter range. However, widespread adoption of traditional VLP solutions faces significant barriers due to the increased costs and operational complexity associated with modulating LEDs, which consequently reduces illumination efficiency by lowering their radiant flux. To address these limitations, recent research has introduced the concept of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals of Opportunity (LSOOP) emitted by unmodulated illumination sources such as conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure alternative for indoor positioning by eliminating the need for modulation hardware and maintaining lighting efficiency. This paper delineates the fundamental principles of uVLP, provides a comparative analysis of uVLP versus conventional VLP methods, and classifies existing uVLP techniques according to receiver technologies into intensity-based methods (e.g., photodiodes, solar cells, etc.) and imaging-based methods. Additionally, we propose a comprehensive taxonomy categorizing techniques into demultiplexed and undemultiplexed approaches. Within this structured framework, we critically review current advancements in uVLP, discuss prevailing challenges, and outline promising research directions essential for developing robust, scalable, and widely deployable uVLP solutions.

Paper number 31:
Title: Angle Estimation of a Single Source with Massive Uniform Circular Arrays
Authors: Mingyan Gong
Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an essential topic in array signal processing. Widely adopted uniform linear arrays can only provide estimates of source azimuth. Thus, uniform circular arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal coverage and additional elevation angle information. Considering that with a massive UCA, its polar angles of array sensors can approximately represent azimuth angles over $360^{\circ}$ using angle quantization, a simple two-dimensional DOA estimation method for a single source is proposed. In this method, the quantized azimuth angle estimate is obtained by only calculating and comparing a number of covariances, based on which the elevation angle estimate is then obtained by an explicit formula. Thus, the proposed method is computationally simple and suitable for real-time signal processing. Numerical results verify that the proposed method can obtain azimuth as well as elevation angle estimates and the estimates can be used as starting points of multidimensional searches for methods with higher accuracy. Additionally, the proposed method can still work in the presence of nonuniform noise.

Paper number 32:
Title: Multifrequency system model for multiport time-modulated scatterers
Authors: Aleksandr D. Kuznetsov (1), Jari Holopainen (1), Ville Viikari (1) ((1) Aalto University)
Abstract: Utilizing scatterers in communication engineering, such as reconfigurable intelligent surfaces (RISs) and backscatter systems, requires physically consistent models for accurate performance prediction. A multiport model, which also accounts for structural scattering, has been developed for non-periodic scatterers. However, many emerging systems operate at multiple frequencies or generate intermodulation harmonics, particularly when incorporating space-time modulation (STM) or dynamic load control. These functionalities demand advanced modeling approaches capable of capturing scattering behavior across several frequencies and directions simultaneously. This article extends a multiport S-parameters-based model for predicting the scattering properties of multifrequency operating structures. The model extends the applicability of convenient S-matrix models to time-modulated multiport structures. Unlike known approaches, this model incorporates structural scattering, mutual coupling, the possibility of non-digital modulation, and non-periodic configurations, enabling precise analysis and optimization for a broad range of communication and sensing systems. Validation against experimental results for a space-time modulated scattering structure demonstrates the accuracy and practical applicability of the proposed model.

Paper number 33:
Title: fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting
Authors: Alicia Durrer, Florentin Bieder, Paul Friedrich, Bjoern Menze, Philippe C. Cattin, Florian Kofler
Abstract: Healthy tissue inpainting has significant applications, including the generation of pseudo-healthy baselines for tumor growth models and the facilitation of image registration. In previous editions of the BraTS Local Synthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion probabilistic models (DDPMs) demonstrated qualitatively convincing results but suffered from low sampling speed. To mitigate this limitation, we adapted a 2D image generation approach, combining DDPMs with generative adversarial networks (GANs) and employing a variance-preserving noise schedule, for the task of 3D inpainting. Our experiments showed that the variance-preserving noise schedule and the selected reconstruction losses can be effectively utilized for high-quality 3D inpainting in a few time steps without requiring adversarial training. We applied our findings to a different architecture, a 3D wavelet diffusion model (WDM3D) that does not include a GAN component. The resulting model, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a PSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these scores using only two time steps, completing the 3D inpainting process in 1.81 s per image. When compared to other DDPMs used for healthy brain tissue inpainting, our model is up to 800 x faster while still achieving superior performance metrics. Our proposed method, fastWDM3D, represents a promising approach for fast and accurate healthy tissue inpainting. Our code is available at this https URL.

Paper number 34:
Title: Disentangling coincident cell events using deep transfer learning and compressive sensing
Authors: Moritz Leuthner, Rafael Vorländer, Oliver Hayden
Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring, and cell therapy, but coincident events - where multiple cells overlap in a sensing zone - can severely compromise signal fidelity. We present a hybrid framework combining a fully convolutional neural network (FCN) with compressive sensing (CS) to disentangle such overlapping events in one-dimensional sensor data. The FCN, trained on bead-derived datasets, accurately estimates coincident event counts and generalizes to immunomagnetically labeled CD4+ and CD14+ cells in whole blood without retraining. Using this count, the CS module reconstructs individual signal components with high fidelity, enabling precise recovery of single-cell features, including velocity, amplitude, and hydrodynamic diameter. Benchmarking against conventional state-machine algorithms shows superior performance - recovering up to 21% more events and improving classification accuracy beyond 97%. Explinability via class activation maps and parameterized Gaussian template fitting ensures transparency and clinical interpretability. Demonstrated with magnetic flow cytometry (MFC), the framework is compatible with other waveform-generating modalities, including impedance cytometry, nanopore, and resistive pulse sensing. This work lays the foundation for next-generation non-optical single-cell sensing platforms that are automated, generalizable, and capable of resolving overlapping events, broadening the utility of cytometry in translational medicine and precision diagnostics, e.g. cell-interaction studies.

Paper number 35:
Title: QTCAJOSA: Low-Complexity Joint Offloading and Subchannel Allocation for NTN-Enabled IoMT
Authors: Alejandro Flores C., Konstantinos Ntontin, Ashok Bandi, Symeon Chatzinotas
Abstract: In this work, we consider the resource allocation problem for task offloading from Internet of Medical Things (IoMT) devices, to a non-terrestrial network. The architecture considers clusters of IoMT devices that offload their tasks to a dedicated unmanned aerial vehicle (UAV) serving as a multi-access edge computing (MEC) server, which can compute the task or further offload it to an available high-altitude platform station (HAPS) or to a low-earth orbit (LEO) satellite for remote computing. We formulate a problem that has as objective the minimization of the weighted sum delay of the tasks. Given the non-convex nature of the problem, and acknowledging that the complexity of the optimization algorithms impact their performance, we derive a low-complexity joint subchannel allocation and offloading decision algorithm with dynamic computing resource initialization, developed as a greedy heuristic based on convex optimization criteria. Simulations show the gain obtained by including the different non-terrestrial nodes against architectures without them.

Paper number 36:
Title: Transient-Stability-Aware Frequency Provision in IBR-Rich Grids via Information Gap Decision Theory and Deep Learning
Authors: Amin Masoumi, Mert Korkali
Abstract: This paper introduces a framework to address the critical loss of transient stability caused by reduced inertia in grids with high inverter-based resource (IBR) penetration. The proposed method integrates a predictive deep learning (DL) model with information gap decision theory (IGDT) to create a risk-averse dispatch strategy. By reformulating the conventional virtual inertia scheduling (VIS) problem, the framework uses early predictions of post-fault dynamics to proactively redispatch resources, ensuring the system's center of inertia remains stable under worst-case contingencies. Validated on the IEEE 39-bus system with 70% IBR penetration, the proposed approach prevents system collapse where a conventional VIS strategy fails, ensuring frequency stability at a cost increase of only 5%.

Paper number 37:
Title: Privacy-Preserving Fusion for Multi-Sensor Systems Under Multiple Packet Dropouts
Authors: Jie Huang, Jason J. R. Liu
Abstract: Wireless sensor networks (WSNs) are critical components in modern cyber-physical systems, enabling efficient data collection and fusion through spatially distributed sensors. However, the inherent risks of eavesdropping and packet dropouts in such networks pose significant challenges to secure state estimation. In this paper, we address the privacy-preserving fusion estimation (PPFE) problem for multi-sensor systems under multiple packet dropouts and eavesdropping attacks. To mitigate these issues, we propose a distributed encoding-based privacy-preserving mechanism (PPM) within a control-theoretic framework, ensuring data privacy during transmission while maintaining the performance of legitimate state estimation. A centralized fusion filter is developed, accounting for the coupling effects of packet dropouts and the encoding-based PPM. Boundedness conditions for the legitimate user's estimation error covariance are derived via a modified algebraic Riccati equation. Additionally, by demonstrating the divergence of the eavesdropper's mean estimation error, the proposed PPFE algorithm's data confidentiality is rigorously analyzed. Simulation results for an Internet-based three-tank system validate the effectiveness of the proposed approach, highlighting its potential to enhance privacy without compromising estimation accuracy.

Paper number 38:
Title: SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution
Authors: Ritik Shah, Marco F. Duarte
Abstract: High-spatial-resolution hyperspectral images (HSI) are essential for applications such as remote sensing and medical imaging, yet HSI sensors inherently trade spatial detail for spectral richness. Fusing high-spatial-resolution multispectral images (HR-MSI) with low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to recover fine spatial structures without sacrificing spectral fidelity. Most state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF) calibration or ground truth high resolution HSI (HR-HSI), both of which are impractical to obtain in real world settings. We present SpectraLift, a fully self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic low-spatial-resolution multispectral image (LR-MSI) obtained by applying the SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an $\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as the optimization objective. At inference, SpectraLift uses the trained network to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in minutes, is agnostic to spatial blur and resolution, and outperforms state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.

Paper number 39:
Title: Real-time control of a magnetohydrodynamic flow
Authors: Adam Uchytil, Milan Korda, Jiří Zemánek
Abstract: We demonstrate the feedback control of a weakly conducting magnetohydrodynamic (MHD) flow via Lorentz forces generated by externally applied electric and magnetic fields. Specifically, we steer the flow of an electrolyte toward prescribed velocity or vorticity patterns using arrays of electrodes and electromagnets positioned around and beneath a fluid reservoir, with feedback provided by planar particle image velocimetry (PIV). Control is implemented using a model predictive control (MPC) framework, in which control signals are computed by minimizing a cost function over the predicted evolution of the flow. The predictor is constructed entirely from data using Koopman operator theory, which enables a linear representation of the underlying nonlinear fluid dynamics. This linearity allows the MPC problem to be solved by alternating between two small and efficiently solvable convex quadratic programs (QPs): one for the electrodes and one for the electromagnets. The resulting controller runs in a closed loop on a standard laptop, enabling real-time control of the flow. We demonstrate the functionality of the approach through experiments in which the flow is shaped to match a range of reference velocity fields and a time-varying vorticity field.

Paper number 40:
Title: Physically Based Neural LiDAR Resimulation
Authors: Richard Marcus, Marc Stamminger
Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the field of LiDAR simulation and large-scale 3D scene reconstruction. While solutions for faster rendering or handling dynamic scenes have been proposed, LiDAR specific effects remain insufficiently addressed. By explicitly modeling sensor characteristics such as rolling shutter, laser power variations, and intensity falloff, our method achieves more accurate LiDAR simulation compared to existing techniques. We demonstrate the effectiveness of our approach through quantitative and qualitative comparisons with state-of-the-art methods, as well as ablation studies that highlight the importance of each sensor model component. Beyond that, we show that our approach exhibits advanced resimulation capabilities, such as generating high resolution LiDAR scans in the camera perspective. Our code and the resulting dataset are available at this https URL.

Paper number 41:
Title: Assessing the economic benefits of space weather mitigation investment decisions: Evidence from Aotearoa New Zealand
Authors: Edward J. Oughton, Andrew Renton, Daniel Mac Marnus, Craig J. Rodger
Abstract: Space weather events pose a growing threat to modern economies, yet their macroeconomic consequences still remain underexplored. This study presents the first dedicated economic assessment of geomagnetic storm impacts on Aotearoa New Zealand, quantifying potential GDP losses across seven disruption and mitigation scenarios due to an extreme coronal mass ejection (CME). The primary focus is upon the damaging impacts of geomagnetically induced currents (GICs) on the electrical power transmission network. The goal is to support decision-making around space weather mitigation investments by providing a first-order approximation of their potential economic benefits. We find that in the absence of mitigation, a severe but realistic storm could result in up to NZ\$8.36 billion in lost GDP, with more than half stemming from cascading supply chain effects. Yet, even less severe scenarios incur losses exceeding NZ\$3 billion. Importantly, research-led operational strategies, such as optimized switching and islanding, can avoid up to NZ\$370 million in losses for as little as NZ\$500,000 in expenditure, delivering a benefit-cost ratio of 740 to 1. Moreover, physical protections such as GIC blocking devices further reduce disruption to as low as NZ\$1.12 billion, with avoided GDP losses up to NZ\$2.3 billion, and benefit-cost returns up to 80 to 1. When also acknowledging unmodelled impacts, including multi-billion losses in capital equipment and long-term revenue, the economic rationale for pre-emptive mitigation becomes even more pertinent. Future research needs to integrate the modelling of capital and revenue losses for strategically important industrial facilities.

Paper number 42:
Title: GLOMIA-Pro: A Generalizable Longitudinal Medical Image Analysis Framework for Disease Progression Prediction
Authors: Shuaitong Zhang, Yuchen Sun, Yong Ao, Xuehuan Zhang, Ruoshui Yang, Jiantao Xu, Zuwu Ai, Haike Zhang, Xiang Yang, Yao Xu, Kunwei Li, Duanduan Chen
Abstract: Longitudinal medical images are essential for monitoring disease progression by capturing spatiotemporal changes associated with dynamic biological processes. While current methods have made progress in modeling spatiotemporal patterns, they face three key limitations: (1) lack of generalizable framework applicable to diverse disease progression prediction tasks; (2) frequent overlook of the ordinal nature inherent in disease staging; (3) susceptibility to representation collapse due to structural similarities between adjacent time points, which can obscure subtle but discriminative progression biomarkers. To address these limitations, we propose a Generalizable LOngitudinal Medical Image Analysis framework for disease Progression prediction (GLOMIA-Pro). GLOMIA-Pro consists of two core components: progression representation extraction and progression-aware fusion. The progression representation extraction module introduces a piecewise orthogonal attention mechanism and employs a novel ordinal progression constraint to disentangle finegrained temporal imaging variations relevant to disease progression. The progression-aware fusion module incorporates a redesigned skip connection architecture which integrates the learned progression representation with current imaging representation, effectively mitigating representation collapse during cross-temporal fusion. Validated on two distinct clinical applications: knee osteoarthritis severity prediction and esophageal cancer treatment response assessment, GLOMIA-Pro consistently outperforms seven state-of-the-art longitudinal analysis methods. Ablation studies further confirm the contribution of individual components, demonstrating the robustness and generalizability of GLOMIA-Pro across diverse clinical scenarios.

Paper number 43:
Title: On the factorization of matrices into products of positive definite ones
Authors: Mahmoud Abdelgalil, Tryphon T. Georgiou
Abstract: The present work revisits and provides a new approach on a result by Charles Ballantine, on the factorization of a square matrix with positive determinant into a product of positive definite factors. {\em Ballantine-type} factorizations, that bound the number of positive definite factors, proved central in solving a basic, yet elusive control problem--the strong controllability of a linear system via control in the form of state feedback. Ballantine's result transcends control engineering, and highlights the little appreciated fact that rotations can be realized by the successive application of irrotational motions. Our approach is constructive and is based on the theory of optimal mass transport, specifically, it relates successive rotations of Gaussian distributions to corresponding optimal transport maps that constitute the sought factors.

Paper number 44:
Title: Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates
Authors: Carlos De La Vega Martin, Rodrigo Diaz Fernandez, Mark Sandler
Abstract: Physical modelling synthesis aims to generate audio from physical simulations of vibrating structures. Thin elastic plates are a common model for drum membranes. Traditional numerical methods like finite differences and finite elements offer high accuracy but are computationally demanding, limiting their use in real-time audio applications. This paper presents a comparative analysis of neural network-based approaches for solving the vibration of nonlinear elastic plates. We evaluate several state-of-the-art models, trained on short sequences, for prediction of long sequences in an autoregressive fashion. We show some of the limitations of these models, and why is not enough to look at the prediction error in the time domain. We discuss the implications for real-time audio synthesis and propose future directions for improving neural approaches to model nonlinear vibration.

Paper number 45:
Title: Robust Resource Allocation for Pinching-Antenna Systems under Imperfect CSI
Authors: Ming Zeng, Xianbin Wang, Yuanwei Liu, Zhiguo Ding, George K. Karagiannidis, H. Vincent Poor
Abstract: Pinching-antenna technology has lately showcased its promising capability for reconfiguring wireless propagation environments, especially in high-frequency communication systems like millimeter-wave and terahertz bands. By dynamically placing the antenna over a dielectric waveguide, line-of-sight (LoS) connections can be made to significantly improve system performance. Although recent research have illustrated the advantages of pinching-antenna-assisted designs, they mainly presuppose complete knowledge of user locations -- an impractical assumption in real-world systems. To address this issue, the robust resource allocation in a multi-user pinching antenna downlink system with uncertain user positions is investigated, aiming to minimize total transmit power while satisfying individual outage probability constraints. First, we address the single-user case, deriving the optimal pinching antenna position and obtaining the corresponding power allocation using a bisection method combined with geometric analysis. We then extend this solution to the multi-user case. In this case, we optimize the pinching antenna position using a particle swarm optimization (PSO) algorithm to handle the resulting non-convex and non-differentiable optimization problem. Simulation results demonstrate that the proposed scheme outperforms conventional fixed-antenna systems and validate the effectiveness of the PSO-based antenna placement strategy under location uncertainty.

Paper number 46:
Title: Keep the beat going: Automatic drum transcription with momentum
Authors: Alisha L. Foster, Robert J. Webber
Abstract: A simple, interpretable way to perform automatic drum transcription is by factoring the magnitude spectrogram of a recorded musical piece using a partially fixed nonnegative matrix factorization. There are two natural ways to optimize the nonnegative matrix factorization, including a multiplicative update rule and projected gradient descent with momentum. The methods differ in their empirical accuracies and theoretical convergence guarantees. This paper summarizes the methods and their time complexities, and it applies the methods to the ENST-Drums data set and an original recording from the author's band, evaluating the empirical accuracy with respect to ground-truth drum annotations. The results indicate that projected gradient descent with momentum leads to higher accuracy for a fixed runtime, and it satisfies stronger convergence guarantees.

Paper number 47:
Title: Boundary Feedback and Observer Synthesis for a Class of Nonlinear Parabolic--Elliptic PDE Systems
Authors: Kamal Fenza, Moussa Labbadi, Mohamed Ouzahra
Abstract: This paper investigates the stabilization of a coupled system comprising a parabolic PDE and an elliptic PDE with nonlinear terms. A rigorous backstepping design provides an explicit boundary control law and exponentially convergent observers from partial boundary measurements. Several theorems ensure exponential stability and well-posedness of the nonlinear closed-loop system.

Paper number 48:
Title: Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning
Authors: David Freire-Obregón, Agnieszka Dubiel, Prasoon Kumar Vinodkumar, Gholamreza Anbarjafari, Dorota Kamińska, Modesto Castrillón-Santana
Abstract: Recent advances have shown promise in emotion recognition from electroencephalogram (EEG) signals by employing bi-hemispheric neural architectures that incorporate neuroscientific priors into deep learning models. However, interpretability remains a significant limitation for their application in sensitive fields such as affective computing and cognitive modeling. In this work, we introduce a post-hoc interpretability framework tailored to dual-stream EEG classifiers, extending the Local Interpretable Model-Agnostic Explanations (LIME) approach to accommodate structured, bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch inputs corresponding to left and right-hemisphere EEG channel groups. It decomposes prediction relevance into per-channel contributions across hemispheres and emotional classes. We apply this framework to a previously validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset of EEG recordings captured during a VR-based emotion elicitation task. The resulting explanations reveal emotion-specific hemispheric activation patterns consistent with known neurophysiological phenomena, such as frontal lateralization in joy and posterior asymmetry in sadness. Furthermore, we aggregate local explanations across samples to derive global channel importance profiles, enabling a neurophysiologically grounded interpretation of the model's decisions. Correlation analysis between symmetric electrodes further highlights the model's emotion-dependent lateralization behavior, supporting the functional asymmetries reported in affective neuroscience.

Paper number 49:
Title: Physics constrained learning of stochastic characteristics
Authors: Pardha Sai Krishna Ala, Ameya Salvi, Venkat Krovi, Matthias Schmid
Abstract: Accurate state estimation requires careful consideration of uncertainty surrounding the process and measurement models; these characteristics are usually not well-known and need an experienced designer to select the covariance matrices. An error in the selection of covariance matrices could impact the accuracy of the estimation algorithm and may sometimes cause the filter to diverge. Identifying noise characteristics has long been a challenging problem due to uncertainty surrounding noise sources and difficulties in systematic noise modeling. Most existing approaches try identifying unknown covariance matrices through an optimization algorithm involving innovation sequences. In recent years, learning approaches have been utilized to determine the stochastic characteristics of process and measurement models. We present a learning-based methodology with different loss functions to identify noise characteristics and test these approaches' performance for real-time vehicle state estimation

Paper number 50:
Title: FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks
Authors: Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak
Abstract: Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: this https URL.

Paper number 51:
Title: Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine
Authors: Anastasia Kuznetsova, Inseon Jang, Wootaek Lim, Minje Kim
Abstract: Neural audio codecs, leveraging quantization algorithms, have significantly impacted various speech/audio tasks. While high-fidelity reconstruction is paramount for human perception, audio coding for machines (ACoM) prioritizes efficient compression and downstream task performance, disregarding perceptual nuances. This work introduces an efficient ACoM method that can compress and quantize any chosen intermediate feature representation of an already trained speech/audio downstream model. Our approach employs task-specific loss guidance alongside residual vector quantization (RVQ) losses, providing ultra-low bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model performance. The resulting tokenizer is adaptable to various bitrates and model sizes for flexible deployment. Evaluated on automatic speech recognition and audio classification, our method demonstrates its efficacy and potential for broader task and architectural applicability through appropriate regularization.

Paper number 52:
Title: AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation
Authors: Potsawee Manakul, Woody Haosheng Gan, Michael J. Ryan, Ali Sartaz Khan, Warit Sirichotedumrong, Kunat Pipatanakul, William Held, Diyi Yang
Abstract: Current speech evaluation suffers from two critical limitations: the need and difficulty of designing specialized systems targeting individual audio characteristics, and poor correlation between automatic evaluation methods and human preferences. This work presents a systematic study of Large Audio Model (LAM) as a Judge, AudioJudge, investigating whether it can provide a unified evaluation framework that addresses both challenges. We systematically explore AudioJudge across audio characteristic detection tasks, including pronunciation, speaking rate, speaker identification and speech quality, and system-level human preference simulation for automated benchmarking. We investigate different prompt engineering strategies, finding that audio concatenation combined with in-context learning significantly improves performance across both audio characteristic detection and human preference simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to enable general-purpose multi-aspect audio evaluation. This method decomposes speech assessment into specialized judges for lexical content, speech quality, and paralinguistic features, achieving up to 0.91 Spearman correlation with human preferences on our system ranking benchmark. Robustness analysis reveals that while LAMs maintain strong performance under acoustic noise, they exhibit significant verbosity and positional biases that require careful mitigation.

Paper number 53:
Title: Estimation of Regions of Attraction for Nonlinear Systems via Coordinate-Transformed TS Models and Piecewise Quadratic Lyapunov Functions
Authors: Artun Sel, Mehmet Koruturk, Erdi Sayar
Abstract: This paper presents a novel approach for computing enlarged Region of Attractions (ROA) for nonlinear dynamical systems through the integration of multiple coordinate transformations and piecewise quadratic Lyapunov functions within the Takagi-Sugeno (TS) modeling framework. While existing methods typically follow a single-path approach of original system $\rightarrow$ TS model $\rightarrow$ ROA computation, the proposed methodology systematically applies a sequence of coordinate transformations to generate multiple system representations, each yielding distinct ROA estimations. Specifically, the approach transforms the original nonlinear system using transformation matrices $T_1, T_2, \ldots, T_N$ to obtain $N$ different coordinate representations, constructs corresponding TS models for each transformed system, and computes individual ROAs using piecewise quadratic Lyapunov functions. The final ROA estimate is obtained as the union of all computed regions, leveraging the flexibility inherent in piecewise quadratic Lyapunov functions compared to traditional quadratic approaches. The enhanced methodology demonstrates significant improvements in ROA size estimation compared to conventional single-transformation techniques, as evidenced through comparative analysis with existing TS-based stability methods.

Paper number 54:
Title: Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries
Authors: Minyoung Kim, Sehwan Park, Sungmin Cha, Paul Hongsuck Seo
Abstract: Recent advances in voice cloning and lip synchronization models have enabled Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are manipulated to mimic a target speaker. This significantly increases the risk of misinformation by making fake content seem real. To address this issue, existing methods detect or localize manipulations but cannot recover the authentic audio that conveys the semantic content of the message. This limitation reduces their effectiveness in combating audiovisual misinformation. In this work, we introduce the task of Authentic Audio Recovery (AAR) and Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal watermarking framework to embed authentic audio into visuals before manipulation. This enables AAR, TLA, and a robust defense against misinformation. Extensive experiments demonstrate the strong performance of our method in AAR and TLA against various manipulations, including voice cloning and lip synchronization.

Paper number 55:
Title: Sample-Constrained Black Box Optimization for Audio Personalization
Authors: Rajalaxmi Rajagopalan, Yu-Lin Wei, Romit Roy Choudhury
Abstract: We consider the problem of personalizing audio to maximize user experience. Briefly, we aim to find a filter $h^*$, which applied to any music or speech, will maximize the user's satisfaction. This is a black-box optimization problem since the user's satisfaction function is unknown. Substantive work has been done on this topic where the key idea is to play audio samples to the user, each shaped by a different filter $h_i$, and query the user for their satisfaction scores $f(h_i)$. A family of ``surrogate" functions is then designed to fit these scores and the optimization method gradually refines these functions to arrive at the filter $\hat{h}^*$ that maximizes satisfaction. In certain applications, we observe that a second type of querying is possible where users can tell us the individual elements $h^*[j]$ of the optimal filter $h^*$. Consider an analogy from cooking where the goal is to cook a recipe that maximizes user satisfaction. A user can be asked to score various cooked recipes (e.g., tofu fried rice) or to score individual ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$ queries, where a query can be of either type, our goal is to find the recipe that will maximize this user's satisfaction. Our proposal builds on Sparse Gaussian Process Regression (GPR) and shows how a hybrid approach can outperform any one type of querying. Our results are validated through simulations and real world experiments, where volunteers gave feedback on music/speech audio and were able to achieve high satisfaction levels. We believe this idea of hybrid querying opens new problems in black-box optimization and solutions can benefit other applications beyond audio personalization.

Paper number 56:
Title: Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features
Authors: J. M. Chan Sri Manukalpa, H. S. Bopage, W. A. M. Jayawardena, P. K. P. G. Panduwawala
Abstract: Structural pests, such as termites, pose a serious threat to wooden buildings, resulting in significant economic losses due to their hidden and progressive damage. Traditional detection methods, such as visual inspections and chemical treatments, are invasive, labor intensive, and ineffective for early stage infestations. To bridge this gap, this study proposes a non invasive deep learning based acoustic classification framework for early termite detection. We aim to develop a robust, scalable model that distinguishes termite generated acoustic signals from background noise. We introduce a hybrid Convolutional Neural Network Long Short Term Memory architecture that captures both spatial and temporal features of termite activity. Audio data were collected from termite infested and clean wooden samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN LSTM model to classify the signals. Experimental results show high performance, with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis reveals that the hybrid model outperforms standalone CNN and LSTM architectures, underscoring its combined strength. Notably, the model yields low false-negative rates, which is essential for enabling timely intervention. This research contributes a non invasive, automated solution for early termite detection, with practical implications for improved pest monitoring, minimized structural damage, and better decision making by homeowners and pest control professionals. Future work may integrate IoT for real time alerts and extend detection to other structural pests.

Paper number 57:
Title: Large Language Models' Internal Perception of Symbolic Music
Authors: Andrew Shin, Kunitake Kaneko
Abstract: Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.

Paper number 58:
Title: Autoregressive Speech Enhancement via Acoustic Tokens
Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli
Abstract: In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.

Paper number 59:
Title: DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning
Authors: Rahel Rickenbach, Bruce Lee, René Zurbrügg, Carmen Amo Alonso, Melanie N. Zeilinger
Abstract: The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator. A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks. Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers. Furthermore, there is often no principled way to evaluate for hallucination before task execution. To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions. To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction. Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples. Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.

Paper number 60:
Title: Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios
Authors: John Hansen, Satwik Dutta, Ellen Grand
Abstract: A child's spoken ability continues to change until their adult age. Until 7-8yrs, their speech sound development and language structure evolve rapidly. This dynamic shift in their spoken communication skills and data privacy make it challenging to curate technology-ready speech corpora for children. This study aims to bridge this gap and provide researchers and practitioners with the best practices and considerations for developing such a corpus based on an intended goal. Although primarily focused on educational goals, applications of child speech data have spread across fields including clinical and forensics fields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of data collection inspired by prior collection efforts and our experience/knowledge. We also provide a guide to establish collaboration, trust, and for navigating the human subjects research protocol. This study concludes with guidelines for corpus quality check, triage, and annotation.

Paper number 61:
Title: Robust Beamforming Design for Secure Near-Field ISAC Systems
Authors: Ziqiang CHen, Feng Wang, Guojun Han, Xin Wang, Vincent K. N. Lau
Abstract: This letter investigates the robust beamforming design for a near-field secure integrated sensing and communication (ISAC) system with multiple communication users (CUs) and targets, as well as multiple eavesdroppers. Taking into account the channel uncertainty constraints, we maximize the minimum sensing beampattern gain for targets, subject to the minimum signal-to-interference-plus-noise ratio (SINR) constraint for each CU and the maximum SINR constraint for each eavesdropper, as well as the ISAC transmit power constraint. The formulated design problem is non-convex. As a low-complexity suboptimal solution, we first apply the S-Procedure to convert semi-infinite channel uncertainty constraints into linear matrix inequalities (LMIs) and then use the state-of-the-art sequential rank-one constraint relaxation (SROCR) method to address the rank-one constraints. The numerical results show that the proposed ISAC beamforming design scheme outperforms the existing semidefinite relaxation (SDR) and other baseline schemes, and it significantly enhances security and robustness for near-field ISAC systems.

Paper number 62:
Title: From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation
Authors: Mengxi Liu, Lala Shakti Swarup Ray, Sizhen Bian, Ko Watanabe, Ankur Bhatt, Joanna Sorysz, Russel Torah, Bo Zhou, Paul Lukowicz
Abstract: We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validate NeckSense on 7 participants using the current SOTA pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact, line-of-sight-free bio-impedance wearable can deliver head-tracking performance comparable to SOTA vision-based methods.

Paper number 63:
Title: Federated Learning for Commercial Image Sources
Authors: Shreyansh Jain, Koteswar Rao Jerripothula
Abstract: Federated Learning is a collaborative machine learning paradigm that enables multiple clients to learn a global model without exposing their data to each other. Consequently, it provides a secure learning platform with privacy-preserving capabilities. This paper introduces a new dataset containing 23,326 images collected from eight different commercial sources and classified into 31 categories, similar to the Office-31 dataset. To the best of our knowledge, this is the first image classification dataset specifically designed for Federated Learning. We also propose two new Federated Learning algorithms, namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from its previous client, updates them through local training, and passes them to the next client, thus forming a cyclic topology. In Fed-Star, a client receives weights from all other clients, updates its local weights through pre-aggregation (to address statistical heterogeneity) and local training, and sends its updated local weights to all other clients, thus forming a star-like topology. Our experiments reveal that both algorithms perform better than existing baselines on our newly introduced dataset.

Paper number 64:
Title: Wireless Multi-Port Sensing: Virtual-VNA-Enabled De-Embedding of an Over-the-Air Fixture
Authors: Philipp del Hougne
Abstract: We develop a multi-port-backscatter-modulation technique to determine, over the air (OTA), the scattering parameters of a linear, passive, time-invariant multi-port device under test (DUT). A set of "not-directly-accessible" (NDA) antennas can be switched between being terminated by the DUT or by a specific, known, tunable load network. Waves can be radiated and captured via a distinct set of "accessible" antennas that couple OTA to the NDA antennas. First, we characterize the OTA fixture between the accessible antennas' ports and the DUT's ports. We achieve this based on our recently introduced "Virtual VNA" technique; specifically, we connect the NDA antennas to the tunable load network and measure the scattering at the accessible antennas' ports for various configurations of the tunable load network. Second, we connect the NDA antennas to the DUT and measure the scattering at the accessible antennas' ports. Third, we de-embed the OTA fixture to retrieve the DUT's scattering parameters. We experimentally validate our technique at 2.45 GHz for 1-port DUTs and 5-port DUTs, considering a rich-scattering OTA fixture inside a reverberation chamber. We systematically study the influence of the number of accessible antennas and various conceivable simplifications in terms of the system model as well as the properties of the tunable load network. Our wireless multi-port sensing technique can find applications in areas like RFID and wireless bioelectronics.

Paper number 65:
Title: ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning
Authors: Rahel Rickenbach, Alan A. Lahoud, Erik Schaffernicht, Melanie N. Zeilinger, Johannes A. Stork
Abstract: The computational burden of model predictive control (MPC) limits its application on real-time systems, such as robots, and often requires the use of short prediction horizons. This not only affects the control performance, but also increases the difficulty of designing MPC cost functions that reflect the desired long-term objective. This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC. It improves performance over alternative methods, such as approximate explicit MPC and automatic cost parameter tuning, in particular in terms of i) optimizing the long term objective; ii) maintaining computational costs comparable to a short-horizon MPC; iii) ensuring constraint satisfaction; and iv) generalizing control behaviour to environments not observed during training. For this purpose, ZipMPC leverages the concept of differentiable MPC with neural networks to propagate gradients of the imitation loss through the MPC optimization. We validate our proposed method in simulation and real-world experiments on autonomous racing. ZipMPC consistently completes laps faster than selected baselines, achieving lap times close to the long-horizon MPC baseline. In challenging scenarios where the short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In particular, these performance gains are also observed on tracks unseen during training.

Paper number 66:
Title: Generalized Scattering Matrix Framework for Modeling Implantable Antennas in Multilayered Spherical Media
Authors: Chenbo Shi, Xin Gu, Shichen Liang, Jin Pan
Abstract: This paper presents a unified and efficient framework for analyzing antennas embedded in spherically stratified media -- a model broadly applicable to implantable antennas in biomedical systems and radome-enclosed antennas in engineering applications. The proposed method decouples the modeling of the antenna and its surrounding medium by combining the antenna's free-space generalized scattering matrix (GSM) with a set of extended spherical scattering operators (SSOs) that rigorously capture the electromagnetic interactions with multilayered spherical environments. This decoupling enables rapid reevaluation under arbitrary material variations without re-simulating the antenna, offering substantial computational advantages over traditional dyadic Green's function (DGF)-based MoM approaches. The framework supports a wide range of spherical media, including radially inhomogeneous and uniaxially anisotropic layers. Extensive case studies demonstrate excellent agreement with full-wave and DGF-based solutions, confirming the method's accuracy, generality, and scalability. Code implementations are provided to facilitate adoption and future development.

Paper number 67:
Title: Secure Pinching Antenna-aided ISAC
Authors: Elmehdi Illi, Marwa Qaraqe, Ali Ghrayeb
Abstract: In this letter, a pinching antenna (PA)-aided scheme for establishing a secure integrated sensing and communication system (ISAC) is investigated. The underlying system comprises a dual-functional radar communication (DFRC) base station (BS) linked to multiple waveguides to serve several downlink users while sensing a set of malicious targets in a given area. The PA-aided BS aims at preserving communication confidentiality with the legitimate users while being able to detect malicious targets. One objective of the proposed scheme is to optimize the PA locations, based on which an optimal design of the legitimate signal beamforming and artificial noise covariance matrices is provided to maximize the network's sensing performance, subject to secrecy and total power constraints. We demonstrate the efficacy of the proposed scheme through numerical examples and compare that against a traditional DFRC ISAC system with a uniform linear array of half-wavelength-spaced antennas. We show that the proposed scheme outperforms the baseline PA-aided scheme with equidistant PAs by $3$ dB in terms of illumination power, while it can provide gains of up to $30$ dB of the same metric against a traditional ISAC system with half-wavelength-space uniform linear arrays.

Paper number 68:
Title: SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks
Authors: Kutub Uddin, Awais Khan, Muhammad Umar Farooq, Khalid Malik
Abstract: Audio plays a crucial role in applications like speaker verification, voice-enabled smart devices, and audio conferencing. However, audio manipulations, such as deepfakes, pose significant risks by enabling the spread of misinformation. Our empirical analysis reveals that existing methods for detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks, particularly those attacked using generative adversarial networks. In this article, we propose a novel collaborative learning method called SHIELD to defend against generative AF attacks. To expose AF signatures, we integrate an auxiliary generative model, called the defense (DF) generative model, which facilitates collaborative learning by combining input and output. Furthermore, we design a triplet model to capture correlations for real and AF attacked audios with real-generated and attacked-generated audios using auxiliary generative models. The proposed SHIELD strengthens the defense against generative AF attacks and achieves robust performance across various generative models. The proposed AF significantly reduces the average detection accuracy from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild, and from 98.41% to 51.18% for HalfTruth for three different generative models. The proposed SHIELD mechanism is robust against AF attacks and achieves an average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%, and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and HalfTruth datasets, respectively.

Paper number 69:
Title: Automatically assessing oral narratives of Afrikaans and isiXhosa children
Authors: R. Louw (1), E. Sharratt (1), F. de Wet (1), C. Jacobs (1), A. Smith (1), H. Kamper (1) ((1) Stellenbosch University)
Abstract: Developing narrative and comprehension skills in early childhood is critical for later literacy. However, teachers in large preschool classrooms struggle to accurately identify students who require intervention. We present a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition followed by a machine learning scoring model to predict narrative and comprehension scores. For scoring predicted transcripts, we compare a linear model to a large language model (LLM). The LLM-based system outperforms the linear model in most cases, but the linear system is competitive despite its simplicity. The LLM-based system is comparable to a human expert in flagging children who require intervention. We lay the foundation for automatic oral assessments in classrooms, giving teachers extra capacity to focus on personalised support for children's learning.

Paper number 70:
Title: Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead Electricity Price Forecasting in European Markets
Authors: Maria Margarida Mascarenhas, Jilles De Blauwe, Mikael Amelin, Hussain Kazmi
Abstract: Accurate short-term electricity price forecasting is crucial for strategically scheduling demand and generation bids in day-ahead markets. While data-driven techniques have shown considerable prowess in achieving high forecast accuracy in recent years, they rely heavily on the quality of input covariates. In this paper, we investigate whether asynchronously published prices as a result of differing gate closure times (GCTs) in some bidding zones can improve forecasting accuracy in other markets with later GCTs. Using a state-of-the-art ensemble of models, we show significant improvements of 22% and 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3) respectively, when including price data from interconnected markets with earlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement holds for both general as well as extreme market conditions. Our analysis also yields further important insights: frequent model recalibration is necessary for maximum accuracy but comes at substantial additional computational costs, and using data from more markets does not always lead to better performance - a fact we delve deeper into with interpretability analysis of the forecast models. Overall, these findings provide valuable guidance for market participants and decision-makers aiming to optimize bidding strategies within increasingly interconnected and volatile European energy markets.

Paper number 71:
Title: Voxtral
Authors: Alexander H. Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devendra Singh Chaplot, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, Jean-Hadrien Chabran, Jessica Chudnovsky, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Kush Jain, Lélio Renard Lavaud, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Mickaël Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Rémi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Shashwat Dalal, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz
Abstract: We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.

Paper number 72:
Title: Taming Diffusion Transformer for Real-Time Mobile Video Generation
Authors: Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov
Abstract: Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and real-time generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable real-time performance on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platform while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max, demonstrating the feasibility of real-time, high-quality video generation on mobile devices.

Paper number 73:
Title: Fast Variational Block-Sparse Bayesian Learning
Authors: Jakob Möderl, Erik Leitinger, Bernard H. Fleury, Franz Pernkopf, Klaus Witrisal
Abstract: We propose a variational Bayesian (VB) implementation of block-sparse Bayesian learning (BSBL) to compute proxy probability density functions (PDFs) that approximate the posterior PDFs of the weights and associated hyperparameters in a block-sparse linear model, resulting in an iterative algorithm coined variational BSBL (VA-BSBL). The priors of the hyperparameters are selected to belong to the family of generalized inverse Gaussian distributions. This family contains as special cases commonly used hyperpriors. Inspired by previous work on classical sparse Bayesian learning (SBL), we investigate the update stage in which the proxy PDFs of a single block of weights and of its associated hyperparameter are successively updated, while keeping the proxy PDFs of the other parameters fixed. This stage defines a nonlinear first-order recurrence relation for the mean of the proxy PDF of the hyperparameter. By iterating this relation "ad infinitum" we obtain a criterion that determines whether the so-generated sequence of hyperparameter means converges or diverges. Incorporating this criterion into the VA-BSBL algorithm yields a fast implementation, coined fast-BSBL (F-BSBL), which achieves a two-order-of-magnitude runtime improvement. We further identify the range of the parameters of the generalized inverse Gaussian distribution which result in an inherent pruning procedure that switches off "weak" components in the model, which is necessary to obtain sparse results. Lastly, we show that expectation-maximization (EM)-based and VB-based implementations of BSBL are identical methods. Thus, we extend a well-known result from classical SBL to BSBL. Consequently, F-BSBL and BSBL using coordinate ascent to maximize the marginal likelihood coincide. These results provide a unified framework for interpreting existing BSBL methods.

Paper number 74:
Title: Learning Lens Blur Fields
Authors: Esther Y. H. Lin, Zhecheng Wang, Rebecca Lin, Daniel Miau, Florian Kainz, Jiawen Chen, Xuaner Cecilia Zhang, David B. Lindell, Kiriakos N. Kutulakos
Abstract: Optical blur is an inherent property of any lens system and is challenging to model in modern cameras because of their complex optical elements. To tackle this challenge, we introduce a high-dimensional neural representation of blur$-$$\textit{the lens blur field}$$-$and a practical method for acquiring it. The lens blur field is a multilayer perceptron (MLP) designed to (1) accurately capture variations of the lens 2D point spread function over image plane location, focus setting and, optionally, depth and (2) represent these variations parametrically as a single, sensor-specific function. The representation models the combined effects of defocus, diffraction, aberration, and accounts for sensor features such as pixel color filters and pixel-specific micro-lenses. To learn the real-world blur field of a given device, we formulate a generalized non-blind deconvolution problem that directly optimizes the MLP weights using a small set of focal stacks as the only input. We also provide a first-of-its-kind dataset of 5D blur fields$-$for smartphone cameras, camera bodies equipped with a variety of lenses, etc. Lastly, we show that acquired 5D blur fields are expressive and accurate enough to reveal, for the first time, differences in optical behavior of smartphone devices of the same make and model. Code and data can be found at this http URL.

Paper number 75:
Title: Input-Output Extension of Underactuated Nonlinear Systems
Authors: Mirko Mizzoni, Amr Afifi, Antonio Franchi
Abstract: This letter proposes a method to integrate auxiliary actuators that enhance the task-space capabilities of commercial underactuated systems, while leaving the internal certified low-level controller untouched. The additional actuators are combined with a feedback-linearizing outer-loop controller, enabling full-pose tracking. We provide conditions under which legacy high-level commands and new actuator inputs can be cohesively coordinated to achieve decoupled control of all degrees of freedom. A comparative study with a standard quadrotor-originally not designed for physical interaction-demonstrates that the proposed modified platform remains stable under contact, while the baseline system diverges. Additionally, simulation results under parameter uncertainty illustrate the robustness of the proposed approach.

Paper number 76:
Title: Using Dynamic Safety Margins as Control Barrier Functions
Authors: Victor Freire, Marco M. Nicotra
Abstract: This paper presents an approach to design control barrier functions (CBFs) for arbitrary state and input constraints using tools from the reference governor literature. In particular, it is shown that dynamic safety margins (DSMs) are CBFs for an augmented system obtained by concatenating the state with a virtual reference. The proposed approach is agnostic to the relative degree and can handle multiple state and input constraints using the control-sharing property of CBFs. The construction of CBFs using Lyapunov-based DSMs is then investigated in further detail. Numerical simulations show that the method outperforms existing DSM-based approaches, while also guaranteeing safety and persistent feasibility of the associated optimization program.

Paper number 77:
Title: Deep Blur Multi-Model (DeepBlurMM) -- a strategy to mitigate the impact of image blur on deep learning model performance in histopathology image analysis
Authors: Yujie Xiang, Bojing Liu, Mattias Rantalainen
Abstract: AI-based models for histopathology whole slide image (WSI) analysis are increasingly common, but unsharp or blurred areas within WSI can significantly reduce prediction performance. In this study, we investigated the effect of image blur on deep learning models and introduced a mixture of experts (MoE) strategy that combines predictions from multiple expert models trained on data with varying blur levels. Using H&E-stained WSIs from 2,093 breast cancer patients, we benchmarked performance on grade classification and IHC biomarker prediction with both CNN- (CNN_CLAM and MoE-CNN_CLAM) and Vision Transformer-based (UNI_CLAM and MoE-UNI_CLAM) models. Our results show that baseline models' performance consistently decreased with increasing blur, but expert models trained on blurred tiles and especially our proposed MoE approach substantially improved performance, and outperformed baseline models in a range of simulated scenarios. MoE-CNN_CLAM outperformed the baseline CNN_CLAM under moderate (AUC: 0.868 vs. 0.702) and mixed blur conditions (AUC: 0.890 vs. 0.875). MoE-UNI_CLAM outperformed the baseline UNI_CLAM model in both moderate (AUC: 0.950 vs. 0.928) and mixed blur conditions (AUC: 0.944 vs. 0.931). This MoE method has the potential to enhance the reliability of AI-based pathology models under variable image quality, supporting broader application in both research and clinical settings.

Paper number 78:
Title: Globally Optimal Movable Antenna-Enhanced multiuser Communication: Discrete Antenna Positioning, Motion Power Consumption, and Imperfect CSI
Authors: Yifei Wu, Dongfang Xu, Derrick Wing Kwan Ng, Wolfgang Gerstacker, Robert Schober
Abstract: Movable antennas (MAs) represent a promising paradigm to enhance the spatial degrees of freedom of conventional multi-antenna systems by dynamically adapting the positions of antenna elements within a designated transmit area. In particular, by employing electro-mechanical MA drivers, the positions of the MA elements can be adjusted to shape a favorable spatial correlation for improving system performance. Although preliminary research has explored beamforming designs for MA systems, the intricacies of the power consumption and the precise positioning of MA elements are not well understood. Moreover, the assumption of perfect CSI adopted in the literature is impractical due to the significant pilot overhead and the extensive time to acquire perfect CSI. To address these challenges, we model the motion of MA elements through discrete steps and quantify the associated power consumption as a function of these movements. Furthermore, by leveraging the properties of the MA channel model, we introduce a novel CSI error model tailored for MA systems that facilitates robust resource allocation design. In particular, we optimize the beamforming and the MA positions at the BS to minimize the total BS power consumption, encompassing both radiated and MA motion power while guaranteeing a minimum required SINR for each user. To this end, novel algorithms exploiting the branch and bound (BnB) method are developed to obtain the optimal solution for perfect and imperfect CSI. Moreover, to support practical implementation, we propose low-complexity algorithms with guaranteed convergence by leveraging successive convex approximation (SCA). Our numerical results validate the optimality of the proposed BnB-based algorithms. Furthermore, we unveil that both proposed SCA-based algorithms approach the optimal performance within a few iterations, thus highlighting their practical advantages.

Paper number 79:
Title: Spacecraft Attitude Control Under Reaction Wheel Constraints Using Control Lyapunov and Control Barrier Functions
Authors: Milad Alipour Shahraki, Laurent Lessard
Abstract: This paper introduces a novel control strategy for agile spacecraft attitude control, addressing reaction wheel-related input and state constraints. An optimal-decay control Lyapunov function quadratic program stabilizes the system and mitigates chattering at low sampling frequencies, while control barrier functions enforce hard state constraints. Numerical simulations validate the method's practicality and efficiency for real-time agile spacecraft attitude control.

Paper number 80:
Title: Rate Splitting Multiple Access for RIS-aided URLLC MIMO Broadcast Channels
Authors: Mohammad Soleymani, Ignacio Santamaria, Eduard Jorswieck, Marco Di Renzo, Robert Schober, Lajos Hanzo
Abstract: The performance of modern wireless communication systems is typically limited by interference. The impact of interference can be even more severe in ultra-reliable and low-latency communication (URLLC) use cases. A powerful tool for managing interference is rate splitting multiple access (RSMA), which encompasses many multiple-access technologies like non-orthogonal multiple access (NOMA), spatial division multiple access (SDMA), and broadcasting. Another effective technology to enhance the performance of URLLC systems and mitigate interference is constituted by reconfigurable intelligent surfaces (RISs). This paper develops RSMA schemes for multi-user multiple-input multiple-output (MIMO) RIS-aided broadcast channels (BCs) based on finite block length (FBL) coding. We show that RSMA and RISs can substantially improve the spectral efficiency (SE) and energy efficiency (EE) of MIMO RIS-aided URLLC systems. Additionally, the gain of employing RSMA and RISs noticeably increases when the reliability and latency constraints are more stringent. Furthermore, RISs impact RSMA differently, depending on the user load. If the system is underloaded, RISs are able to manage the interference sufficiently well, making the gains of RSMA small. However, when the user load is high, RISs and RSMA become synergetic.

Paper number 81:
Title: Uncertainty quantification for White Matter Hyperintensity segmentation detects silent failures and improves automated Fazekas quantification
Authors: Ben Philps, Maria del C. Valdes Hernandez, Chen Qin, Una Clancy, Eleni Sakka, Susana Munoz Maniega, Mark E. Bastin, Angela C.C. Jochems, Joanna M. Wardlaw, Miguel O. Bernabeu, Alzheimers Disease Neuroimaging Initiative
Abstract: White Matter Hyperintensities (WMH) are key neuroradiological markers of small vessel disease present in brain MRI. Assessment of WMH is important in research and clinics. However, WMH are challenging to segment due to their high variability in shape, location, size, poorly defined borders, and similar intensity profile to other pathologies (e.g stroke lesions) and artefacts (e.g head motion). In this work, we assess the utility and semantic properties of the most effective techniques for uncertainty quantification (UQ) in segmentation for the WMH segmentation task across multiple test-time data distributions. We find UQ techniques reduce 'silent failure' by identifying in UQ maps small WMH clusters in the deep white matter that are unsegmented by the model. A combination of Stochastic Segmentation Networks with Deep Ensembles also yields the highest Dice and lowest Absolute Volume Difference % (AVD) score and can highlight areas where there is ambiguity between WMH and stroke lesions. We further demonstrate the downstream utility of UQ, proposing a novel method for classification of the clinical Fazekas score using spatial features extracted from voxelwise WMH probability and UQ maps. We show that incorporating WMH uncertainty information improves Fazekas classification performance and calibration. Our model with (UQ and spatial WMH features)/(spatial WMH features)/(WMH volume only) achieves a balanced accuracy score of 0.74/0.67/0.62, and root brier score of 0.65/0.72/0.74 in the Deep WMH and balanced accuracy of 0.74/0.73/0.71 and root brier score of 0.64/0.66/0.68 in the Periventricular region. We further demonstrate that stochastic UQ techniques with high sample diversity can improve the detection of poor quality segmentations.

Paper number 82:
Title: A Weighted Hankel Approach and Cramér-Rao Bound Analysis for Quantitative Acoustic Microscopy Imaging
Authors: Lorena Leon, Jonathan Mamou, Denis Kouamé, Adrian Basarab
Abstract: Quantitative acoustic microscopy (QAM) is a cutting-edge imaging modality that leverages very high-frequency ultrasound to characterize the acoustic and mechanical properties of biological tissues at microscopic resolutions. Radio-frequency echo signals are digitized and processed to yield two-dimensional maps. This paper introduces a weighted Hankel-based spectral method with a reweighting strategy to enhance robustness with regard to noise and reduce unreliable acoustic parameter estimates. Additionally, we derive, for the first time in QAM, Cramér-Rao bounds to establish theoretical performance benchmarks for acoustic parameter estimation. Simulations and experimental results demonstrate that the proposed method consistently outperform standard autoregressive approach, particularly under challenging conditions. These advancements promise to improve the accuracy and reliability of tissue characterization, enhancing the potential of QAM for biomedical applications.

Paper number 83:
Title: A lightweight and robust method for blind wideband-to-fullband extension of speech
Authors: Jan Büthe, Jean-Marc Valin
Abstract: Reducing the bandwidth of speech is common practice in resource constrained environments like low-bandwidth speech transmission or low-complexity vocoding. We propose a lightweight and robust method for extending the bandwidth of wideband speech signals that is inspired by classical methods developed in the speech coding context. The resulting model has just ~370K parameters and a complexity of ~140 MFLOPS (or ~70 MMACS). With a frame size of 10 ms and a lookahead of only 0.27 ms, the model is well-suited for use with common wideband speech codecs. We evaluate the model's robustness by pairing it with the Opus SILK speech codec (1.5 release) and verify in a P.808 DCR listening test that it significantly improves quality from 6 to 12 kb/s. We also demonstrate that Opus 1.5 together with the proposed bandwidth extension at 9 kb/s meets the quality of 3GPP EVS at 9.6 kb/s and that of Opus 1.4 at 18 kb/s showing that the blind bandwidth extension can meet the quality of classical guided bandwidth extensions thus providing a way for backward-compatible quality improvement.

Paper number 84:
Title: Exploiting Polarization Domain of the IOS for Enhanced Full-Dimensional Transmission
Authors: Weiqiao Zhu, Zizhou Zheng, Yang Yang, Huan Huang, Weijun Hao, Xiaofei Jia, Hongliang Zhang
Abstract: Intelligent omni-surface (IOS), capable of providing service coverage to mobile users (MUs) in a reflective and refractive manner, has recently attracted widespread attention. However, the performance of power-domain IOS-assisted systems is limited by the intimate coupling between the refraction and reflection behavior of IOS elements. In this paper, we introduce the concept of dual-polarized IOS-assisted communication to overcome this challenge. By employing the polarization domain in the design of IOS, full independent refraction and reflection modes can be delivered. We consider a downlink dual-polarized IOS-aided system while also accounting for the leakage between different polarizations. To maximize the sum rate, we formulated a joint base station (BS) digital and IOS analog beamforming problem and proposed an iterative algorithm to solve the nonconvex program. Simulation results validate that dual-polarized IOS significantly enhances the performance than that of the power-domain one.

Paper number 85:
Title: Performance Analysis of Multi-IRS Aided Multiple Operator Systems at mmWave Frequencies
Authors: Souradeep Ghosh, L. Yashvanth, Chandra R. Murthy
Abstract: Intelligent reflecting surfaces (IRSs) are envisioned to enhance the performance of mmWave wireless systems. In practice, multiple mobile operators (MO) coexist in an area and provide simultaneous and independent services to user-equipments (UEs) on different frequency bands. Then, if each MO deploys an IRS to enhance its performance, the IRSs also alter the channels of UEs of other MOs. In this context, this paper addresses the following questions: can an MO still continue to control its IRS independently of other MOs and IRSs? Is joint optimization of IRSs deployed by different MOs and inter-MO cooperation needed? To that end, by considering the mmWave bands, we first derive the ergodic sum spectral efficiency (SE) in a $2$-MO system for the following schemes: 1) joint optimization of an overall phase angle of the IRSs with MO cooperation, 2) MO cooperation via time-sharing, and 3) no cooperation between the MOs. We find that even with no cooperation between the MOs, the performance of a given MO is not degraded by the presence of an out-of-band (OOB) MO deploying and independently controlling its own IRS. On the other hand, the SE gain obtained at a given MO using joint optimization and cooperation over the no-cooperation scheme decreases inversely with the number of elements in the IRS deployed by the other MO. We generalize our results to a multiple MO setup and show that the gain in the sum-SE over the no-cooperation case increases at least linearly with the number of OOB MOs. Finally, we numerically verify our findings and conclude that every MO can independently operate and tune its IRS; cooperation via optimizing an overall phase only brings marginal benefits in practice.

Paper number 86:
Title: Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography
Authors: Zhijin He, Alan B. McMillan
Abstract: The application of artificial intelligence (AI) in medical imaging has revolutionized diagnostic practices, enabling advanced analysis and interpretation of radiological data. This study presents a comprehensive evaluation of radiomics-based and deep learning-based approaches for disease detection in chest radiography, focusing on COVID-19, lung opacity, and viral pneumonia. While deep learning models, particularly convolutional neural networks and vision transformers, learn directly from image data, radiomics-based models extract handcrafted features, offering potential advantages in data-limited scenarios. We systematically compared the diagnostic performance of various AI models, including Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines, and Multi-Layer Perceptrons for radiomics, against state-of-the-art deep learning models such as InceptionV3, EfficientNetL, and ConvNeXtXLarge. Performance was evaluated across multiple sample sizes. At 24 samples, EfficientNetL achieved an AUC of 0.839, outperforming SVM with an AUC of 0.762. At 4000 samples, InceptionV3 achieved the highest AUC of 0.996, compared to 0.885 for Random Forest. A Scheirer-Ray-Hare test confirmed significant main and interaction effects of model type and sample size on all metrics. Post hoc Mann-Whitney U tests with Bonferroni correction further revealed consistent performance advantages for deep learning models across most conditions. These findings provide statistically validated, data-driven recommendations for model selection in diagnostic AI. Deep learning models demonstrated higher performance and better scalability with increasing data availability, while radiomics-based models may remain useful in low-data contexts. This study addresses a critical gap in AI-based diagnostic research by offering practical guidance for deploying AI models across diverse clinical environments.

Paper number 87:
Title: UNet-3D with Adaptive TverskyCE Loss for Pancreas Medical Image Segmentation
Authors: Xubei Zhang, Mikhail Y. Shalaginov, Tingying Helen Zeng
Abstract: Pancreatic cancer, which has a low survival rate, is one of the most challenging cancers to diagnose and treat effectively. Early detection through abdominal computed tomography (CT) scans is crucial, yet complicated by the pancreas' obscure anatomical position, small size, and frequent occlusion by surrounding organs. These factors make the pancreas particularly difficult to identify and segment accurately. While deep learning (DL) models have shown promise for segmentation tasks, their performance still requires significant improvement to address these challenges. In this research, we propose a novel adaptive TverskyCE loss for DL model training, which combines Tversky loss with cross-entropy loss through learnable weights. Our method enables automatic adjustment of loss contributions during training, dynamically optimizing the objective function for improved performance. All experiments were conducted on the National Institutes of Health (NIH) Pancreas-CT dataset. We evaluated the adaptive TverskyCE loss on the UNet-3D and Dilated UNet-3D, and our method achieved a Dice Similarity Coefficient (DSC) of 85.59%, with peak performance up to 95.24%, and the score of 85.14%. DSC and the score score were improved by 9.47% and 8.98% respectively compared with the baseline UNet-3D with Tversky loss for pancreas segmentation. Keywords: Pancreas segmentation, Tversky loss, Cross-entropy loss, UNet-3D, Dilated UNet-3D

Paper number 88:
Title: RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions
Authors: Junzhi Ning, Cheng Tang, Kaijing Zhou, Diping Song, Lihao Liu, Ming Hu, Wei Li, Huihui Xu, Yanzhou Su, Tianbin Li, Jiyao Liu, Jin Ye, Sheng Zhang, Yuanfeng Ji, Junjun He
Abstract: The scarcity of high-quality, labelled retinal imaging data, which presents a significant challenge in the development of machine learning models for ophthalmology, hinders progress in the field. Existing methods for synthesising Colour Fundus Photographs (CFPs) largely rely on predefined disease labels, which restricts their ability to generate images that reflect fine-grained anatomical variations, subtle disease stages, and diverse pathological features beyond coarse class categories. To overcome these challenges, we first introduce an innovative pipeline that creates a large-scale, captioned retinal dataset comprising 1.4 million entries, called RetinaLogos-1400k. Specifically, RetinaLogos-1400k uses the visual language model(VLM) to describe retinal conditions and key structures, such as optic disc configuration, vascular distribution, nerve fibre layers, and pathological features. Building on this dataset, we employ a novel three-step training framework, RetinaLogos, which enables fine-grained semantic control over retinal images and accurately captures different stages of disease progression, subtle anatomical variations, and specific lesion types. Through extensive experiments, our method demonstrates superior performance across multiple datasets, with 62.07% of text-driven synthetic CFPs indistinguishable from real ones by ophthalmologists. Moreover, the synthetic data improves accuracy by 5%-10% in diabetic retinopathy grading and glaucoma detection. Codes are available at this https URL.

Paper number 89:
Title: BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia
Authors: Rachit Saluja, Arzu Kovanlikaya, Candace Chien, Lauren Kathryn Blatt, Jeffrey M. Perlman, Stefan Worgall, Mert R. Sabuncu, Jonathan P. Dyke
Abstract: Bronchopulmonary dysplasia (BPD) is a common complication among preterm neonates, with portable X-ray imaging serving as the standard diagnostic modality in neonatal intensive care units (NICUs). However, lung magnetic resonance imaging (MRI) offers a non-invasive alternative that avoids sedation and radiation while providing detailed insights into the underlying mechanisms of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and semantic segmentation algorithms can be developed to assist clinicians in identifying the etiology of BPD. In this dataset, we present MRI scans paired with corresponding semantic segmentations of the lungs and trachea for 40 neonates, the majority of whom are diagnosed with BPD. The imaging data consist of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as the StarVIBE series. Additionally, we provide comprehensive clinical data and baseline segmentation models, validated against clinical assessments, to support further research and development in neonatal lung imaging.

Paper number 90:
Title: Context-Aware Deep Learning for Robust Channel Extrapolation in Fluid Antenna Systems
Authors: Yanliang Jin, Runze Yu, Yuan Gao, Shengli Liu, Xiaoli Chu, Kai-Kit Wong, Chan-Byoung Chae
Abstract: Fluid antenna systems (FAS) offer remarkable spatial flexibility but face significant challenges in acquiring high-resolution channel state information (CSI), leading to considerable overhead. To address this issue, we propose CANet, a robust deep learning model for channel extrapolation in FAS. CANet combines context-adaptive modeling with a cross-scale attention mechanism and is built on a ConvNeXt v2 backbone to improve extrapolation accuracy for unobserved antenna ports. To further enhance robustness, we introduce a novel spatial amplitude perturbation strategy, inspired by frequency-domain augmentation techniques in image processing. This motivates the incorporation of a Fourier-domain loss function, capturing frequency-domain consistency, alongside a spectral structure consistency loss that reinforces learning stability under perturbations. Our simulation results demonstrate that CANet outperforms benchmark models across a wide range of signal-to-noise ratio (SNR) levels.

Paper number 91:
Title: Stereo Reproduction in the Presence of Sample Rate Offsets
Authors: Srikanth Korse, Andreas Walther, Emanuel A. P. Habets
Abstract: One of the main challenges in synchronizing wirelessly connected loudspeakers for spatial audio reproduction is clock skew. Clock skew arises from sample rate offsets ( SROs) between the loudspeakers, caused by the use of independent device clocks. While network-based protocols like Precision Time Protocol (PTP) and Network Time Protocol (NTP) are explored, the impact of SROs on spatial audio reproduction and its perceptual consequences remains underexplored. We propose an audio-domain SRO compensation method using spatial filtering to isolate loudspeaker contributions. These filtered signals, along with the original playback signal, are used to estimate the SROs, and their influence is compensated for prior to spatial audio reproduction. We evaluate the effect of the compensation method in a subjective listening test. The results of these tests as well as objective metrics demonstrate that the proposed method mitigates the perceptual degradation introduced by SROs by preserving the spatial cues.

Paper number 92:
Title: Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification Mapping on Non-Contrast CT
Authors: Xiangjian Hou, Ebru Yaman Akcicek, Xin Wang, Kazem Hashemizadeh, Scott Mcnally, Chun Yuan, Xiaodong Ma
Abstract: While total intracranial carotid artery calcification (ICAC) volume is an established stroke biomarker, growing evidence shows this aggregate metric ignores the critical influence of plaque location, since calcification in different segments carries distinct prognostic and procedural risks. However, a finer-grained, segment-specific quantification has remained technically infeasible. Conventional 3D models are forced to process downsampled volumes or isolated patches, sacrificing the global context required to resolve anatomical ambiguity and render reliable landmark localization. To overcome this, we reformulate the 3D challenge as a \textbf{Parallel Probabilistic Landmark Localization} task along the 1D axial dimension. We propose the \textbf{Depth-Sequence Transformer (DST)}, a framework that processes full-resolution CT volumes as sequences of 2D slices, learning to predict $N=6$ independent probability distributions that pinpoint key anatomical landmarks. Our DST framework demonstrates exceptional accuracy and robustness. Evaluated on a 100-patient clinical cohort with rigorous 5-fold cross-validation, it achieves a Mean Absolute Error (MAE) of \textbf{0.1 slices}, with \textbf{96\%} of predictions falling within a $\pm1$ slice tolerance. Furthermore, to validate its architectural power, the DST backbone establishes the best result on the public Clean-CC-CCII classification benchmark under an end-to-end evaluation protocol. Our work delivers the first practical tool for automated segment-specific ICAC analysis. The proposed framework provides a foundation for further studies on the role of location-specific biomarkers in diagnosis, prognosis, and procedural planning.

Paper number 93:
Title: A Generalized Stability Analysis Method with Dynamic Phasors for LV AC Microgrids
Authors: Bülent Dağ
Abstract: Representation of inductive coupling lines with conventional static phasors is the main reason of inadequacy of the existing phasors based simplified stability analysis methods for microgrids with inductive coupling lines. In the literature, dynamic phasors have been proposed for the dynamic modelling of inductive lines to conserve the simplified structure of the analysis method. In this study a generalized stability analysis method for LV AC microgrids, composed of droop controlled inverters, is presented. The proposed analysis method is based on the inclusion of dynamic phasors for inductive coupling lines into the existing phasors based stability analysis method. The results show that the stability analysis method with dynamic phasors successfully predicts the instability boundaries of LV AC microgrids.

Paper number 94:
Title: A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion
Authors: Mingda Zhang
Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is essential for neuro-oncology diagnosis and treatment planning. Despite advances in deep learning methods, automatic segmentation remains challenging due to tumor morphological heterogeneity and complex three-dimensional spatial relationships. Current techniques primarily rely on visual features extracted from MRI sequences while underutilizing semantic knowledge embedded in medical reports. This research presents a multi-level fusion architecture that integrates pixel-level, feature-level, and semantic-level information, facilitating comprehensive processing from low-level data to high-level concepts. The semantic-level fusion pathway combines the semantic understanding capabilities of Contrastive Language-Image Pre-training (CLIP) models with the spatial feature extraction advantages of 3D U-Net through three mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention mechanisms. Experimental validation on the BraTS 2020 dataset demonstrates that the proposed model achieves an overall Dice coefficient of 0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with a 7.3% Dice coefficient increase in the clinically important enhancing tumor (ET) region.

Paper number 95:
Title: Predictive & Trust-based Multi-Agent Coordination
Authors: Venkatraman Renganathan, Sabyasachi Mondal, Antonios Tsourdos
Abstract: This paper presents a trust-based predictive multi-agent consensus protocol that analyses neighbours' anticipation data and makes coordination decisions. Agents in the network share their future predicted data over a finite look-ahead horizon with their neighbours and update their predictions in a rolling-horizon fashion. The prediction data is then used by agents to learn both the trust and the commitment traits exhibited by their neighbours over time. The proposed protocol is named as the Anticipatory Distributed Coordination (ADC) protocol. Lyapunov theory-based agreement convergence between agents is provided, followed by demonstrations using numerical simulations.

Paper number 96:
Title: DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference via Distributed Split Speculative Decoding
Authors: Jiahong Ning, Ce Zheng, Tingting Yang
Abstract: Large language models (LLMs) have transformed natural language processing but face critical deployment challenges in device-edge systems due to resource limitations and communication overhead. To address these issues, collaborative frameworks have emerged that combine small language models (SLMs) on devices with LLMs at the edge, using speculative decoding (SD) to improve efficiency. However, existing solutions often trade inference accuracy for latency or suffer from high uplink transmission costs when verifying candidate tokens. In this paper, we propose Distributed Split Speculative Decoding (DSSD), a novel architecture that not only preserves the SLM-LLM split but also partitions the verification phase between the device and edge. In this way, DSSD replaces the uplink transmission of multiple vocabulary distributions with a single downlink transmission, significantly reducing communication latency while maintaining inference quality. Experiments show that our solution outperforms current methods, and codes are at: this https URL

Paper number 97:
Title: VoxATtack: A Multimodal Attack on Voice Anonymization Systems
Authors: Ahmad Aloradi, Ünal Ege Gaznepoglu, Emanuël A. P. Habets, Daniel Tenbrinck
Abstract: Voice anonymization systems aim to protect speaker privacy by obscuring vocal traits while preserving the linguistic content relevant for downstream applications. However, because these linguistic cues remain intact, they can be exploited to identify semantic speech patterns associated with specific speakers. In this work, we present VoxATtack, a novel multimodal de-anonymization model that incorporates both acoustic and textual information to attack anonymization systems. While previous research has focused on refining speaker representations extracted from speech, we show that incorporating textual information with a standard ECAPA-TDNN improves the attacker's performance. Our proposed VoxATtack model employs a dual-branch architecture, with an ECAPA-TDNN processing anonymized speech and a pretrained BERT encoding the transcriptions. Both outputs are projected into embeddings of equal dimensionality and then fused based on confidence weights computed on a per-utterance basis. When evaluating our approach on the VoicePrivacy Attacker Challenge (VPAC) dataset, it outperforms the top-ranking attackers on five out of seven benchmarks, namely B3, B4, B5, T8-5, and T12-5. To further boost performance, we leverage anonymized speech and SpecAugment as augmentation techniques. This enhancement enables VoxATtack to achieve state-of-the-art on all VPAC benchmarks, after scoring 20.6% and 27.2% average equal error rate on T10-2 and T25-1, respectively. Our results demonstrate that incorporating textual information and selective data augmentation reveals critical vulnerabilities in current voice anonymization methods and exposes potential weaknesses in the datasets used to evaluate them.

Paper number 98:
Title: Distributed Truncated Predictive Control for Networked Systems under Uncertainty: Stability and Near-Optimality Guarantee
Authors: Eric Xu, Soummya Kar, Guannan Qu
Abstract: We study the problem of distributed online control of networked systems with time-varying cost functions and disturbances, where each node only has local information of the states and forecasts of the costs and disturbances. We develop a distributed truncated predictive control (DTPC) algorithm, where each node solves a ``truncated'' predictive optimal control problem with horizon $k$, but only involving nodes in a $\kappa$-hop neighborhood (ignoring nodes outside). We show that the DTPC algorithm satisfies input-to-state stability (ISS) bounds and has regret decaying exponentially in $k$ and $\kappa$, meaning a short predictive horizon $k$ and a small truncation radius $\kappa$ is sufficient to achieve near-optimal performance. Furthermore, we show that when the future costs and disturbances are not exactly known, the regret has exponentially decaying sensitivity to the forecast errors in terms of predictive horizon, meaning near-term forecast errors play a much more important role than longer-term forecasts.

Paper number 99:
Title: Robo-Platform: A Robotic System for Recording Sensors and Controlling Robots
Authors: Masoud Dayani Najafabadi, Khoshnam Shojaei
Abstract: Mobile smartphones compactly provide sensors such as cameras, IMUs, GNSS measurement units, and wireless and wired communication channels required for robotics projects. They are affordable, portable, and programmable, which makes them ideal for testing, data acquisition, controlling mobile robots, and many other robotic applications. A robotic system is proposed in this paper, consisting of an Android phone, a microcontroller board attached to the phone via USB, and a remote wireless controller station. In the data acquisition mode, the Android device can record a dataset of a diverse configuration of multiple cameras, IMUs, GNSS units, and external USB ADC channels in the rawest format used for, but not limited to, pose estimation and scene reconstruction applications. In robot control mode, the Android phone, a microcontroller board, and other peripherals constitute the mobile or stationary robotic system. This system is controlled using a remote server connected over Wi-Fi or Bluetooth. Experiments show that although the SLAM and AR applications can utilize the acquired data, the proposed system can pave the way for more advanced algorithms for processing these noisy and sporadic measurements. Moreover, the characteristics of the communication media are studied, and two example robotic projects, which involve controlling a toy car and a quadcopter, are included.

Paper number 100:
Title: Nash equilibrium seeking for a class of quadratic-bilinear Wasserstein distributionally robust games
Authors: Georgios Pantazis, Reza Rahimi Baghbadorani, Sergio Grammatico
Abstract: We consider a class of Wasserstein distributionally robust Nash equilibrium problems, where agents construct heterogeneous data-driven Wasserstein ambiguity sets using private samples and radii, in line with their individual risk-averse behaviour. By leveraging relevant properties of this class of games, we show that equilibria of the original seemingly infinite-dimensional problem can be obtained as a solution to a finite-dimensional Nash equilibrium problem. We then reformulate the problem as a finite-dimensional variational inequality and establish the connection between the corresponding solution sets. Our reformulation has scalable behaviour with respect to the data size and maintains a fixed number of constraints, independently of the number of samples. To compute a solution, we leverage two algorithms, based on the golden ratio algorithm. The efficiency of both algorithmic schemes is corroborated through extensive simulation studies on an illustrative example and a stochastic portfolio allocation game, where behavioural coupling among investors is modeled.

Paper number 101:
Title: A Progressive Image Restoration Network for High-order Degradation Imaging in Remote Sensing
Authors: Yujie Feng, Yin Yang, Xiaohong Fan, Zhengpeng Zhang, Lijing Bu, Jianping Zhang
Abstract: Recently, deep learning methods have gained remarkable achievements in the field of image restoration for remote sensing (RS). However, most existing RS image restoration methods focus mainly on conventional first-order degradation models, which may not effectively capture the imaging mechanisms of remote sensing images. Furthermore, many RS image restoration approaches that use deep learning are often criticized for their lacks of architecture transparency and model interpretability. To address these problems, we propose a novel progressive restoration network for high-order degradation imaging (HDI-PRNet), to progressively restore different image degradation. HDI-PRNet is developed based on the theoretical framework of degradation imaging, also Markov properties of the high-order degradation process and Maximum a posteriori (MAP) estimation, offering the benefit of mathematical interpretability within the unfolding network. The framework is composed of three main components: a module for image denoising that relies on proximal mapping prior learning, a module for image deblurring that integrates Neumann series expansion with dual-domain degradation learning, and a module for super-resolution. Extensive experiments demonstrate that our method achieves superior performance on both synthetic and real remote sensing images.

Paper number 102:
Title: Speech-Forensics: Towards Comprehensive Synthetic Speech Dataset Establishment and Analysis
Authors: Zhoulin Ji, Chenhao Lin, Hang Wang, Chao Shen
Abstract: Detecting synthetic from real speech is increasingly crucial due to the risks of misinformation and identity impersonation. While various datasets for synthetic speech analysis have been developed, they often focus on specific areas, limiting their utility for comprehensive research. To fill this gap, we propose the Speech-Forensics dataset by extensively covering authentic, synthetic, and partially forged speech samples that include multiple segments synthesized by different high-quality algorithms. Moreover, we propose a TEmporal Speech LocalizaTion network, called TEST, aiming at simultaneously performing authenticity detection, multiple fake segments localization, and synthesis algorithms recognition, without any complex post-processing. TEST effectively integrates LSTM and Transformer to extract more powerful temporal speech representations and utilizes dense prediction on multi-scale pyramid features to estimate the synthetic spans. Our model achieves an average mAP of 83.55% and an EER of 5.25% at the utterance level. At the segment level, it attains an EER of 1.07% and a 92.19% F1 score. These results highlight the model's robust capability for a comprehensive analysis of synthetic speech, offering a promising avenue for future research and practical applications in this field.

Paper number 103:
Title: The Role of Integrity Monitoring in Connected and Automated Vehicles: Current State-of-Practice and Future Directions
Authors: Saswat Priyadarshi Nayak, Matthew Barth
Abstract: Positioning integrity refers to the trust in the performance of a navigation system. Accurate and reliable position information is needed to meet the requirements of connected and Automated Vehicle (CAV) applications, particularly in safety-critical scenarios. Receiver Autonomous Integrity Monitoring (RAIM) and its variants have been widely studied for Global Navigation Satellite System (GNSS)-based vehicle positioning, often fused with kinematic (e.g., Odometry) and perception sensors (e.g., camera). However, integrity monitoring (IM) for cooperative positioning solutions leveraging Vehicle-to-Everything (V2X) communication has received comparatively limited attention. This paper reviews existing research in the field of positioning IM and identifies various research gaps. Particular attention has been placed on identifying research that highlights cooperative IM methods. It also examines key automotive safety standards and public V2X datasets to map current research priorities and uncover critical gaps. Finally, the paper outlines promising future directions, highlighting research topics aimed at advancing and benchmarking positioning integrity.

Paper number 104:
Title: Safety-Critical Human-Machine Shared Driving for Vehicle Collision Avoidance based on Hamilton-Jacobi reachability
Authors: Shiyue Zhao, Junzhi Zhang, Rui Zhou, Neda Masoud, Jianxiong Li, Helai Huang, Shijie Zhao
Abstract: Road safety continues to be a pressing global issue, with vehicle collisions imposing significant human, societal, and economic burdens. Human-machine shared collision avoidance in critical collision scenarios aims to aid drivers' accident avoidance through intervening only when necessary. Existing methods count on replanning collision-free trajectories and imposing human-machine tracking, which usually interrupts the driver's intent and increases the risk of conflict. This paper introduces a Reachability-Aware Reinforcement Learning (RL) framework for shared control, guided by Hamilton-Jacobi (HJ) reachability analysis. Machine intervention is activated only when the vehicle approaches the Collision Avoidance Reachable Set (CARS), which represents states where collision is unavoidable. First, we precompute the reachability distributions and the CARS by solving the Bellman equation using offline data. To reduce human-machine conflicts, we develop a driver model for sudden obstacles and propose an authority allocation strategy considering key collision avoidance features. Finally, we train a RL agent to reduce human-machine conflicts while enforcing the hard constraint of avoiding entry into the CARS. The proposed method was tested on a real vehicle platform. Results show that the controller intervenes effectively near CARS to prevent collisions while maintaining improved original driving task performance. Robustness analysis further supports its flexibility across different driver attributes.

Paper number 105:
Title: Token Communications: A Large Model-Driven Framework for Cross-modal Context-aware Semantic Communications
Authors: Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
Abstract: In this paper, we introduce token communications (TokCom), a large model-driven framework to leverage cross-modal context information in generative semantic communications (GenSC). TokCom is a new paradigm, motivated by the recent success of generative foundation models and multimodal large language models (GFM/MLLMs), where the communication units are tokens, enabling efficient transformer-based token processing at the transmitter and receiver. In this paper, we introduce the potential opportunities and challenges of leveraging context in GenSC, explore how to integrate GFM/MLLMs-based token processing into semantic communication systems to leverage cross-modal context effectively at affordable complexity, present the key principles for efficient TokCom at various layers in future wireless networks. In a typical image semantic communication setup, we demonstrate a significant improvement of the bandwidth efficiency, achieved by TokCom by leveraging the context information among tokens. Finally, the potential research directions are identified to facilitate adoption of TokCom in future wireless networks.

Paper number 106:
Title: Deep Q-Learning with Gradient Target Tracking
Authors: Donghwan Lee, Bum Geun Park, Taeho Lee
Abstract: This paper introduces Q-learning with gradient target tracking, a novel reinforcement learning framework that provides a learned continuous target update mechanism as an alternative to the conventional hard update paradigm. In the standard deep Q-network (DQN), the target network is a copy of the online network's weights, held fixed for a number of iterations before being periodically replaced via a hard update. While this stabilizes training by providing consistent targets, it introduces a new challenge: the hard update period must be carefully tuned to achieve optimal performance. To address this issue, we propose two gradient-based target update methods: DQN with asymmetric gradient target tracking (AGT2-DQN) and DQN with symmetric gradient target tracking (SGT2-DQN). These methods replace the conventional hard target updates with continuous and structured updates using gradient descent, which effectively eliminates the need for manual tuning. We provide a theoretical analysis proving the convergence of these methods in tabular settings. Additionally, empirical evaluations demonstrate their advantages over standard DQN baselines, which suggest that gradient-based target updates can serve as an effective alternative to conventional target update mechanisms in Q-learning.

Paper number 107:
Title: Learning Separated Representations for Instrument-based Music Similarity
Authors: Yuka Hashizume, Li Li, Atsushi Miyashita, Tomoki Toda
Abstract: A flexible recommendation and retrieval system requires music similarity in terms of multiple partial elements of musical pieces to allow users to select the element they want to focus on. A method for music similarity learning using multiple networks with individual instrumental signals is effective but faces the problem that using each clean instrumental signal as a query is impractical for retrieval systems and using separated instrumental signals reduces accuracy owing to artifacts. In this paper, we present instrumental-part-based music similarity learning with a single network that takes mixed signals as input instead of individual instrumental signals. Specifically, we designed a single similarity embedding space with separated subspaces for each instrument, extracted by Conditional Similarity Networks, which are trained using the triplet loss with masks. Experimental results showed that (1) the proposed method can obtain more accurate embedding representation than using individual networks using separated signals as input in the evaluation of an instrument that had low accuracy, (2) each sub-embedding space can hold the characteristics of the corresponding instrument, and (3) the selection of similar musical pieces focusing on each instrumental sound by the proposed method can obtain human acceptance, especially when focusing on timbre.

Paper number 108:
Title: Generative Diffusion Models for Resource Allocation in Wireless Networks
Authors: Yigit Berkay Uslu, Samar Hadou, Shirin Saeedi Bidokhti, Alejandro Ribeiro
Abstract: This paper proposes a supervised training algorithm for learning stochastic resource allocation policies with generative diffusion models (GDMs). We formulate the allocation problem as the maximization of an ergodic utility function subject to ergodic Quality of Service (QoS) constraints. Given samples from a stochastic expert policy that yields a near-optimal solution to the constrained optimization problem, we train a GDM policy to imitate the expert and generate new samples from the optimal distribution. We achieve near-optimal performance through the sequential execution of the generated samples. To enable generalization to a family of network configurations, we parameterize the backward diffusion process with a graph neural network (GNN) architecture. We present numerical results in a case study of power control.

Paper number 109:
Title: ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications
Authors: Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Robert Schober, Deniz Gündüz
Abstract: Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as token domain multiple access (ToDMA), where a large number of devices share a token codebook and a modulation codebook for source and channel coding, respectively. Specifically, each transmitter first tokenizes its source signal and modulate each token to a codeword. At the receiver, compressed sensing is employed first to detect active tokens and the corresponding channel state information (CSI) from the superposed signals. Then, the source token sequences are reconstructed by clustering the token-associated CSI across multiple time slots. In case of token collisions, some active tokens cannot be assigned and some positions in the reconstructed token sequences are empty. We propose to use pre-trained MLLMs to leverage the context, predict masked tokens, and thus mitigate token collisions. Simulation results demonstrate the effectiveness of the proposed ToDMA framework for both text and image transmission tasks, achieving significantly lower latency compared to context-unaware orthogonal communication schemes, while also delivering superior distortion and perceptual quality compared to state-of-the-art context-unaware non-orthogonal communication methods.

Paper number 110:
Title: Can Large Language Models Predict Audio Effects Parameters from Natural Language?
Authors: Seungheon Doh, Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Juhan Nam, Yuki Mitsufuji
Abstract: In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation. We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship between timbre semantics and audio effects in music production. To enhance performance, we introduce three types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous optimization approaches, offering competitive performance in translating natural language descriptions to appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving the way for more intuitive and accessible music production tools.

Paper number 111:
Title: Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application
Authors: Xiucheng Wang, Honggang Jia, Nan Cheng
Abstract: In this paper, a novel semantic communication framework empowered by generative artificial intelligence (GAI) is proposed, to enhance the robustness against both channel noise and transmission data distribution shifts. A theoretical foundation is established using stochastic differential equations (SDEs), from which a closed-form mapping between any signal-to-noise ratio (SNR) and the optimal denoising timestep is derived. Moreover, to address distribution mismatch, a mathematical scaling method is introduced to align received semantic features with the training distribution of the GAI. Built on this theoretical foundation, a latent diffusion model (LDM)-based semantic communication framework is proposed that combines a variational autoencoder for semantic features extraction, where a pretrained diffusion model is used for denoising. The proposed system is a training-free framework that supports zero-shot generalization, and achieves superior performance under low-SNR and out-of-distribution conditions, offering a scalable and robust solution for future 6G semantic communication systems. Experimental results demonstrate that the proposed semantic communication framework achieves state-of-the-art performance in both pixel-level accuracy and semantic perceptual quality, consistently outperforming baselines across a wide range of SNRs and data distributions without any fine-tuning or post-training.

Paper number 112:
Title: Monocular 3D Hand Pose Estimation with Implicit Camera Alignment
Authors: Christos Pantazopoulos, Spyridon Thermos, Gerasimos Potamianos
Abstract: Estimating the 3D hand articulation from a single color image is an important problem with applications in Augmented Reality (AR), Virtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart from the absence of depth information, occlusions, articulation complexity, and the need for camera parameters knowledge pose additional challenges. In this work, we propose an optimization pipeline for estimating the 3D hand articulation from 2D keypoint input, which includes a keypoint alignment step and a fingertip loss to overcome the need to know or estimate the camera parameters. We evaluate our approach on the EgoDexter and Dexter+Object benchmarks to showcase that it performs competitively with the state-of-the-art, while also demonstrating its robustness when processing "in-the-wild" images without any prior camera knowledge. Our quantitative analysis highlights the sensitivity of the 2D keypoint estimation accuracy, despite the use of hand priors. Code is available at the project page this https URL

Paper number 113:
Title: Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop Mapping with Deep Learning
Authors: Mehmet Ozgur Turkoglu, Selene Ledain, Helge Aasen
Abstract: Crop type classification using optical satellite time series remains limited in its ability to generalize across seasons, particularly when crop phenology shifts due to inter-annual weather variability. This hampers real-world applicability in scenarios where current-year labels are unavailable. In addition, uncertainty quantification is often overlooked, which reduces the reliability of such approaches for operational crop monitoring. Inspired by ecophysiological principles of plant growth, we propose a simple, model-agnostic Thermal-Time-based Temporal Sampling (T3S) method that replaces calendar time with thermal time. By subsampling time series in this biologically meaningful way, our method highlights key periods within the growing season while reducing temporal redundancy and noise. We evaluate the T3S on a multi-year Sentinel-2 dataset covering the entirety of Switzerland, which allows us to assess all applied methods on unseen years. Compared to state-of-the-art baselines, our approach yields substantial improvements in classification accuracy and, critically, provides well-calibrated uncertainty estimates. Moreover, the T3S method excels in low-data regimes and enables significantly more accurate early-season classification. With just 10% of the training labels, it outperforms the current baseline in both accuracy and uncertainty calibration, and by the end of June, it achieves a performance similar to the full-season baseline model.

Paper number 114:
Title: Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model
Authors: Koki Yamane, Yunhan Li, Masashi Konosu, Koki Inami, Junji Oaki, Sho Sakaino, Toshiaki Tsuji
Abstract: In recent years, the advancement of imitation learning has led to increased interest in teleoperating low-cost manipulators to collect demonstration data. However, most existing systems rely on unilateral control, which only transmits target position values. While this approach is easy to implement and suitable for slow, non-contact tasks, it struggles with fast or contact-rich operations due to the absence of force feedback. This work demonstrates that fast teleoperation with force feedback is feasible even with force-sensorless, low-cost manipulators by leveraging 4-channel bilateral control. Based on accurately identified manipulator dynamics, our method integrates nonlinear terms compensation, velocity and external force estimation, and variable gain corresponding to inertial variation. Furthermore, using data collected by 4-channel bilateral control, we show that incorporating force information into both the input and output of learned policies improves performance in imitation learning. These results highlight the practical effectiveness of our system for high-fidelity teleoperation and data collection on affordable hardware.

Paper number 115:
Title: Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music
Authors: Maziar Kanani, Sean O Leary, James McDermott
Abstract: Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy. In this study, we introduce the first digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus. Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.

Paper number 116:
Title: A Roadmap for Climate-Relevant Robotics Research
Authors: Alan Papalia, Charles Dawson, Laurentiu L. Anton, Norhan Magdy Bayomi, Bianca Champenois, Jung-Hoon Cho, Levi Cai, Joseph DelPreto, Kristen Edwards, Bilha-Catherine Githinji, Cameron Hickert, Vindula Jayawardana, Matthew Kramer, Shreyaa Raghavan, David Russell, Shide Salimi, Jingnan Shi, Soumya Sudhakar, Yanwei Wang, Shouyi Wang, Luca Carlone, Vijay Kumar, Daniela Rus, John E. Fernandez, Cathy Wu, George Kantor, Derek Young, Hanumant Singh
Abstract: Climate change is one of the defining challenges of the 21st century, and many in the robotics community are looking for ways to contribute. This paper presents a roadmap for climate-relevant robotics research, identifying high-impact opportunities for collaboration between roboticists and experts across climate domains such as energy, the built environment, transportation, industry, land use, and Earth sciences. These applications include problems such as energy systems optimization, construction, precision agriculture, building envelope retrofits, autonomous trucking, and large-scale environmental monitoring. Critically, we include opportunities to apply not only physical robots but also the broader robotics toolkit - including planning, perception, control, and estimation algorithms - to climate-relevant problems. A central goal of this roadmap is to inspire new research directions and collaboration by highlighting specific, actionable problems at the intersection of robotics and climate. This work represents a collaboration between robotics researchers and domain experts in various climate disciplines, and it serves as an invitation to the robotics community to bring their expertise to bear on urgent climate priorities.
    