
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Uncertainty Quantification for Data-Driven Machine Learning Models in Nuclear Engineering Applications: Where We Are and What Do We Need?
Authors: Xu Wu, Lesego E. Moloko, Pavel M. Bokov, Gregory K. Delipei, Joshua Kaizer, Kostadin N. Ivanov
Abstract: Machine learning (ML) has been leveraged to tackle a diverse range of tasks in almost all branches of nuclear engineering. Many of the successes in ML applications can be attributed to the recent performance breakthroughs in deep learning, the growing availability of computational power, data, and easy-to-use ML libraries. However, these empirical successes have often outpaced our formal understanding of the ML algorithms. An important but under-rated area is uncertainty quantification (UQ) of ML. ML-based models are subject to approximation uncertainty when they are used to make predictions, due to sources including but not limited to, data noise, data coverage, extrapolation, imperfect model architecture and the stochastic training process. The goal of this paper is to clearly explain and illustrate the importance of UQ of ML. We will elucidate the differences in the basic concepts of UQ of physics-based models and data-driven ML models. Various sources of uncertainties in physical modeling and data-driven modeling will be discussed, demonstrated, and compared. We will also present and demonstrate a few techniques to quantify the ML prediction uncertainties. Finally, we will discuss the need for building a verification, validation and UQ framework to establish ML credibility.

Paper number 2:
Title: A new graph-based surrogate model for rapid prediction of crashworthiness performance of vehicle panel components
Authors: Haoran Li, Yingxue Zhao, Haosu Zhou, Tobias Pfaff, Nan Li
Abstract: During the design cycle of safety critical vehicle components such as B-pillars, crashworthiness performance is a key metric for passenger protection assessment in vehicle accidents. Traditional finite element simulations for crashworthiness analysis involve complex modelling, leading to an increased computational demand. Although a few machine learning-based surrogate models have been developed for rapid predictions for crashworthiness analysis, they exhibit limitations in detailed representation of complex 3D components. Graph Neural Networks (GNNs) have emerged as a promising solution for processing data with complex structures. However, existing GNN models often lack sufficient accuracy and computational efficiency to meet industrial demands. This paper proposes Recurrent Graph U-Net (ReGUNet), a new graph-based surrogate model for crashworthiness analysis of vehicle panel components. ReGUNet adoptes a U-Net architecture with multiple graph downsampling and upsampling layers, which improves the model's computational efficiency and accuracy; the introduction of recurrence enhances the accuracy and stability of temporal predictions over multiple time steps. ReGUNet is evaluated through a case study of side crash testing of a B-pillar component with variation in geometric design. The trained model demonstrates great accuracy in predicting the dynamic behaviour of previously unseen component designs within a relative error of 0.74% for the maximum B-pillar intrusion. Compared to the baseline models, ReGUNet can reduce the averaged mean prediction error of the component's deformation by more than 51% with significant improvement in computational efficiency. Provided enhanced accuracy and efficiency, ReGUNet shows greater potential in accurate predictions of large and complex graphs compared to existing models.

Paper number 3:
Title: CP-NCBF: A Conformal Prediction-based Approach to Synthesize Verified Neural Control Barrier Functions
Authors: Manan Tayal, Aditya Singh, Pushpak Jagtap, Shishir Kolathaya
Abstract: Control Barrier Functions (CBFs) are a practical approach for designing safety-critical controllers, but constructing them for arbitrary nonlinear dynamical systems remains a challenge. Recent efforts have explored learning-based methods, such as neural CBFs (NCBFs), to address this issue. However, ensuring the validity of NCBFs is difficult due to potential learning errors. In this letter, we propose a novel framework that leverages split-conformal prediction to generate formally verified neural CBFs with probabilistic guarantees based on a user-defined error rate, referred to as CP-NCBF. Unlike existing methods that impose Lipschitz constraints on neural CBF-leading to scalability limitations and overly conservative safe sets--our approach is sample-efficient, scalable, and results in less restrictive safety regions. We validate our framework through case studies on obstacle avoidance in autonomous driving and geo-fencing of aerial vehicles, demonstrating its ability to generate larger and less conservative safe sets compared to conventional techniques.

Paper number 4:
Title: Reachable Sets-based Trajectory Planning Combining Reinforcement Learning and iLQR
Authors: Wenjie Huang, Yang Li, Shijie Yuan, Jingjia Teng, Hongmao Qin, Yougang Bian
Abstract: The driving risk field is applicable to more complex driving scenarios, providing new approaches for safety decision-making and active vehicle control in intricate environments. However, existing research often overlooks the driving risk field and fails to consider the impact of risk distribution within drivable areas on trajectory planning, which poses challenges for enhancing safety. This paper proposes a trajectory planning method for intelligent vehicles based on the risk reachable set to further improve the safety of trajectory planning. First, we construct the reachable set incorporating the driving risk field to more accurately assess and avoid potential risks in drivable areas. Then, the initial trajectory is generated based on safe reinforcement learning and projected onto the reachable set. Finally, we introduce a trajectory planning method based on a constrained iterative quadratic regulator to optimize the initial solution, ensuring that the planned trajectory achieves optimal comfort, safety, and efficiency. We conduct simulation tests of trajectory planning in high-speed lane-changing scenarios. The results indicate that the proposed method can guarantee trajectory comfort and driving efficiency, with the generated trajectory situated outside high-risk boundaries, thereby ensuring vehicle safety during operation.

Paper number 5:
Title: TamedPUMA: safe and stable imitation learning with geometric fabrics
Authors: Saray Bakker, Rodrigo Pérez-Dattari, Cosimo Della Santina, Wendelin Böhmer, Javier Alonso-Mora
Abstract: Using the language of dynamical systems, Imitation learning (IL) provides an intuitive and effective way of teaching stable task-space motions to robots with goal convergence. Yet, IL techniques are affected by serious limitations when it comes to ensuring safety and fulfillment of physical constraints. With this work, we solve this challenge via TamedPUMA, an IL algorithm augmented with a recent development in motion generation called geometric fabrics. As both the IL policy and geometric fabrics describe motions as artificial second-order dynamical systems, we propose two variations where IL provides a navigation policy for geometric fabrics. The result is a stable imitation learning strategy within which we can seamlessly blend geometrical constraints like collision avoidance and joint limits. Beyond providing a theoretical analysis, we demonstrate TamedPUMA with simulated and real-world tasks, including a 7-DoF manipulator.

Paper number 6:
Title: Stabilizing NMPC Approaches for Underactuated Mechanical Systems on the SE(3) Manifold
Authors: Jean C. Pereira, Valter J. S. Leite, Guilherme V. Raffo
Abstract: This paper addresses the motion control problem for underactuated mechanical systems with full attitude control and one translational force input to manage the six degrees of freedom involved in the three-dimensional Euclidean space. These systems are often classified as second-order nonholonomic due to their completely nonintegrable acceleration constraints. To tackle this complex control problem, we propose two nonlinear model predictive control (NMPC) schemes that ensure closed-loop stability and recursive feasibility without terminal conditions. The system dynamics are modeled on the SE(3) manifold for a globally and unique description of rigid body configurations. One NMPC scheme also aims to reduce mission time as an economic criterion. The controllers' effectiveness is validated through numerical experiments on a quadrotor UAV.

Paper number 7:
Title: Anatomically Guided Motion Correction for Placental IVIM Parameter Estimation with Accelerated Sampling Method
Authors: Mbaimou Auxence Ngremmadji, Freddy Odille, Charline Bertholdt, Marine Beaumont, Olivier Morel, Bailiang Chen
Abstract: Intravoxel incoherent motion (IVIM) is a diffusion-weighted magnetic resonance imaging (MRI) method that may be applied to the placenta to help diagnose abnormal pregnancies. IVIM requires prolonged scan times, followed by a model-based estimation procedure. Maternal or fetal motion during the scan affects the accuracy of this estimation. In this work, we proposed to address this challenging motion correction and data fitting problem by using additional anatomical information that is routinely collected at the beginning of the examination. Super-resolution reconstruction (SRR) was applied to these anatomical data, to provide a patient-specific, 3D isotropic, anatomic reference. Our first contribution is a novel framework with a two-step motion correction that uses both IVIM and the SRR anatomic data, accounting for both intra- and inter-scan, non-rigid motion. Our second contribution is an automation and acceleration of the IVIM data fitting, using a state-of-the-art Bayesian-type algorithm, modified with a preconditioned Crank-Nicholson (pCN) sampling strategy. The accuracy of the IVIM parameter fitting was improved by the proposed motion correction strategy, as assessed by the mean absolute fitting error in the region of interest, which was 4.14 before and 3.02 after correction (arbitrary units of signal intensity). The novel sampling strategy accelerated parameter estimation by 39% in average, with the same accuracy as that of the conventional Bayesian approach. In conclusion, the proposed method may be applied to obtain fast and reliable IVIM parameter estimates in challenging scenarios such as prenatal MRI.

Paper number 8:
Title: Sharp Hybrid Zonotopes: Set Operations and the Reformulation-linearization Technique
Authors: Jonah J. Glunt, Joshua A. Robbins, Daniel Silvestre, Herschel C. Pangborn
Abstract: Mixed integer set representations, and specifically hybrid zonotopes, have enabled new techniques for reachability and verification of nonlinear and hybrid systems. Mixed-integer sets which have the property that their convex relaxation is equal to their convex hull are said to be sharp. This property allows the convex hull to be computed with minimal overhead, and is known to be important for improving the convergence rates of mixed-integer optimization algorithms that rely on convex relaxations. This paper examines methods for formulating sharp hybrid zonotopes and provides sharpness-preserving methods for performing several key set operations. The paper then shows how the reformulation-linearization technique can be applied to create a sharp realization of a hybrid zonotope that is initially not sharp. A numerical example applies this technique to find the convex hull of a level set of a feedforward ReLU neural network.

Paper number 9:
Title: The Impedance Space: A Look at Mechanical Impedance Ellipses in 3D
Authors: Leonardo F. Dos Santos (1), Cícero Zanette (1), Elisa G. Vergamini (1), Lucca Maitan (1), Thiago Boaventura (1) ((1) São Carlos School of Engineering - University of São Paulo)
Abstract: This paper introduces a novel 3D graphical representation of mechanical impedance, named impedance space, to enhance the analysis of the dynamic behavior of compliant systems. This approach addresses the limitations of existing 2D graphical methods by associating the impedance control parameters with linear transformations to plot a parametric 3D ellipse, for a known oscillatory input. The well-known 2D impedance ellipses are shown to be particular projections of this 3D ellipse. Experimental results demonstrate the effectiveness of the proposed representation.

Paper number 10:
Title: MM-UNet: Meta Mamba UNet for Medical Image Segmentation
Authors: Bin Xie, Yan Yan, Gady Agam
Abstract: State Space Models (SSMs) have recently demonstrated outstanding performance in long-sequence modeling, particularly in natural language processing. However, their direct application to medical image segmentation poses several challenges. SSMs, originally designed for 1D sequences, struggle with 3D spatial structures in medical images due to discontinuities introduced by flattening. Additionally, SSMs have difficulty fitting high-variance data, which is common in medical imaging. In this paper, we analyze the intrinsic limitations of SSMs in medical image segmentation and propose a unified U-shaped encoder-decoder architecture, Meta Mamba UNet (MM-UNet), designed to leverage the advantages of SSMs while mitigating their drawbacks. MM-UNet incorporates hybrid modules that integrate SSMs within residual connections, reducing variance and improving performance. Furthermore, we introduce a novel bi-directional scan order strategy to alleviate discontinuities when processing medical images. Extensive experiments on the AMOS2022 and Synapse datasets demonstrate the superiority of MM-UNet over state-of-the-art methods. MM-UNet achieves a Dice score of 91.0% on AMOS2022, surpassing nnUNet by 3.2%, and a Dice score of 87.1% on Synapse. These results confirm the effectiveness of integrating SSMs in medical image segmentation through architectural design optimizations.

Paper number 11:
Title: Echo-E$^3$Net: Efficient Endo-Epi Spatio-Temporal Network for Ejection Fraction Estimation
Authors: Moein Heidari, Afshin Bozorgpour, AmirHossein Zarif-Fakharnia, Dorit Merhof, Ilker Hacihaliloglu
Abstract: Left ventricular ejection fraction (LVEF) is a critical metric for assessing cardiac function, widely used in diagnosing heart failure and guiding clinical decisions. Despite its importance, conventional LVEF estimation remains time-consuming and operator-dependent. Recent deep learning advancements have enhanced automation, yet many existing models are computationally demanding, hindering their feasibility for real-time clinical applications. Additionally, the interplay between spatial and temporal features is crucial for accurate estimation but is often overlooked. In this work, we propose Echo-E$^3$Net, an efficient Endo-Epi spatio-temporal network tailored for LVEF estimation. Our method introduces the Endo-Epi Cardial Border Detector (E$^2$CBD) module, which enhances feature extraction by leveraging spatial and temporal landmark cues. Complementing this, the Endo-Epi Feature Aggregator (E$^2$FA) distills statistical descriptors from backbone feature maps, refining the final EF prediction. These modules, along with a multi-component loss function tailored to align with the clinical definition of EF, collectively enhance spatial-temporal representation learning, ensuring robust and efficient EF estimation. We evaluate Echo-E$^3$Net on the EchoNet-Dynamic dataset, achieving a RMSE of 5.15 and an R$^2$ score of 0.82, setting a new benchmark in efficiency with 6.8 million parameters and only 8.49G Flops. Our model operates without pre-training, data augmentation, or ensemble methods, making it well-suited for real-time point-of-care ultrasound (PoCUS) applications. Our Code is publicly available on~\href{this https URL}{\textcolor{magenta}{GitHub}}.

Paper number 12:
Title: Electric Vehicle Integration using Large-Scale Combined Transmission and Distribution Grid Models
Authors: Diana Wallison, Lyric Haylow, Jessica Wert, Jonathan M. Snodgrass, Thomas J. Overbye, Yanzhi (Ann)Xu
Abstract: In this paper, we propose a unifying co-simulation framework integrating transportation demand, grid assets, land use, demographics, and emissions to optimally accelerate electric vehicle (EV) development as well as measure the impact of EV integration. 96 urban and long-haul truck charging demand simulations were developed and integrated into a combined transmission and distribution (T&D) simulation, encompassing the Houston/Dallas/Fort Worth area. The T&D scenarios are then used to develop cost optimization strategies to determine optimal placement and sizing of truck charging infrastructure that minimize infrastructure costs.

Paper number 13:
Title: A robust mechanical sensorless control strategy for active rectification of small wind turbines
Authors: Adrien Prévost, Vincent Léchappé, Romain Delpoux, Xavier Brun
Abstract: This article proposes a mechanical sensorless control strategy for the synchronous rectification of small wind turbines equipped with a surface-mounted Permanent Magnet Synchronous Generator (PMSG). By means of Lyapunov theory, the Global Asymptotic Stability (GAS) of the closed loop system is proven. It allows the use of a classical Sliding Mode Observer (SMO) to remove the mechanical sensor in the control loop despite uncertainties on the resistance and inductance parameters. The analysis of the equilibrium points have made it possible to propose an analytic model of the angular misalignment between the true and the observer rotating frames, responsible for current tracking errors. Experimental tests on a wind turbine emulator show that despite large errors on the the resistance and inductance parameters, the impact on the energy harvest is low, proving that the strategy's performance is robust to high uncertainties.

Paper number 14:
Title: Sequential HW-Aware Precoding: Over-the-air cancellation of HWI in Downlink Cell-Free Massive MIMO with Serial Fronthaul
Authors: Antoine Durant, Asma Mabrouk, Rafik Zayani
Abstract: This paper addresses the critical challenge of mitigating hardware impairments (HWIs) in downlink cell-free massive MIMO (CF-mMIMO) networks while ensuring computational scalability. We propose a novel sequential hardware-aware (HW-aware) precoding technique that leverages the serial fronthaul topology to perform over-the-air HWI cancellation. This approach involves sequentially exchanging approximated user-perceived distortion information among successive access points (APs) for over-the-air HWI mitigation. Each AP independently computes its spatial multiplexing weights and transmits signals that counteract the distortions introduced by the preceding AP. We develop a problem formulation and present a closed-form solution for this method. For performance evaluation, we study two reference methods taken either from centralized massive MIMO literature (Tone Reservation [TR]) or tailored for CF-mMIMO networks (PAPR-aware precoding), both focusing on reducing the PAPR of OFDM signals in the downlink. Results indicate that the sequential HW-aware approach achieves a substantial increase in spectral efficiency (SE) in high-distortion scenarios, with an average SE increase factor of 1.8 under severe distortions. Additionally, the proposed method, which is executed locally, demonstrates better scalability, achieving a reduction of up to 40\% and 72\% in the total number of complex multiplications compared to the PAPR-aware and TR approaches, respectively. Finally, the sequential HW-aware precoder offers high performance even when applied to cost-effective APs with few antennas, presenting a promising and practical solution for HWI compensation in CF-mMIMO systems with serial fronthaul.

Paper number 15:
Title: ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology
Authors: Vishwesh Ramanathan, Tony Xu, Pushpak Pati, Faruk Ahmed, Maged Goubran, Anne L. Martel
Abstract: Prediction tasks in digital pathology are challenging due to the massive size of whole-slide images (WSIs) and the weak nature of training signals. Advances in computing, data availability, and self-supervised learning (SSL) have paved the way for slide-level foundation models (SLFMs) that can improve prediction tasks in low-data regimes. However, working with these models is challenging, with issues such as catastrophic forgetting during fine-tuning and under-utilization of shared information between tasks and modalities. To overcome these two challenges, we propose ModalTune, a novel fine-tuning framework which introduces the Modal Adapter to integrate new modalities without modifying SLFM weights. Additionally, we use large-language models (LLMs) to encode labels as text, capturing semantic relationships and enhancing generalization across multiple tasks and cancer types in a single training recipe. ModalTune achieves state-of-the-art (SOTA) results against both uni-modal and multi-modal models across four cancer types, jointly improving survival and cancer subtype prediction while remaining competitive in pan-cancer settings. Additionally, we show ModalTune is highly generalizable to two out-of-distribution (OOD) datasets. To our knowledge, this is the first unified fine-tuning framework for multi-modal, multi-task, and pan-cancer modeling in digital pathology.

Paper number 16:
Title: Joint Superimposed Pilot-aided Channel Estimation and Data Detection for FTN Signaling over Doubly-Selective Channels
Authors: Simin Keykhosravi, Ebrahim Bedeer
Abstract: Faster-than-Nyquist (FTN) signaling and superimposed pilot (SP) techniques are effective solutions for significantly enhancing the spectral efficiency (SE) in next-generation wireless communication systems. This paper proposes an innovative SP-aided channel estimation method for FTN signaling enhancing the SE over doubly-selective (i.e., time- and frequency-selective) channels. To avoid complex channel tracking, we utilize a basis expansion model (BEM) to characterize doubly-selective channel variations. We propose a frame structure that superimposes a known periodic pilot sequence onto the information sequence, avoiding SE loss by eliminating the additional overhead of multiplexed pilot (MP). Additionally, we find the optimal FTN signaling SP sequence that minimizes the mean square error (MSE) of the channel estimation. Expanding on our proposed SP-aided channel estimation method, we propose two detection methods: (1) an SP-aided separate channel estimation and data detection (SCEDD) method performing a single channel estimation followed by iterative data detection via a turbo equalizer, serving as a baseline for evaluating the SP-aided channel estimation method, and (2) an SP-aided joint channel estimation and data detection (JCEDD) method, which extends the SCEDD by updating the channel estimate in each turbo equalization iteration, becoming our primary focus for its superior performance. At equivalent SE and a higher fading rate on the order of $10^{-3}$, our simulations show that SP-aided SCEDD method outperforms MP-aided techniques in both MSE and BER, while the SP-aided JCEDD method delivers remarkable performance, where reference approaches fail to track rapid channel variations.

Paper number 17:
Title: Wideband Cognitive Radio for Joint Communication and Sensing: Optimization of Subcarrier Allocation and beamforming
Authors: Diluka Galappaththige, Chintha Tellambura
Abstract: As data traffic grows, wireless systems shift to higher frequency bands (6 GHz and above), where radar systems also operate. This coexistence demands effective interference management and efficient wideband utilization. Cognitive Radio (CR) offers a solution but remains limited to single-node or narrowband systems. This paper introduces a generalized wideband CR-enabled communication and sensing system with multiple users and targets. We propose a communication and sensing sub-carrier allocations framework, followed by transmit beamforming for the primary communication BS and sensing signal design for the secondary radar BS. The goal is to maximize the communication sum rate while ensuring sensing requirements, minimizing interference, and adhering to power constraints. To solve the resulting non-convex problem, we develop a manifold optimization algorithm for communication-only sub-carriers and an alternating optimization approach using the generalized Rayleigh quotient and semidefinite relaxation for communication-sensing sub-carriers. Compared to a non-cooperative benchmark, the proposed system achieves a \qty{10}{\percent} gain in communication sum rate and a \qty{32}{\percent} gain in sensing sum rate with \num{12} BS antennas.

Paper number 18:
Title: Mixed-gradients Distributed Filtered Reference Least Mean Square Algorithm -- A Robust Distributed Multichannel Active Noise Control Algorithm
Authors: Junwei Ji, Dongyuan Shi, Woon-Seng Gan
Abstract: Distributed multichannel active noise control (DMCANC), which utilizes multiple individual processors to achieve a global noise reduction performance comparable to conventional centralized multichannel active noise control (MCANC), has become increasingly attractive due to its high computational efficiency. However, the majority of current DMCANC algorithms disregard the impact of crosstalk across nodes and impose the assumption of an ideal network devoid of communication limitations, which is an unrealistic assumption. Therefore, this work presents a robust DMCANC algorithm that employs the compensating filter to mitigate the impact of crosstalk. The proposed solution enhances the DMCANC system's flexibility and security by utilizing local gradients instead of local control filters to convey enhanced information, resulting in a mixed-gradients distributed filtered reference least mean square (MGDFxLMS) algorithm. The performance investigation demonstrates that the proposed approach performs well with the centralized method. Furthermore, to address the issue of communication delay in the distributed network, a practical strategy that auto-shrinks the step size value in response to the delayed samples is implemented to improve the system's resilience. The numerical simulation results demonstrate the efficacy of the proposed auto-shrink step size MGDFxLMS (ASSS-MGDFxLMS) algorithm across various communication delays, highlighting its practical value.

Paper number 19:
Title: DGAR: A Unified Domain Generalization Framework for RF-Enabled Human Activity Recognition
Authors: Junshuo Liu, Xin Shi, Robert C. Qiu
Abstract: Radio-frequency (RF)-based human activity recognition (HAR) is a non-intrusive and privacy-preserving technology with applications in smart homes, healthcare, and security systems. However, real-world deployments face challenges from domain shifts caused by user behavior, physical attributes, and environmental conditions, leading to performance degradation. To address this, we propose DGAR, a domain-generalized activity recognition framework that learns domain-invariant and domain-specific representations without requiring target domain data. DGAR leverages correlation alignment to reduce inter-domain discrepancies and integrates a squeeze-and-excitation (SE) block to enhance the extraction of salient spatial and temporal features from RF data. Extensive experiments on multiple public datasets, including HUST-HAR, Lab-LFM, and Office-LFM, validate DGAR's effectiveness, achieving F1-score improvements ranging from 2.09% to 5.81% over state-of-the-art methods. These results demonstrate DGAR's ability to address domain shift challenges, paving the way for robust, real-world HAR applications in diverse and dynamic scenarios.

Paper number 20:
Title: Causal Inference based Transfer Learning with LLMs: An Efficient Framework for Industrial RUL Prediction
Authors: Yan Chen, Cheng Liu
Abstract: Accurate prediction of Remaining Useful Life (RUL) for complex industrial machinery is critical for the reliability and maintenance of mechatronic systems, but it is challenged by high-dimensional, noisy sensor data. We propose the Causal-Informed Data Pruning Framework (CIDPF), which pioneers the use of causal inference to identify sensor signals with robust causal relationships to RUL through PCMCI-based stability analysis, while a Gaussian Mixture Model (GMM) screens for anomalies. By training on only 10% of the pruned data, CIDPF fine-tunes pre-trained Large Language Models (LLMs) using parameter-efficient strategies, reducing training time by 90% compared to traditional approaches. Experiments on the N-CMAPSS dataset demonstrate that CIDPF achieves a 26% lower RMSE than existing methods and a 25% improvement over full-data baselines, showcasing superior accuracy and computational efficiency in industrial mechatronic systems. The framework's adaptability to multi-condition scenarios further underscores its practicality for industrial deployment.

Paper number 21:
Title: Enhancing Fault Detection in CO2 Refrigeration Systems: Optimal Sensor Selection and Robustness Analysis Using Tree-Based Machine Learning
Authors: Masoud Kishani Farahani, Morteza Kolivandi, Abbas Rajabi Ghahnavieh, Mohammad Talaei
Abstract: This study investigates the reliability and robustness of data-driven Fault Detection and Diagnosis (FDD) models for CO2 refrigeration systems (CO2-RS) in supermarkets, focusing on optimal sensor selection and resilience against sensor noise. Using tree-based machine learning algorithms - Random Forest (RF), XGBoost, CatBoost, and LightGBM - we developed FDD models to classify six common faults in a laboratory-scale CO2-RS. The Recursive Feature Addition (RFA) approach identified optimal sensor sets, achieving a 99% F1-score with minimal sensors: four for RF, seven for XGBoost, five for CatBoost, and five for LightGBM. Condenser-side sensors consistently ranked as critical for fault detection. Robustness was assessed by injecting Additive White Gaussian Noise (AWGN) at a signal-to-noise ratio (SNR) of 3 dB into the most important sensor, with XGBoost showing superior resilience at 85.24%, followed by CatBoost (57.07%), LightGBM (49.1%), and RF (49.46%). Sensitivity analysis across high-SNR (10 dB), low-SNR (0 dB), and sensor failure scenarios revealed XGBoost's robustness peaking at 90.23% and retaining 79% under failure, contrasting with sharper declines in other models. These findings highlight a trade-off between sensor count, cost, and reliability, with larger ensembles enhancing noise resilience. This work bridges gaps in FDD literature by integrating sensor optimization with comprehensive robustness analysis, offering a practical framework for improving energy efficiency and fault management in CO2-RS. Future efforts could explore adaptive SNR thresholds and redundant sensor configurations to enhance real-world applicability.

Paper number 22:
Title: Flexible WMMSE Beamforming for MU-MIMO Movable Antenna Communications
Authors: Songjie Yang, Zihang Wan, Yue Xiu, Boyu Ning, Yong Li, Yuenwei Liu, Chau Yuen
Abstract: Movable antennas offer new potential for wireless communication by introducing degrees of freedom in antenna positioning, which has recently been explored for improving sum rates. In this paper, we aim to fully leverage the capabilities of movable antennas (MAs) by assuming that both the transmitter and receiver can optimize their antenna positions in multi-user multiple-input multiple-output (MU-MIMO) communications. Recognizing that WMMSE beamforming is a highly effective method for maximizing the MU-MIMO sum rate, we modify it to integrate antenna position optimization for MA systems, which we refer to as flexible WMMSE (F-WMMSE) beamforming. Importantly, we reformulate the subproblems within WMMSE to develop regularized sparse optimization frameworks to achieve joint beamforming (antenna coefficient optimization) and element movement (antenna position optimization). We then propose a regularized least squares-based simultaneous orthogonal matching pursuit (RLS-SOMP) algorithm to address the resulting sparse optimization problem. To enhance practical applications, the low-complexity implementation of the proposed framework is developed based on the pre-calculations and matrix inverse lemma. The overall F-WMMSE algorithm converges similarly to WMMSE, and our findings indicate that F-WMMSE achieves a significant sum rate improvement compared to traditional WMMSE, exceeding 20% under appropriate simulation conditions

Paper number 23:
Title: RIS-based Physical Layer Security for Integrated Sensing and Communication: A Comprehensive Survey
Authors: Yongxiao Li, Feroz Khan, Manzoor Ahmed, Aized Amin Soofi, Wali Ullah Khan, Chandan Kumar Sheemar, Muhammad Asif, Zhu Han
Abstract: Integrated Sensing and Communication (ISAC) is a crucial component of future wireless networks, enabling seamless integration of Communication and Sensing (C\&S) functionalities. However, ensuring security in ISAC systems remains a significant challenge, as both C\&S data are susceptible to adversarial threats. Physical Layer Security (PLS) has emerged as a key framework for mitigating these risks at the transmission level. Reconfigurable Intelligent Surfaces (RIS) further enhance PLS by dynamically shaping the radio environment to improve both secrecy along with C\&S performance. This survey begins with an overview of RIS, PLS, and ISAC fundamentals, establishing a foundation for understanding their integration. The state-of-the-art RIS-assisted PLS approaches in ISAC systems are then categorized into passive RIS and Active RIS (ARIS) paradigms. Passive RIS-based techniques focus on optimizing system throughput, covert communication, and Secrecy Rates (SRs), alongside improving sensing Signal-to-Noise Ratio (SNR) and Weighted Sum Rate (WSR) under various constraints. ARIS-based strategies extend these capabilities by actively optimizing beamforming to enhance secrecy and covert rates while ensuring robust sensing under communication and security constraints. By reviewing both passive and ARIS-based security frameworks, this survey highlights the transformative role of RIS in strengthening ISAC security. Furthermore, it explores key optimization methodologies, technical challenges, and future research directions for integrating RIS with PLS to ensure secure and efficient ISAC in next-generation 6G wireless networks.

Paper number 24:
Title: Investigation into the role of the Bessel function order in the Fourier-Bessel series and the Hankel Transform
Authors: Suketu P Patni, Vikram M Gadre
Abstract: This work focuses on estimating the number of terms of a Fourier-Bessel series of order $p'$ required to get within a certain error of a Bessel function of a fixed order $p$ where $p \neq p'$. Our approach consists of two steps: one, constructing an invariant over $n$ of the $n^{\text{th}}$ order Hankel transform; and two, observing the effect of expanding a suitably scaled Bessel function of a fixed order $p$ in its Fourier-Bessel series of order $p'$. We demonstrate a new error metric to simplify the error computations. Further, we generate an empirical model using numerical simulations and examine its capabilities in predicting the number of terms required.

Paper number 25:
Title: A Novel Design Method for Seeking Sparse Linear Arrays With Low Redundancy and Enhanced DOF
Authors: Si Wang, Guoqiang Xiao
Abstract: Sparse arrays with $N$-sensors can provide up to $O(N^2)$ degrees of freedom (DOF) by second-order cumulants. However, these sparse arrays like minimum-/low-redundancy arrays (MRAs/LRAs), nested arrays and coprime arrays can only provide limited DOF and array aperture with the same number of physical sensors. However, further increasing DOF would increase costs in practical applications. The paper aims to design a sparse linear array (SLA) with higher DOF and lower redundancy via exploring different cases of third-order cumulants. Based on the framework third-order exhaustive co-array (TO-ECA), a general third-order array (GTOA) with any generator is proposed in the paper. Further, three novel arrays are designed based on GTOA with different generators, namely third-order sum and difference array (generator) (TO-SDA(CNA)), (TO-SDA(SCNA)) and (TO-SDA(TNA-II)) which can provide closed-form expressions for the sensor locations and enhance DOF in order to resolve more signal sources in the estimation of direction of arrival (DOA). The three TO-SDAs are all consisted of two sub-arrays, where the first is the generator and another is a ULA with big inter-spacing between sensors. For the three TO-SDAs, the maximum DOF under the given number of total physical sensors can be derived and the TO-ECA of the three TO-SDAs are hole-free. Additionally, the redundancy of the three TO-SDAs is defined, and the lower band of the redundancy for the three TO-SDAs is derived. Furthermore, the proposed TO-SDA(TNA-II) not only achieves higher DOF than those of existing TONA and even SE-FL-NA but also reduces mutual coupling effects. Meanwhile it realizes higher resolution and decreases redundancy. Numerical simulations are conducted to verify the superiority of TO-SDA(TNA-II) on DOA estimation performance and enhanced DOF over other existing DCAs.

Paper number 26:
Title: Probabilistic Net Load Forecasting for High-Penetration RES Grids Utilizing Enhanced Conditional Diffusion Model
Authors: Yixiang Huang, Jianhua Pei, Luocheng Chen, Zhenchang Du, Jinfu Chen, Zirui Peng
Abstract: The proliferation of intermittent distributed renewable energy sources (RES) in modern power systems has fundamentally compromised the reliability and accuracy of deterministic net load forecasting. Generative models, particularly diffusion models, demonstrate exceptional potential in uncertainty quantification for scenario forecasting. Nevertheless, their probabilistic predictive capabilities and conditional bootstrapping mechanisms still remain underexplored. In this paper, a day-ahead probabilistic net load forecasting framework is developed by systematically quantifying epistemic uncertainty and aleatoric variability using the feature-informed enhanced conditional diffusion model (ECDM). The ECDM architecture implements the net load distribution generation process using an imputation-based conditional diffusion model, where multi-modal conditional inputs, such as weather and calendar data, are fused via cross-attention mechanisms. Specifically, historical net load profiles are utilized to guide the reverse diffusion trajectory through non-parametric imputation operators preserving spatial-temporal integrity. To capture periodic characteristics, a novel weekly arrangement method is also introduced, while an unconditional model is integrated to ensure diversity in the generated scenarios. Subsequently, the maximum probabilistic points and probability intervals of predicted net load are obtained by the adaptive kernel density estimation under RES intermittency. Moreover, ECDM is extented to multi-energy forecast framework, attempting to increase interpretability of the net load predictions. Numerical experiments on a publicly available dataset demonstrate the superior forecasting performance of the proposed method compared to existing state-of-the-art approaches.

Paper number 27:
Title: Hierarchy-Aware and Channel-Adaptive Semantic Communication for Bandwidth-Limited Data Fusion
Authors: Lei Guo, Wei Chen, Yuxuan Sun, Bo Ai, Nikolaos Pappas, Tony Quek
Abstract: Obtaining high-resolution hyperspectral images (HR-HSI) is costly and data-intensive, making it necessary to fuse low-resolution hyperspectral images (LR-HSI) with high-resolution RGB images (HR-RGB) for practical applications. However, traditional fusion techniques, which integrate detailed information into the reconstruction, significantly increase bandwidth consumption compared to directly transmitting raw data. To overcome these challenges, we propose a hierarchy-aware and channel-adaptive semantic communication approach for bandwidth-limited data fusion. A hierarchical correlation module is proposed to preserve both the overall structural information and the details of the image required for super-resolution. This module efficiently combines deep semantic and shallow features from LR-HSI and HR-RGB. To further reduce bandwidth usage while preserving reconstruction quality, a channel-adaptive attention mechanism based on Transformer is proposed to dynamically integrate and transmit the deep and shallow features, enabling efficient data transmission and high-quality HR-HSI reconstruction. Experimental results on the CAVE and Washington DC Mall datasets demonstrate that our method outperforms single-source transmission, achieving up to a 2 dB improvement in peak signal-to-noise ratio (PSNR). Additionally, it reduces bandwidth consumption by two-thirds, confirming its effectiveness in bandwidth-constrained environments for HR-HSI reconstruction tasks.

Paper number 28:
Title: Assessing workflow impact and clinical utility of AI-assisted brain aneurysm detection: a multi-reader study
Authors: Tommaso Di Noto, Sofyan Jankowski, Francesco Puccinelli, Guillaume Marie, Sebastien Tourbier, Yasser Aleman-Gomez, Oscar Esteban, Ricardo Corredor-Jerez, Guillaume Saliou, Patric Hagmann, Meritxell Bach Cuadra, Jonas Richiardi
Abstract: Despite the plethora of AI-based algorithms developed for anomaly detection in radiology, subsequent integration into clinical setting is rarely evaluated. In this work, we assess the applicability and utility of an AI-based model for brain aneurysm detection comparing the performance of two readers with different levels of experience (2 and 13 years). We aim to answer the following questions: 1) Do the readers improve their performance when assisted by the AI algorithm? 2) How much does the AI algorithm impact routine clinical workflow? We reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance Angiography dataset (N=460). We use 360 subjects for training/validating our algorithm and 100 as unseen test set for the reading session. Even though our model reaches state-of-the-art results on the test set (sensitivity=74%, false positive rate=1.6), we show that neither the junior nor the senior reader significantly increase their sensitivity (p=0.59, p=1, respectively). In addition, we find that reading time for both readers is significantly higher in the "AI-assisted" setting than in the "Unassisted" (+15 seconds, on average; p=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers is unchanged across the two settings, indicating that the AI assistance does not influence the certainty of the diagnosis. Our findings highlight the importance of clinical validation of AI algorithms in a clinical setting involving radiologists. This study should serve as a reminder to the community to always examine the real-word effectiveness and workflow impact of proposed algorithms.

Paper number 29:
Title: Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low Earth Orbit
Authors: Zachary L. Clements, Patrick B. Ellis, Iain Goodridge, Matthew J. Murrian, Mark L. Psiaki, Todd E. Humphreys
Abstract: This paper presents an analysis and experimental demonstration of single-satellite single-pass geolocation of a terrestrial broadcast Global Navigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The proliferation of LEO-based GNSS receivers offers the prospect of unprecedented spectrum awareness, enabling persistent GNSS interference detection and geolocation. Accurate LEO-based single-receiver emitter geolocation is possible when a range-rate time history can be extracted for the emitter. This paper presents a technique crafted specifically for indiscriminate broadcast-type GNSS spoofing signals. Furthermore, it explores how unmodeled oscillator instability and worst-case spoofer-introduced signal variations degrade the geolocation estimate. The proposed geolocation technique is validated by a controlled experiment, in partnership with Spire Global, in which a LEO-based receiver captures broadcast GNSS spoofing signals transmitted from a known ground station on a non-GNSS frequency band.

Paper number 30:
Title: DVG-Diffusion: Dual-View Guided Diffusion Model for CT Reconstruction from X-Rays
Authors: Xing Xie, Jiawei Liu, Huijie Fan, Zhi Han, Yandong Tang, Liangqiong Qu
Abstract: Directly reconstructing 3D CT volume from few-view 2D X-rays using an end-to-end deep learning network is a challenging task, as X-ray images are merely projection views of the 3D CT volume. In this work, we facilitate complex 2D X-ray image to 3D CT mapping by incorporating new view synthesis, and reduce the learning difficulty through view-guided feature alignment. Specifically, we propose a dual-view guided diffusion model (DVG-Diffusion), which couples a real input X-ray view and a synthesized new X-ray view to jointly guide CT reconstruction. First, a novel view parameter-guided encoder captures features from X-rays that are spatially aligned with CT. Next, we concatenate the extracted dual-view features as conditions for the latent diffusion model to learn and refine the CT latent representation. Finally, the CT latent representation is decoded into a CT volume in pixel space. By incorporating view parameter guided encoding and dual-view guided CT reconstruction, our DVG-Diffusion can achieve an effective balance between high fidelity and perceptual quality for CT reconstruction. Experimental results demonstrate our method outperforms state-of-the-art methods. Based on experiments, the comprehensive analysis and discussions for views and reconstruction are also presented.

Paper number 31:
Title: Achievable Sum Secrecy Rate of STAR-RIS-Enabled MU-MIMO ISAC
Authors: Ainara Kazymova, Vaibhav Kumar, Christina Pöpper, Marwa Chafii
Abstract: Various emerging applications in sixth-generation (6G) wireless demand a seamless integration of communication and sensing services, driving the development of integrated sensing and communication (ISAC) systems. Using a common waveform for both functions introduces additional security challenges, as information-bearing signals are vulnerable to eavesdropping. While a variety of secure beamforming designs are proposed in the literature for metasurface-enabled ISAC systems with single-antenna users and eavesdroppers, optimal designs for multi-antenna scenarios remain unexplored. This paper addresses this gap by studying a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-enabled multi-user multiple-input multiple-output (MU-MIMO) ISAC system and proposing an optimal beamforming design to maximize the sum secrecy rate while ensuring sensing quality for multiple targets. An alternating-optimization based iterative solution is developed to tackle the non-convex optimization problem. The presented analysis confirms that the computational complexity of the proposed algorithm grows linearly with the number of metasurface elements. Our numerical results show the benefit of using STAR-RIS to increase the sum secrecy rate of the MU-MIMO ISAC system compared to its corresponding conventional RIS (cRIS)-enabled and w/o RIS systems.

Paper number 32:
Title: Green Integration of Sensing, Communication, and Power Transfer via STAR-RIS
Authors: Vaibhav Kumar, Marwa Chafii
Abstract: The upcoming sixth-generation (6G) wireless standard is anticipated to support a variety of applications that will require a seamless integration of sensing and communication services in a single network infrastructure. At the same time, 6G is also expected to support millions of low-powered Internet-of-Things (IoT) devices. Previous studies have demonstrated that the power requirements of these IoT devices can be met through wireless power transfer facilitated by intelligent metasurfaces. Therefore, in this paper, we try to formulate and answer a fundamental question: How much transmit power is required for an integrated sensing, communication, and power transfer (ISCPT) system? More specifically, we consider the problem of optimal active, passive, and receive beamforming design for a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-enabled ISCPT system with multiple sensing targets, multiple information receivers, and multiple energy receivers, to minimize the required transmit power from the base station, while guaranteeing predefined sensing, communication, and energy harvesting requirements. To tackle the challenging non-convex optimization problem, we use an alternating optimization (AO)-based approach, where the receive beamforming is obtained in closed form while the active and passive beamforming are obtained using a second-order cone program (SOCP) approach. Our numerical results show the benefit of using STAR-RIS to reduce the transmit power requirement for the ISCPT system compared to its corresponding conventional RIS (cRIS)-enabled and non-RIS ISCPT systems.

Paper number 33:
Title: FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation
Authors: Qingshan Hou, Meng Wang, Peng Cao, Zou Ke, Xiaoli Liu, Huazhu Fu, Osmar R. Zaiane
Abstract: Recent advancements in ophthalmology foundation models such as RetFound have demonstrated remarkable diagnostic capabilities but require massive datasets for effective pre-training, creating significant barriers for development and deployment. To address this critical challenge, we propose FundusGAN, a novel hierarchical feature-aware generative framework specifically designed for high-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid Network within its encoder to comprehensively extract multi-scale information, capturing both large anatomical structures and subtle pathological features. The framework incorporates a modified StyleGAN-based generator with dilated convolutions and strategic upsampling adjustments to preserve critical retinal structures while enhancing pathological detail representation. Comprehensive evaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN consistently outperforms state-of-the-art methods across multiple metrics (SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease classification experiments reveal that augmenting training data with FundusGAN-generated images significantly improves diagnostic accuracy across multiple CNN architectures (up to 6.49\% improvement with ResNet50). These results establish FundusGAN as a valuable foundation model component that effectively addresses data scarcity challenges in ophthalmological AI research, enabling more robust and generalizable diagnostic systems while reducing dependency on large-scale clinical data collection.

Paper number 34:
Title: Accelerated Cardiac Parametric Mapping using Deep Learning-Refined Subspace Models
Authors: Calder D. Sheagren, Brenden T. Kadota, Jaykumar H. Patel, Mark Chiew, Graham A. Wright
Abstract: Cardiac parametric mapping is useful for evaluating cardiac fibrosis and edema. Parametric mapping relies on single-shot heartbeat-by-heartbeat imaging, which is susceptible to intra-shot motion during the imaging window. However, reducing the imaging window requires undersampled reconstruction techniques to preserve image fidelity and spatial resolution. The proposed approach is based on a low-rank tensor model of the multi-dimensional data, which jointly estimates spatial basis images and temporal basis time-courses from an auxiliary parallel imaging reconstruction. The tensor-estimated spatial basis is then further refined using a deep neural network, trained in a fully supervised fashion, improving the fidelity of the spatial basis using learned representations of cardiac basis functions. This two-stage spatial basis estimation will be compared against Fourier-based reconstructions and parallel imaging alone to demonstrate the sharpening and denoising properties of the deep learning-based subspace analysis.

Paper number 35:
Title: Geometry-Based Channel Estimation, Prediction, and Fusion
Authors: Benjamin J. B. Deutschmann, Erik Leitinger, Klaus Witrisal
Abstract: Reciprocity-based beamforming-most commonly employed in time-division duplexing-uses noisy, estimated (i.e., measured) channel state information (CSI) acquired on the uplink. While computationally efficient, reciprocity-based beamforming suffers severe losses under (i) low signal-to-noise ratio (SNR) and (ii) user mobility because it ignores the underlying physics of the radio channel beyond its reciprocity. Based on a physics-driven geometry-based channel model, we propose a method that jointly infers the mobile user's position and environment map on the uplink. It then leverages the estimated user position and environment map to predict CSI on the downlink. We demonstrate significant efficiency gains under both (i) low SNR and (ii) user mobility on measured data. While the user position may allow efficient beamforming in strong line-of-sight (LoS) channels, inferring an environment map allows bypassing obstructed LoS conditions using non-LoS beamforming via multipath components. We further propose "channel fusion," a probabilistic (Bayesian) combination of estimated and predicted CSI, which increases the beamforming robustness, particularly when either source of CSI is unreliable. Notably, this approach shares similarities with the minimum mean square error (MMSE) channel estimator, with geometry-based prior parameters inferred from the data.

Paper number 36:
Title: An AI-enabled dual-hormone model predictive control algorithm that delivers insulin and pramlintide
Authors: Peter G. Jacobs, Wade Hilts, Robert Dodier, Joseph Leitschuh, Jae H. Eom, Deborah Branigan, Forrest Ling, Matthew Howard, Clara Mosquera-Lopez, Leah Wilson
Abstract: Current closed-loop insulin delivery algorithms need to be informed of carbohydrate intake disturbances. This can be a burden on people using these systems. Pramlintide is a hormone that delays gastric emptying, which enables insulin kinetics to align with the kinetics of carbohydrate absorption. Integrating pramlintide into an automated insulin delivery system can be helpful in reducing the postprandial glucose excursion and may be helpful in enabling fully-closed loop whereby meals do not need to be announced. We present an AI-enabled dual-hormone model predictive control (MPC) algorithm that delivers insulin and pramlintide without requiring meal announcements that uses a neural network to automatically detect and deliver meal insulin. The MPC algorithm includes a new pramlintide pharmacokinetics and pharmacodynamics model that was identified using data collected from people with type 1 diabetes undergoing a meal challenge. Using a simulator, we evaluated the performance of various pramlintide delivery methods and controller models, as well as the baseline insulin-only scenario. Meals were automatically dosed using a neural network meal detection and dosing (MDD) algorithm. The primary outcome was the percent time glucose is in the target range (TIR: 70-180 mg/dL). Results show that delivering pramlintide at a fixed ratio of 6 mcg pramlintide:1 u insulin using an MPC that is aware of the pramlintide achieved the most significant improved TIR compared with an insulin-only MPC (91.6% vs. 64.1%). Delivering pramlintide as a fixed ratio was better than delivering basal pramlintide at a constant rate, indicating the benefit of the MDD algorithm. There was no advantage of independent control of insulin and pramlintide compared with insulin and pramlintide delivered as a fixed ratio. Preliminary real-world results from a human subject further indicate the benefit of pramlintide delivery.

Paper number 37:
Title: Multi-Disease-Aware Training Strategy for Cardiac MR Image Segmentation
Authors: Hong Zheng (1 and 2 and 4), Yucheng Chen (3), Nan Mu (1 and 4 and 5), Xiaoning Li (1 and 4 and 5) ((1) College of Computer Science, Sichuan Normal University, Chengdu, China, (2) School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China, (3) Department of Cardiology, West China Hospital, Sichuan University, Chengdu, China, (4) Visual Computing and Virtual Reality Key Laboratory of Sichuan Province, Chengdu, China, (5) Sichuan 2011 Collaborative Innovation Center for Educational Big Data, Chengdu, China)
Abstract: Accurate segmentation of the ventricles from cardiac magnetic resonance images (CMRIs) is crucial for enhancing the diagnosis and analysis of heart conditions. Deep learning-based segmentation methods have recently garnered significant attention due to their impressive performance. However, these segmentation methods are typically good at partitioning regularly shaped organs, such as the left ventricle (LV) and the myocardium (MYO), whereas they perform poorly on irregularly shaped organs, such as the right ventricle (RV). In this study, we argue that this limitation of segmentation models stems from their insufficient generalization ability to address the distribution shift of segmentation targets across slices, cardiac phases, and disease conditions. To overcome this issue, we present a Multi-Disease-Aware Training Strategy (MTS) and restructure the introduced CMRI datasets into multi-disease datasets. Additionally, we propose a specialized data processing technique for preprocessing input images to support the MTS. To validate the effectiveness of our method, we performed control group experiments and cross-validation tests. The experimental results show that (1) network models trained using our proposed strategy achieved superior segmentation performance, particularly in RV segmentation, and (2) these networks exhibited robust performance even when applied to data from unknown diseases.

Paper number 38:
Title: Adaptive Koopman Model Predictive Control of Simple Serial Robots
Authors: Adriano del Río, Christoph Stoeffler
Abstract: Approximating nonlinear systems as linear ones is a common workaround to apply control tools tailored for linear systems. This motivates our present work where we developed a data-driven model predictive controller (MPC) based on the Koopman operator framework, allowing the embedding of nonlinear dynamics in a higher dimensional, but linear function space. The controller, termed adaptive Koopman model predictive control (KMPC), uses online closed-loop feedback to learn and incrementally update a linear representation of nonlinear system dynamics, without the prior knowledge of a model. Adaptive KMPC differs from most other Koopman-based control frameworks that aim to identify high-validity-range models in advance and then enter closed-loop control without further model adaptations. To validate the controller, trajectory tracking experiments are conducted with 1R and 2R robots under force disturbances and changing model parameters. We compare the controller to classical linearization MPC and Koopman-based MPC without model updates, denoted static KMPC. The results show that adaptive KMPC can, opposed to static KMPC, generalize over unforeseen force disturbances and can, opposed to linearization MPC, handle varying dynamic parameters, while using a small set of basis functions to approximate the Koopman operator.

Paper number 39:
Title: Opportunistic Subarray Grouping for RIS-Aided Massive Random Access in Cellular Connectivity
Authors: Yizhu Wang, Zhou Zhang, Saman Atapattu, Marco Di Renzo
Abstract: In Reconfigurable Intelligent Surfaces (RIS), reflective elements (REs) are typically configured as a single array, but as RE numbers increase, this approach incurs high overhead for optimal configuration. Subarray grouping provides an effective tradeoff between performance and overhead. This paper studies RIS-aided massive random access (RA) at the Medium Access Control (MAC) layer in cellular networks to enhance throughput. We introduce an opportunistic scheduling scheme that integrates multi-round access requests, subarray grouping for efficient RIS link acquisition, and multi-user data transmission. To optimize access request timing, RIS estimation overhead and throughput, we propose a multi-user RA strategy using sequential decision optimization to maximize average system throughput. A low-complexity algorithm is also developed for practical implementation. Both theoretical analysis and numerical simulations demonstrate that the proposed strategy significantly outperforms the extremes of full-array grouping and element-wise grouping.

Paper number 40:
Title: Cat-AIR: Content and Task-Aware All-in-One Image Restoration
Authors: Jiachen Jiang, Tianyu Ding, Ke Zhang, Jinxin Zhou, Tianyi Chen, Ilya Zharkov, Zhihui Zhu, Luming Liang
Abstract: All-in-one image restoration seeks to recover high-quality images from various types of degradation using a single model, without prior knowledge of the corruption source. However, existing methods often struggle to effectively and efficiently handle multiple degradation types. We present Cat-AIR, a novel \textbf{C}ontent \textbf{A}nd \textbf{T}ask-aware framework for \textbf{A}ll-in-one \textbf{I}mage \textbf{R}estoration. Cat-AIR incorporates an alternating spatial-channel attention mechanism that adaptively balances the local and global information for different tasks. Specifically, we introduce cross-layer channel attentions and cross-feature spatial attentions that allocate computations based on content and task complexity. Furthermore, we propose a smooth learning strategy that allows for seamless adaptation to new restoration tasks while maintaining performance on existing ones. Extensive experiments demonstrate that Cat-AIR achieves state-of-the-art results across a wide range of restoration tasks, requiring fewer FLOPs than previous methods, establishing new benchmarks for efficient all-in-one image restoration.

Paper number 41:
Title: PathoHR: Breast Cancer Survival Prediction on High-Resolution Pathological Images
Authors: Yang Luo, Shiru Wang, Jun Liu, Jiaxuan Xiao, Rundong Xue, Zeyu Zhang, Hao Zhang, Yu Lu, Yang Zhao, Yutong Xie
Abstract: Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumor's aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at this https URL.

Paper number 42:
Title: Rejuvenating IRS: AoI-based Low Overhead Reconfiguration Design
Authors: Jorge Torres Gómez, Joana Angjo, Moritz Garkisch, Vahid Jamali, Robert Schober, Falko Dressler
Abstract: Intelligent reflective surface (IRS) technologies help mitigate undesirable effects in wireless links by steering the communication signal between transmitters and receivers. IRS elements are configured to adjust the phase of the reflected signal for a user's location and enhance the perceived signal-to-noise ratio (SNR). In this way, an IRS improves the communication link but inevitably introduces more communication overhead. This occurs especially in mobile scenarios, where the user's position must be frequently estimated to re-adjust the IRS elements periodically. Such an operation requires balancing the amount of training versus the data time slots to optimize the communication performance in the link. Aiming to study this balance with the age of information (AoI) framework, we address the question of how often an IRS needs to be updated with the lowest possible overhead and the maximum of freshness of information. We derive the corresponding analytical solution for a mobile scenario, where the transmitter is static and the mobile user (MU) follows a random waypoint mobility model. We provide a closed-form expression for the average peak age of information (PAoI), as a metric to evaluate the impact of the IRS update frequency. As for the performance evaluation, we consider a realistic scenario following the IEEE 802.11ad standard, targeting the mmWave band. Our results reveal that the minimum achievable average PAoI is in the microsecond range and the optimal IRS update period is in the seconds range, causing 9% overhead in the link when the MU moves at a velocity of 1 m/s.

Paper number 43:
Title: A State-of-the-Art Review on Acoustic Preservation of Historical Worship Spaces through Auralization
Authors: Hannes Rosseel, Toon van Waterschoot
Abstract: Historical Worship Spaces (HWS) are significant architectural landmarks which hold both cultural and spiritual value. The acoustic properties of these spaces play a crucial role in historical and contemporary religious liturgies, rituals, and ceremonies, as well as in the performance of sacred music. However, the original acoustic characteristics of these spaces are often at risk due to repurposing, renovations, natural disasters, or deterioration over time. This paper presents a comprehensive review of the current state of research on the acquisition, analysis, and synthesis of acoustics, with a focus on HWS. An example case study of the Nassau chapel in Brussels, Belgium, is presented to demonstrate the application of these techniques for the preservation and auralization of historical worship space acoustics. The paper concludes with a discussion of the challenges and opportunities in the field, and outlines future research directions.

Paper number 44:
Title: Multiple-Particle Autofocusing Algorithm Using Axial Resolution and Morphological Analyses Based on Digital Holography
Authors: Wei-Na Li, Yi Zhou, Jiatai Chen, Hongjie Ou, XiangSheng Xie
Abstract: We propose an autofocusing algorithm to obtain, relatively accurately, the 3D position of each particle, particularly its axial location, and particle number of a dense transparent particle solution via its hologram. First, morphological analyses and constrained intensity are used on raw reconstructed images to obtain information on candidate focused particles. Second, axial resolution is used to obtain the real focused particles. Based on the mean intensity and equivalent diameter of each candidate focused particle, all focused particles are eventually secured. Our proposed method can rapidly provide relatively accurate ground-truth axial positions to solve the autofocusing problem that occurs with dense particles.

Paper number 45:
Title: FS-SS: Few-Shot Learning for Fast and Accurate Spike Sorting of High-channel Count Probes
Authors: Tao Fang, Majid Zamani
Abstract: There is a need for fast adaptation in spike sorting algorithms to implement brain-machine interface (BMIs) in different applications. Learning and adapting the functionality of the sorting process in real-time can significantly improve the performance. However, deep neural networks (DNNs) depend on large amounts of data for training models and their performance sustainability decreases when data is limited. Inspired by meta-learning, this paper proposes a few-shot spike sorting framework (FS-SS) with variable network model size that requires minimal learning training and supervision. The framework is not only compatible with few-shot adaptations, but also it uses attention mechanism and dilated convolutional neural networks. This allows scaling the network parameters to learn the important features of spike signals and to quickly generalize the learning ability to new spike waveforms in recording channels after few observations. The FS-SS was evaluated by using freely accessible datasets, also compared with the other state-of-the-art algorithms. The average classification accuracy of the proposed method is 99.28%, which shows extreme robustness to background noise and similarity of the spike waveforms. When the number of training samples is reduced by 90%, the parameter scale is reduced by 68.2%, while the accuracy only decreased by 0.55%. The paper also visualizes the model's attention distribution under spike sorting tasks of different difficulty levels. The attention distribution results show that the proposed model has clear interpretability and high robustness.

Paper number 46:
Title: WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression
Authors: Yu Mao, Jun Wang, Nan Guan, Chun Jason Xue
Abstract: Whole-Slide Images (WSIs) have revolutionized medical analysis by presenting high-resolution images of the whole tissue slide. Despite avoiding the physical storage of the slides, WSIs require considerable data volume, which makes the storage and maintenance of WSI records costly and unsustainable. To this end, this work presents the first investigation of lossless compression of WSI images. Interestingly, we find that most existing compression methods fail to compress the WSI images effectively. Furthermore, our analysis reveals that the failure of existing compressors is mainly due to information irregularity in WSI images. To resolve this issue, we developed a simple yet effective lossless compressor called WISE, specifically designed for WSI images. WISE employs a hierarchical encoding strategy to extract effective bits, reducing the entropy of the image and then adopting a dictionary-based method to handle the irregular frequency patterns. Through extensive experiments, we show that WISE can effectively compress the gigapixel WSI images to 36 times on average and up to 136 times.

Paper number 47:
Title: GenMetaLoc: Learning to Learn Environment-Aware Fingerprint Generation for Sample Efficient Wireless Localization
Authors: Jun Gao, Feng Yin, Wenzhong Yan, Qinglei Kong, Lexi Xu, Shuguang Cui
Abstract: Existing fingerprinting-based localization methods often require extensive data collection and struggle to generalize to new environments. In contrast to previous environment-unknown MetaLoc, we propose GenMetaLoc in this paper, which first introduces meta-learning to enable the generation of dense fingerprint databases from an environment-aware perspective. In the model aspect, the learning-to-learn mechanism accelerates the fingerprint generation process by facilitating rapid adaptation to new environments with minimal data. Additionally, we incorporate 3D point cloud data from the first Fresnel zone between the transmitter and receiver, which describes the obstacles distribution in the environment and serves as a condition to guide the diffusion model in generating more accurate fingerprints. In the data processing aspect, unlike most studies that focus solely on channel state information (CSI) amplitude or phase, we present a comprehensive processing that addresses both, correcting errors from WiFi hardware limitations such as amplitude discrepancies and frequency offsets. For the data collection platform, we develop an uplink wireless localization system that leverages the sensing capabilities of existing commercial WiFi devices and mobile phones, thus reducing the need for additional deployment costs. Experimental results on real datasets show that our framework outperforms baseline methods.

Paper number 48:
Title: A Two-Stage Rotation-Based Super-Resolution Signature Estimation for Spatial Wideband Systems
Authors: Chandrashekhar Rai, Debarati Sen
Abstract: Spatial and temporal delays in a wireless multi-antenna system, paired with an orthogonal frequency division multiplexing (OFDM) waveform, can be utilized to estimate the Angle of Arrival (AoA) and Time of Arrival (ToA) of scatterers in the radio channel through spectral estimation techniques. However, in millimeter-wave (mmWave) and TeraHertz (THz) systems, the spatial delays across the aperture of massive array elements are comparable to the inverse of the signal bandwidth. As a result, these delays cannot be approximated solely by phase terms, necessitating consideration of the Spatial Wideband Effect (SWE). The SWE in the mmWave/THz system causes migration of the actual AoA-ToA coarse bins. Moreover, a finite grid measurement of complex sinusoidal signals of continuous frequencies results in spectral leakage whenever there is a grid mismatch. In this work, given the Discrete Fourier Transform's computational efficiency and broad practical applicability, we propose utilizing the inverse Discrete Fourier Transform (DFT) for the initial 2-D spectrum estimation of the channel response. Further, in this paper, we propose a two-stage efficient rotation-based algorithm for fine-tuned signature estimation of spatial wideband systems with uniform linear arrays. Specifically, we utilize the rotation-based method to identify the correct coarse bin in the first stage followed by 2D-rotation based fine-tuning around the corrected coarse bin in the second stage. The proposed technique in this work can be used for handling beam squint effect in different applications like near-filed communications, wideband Multiple Input Multiple Output (MIMO) radar, channel estimation in Extremely Large (XL)-MIMO for 6G and beyond systems etc. The effectiveness of our proposed algorithm over the existing narrowband super-resolution estimation algorithms is established numerically through computer simulations.

Paper number 49:
Title: Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging
Authors: Siwon Kim, Wooyung Yun, Jeongbin Oh, Soomok Lee
Abstract: Deep learning has emerged as the predominant solution for classifying medical images. We intend to apply these developments to the ultra-widefield (UWF) retinal imaging dataset. Since UWF images can accurately diagnose various retina diseases, it is very important to clas sify them accurately and prevent them with early treatment. However, processing images manually is time-consuming and labor-intensive, and there are two challenges to automating this process. First, high perfor mance usually requires high computational resources. Artificial intelli gence medical technology is better suited for places with limited medical resources, but using high-performance processing units in such environ ments is challenging. Second, the problem of the accuracy of colour fun dus photography (CFP) methods. In general, the UWF method provides more information for retinal diagnosis than the CFP method, but most of the research has been conducted based on the CFP method. Thus, we demonstrate that these problems can be efficiently addressed in low performance units using methods such as strategic data augmentation and model ensembles, which balance performance and computational re sources while utilizing UWF images.

Paper number 50:
Title: Joint State-Parameter Observer-Based Robust Control of a UAV for Heavy Load Transportation
Authors: Brenner S. Rego, Daniel N. Cardoso, Marco. H. Terra, Guilherme V. Raffo
Abstract: This paper proposes a joint state-parameter observer-based controller for trajectory tracking of an octocopter unmanned aerial vehicle (OUAV), for transportation of a heavy load with unknown mass and size. The multi-body dynamic model of the OUAV with a rigidly attached load is obtained, effectively considering the effects of the load parameters into the dynamics of the system. A robust nonlinear W-infinity control strategy is designed for optimal trajectory tracking of the OUAV, with information of the states and load parameters provided by a joint estimation unscented Kalman filter. The effectiveness of the proposed strategy is corroborated by numerical results.

Paper number 51:
Title: ZECO: ZeroFusion Guided 3D MRI Conditional Generation
Authors: Feiran Wang, Bin Duan, Jiachen Tao, Nikhil Sharma, Dawen Cai, Yan Yan
Abstract: Medical image segmentation is crucial for enhancing diagnostic accuracy and treatment planning in Magnetic Resonance Imaging (MRI). However, acquiring precise lesion masks for segmentation model training demands specialized expertise and significant time investment, leading to a small dataset scale in clinical practice. In this paper, we present ZECO, a ZeroFusion guided 3D MRI conditional generation framework that extracts, compresses, and generates high-fidelity MRI images with corresponding 3D segmentation masks to mitigate data scarcity. To effectively capture inter-slice relationships within volumes, we introduce a Spatial Transformation Module that encodes MRI images into a compact latent space for the diffusion process. Moving beyond unconditional generation, our novel ZeroFusion method progressively maps 3D masks to MRI images in latent space, enabling robust training on limited datasets while avoiding overfitting. ZECO outperforms state-of-the-art models in both quantitative and qualitative evaluations on Brain MRI datasets across various modalities, showcasing its exceptional capability in synthesizing high-quality MRI images conditioned on segmentation masks.

Paper number 52:
Title: Koopman-Nemytskii Operator: A Linear Representation of Nonlinear Controlled Systems
Authors: Wentao Tang
Abstract: While Koopman operator lifts a nonlinear system into an infinite-dimensional function space and represents it as a linear dynamics, its definition is restricted to autonomous systems, i.e., does not incorporate inputs or disturbances. To the end of designing state-feedback controllers, the existing extensions of Koopman operator, which only account for the effect of open-loop values of inputs, does not involve feedback laws on closed-loop systems. Hence, in order to generically represent any nonlinear controlled dynamics linearly, this paper proposes a Koopman-Nemytskii operator, defined as a linear mapping from a product reproducing kernel Hilbert space (RKHS) of states and feedback laws to an RKHS of states. Using the equivalence between RKHS and Sobolev-Hilbert spaces under certain regularity conditions on the dynamics and kernel selection, this operator is well-defined. Its data-based approximation, which follows a kernel extended dynamic mode decomposition (kernel EDMD) approach, have established errors in single-step and multi-step state predictions as well as accumulated cost under control.

Paper number 53:
Title: Transient synchronization stability analysis and assessment of DFIG system under severe faults
Authors: Hongsheng Xu, Meng Zhan
Abstract: In the transient stability analysis of renewable energy grid-tied systems, although a large amount of works have devoted to the detailed electromagnetic transient simulation and the stability analyses of during-fault stage, the whole low-voltage ride through (LVRT) process and relevant transient stability mechanism remain to be uncovered. Taking the doubly fed induction generator system as the objective, this paper divides the transient processes into four different stages, including the pre-fault, during-fault, early post-fault, and late post-fault ones, establishes the full mechanism models for each stage, and studies the switching dynamics in detail. It is found that the during-fault dynamics can be determined by the phase-lock loop second-order equation within the framework of the generalized swing equation (GSE). For the early post-fault stage, it can be treated as a series of quasi-steady states and its dominant driving system dynamics can still be described by the GSE. Based on the local dynamics of unstable equilibrium point, the system transient stability can be completely determined by whether the initial state of the early post-fault stage is within or out of its basin of attraction (BOA). Based on these observations, the BOA-based and equal area criterion (EAC)-based transient stability assessment methods are developed, which are supported by broad numerical simulations and hardware-in-the-loop experiments. This work provides a clear physical picture and perfectly solves the difficult stability analysis problem when severe faults and LVRT have to be considered in most of DFIG engineering situations.

Paper number 54:
Title: Optimized Contact Plan Design for Reflector and Phased Array Terminals in Cislunar Space Networks
Authors: Huan Yan, Juan A. Fraire, Ziqi Yang, Kanglian Zhao, Wenfeng Li, Yuan Fang, Jinjun Zheng, Chengbin Kang, Huichao Zhou, Xinuo Chang, Lu Wang, Linshan Xue
Abstract: Cislunar space is emerging as a critical domain for human exploration, requiring robust infrastructure to support spatial users - spacecraft with navigation and communication demands. Deploying satellites at Earth-Moon libration points offers an effective solution. This paper introduces a novel Contact Plan Design (CPD) scheme that considers two classes of cislunar transponders: Reflector Links (RL) for high-volume data transfer and Phased Array Links (PL) for fast switching and navigation this http URL approach addresses the needs of both satellites and spatial users within the Earth-Moon Libration Point Communication and Navigation Constellation (EMLP-CNC). Simulations validate the proposed scheme, demonstrating its effectiveness in serving spatial users while meeting satellite ranging and communication requirements. These findings provide essential insights for developing future Cislunar Space Infrastructures.

Paper number 55:
Title: MCE-based Direct FTC Method for Dynamic Positioning of Underwater Vehicles with Thruster Redundancy
Authors: Ji-Hong Li
Abstract: This paper presents an active model-based FTC (fault-tolerant control) method for the dynamic positioning of a class of underwater vehicles with thruster redundancy. Compared to the widely used state and parameter estimation methods, this proposed scheme directly utilizes the vehicle's motion control error (MCE) to construct a residual for detecting thruster faults and failures in the steady state of the control system. In the case of thruster fault identification, the most difficult aspect is that the actual control input with thruster faults is unknown. However, through a detailed and precise analyses of MCE variation trends in the case of thruster faults, highly useful information about this unknown control input can be extracted. This characteristic also serves as the foundation for the novel scheme proposed in this paper. As for control reconfiguration, it is straightforward since the thrust losses can be directly estimated as a result of the identification process. Numerical studies with the real world vehicle model are also carried out to demonstrate the effectiveness of the proposed method.

Paper number 56:
Title: Contact Plan Design for Cross-Linked GNSSs: An ILP Approach for Extended Applications
Authors: Huan Yan, Juan A. Fraire, Ziqi Yang, Kanglian Zhao, Wenfeng Li, Xiyun Hou, Haohan Li, Yuxuan Miao, Jinjun Zheng, Chengbin Kang, Huichao Zhou, Xinuo Chang, Lu Wang
Abstract: Global Navigation Satellite Systems (GNSS) employ inter-satellite links (ISLs) to reduce dependency on ground stations, enabling precise ranging and communication across satellites. Beyond their traditional role, ISLs can support extended applications, including providing navigation and communication services to external entities. However, designing effective contact plan design (CPD) schemes for these multifaceted ISLs, operating under a polling time-division duplex (PTDD) framework, remains a critical challenge. Existing CPD approaches focus solely on meeting GNSS satellites' internal ranging and communication demands, neglecting their extended applications. This paper introduces the first CPD scheme capable of supporting extended GNSS ISLs. By modeling GNSS requirements and designing a tailored service process, our approach ensures the allocation of essential resources for internal operations while accommodating external user demands. Based on the BeiDou constellation, simulation results demonstrate the proposed scheme's efficacy in maintaining core GNSS functionality while providing extended ISLs on a best-effort basis. Additionally, the results highlight the significant impact of GNSS ISLs in enhancing orbit determination and clock synchronization for the Earth-Moon libration point constellation, underscoring the importance of extended GNSS ISL applications.

Paper number 57:
Title: Dominant Groups and Asymmetric Polarization in Generalized Quasi-Structurally Balanced Networks
Authors: Vishnudatta Thota, Swati Priya, Twinkle Tripathy
Abstract: The paper focuses on the phenomenon of asymmetric polarization arising in the presence of a dominant group in the network. The existing works in the literature analyze polarization primarily in structurally and quasi-structurally balanced networks. In this work, we introduce generalized quasi-structurally balanced (GQSB) networks, which include both of these networks as special cases. In the presence of a dominant group, a GQSB network has a unique bipartition: the dominant group (and its allies) and the remaining agents. The dominant group's superior influence results in an asymmetry in how the inter-subset antagonistic interactions are perceived by both of the subsets. This, in turn, leads to asymmetry in the final polarized opinions. To model this behavior, we propose a generalized Laplacian flow for undirected GQSB networks with a dominant group and establish necessary and sufficient conditions for achieving asymmetric polarization. The theoretical results presented in this paper are validated through numerical simulations on the Highland Tribes real-world dataset.

Paper number 58:
Title: Novel Constructions of 2-D Golay Complementary Array Sets of Flexible Sizes
Authors: Zhaoyu Zhang, Xiaoyu Chen, Yuqiong Du, Luyi Zheng
Abstract: One-dimensional (1-D) Golay complementary sets(GCSs) possess numerous well-known properties and have achieved extensive use in communication engineering. The concept of 1-D GCSs can be extended to two-dimensional (2-D) Golay complementary array sets(GCASs). The letter proposes two constructions of 2-D GCASs based on two-dimensional extended generalized Boolean functions, where the set sizes and matrix parameters are flexible. The set size of GCASs with an array size of b^m b^n in the letter can reach N^m+n+1, which has not been cited in the existing literature. Both structures are assembled directly and are not restricted by the parameters of other existing sequences, thus expanding the theoretical and applicative boundaries of Golay complementary array sets.

Paper number 59:
Title: Constraint Horizon in Model Predictive Control
Authors: Allan Andre Do Nascimento, Han Wang, Antonis Papachristodoulou, Kostas Margellos
Abstract: In this work, we propose a Model Predictive Control (MPC) formulation incorporating two distinct horizons: a prediction horizon and a constraint horizon. This approach enables a deeper understanding of how constraints influence key system properties such as suboptimality, without compromising recursive feasibility and constraint satisfaction. In this direction, our contributions are twofold. First, we provide a framework to estimate closed-loop optimality as a function of the number of enforced constraints. This is a generalization of existing results by considering partial constraint enforcement over the prediction horizon. Second, when adopting this general framework under the lens of safety-critical applications, our method improves conventional Control Barrier Function (CBF) based approaches. It mitigates myopic behaviour in Quadratic Programming (QP)-CBF schemes, and resolves compatibility issues between Control Lyapunov Function (CLF) and CBF constraints via the prediction horizon used in the optimization. We show the efficacy of the method via numerical simulations for a safety critical application.

Paper number 60:
Title: Movable Antenna Enabled ISAC: Tackling Slow Antenna Movement, Dynamic RCS, and Imperfect CSI via Two-timescale Optimizati
Authors: Ata Khalili, Robert Schober
Abstract: We investigate resource allocation for a movable antenna (MA) enabled integrated sensing and communication (ISAC) system scanning a sector for sensing and simultaneously serving multiple communication users using multiple variable-length snapshots. To tackle the critical challenges of slow antenna movement speed, dynamic radar cross section (RCS) variation, imperfect channel state information (CSI), and finite precision antenna positioning encountered in practice, we propose a novel two-timescale (TTS) optimization framework. In particular, we jointly optimize the discrete MA positions, the communication and sensing beamforming vectors, and the snapshot durations for minimization of the average transmit power at the base station (BS) while guaranteeing a minimum sensing and communication quality of service (QoS) and accounting for imperfect CSI. To overcome the slow antenna movement speed, the MA positions are adjusted only once per scanning period whereas the beamforming vectors and snapshot durations are adapted in every snapshot. Furthermore, to manage the impact of varying RCSs, a novel chance constraint for the sensing QoS is introduced. To solve the resulting challenging highly non-convex mixed integer non-linear program (MINLP), an efficient iterative algorithm exploiting alternative optimization (AO) is developed and shown to yield a high-quality suboptimal solution. Our simulation results reveal that the proposed MA enabled ISAC system cannot only significantly reduce the BS transmit power compared to systems relying on fixed-position antennas and antenna selection but also exhibits a remarkable robustness to RCS fluctuations and imperfect CSI. Furthermore, the proposed TTS framework achieves a similar performance as a system adjusting the MA positions in every snapshot, while the TTS approach significantly reduces the time used for MA adjustment.

Paper number 61:
Title: Unsupervised Variational Acoustic Clustering
Authors: Luan Vinícius Fiorio, Bruno Defraene, Johan David, Frans Widdershoven, Wim van Houtum, Ronald M. Aarts
Abstract: We propose an unsupervised variational acoustic clustering model for clustering audio data in the time-frequency domain. The model leverages variational inference, extended to an autoencoder framework, with a Gaussian mixture model as a prior for the latent space. Specifically designed for audio applications, we introduce a convolutional-recurrent variational autoencoder optimized for efficient time-frequency processing. Our experimental results considering a spoken digits dataset demonstrate a significant improvement in accuracy and clustering performance compared to traditional methods, showcasing the model's enhanced ability to capture complex audio patterns.

Paper number 62:
Title: Target Speaker Selection for Neural Network Beamforming in Multi-Speaker Scenarios
Authors: Luan Vinícius Fiorio, Bruno Defraene, Johan David, Alex Young, Frans Widdershoven, Wim van Houtum, Ronald M. Aarts
Abstract: We propose a speaker selection mechanism (SSM) for the training of an end-to-end beamforming neural network, based on recent findings that a listener usually looks to the target speaker with a certain undershot angle. The mechanism allows the neural network model to learn toward which speaker to focus, during training, in a multi-speaker scenario, based on the position of listener and speakers. However, only audio information is necessary during inference. We perform acoustic simulations demonstrating the feasibility and performance when the SSM is employed in training. The results show significant increase in speech intelligibility, quality, and distortion metrics when compared to the minimum variance distortionless filter and the same neural network model trained without SSM. The success of the proposed method is a significant step forward toward the solution of the cocktail party problem.

Paper number 63:
Title: Joint Spectrogram Separation and TDOA Estimation using Optimal Transport
Authors: Linda Fabiani, Sebastian J. Schlecht, Isabel Haasler, Filip Elvander
Abstract: Separating sources is a common challenge in applications such as speech enhancement and telecommunications, where distinguishing between overlapping sounds helps reduce interference and improve signal quality. Additionally, in multichannel systems, correct calibration and synchronization are essential to separate and locate source signals accurately. This work introduces a method for blind source separation and estimation of the Time Difference of Arrival (TDOA) of signals in the time-frequency domain. Our proposed method effectively separates signal mixtures into their original source spectrograms while simultaneously estimating the relative delays between receivers, using Optimal Transport (OT) theory. By exploiting the structure of the OT problem, we combine the separation and delay estimation processes into a unified framework, optimizing the system through a block coordinate descent algorithm. We analyze the performance of the OT-based estimator under various noise conditions and compare it with conventional TDOA and source separation methods. Numerical simulation results demonstrate that our proposed approach can achieve a significant level of accuracy across diverse noise scenarios for physical speech signals in both TDOA and source separation tasks.

Paper number 64:
Title: Matrix Pencil-Based Analysis of Multirate Simulation Schemes
Authors: Liya Huang, Georgios Tzounas
Abstract: This paper focuses on multirate time-domain simulations of power system models. It proposes a matrix pencil-based approach to evaluate the spurious numerical deformation introduced to power system dynamics by a given multirate integration scheme. Moreover, it considers the problem of multirate partitioning and discusses a strategy for allocating state and algebraic variables to fast and slow subsystems based on modal participation factors (PFs). The suitability and features of the proposed approach are illustrated through numerical simulations that assess the accuracy effects of interfacing as well as of various prediction and solution methods.

Paper number 65:
Title: RIS-Assisted Localization: A Novel Conditional Sample Mean Approach without CSI
Authors: Jiawei Yao, Yijie Mao, Mingzhe Chen
Abstract: Reconfigurable intelligent surface (RIS) has been recognized as a promising solution for enhancing localization accuracy. Traditional RIS-based localization methods typically rely on prior channel knowledge, beam scanning, and pilot-based assistance. These approaches often result in substantial energy and computational overhead, and require real-time coordination between the base station (BS) and the RIS. In this work, we propose a novel multiple RISs aided localization approach to address these challenges. The proposed method first estimates the angle-of-directions (AoDs) between the RISs and the user using the conditional sample mean approach, and then uses the estimated multiple AoD pairs to determine the user's position. This approach only requires measuring the received signal strength at the BS for a set of randomly generated phase shifts across all RISs, thereby eliminating the need for real-time RIS phase shift design or user-to-BS pilot transmissions. Numerical results show that the proposed localization approach improves localization accuracy while significantly reducing energy and signaling overhead compared to conventional methods.

Paper number 66:
Title: Maximum Likelihood Estimation Based Complex-Valued Robust Chinese Remainder Theorem and Its Fast Algorithm
Authors: Xiaoping Li, Shiyang Sun, Qunying Liao, Xiang-Gen Xia
Abstract: Recently, a multi-channel self-reset analog-to-digital converter (ADC) system with complex-valued moduli has been proposed. This system enables the recovery of high dynamic range complex-valued bandlimited signals at low sampling rates via the Chinese remainder theorem (CRT). In this paper, we investigate complex-valued CRT (C-CRT) with erroneous remainders, where the errors follow wrapped complex Gaussian distributions. Based on the existing real-valued CRT utilizing maximum likelihood estimation (MLE), we propose a fast MLE-based C-CRT (MLE C-CRT). The proposed algorithm requires only $2L$ searches to obtain the optimal estimate of the common remainder, where $L$ is the number of moduli. Once the common remainder is estimated, the complex number can be determined using the C-CRT. Furthermore, we obtain a necessary and sufficient condition for the fast MLE C-CRT to achieve robust estimation. Finally, we apply the proposed algorithm to ADCs. The results demonstrate that the proposed algorithm outperforms the existing methods.

Paper number 67:
Title: Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata Integration
Authors: Taejin Jeong, Joohyeok Kim, Jaehoon Joo, Yeonwoo Jung, Hyeonmin Kim, Seong Jae Hwang
Abstract: Glaucoma is an incurable ophthalmic disease that damages the optic nerve, leads to vision loss, and ranks among the leading causes of blindness worldwide. Diagnosing glaucoma typically involves fundus photography, optical coherence tomography (OCT), and visual field testing. However, the high cost of OCT often leads to reliance on fundus photography and visual field testing, both of which exhibit inherent inter-observer variability. This stems from glaucoma being a multifaceted disease that influenced by various factors. As a result, glaucoma diagnosis is highly subjective, emphasizing the necessity of calibration, which aligns predicted probabilities with actual disease likelihood. Proper calibration is essential to prevent overdiagnosis or misdiagnosis, which are critical concerns for high-risk diseases. Although AI has significantly improved diagnostic accuracy, overconfidence in models have worsen calibration performance. Recent study has begun focusing on calibration for glaucoma. Nevertheless, previous study has not fully considered glaucoma's systemic nature and the high subjectivity in its diagnostic process. To overcome these limitations, we propose V-ViT (Voting-based ViT), a novel framework that enhances calibration by incorporating disease-specific characteristics. V-ViT integrates binocular data and metadata, reflecting the multi-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC dropout-based Voting System to address high subjectivity. Our approach achieves state-of-the-art performance across all metrics, including accuracy, demonstrating that our proposed methods are effective in addressing calibration issues. We validate our method using a custom dataset including binocular data.

Paper number 68:
Title: EVOLVE: a Value-Added Services Platform for Electric Vehicle Charging Stations
Authors: Erick Silva, Tadeu Freitas, Rehana Yasmin, Ali Shoker, Paulo Esteves-Verissimo
Abstract: A notable challenge in Electric Vehicle (EV) charging is the time required to fully charge the battery, which can range from 15 minutes to 2-3 hours. This idle period, however, presents an opportunity to offer time-consuming or data-intensive services such as vehicular software updates. ISO 15118 referred to the concept of Value-Added Services (VAS) in the charging scenario, but it remained underexplored in the literature. Our paper addresses this gap by proposing \acronym, the first EV charger compute architecture that supports secure on-charger universal applications with upstream and downstream communication. The architecture covers the end-to-end hardware/software stack, including standard API for vehicles and IT infrastructure. We demonstrate the feasibility and advantages of \acronym by employing and evaluating three suggested value-added services: vehicular software updates, security information and event management (SIEM), and secure payments. The results demonstrate significant reductions in bandwidth utilization and latency, as well as high throughput, which supports this novel concept and suggests a promising business model for Electric Vehicle charging station operation.

Paper number 69:
Title: Clutter Tracking using Variational Message Passing
Authors: Anders Malthe Westerkam, Troels Pedersen
Abstract: We propose a message passing algorithm for tracking of clutter signals in MIMO radar. The method exploits basis expansion to linearise the signal model, to enable mean field approach for tracking the posterior distribution of the clutter as it evolves across time, as well as the mean and precision of the clutter map. The method shows good estimation accuracy in simulations for a scenario that adhere to the statistical model used for derivation as well as one that does not. The complexity of the method is linear in both the amount of parameters chosen and the amount of data under consideration.

Paper number 70:
Title: Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles
Authors: Der-Hau Lee
Abstract: A robust control strategy for autonomous vehicles can improve system stability, enhance riding comfort, and prevent driving accidents. This paper presents a novel interpolation tube-based constrained iterative linear quadratic regulator (itube-CILQR) algorithm for autonomous computer-vision-based vehicle lane-keeping. The goal of the algorithm is to enhance robustness during high-speed cornering on tight turns. The advantages of itube-CILQR over the standard tube-approach include reduced system conservatism and increased computational speed. Numerical and vision-based experiments were conducted to examine the feasibility of the proposed algorithm. The proposed itube-CILQR algorithm is better suited to vehicle lane-keeping than variational CILQR-based methods and model predictive control (MPC) approaches using a classical interior-point solver. Specifically, in evaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to generate a control signal to guide a self-driving vehicle; itube-MPC typically required a 4.67-times longer computation time to complete the same task. Moreover, the influence of conservatism on system behavior was investigated by exploring the interpolation variable trajectories derived from the proposed itube-CILQR algorithm during lane-keeping maneuvers.

Paper number 71:
Title: On the Optimality of Single-label and Multi-label Neural Network Decoders
Authors: Yunus Can Gültekin, Péter Scheepers, Yuncheng Yuan, Federico Corradi, Alex Alvarado
Abstract: We investigate the design of two neural network (NN) architectures recently proposed as decoders for forward error correction: the so-called single-label NN (SLNN) and multi-label NN (MLNN) decoders. These decoders have been reported to achieve near-optimal codeword- and bit-wise performance, respectively. Results in the literature show near-optimality for a variety of short codes. In this paper, we analytically prove that certain SLNN and MLNN architectures can, in fact, always realize optimal decoding, regardless of the code. These optimal architectures and their binary weights are shown to be defined by the codebook, i.e., no training or network optimization is required. Our proposed architectures are in fact not NNs, but a different way of implementing the maximum likelihood decoding rule. Optimal performance is numerically demonstrated for Hamming $(7,4)$, Polar $(16,8)$, and BCH $(31,21)$ codes. The results show that our optimal architectures are less complex than the SLNN and MLNN architectures proposed in the literature, which in fact only achieve near-optimal performance. Extension to longer codes is still hindered by the curse of dimensionality. Therefore, even though SLNN and MLNN can perform maximum likelihood decoding, such architectures cannot be used for medium and long codes.

Paper number 72:
Title: Channel modeling for 60 GHz fixed mmWave O2I and O2O uplink with angular misalignment
Authors: Nitisha Singh, Sahaj K. Jha, Aniruddha Chandra, Radek Zavorka, Petr Horky, Tomas Mikulasek, Jiri Blumenstein, Ales Prokes, Jaroslaw Wojtun, Jan M. Kelner, Cezary Ziolkowski
Abstract: In this letter, we examine the effect of misalignment angle on cluster-based power delay profile (PDP) modeling for a 60 GHz millimeter-wave uplink. The analysis uses real-world data, where fixed uplink scenarios are realized by placing the transmitter at ground level and the receiver at the building level. Both outdoor-to-indoor (O2I) and outdoor-to-outdoor (O2O) scenarios are studied. Using the misalignment angle and the scenario as inputs, we propose a statistical PDP simulation algorithm based on the Saleh-Valenzuela model. Different goodness-of-fit metrics reveal that our proposed algorithm is robust to both O2I and O2O scenarios and can approximate the PDPs fairly well, even in case of misalignment.

Paper number 73:
Title: Vehicle to vehicle path loss modeling at millimeter wave band for crossing cars
Authors: Anirban Ghosh, Aniruddha Chandra, Tomas Mikulasek, Ales Prokes, Jaroslaw Wojtun, Jan M. Kelner, Cezary Ziolkowski
Abstract: Fifth generation (5G) new radio is now offering sidelink capability, which allows direct vehicle-to-vehicle (V2V) communication. Millimeter wave (mmWave) enables low-latency mission-critical V2V communications, such as forward crash warning, between two vehicles crossing on a road without dividers. In this article, we present a measurement-based path loss (PL) model for V2V links operating at 59.6 GHz mmWave when two vehicles approach from opposite sides and cross each other. Our model outperforms other existing PL models and can reliably model both approaching and departing vehicle scenarios.

Paper number 74:
Title: Long-term channel analysis at 60 and 80 GHz for autonomous ground vehicles
Authors: Radek Zavorka, Tomas Mikulasek, Josef Vychodil, Jiri Blumenstein, Hussein Hammoud, Jaroslaw Wojtun, Aniruddha Chandra, Jan M. Kelner, Cezary Ziolkowski, Ales Prokes
Abstract: This paper presents a comprehensive measurement campaign aimed at evaluating indoor-to-indoor radio channels in dynamic scenarios, with a particular focus on applications such as autonomous ground vehicles (AGV). These scenarios are characterized by the height of the antennas, addressing the unique challenges of near-ground communication. Our study involves long-term measurements (20 minutes of continuous recording per measurement) of the channel impulse response (CIR) in the 60 GHz and 80 GHz frequency bands, each with a bandwidth of 2.048 GHz. We investigate the variations in channel characteristics, focusing on parameters such as root mean square (RMS) delay spread and the Rician factor.

Paper number 75:
Title: Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction
Authors: Yuxuan Zhang, Jinkui Hao, Bo Zhou
Abstract: Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its inherently long acquisition times reduce clinical efficiency and patient comfort. Recent advancements in deep learning, particularly diffusion models, have improved accelerated MRI reconstruction. However, existing diffusion models' training often relies on fully sampled data, models incur high computational costs, and often lack uncertainty estimation, limiting their clinical applicability. To overcome these challenges, we propose a novel framework, called Dual-domain Multi-path Self-supervised Diffusion Model (DMSM), that integrates a self-supervised dual-domain diffusion model training scheme, a lightweight hybrid attention network for the reconstruction diffusion model, and a multi-path inference strategy, to enhance reconstruction accuracy, efficiency, and explainability. Unlike traditional diffusion-based models, DMSM eliminates the dependency on training from fully sampled data, making it more practical for real-world clinical settings. We evaluated DMSM on two human MRI datasets, demonstrating that it achieves favorable performance over several supervised and self-supervised baselines, particularly in preserving fine anatomical structures and suppressing artifacts under high acceleration factors. Additionally, our model generates uncertainty maps that correlate reasonably well with reconstruction errors, offering valuable clinically interpretable guidance and potentially enhancing diagnostic confidence.

Paper number 76:
Title: Learning to segment anatomy and lesions from disparately labeled sources in brain MRI
Authors: Meva Himmetoglu, Ilja Ciernik, Ender Konukoglu
Abstract: Segmenting healthy tissue structures alongside lesions in brain Magnetic Resonance Images (MRI) remains a challenge for today's algorithms due to lesion-caused disruption of the anatomy and lack of jointly labeled training datasets, where both healthy tissues and lesions are labeled on the same images. In this paper, we propose a method that is robust to lesion-caused disruptions and can be trained from disparately labeled training sets, i.e., without requiring jointly labeled samples, to automatically segment both. In contrast to prior work, we decouple healthy tissue and lesion segmentation in two paths to leverage multi-sequence acquisitions and merge information with an attention mechanism. During inference, an image-specific adaptation reduces adverse influences of lesion regions on healthy tissue predictions. During training, the adaptation is taken into account through meta-learning and co-training is used to learn from disparately labeled training images. Our model shows an improved performance on several anatomical structures and lesions on a publicly available brain glioblastoma dataset compared to the state-of-the-art segmentation methods.

Paper number 77:
Title: Choose Wisely: Data-Enabled Predictive Control for Nonlinear Systems Using Online Data Selection
Authors: Joshua Näf, Keith Moffat, Jaap Eising, Florian Dörfler
Abstract: This paper proposes Select-Data-Enabled Predictive Control (Select-DeePC), a new method for controlling nonlinear systems using output-feedback for which data are available but an explicit model is not. At each timestep, Select-DeePC employs only the most relevant data to implicitly linearize the dynamics in ``trajectory space.'' Then, taking user-defined output constraints into account, it makes control decisions using a convex optimization. This optimal control is applied in a receding-horizon manner. As the online data-selection is the core of Select-DeePC, we propose and verify both norm-based and manifold-embedding-based selection methods. We evaluate Select-DeePC on three benchmark nonlinear system simulators -- rocket-landing, a robotic arm and cart-pole inverted pendulum swing-up -- comparing them with standard DeePC and Time-Windowed DeePC, and find that Select-DeePC outperforms both methods.

Paper number 78:
Title: Zak-OTFS for Identification of Linear Time-Varying Systems
Authors: Danish Nisar, Saif Khan Mohammed, Ronny Hadani, Ananthanarayanan Chockalingam, Robert Calderbank
Abstract: Linear time-varying (LTV) systems model radar scenes where each reflector/target applies a delay, Doppler shift and complex amplitude scaling to a transmitted waveform. The receiver processes the received signal using the transmitted signal as a reference. The self-ambiguity function of the transmitted signal captures the cross-correlation of delay and Doppler shifts of the transmitted waveform. It acts as a blur that limits resolution, at the receiver, of the delay and Doppler shifts of targets in close proximity. This paper considers resolution of multiple targets and compares performance of traditional chirp waveforms with the Zak-OTFS waveform. The self-ambiguity function of a chirp is a line in the delay-Doppler domain, whereas the self-ambiguity function of the Zak-OTFS waveform is a lattice. The advantage of lattices over lines is better localization, and we show lattices provide superior noise-free estimation of the range and velocity of multiple targets. When the delay spread of the radar scene is less than the delay period of the Zak-OTFS modulation, and the Doppler spread is less than the Doppler period, we describe how to localize targets by calculating cross-ambiguities in the delay-Doppler domain. We show that the signal processing complexity of our approach is superior to the traditional approach of computing cross-ambiguities in the continuous time / frequency domain.

Paper number 79:
Title: Inertial-Based LQG Control: A New Look at Inverted Pendulum Stabilization
Authors: Daniel Engelsman, Itzik Klein
Abstract: Linear quadratic Gaussian (LQG) control is a well-established method for optimal control through state estimation, particularly in stabilizing an inverted pendulum on a cart. In standard laboratory setups, sensor redundancy enables direct measurement of configuration variables using displacement sensors and rotary encoders. However, in outdoor environments, dynamically stable mobile platforms-such as Segways, hoverboards, and bipedal robots-often have limited sensor availability, restricting state estimation primarily to attitude stabilization. Since the tilt angle cannot be directly measured, it is typically estimated through sensor fusion, increasing reliance on inertial sensors and necessitating a lightweight, self-contained perception module. Prior research has not incorporated accelerometer data into the LQG framework for stabilizing pendulum-like systems, as jerk states are not explicitly modeled in the Newton-Euler formalism. In this paper, we address this gap by leveraging local differential flatness to incorporate higher-order dynamics into the system model. This refinement enhances state estimation, enabling a more robust LQG controller that predicts accelerations for dynamically stable mobile platforms.

Paper number 80:
Title: Autonomous Exploration-Based Precise Mapping for Mobile Robots through Stepwise and Consistent Motions
Authors: Muhua Zhang, Lei Ma, Ying Wu, Kai Shen, Yongkui Sun, Henry Leung
Abstract: This paper presents an autonomous exploration framework. It is designed for indoor ground mobile robots that utilize laser Simultaneous Localization and Mapping (SLAM), ensuring process completeness and precise mapping results. For frontier search, the local-global sampling architecture based on multiple Rapidly Exploring Random Trees (RRTs) is employed. Traversability checks during RRT expansion and global RRT pruning upon map updates eliminate unreachable frontiers, reducing potential collisions and deadlocks. Adaptive sampling density adjustments, informed by obstacle distribution, enhance exploration coverage potential. For frontier point navigation, a stepwise consistent motion strategy is adopted, wherein the robot strictly drives straight on approximately equidistant line segments in the polyline path and rotates in place at segment junctions. This simplified, decoupled motion pattern improves scan-matching stability and mitigates map drift. For process control, the framework serializes frontier point selection and navigation, avoiding oscillation caused by frequent goal changes in conventional parallelized processes. The waypoint retracing mechanism is introduced to generate repeated observations, triggering loop closure detection and backend optimization in graph-based SLAM, thereby improving map consistency and precision. Experiments in both simulation and real-world scenarios validate the effectiveness of the framework. It achieves improved mapping coverage and precision in more challenging environments compared to baseline 2D exploration algorithms. It also shows robustness in supporting resource-constrained robot platforms and maintaining mapping consistency across various LiDAR field-of-view (FoV) configurations.

Paper number 81:
Title: High Efficiency Wiener Filter-based Point Cloud Quality Enhancement for MPEG G-PCC
Authors: Yuxuan Wei, Zehan Wang, Tian Guo, Hao Liu, Liquan Shen, Hui Yuan
Abstract: Point clouds, which directly record the geometry and attributes of scenes or objects by a large number of points, are widely used in various applications such as virtual reality and immersive communication. However, due to the huge data volume and unstructured geometry, efficient compression of point clouds is very crucial. The Moving Picture Expert Group is establishing a geometry-based point cloud compression (G-PCC) standard for both static and dynamic point clouds in recent years. Although lossy compression of G-PCC can achieve a very high compression ratio, the reconstruction quality is relatively low, especially at low bitrates. To mitigate this problem, we propose a high efficiency Wiener filter that can be integrated into the encoder and decoder pipeline of G-PCC to improve the reconstruction quality as well as the rate-distortion performance for dynamic point clouds. Specifically, we first propose a basic Wiener filter, and then improve it by introducing coefficients inheritance and variance-based point classification for the Luma component. Besides, to reduce the complexity of the nearest neighbor search during the application of the Wiener filter, we also propose a Morton code-based fast nearest neighbor search algorithm for efficient calculation of filter coefficients. Experimental results demonstrate that the proposed method can achieve average Bjøntegaard delta rates of -6.1%, -7.3%, and -8.0% for Luma, Chroma Cb, and Chroma Cr components, respectively, under the condition of lossless-geometry-lossy-attributes configuration compared to the latest G-PCC encoding platform (i.e., geometry-based solid content test model version 7.0 release candidate 2) by consuming affordable computational complexity.

Paper number 82:
Title: Optimization over Trained Neural Networks: Difference-of-Convex Algorithm and Application to Data Center Scheduling
Authors: Xinwei Liu, Vladimir Dvorkin
Abstract: When solving decision-making problems with mathematical optimization, some constraints or objectives may lack analytic expressions but can be approximated from data. When an approximation is made by neural networks, the underlying problem becomes optimization over trained neural networks. Despite recent improvements with cutting planes, relaxations, and heuristics, the problem remains difficult to solve in practice. We propose a new solution based on a bilinear problem reformulation that penalizes ReLU constraints in the objective function. This reformulation makes the problem amenable to efficient difference-of-convex algorithms (DCA), for which we propose a principled approach to penalty selection that facilitates convergence to stationary points of the original problem. We apply the DCA to the problem of the least-cost allocation of data center electricity demand in a power grid, reporting significant savings in congested cases.

Paper number 83:
Title: A Predictive Services Architecture for Efficient Airspace Operations
Authors: Ítalo Romani de Oliveira, Samet Ayhan, Glaucia Balvedi, Michael Biglin, Pablo Costas, Euclides C. Pinto Neto, Alexandre Leite, Felipe C. F. de Azevedo
Abstract: Predicting air traffic congestion and flow management is essential for airlines and Air Navigation Service Providers (ANSP) to enhance operational efficiency. Accurate estimates of future airport capacity and airspace density are vital for better airspace management, reducing air traffic controller workload and fuel consumption, ultimately promoting sustainable aviation. While existing literature has addressed these challenges, data management and query processing remain complex due to the vast volume of high-rate air traffic data. Many analytics use cases require a common pre-processing infrastructure, as ad-hoc approaches are insufficient. Additionally, linear prediction models often fall short, necessitating more advanced techniques. This paper presents a data processing and predictive services architecture that ingests large, uncorrelated, and noisy streaming data to forecast future airspace system states. The system continuously collects raw data, periodically compresses it, and stores it in NoSQL databases for efficient query processing. For prediction, the system learns from historical traffic by extracting key features such as airport arrival and departure events, sector boundary crossings, weather parameters, and other air traffic data. These features are input into various regression models, including linear, non-linear, and ensemble models, with the best-performing model selected for predictions. We evaluate this infrastructure across three prediction use cases in the US National Airspace System (NAS) and a segment of European airspace, using extensive real operations data, confirming that our system can predict future system states efficiently and accurately.

Paper number 84:
Title: Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion
Authors: Yu Sun, Yin Li, Ruixiao Sun, Chunhui Liu, Fangming Zhou, Ze Jin, Linjie Wang, Xiang Shen, Zhuolin Hao, Hongyu Xiong
Abstract: Transformer-based multimodal models are widely used in industrial-scale recommendation, search, and advertising systems for content understanding and relevance ranking. Enhancing labeled training data quality and cross-modal fusion significantly improves model performance, influencing key metrics such as quality view rates and ad revenue. High-quality annotations are crucial for advancing content modeling, yet traditional statistical-based active learning (AL) methods face limitations: they struggle to detect overconfident misclassifications and are less effective in distinguishing semantically similar items in deep neural networks. Additionally, audio information plays an increasing role, especially in short-video platforms, yet most pre-trained multimodal architectures primarily focus on text and images. While training from scratch across all three modalities is possible, it sacrifices the benefits of leveraging existing pre-trained visual-language (VL) and audio models. To address these challenges, we propose kNN-based Latent Space Broadening (LSB) to enhance AL efficiency and Vision-Language Modeling with Audio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL models. This system deployed in production systems, leading to significant business gains.

Paper number 85:
Title: A Relaxed Primal-Dual Hybrid Gradient Method with Line Search
Authors: Alex McManus, Stephen Becker, Nicholas Dwork
Abstract: The primal-dual hybrid gradient method (PDHG) is useful for optimization problems that commonly appear in image reconstruction. A downside of PDHG is that there are typically three user-set parameters and performance of the algorithm is sensitive to their values. Toward a parameter-free algorithm, we combine two existing line searches. The first, by Malitsky et al., is over two of the step sizes in the PDHG iterations. We then use the connection between PDHG and the primal-dual form of Douglas-Rachford splitting to construct a line search over the relaxation parameter. We demonstrate the efficacy of the combined line search on multiple problems, including a novel inverse problem in magnetic resonance image reconstruction. The method presented in this manuscript is the first parameter-free variant of PDHG (across all numerical experiments, there were no changes to line search hyperparameters).

Paper number 86:
Title: Extending First-order Motion Planners to Second-order Dynamics
Authors: Mayur Sawant, Abdelhamid Tayebi
Abstract: This paper extends first-order motion planners to robots governed by second-order dynamics. Two control schemes are proposed based on the knowledge of a scalar function whose negative gradient aligns with a given first-order motion planner. When such a function is known, the first-order motion planner is combined with a damping velocity vector with a dynamic gain to extend the safety and convergence guarantees of the first-order motion planner to second-order systems. If no such function is available, we propose an alternative control scheme ensuring that the error between the robot's velocity and the first-order motion planner converges to zero. The theoretical developments are supported by simulation results demonstrating the effectiveness of the proposed approaches.

Paper number 87:
Title: A Deep Learning Model for Estimating Vessel Size Distribution from MR Gradient Echo Sampling of the Free Induction Decay and Spin Echo Sequence Data
Authors: Natenael B. Semmineh, Indranil Guha, Deborah Healey, Anagha Chandrasekharan, Jerrold L. Boxerman, C. Chad Quarles
Abstract: Vascular remodelling is inherent to the pathogenesis of many diseases including cancer, neurodegeneration, fibrosis, hypertension, and diabetes. In this paper, a new susceptibility-contrast based MRI approach is established to non-invasively image intravoxel vessel size distribution (VSD), enabling a more comprehensive and quantitative assessment of vascular remodelling. The approach is founded on imaging vascular structures across a rodent brain using high-resolution, light-sheet fluorescence microscopy, simulating gradient echo sampling of free induction decay and spin echo (GESFIDE) MRI signals for the three-dimensional vascular networks, and training a deep learning model to predict cerebral blood volume (CBV) and VSD from GESFIDE signals. The results from ex vivo experiments demonstrated strong correlation (r = 0.96) between the true and predicted CBV. Also, high similarity between true and predicted VSDs was observed (mean Bhattacharya Coefficient = 0.92). With further in vivo validation, intravoxel VSD imaging could become a transformative preclinical and clinical tool for interrogating disease and treatment induced vascular remodelling.

Paper number 88:
Title: On the Hopf-Cole Transform for Control-affine Schrödinger Bridge
Authors: Alexis Teter, Abhishek Halder
Abstract: The purpose of this note is to clarify the importance of the relation $\boldsymbol{gg}^{\top}\propto \boldsymbol{\sigma\sigma}^{\top}$ in solving control-affine Schrödinger bridge problems via the Hopf-Cole transform, where $\boldsymbol{g},\boldsymbol{\sigma}$ are the control and noise coefficients, respectively. We show that the Hopf-Cole transform applied to the conditions of optimality for generic control-affine Schrödinger bridge problems, i.e., without the assumption $\boldsymbol{gg}^{\top}\propto\boldsymbol{\sigma\sigma}^{\top}$, gives a pair of forward-backward PDEs that are neither linear nor equation-level decoupled. We explain how the resulting PDEs can be interpreted as nonlinear forward-backward advection-diffusion-reaction equations, where the nonlinearity stem from additional drift and reaction terms involving the gradient of the log-likelihood a.k.a. the score. These additional drift and reaction vanish when $\boldsymbol{gg}^{\top}\propto\boldsymbol{\sigma\sigma}^{\top}$, and the resulting boundary-coupled system of linear PDEs can then be solved by dynamic Sinkhorn recursions. A key takeaway of our work is that the numerical solution of the generic control-affine Schrödinger bridge requires further algorithmic development, possibly generalizing the dynamic Sinkhorn recursion or otherwise.

Paper number 89:
Title: Quantized Analog Beamforming Enabled Multi-task Federated Learning Over-the-air
Authors: Jiacheng Yao, Wei Xu, Guangxu Zhu, Zhaohui Yang, Kaibin Huang, Dusit Niyato
Abstract: Over-the-air computation (AirComp) has recently emerged as a pivotal technique for communication-efficient federated learning (FL) in resource-constrained wireless networks. Though AirComp leverages the superposition property of multiple access channels for computation, it inherently limits its ability to manage inter-task interference in multi-task computing. In this paper, we propose a quantized analog beamforming scheme at the receiver to enable simultaneous multi-task FL. Specifically, inspiring by the favorable propagation and channel hardening properties of large-scale antenna arrays, a targeted analog beamforming method in closed form is proposed for statistical interference elimination. Analytical results reveal that the interference power vanishes by an order of $\mathcal{O}\left(1/N_r\right)$ with the number of analog phase shifters, $N_r$, irrespective of their quantization precision. Numerical results demonstrate the effectiveness of the proposed analog beamforming method and show that the performance upper bound of ideal learning without errors can be achieved by increasing the number of low-precision analog phase shifters.

Paper number 90:
Title: RAISE: Optimizing RIS Placement to Maximize Task Throughput in Multi-Server Vehicular Edge Computing
Authors: Yanan Ma, Zhengru Fang, Longzhi Yuan, Yiqin Deng, Xianhao Chen, Yuguang Fang
Abstract: Given the limited computing capabilities on autonomous vehicles, onboard processing of large volumes of latency-sensitive tasks presents significant challenges. While vehicular edge computing (VEC) has emerged as a solution, offloading data-intensive tasks to roadside servers or other vehicles is hindered by large obstacles like trucks/buses and the surge in service demands during rush hours. To address these challenges, Reconfigurable Intelligent Surface (RIS) can be leveraged to mitigate interference from ground signals and reach more edge servers by elevating RIS adaptively. To this end, we propose RAISE, an optimization framework for RIS placement in multi-server VEC systems. Specifically, RAISE optimizes RIS altitude and tilt angle together with the optimal task assignment to maximize task throughput under deadline constraints. To find a solution, a two-layer optimization approach is proposed, where the inner layer exploits the unimodularity of the task assignment problem to derive the efficient optimal strategy while the outer layer develops a near-optimal hill climbing (HC) algorithm for RIS placement with low complexity. Extensive experiments demonstrate that the proposed RAISE framework consistently outperforms existing benchmarks.

Paper number 91:
Title: Slide2Text: Leveraging LLMs for Personalized Textbook Generation from PowerPoint Presentations
Authors: Yizhou Zhou
Abstract: The rapid advancements in Large Language Models (LLMs) have revolutionized educational technology, enabling innovative approaches to automated and personalized content creation. This paper introduces Slide2Text, a system that leverages LLMs to transform PowerPoint presentations into customized textbooks. By extracting slide content using OCR, organizing it into a coherent structure, and generating tailored materials such as explanations, exercises, and references, Slide2Text streamlines the textbook creation process. Flexible customization options further enhance its adaptability to diverse educational needs. The system highlights the potential of LLMs in modernizing textbook creation and improving educational accessibility. Future developments will explore multimedia inputs and advanced user customization features.

Paper number 92:
Title: Tensor-based homogeneous polynomial dynamical system analysis from data
Authors: Xin Mao, Anqi Dong, Ziqin He, Yidan Mei, Shenghan Mei, Can Chen
Abstract: Numerous complex real-world systems, such as those in biological, ecological, and social networks, exhibit higher-order interactions that are often modeled using polynomial dynamical systems or homogeneous polynomial dynamical systems (HPDSs). However, identifying system parameters and analyzing key system-theoretic properties remain challenging due to their inherent nonlinearity and complexity, particularly for large-scale systems. To address these challenges, we develop an innovative computational framework in this article that leverages advanced tensor decomposition techniques, namely tensor train and hierarchical Tucker decompositions, to facilitate efficient identification and analysis of HPDSs that can be equivalently represented by tensors. Specifically, we introduce memory-efficient system identification techniques for directly estimating system parameters represented through tensor decompositions from time-series data. Additionally, we develop necessary and sufficient conditions for determining controllability and observability using the tensor decomposition-based representations of HPDSs, accompanied by detailed complexity analyses that demonstrate significant reductions in computational demands. The effectiveness and efficiency of our framework are validated through numerical examples.

Paper number 93:
Title: Enhancing Fourier Neural Operators with Local Spatial Features
Authors: Chaoyu Liu, Davide Murari, Chris Budd, Lihao Liu, Carola-Bibiane Schönlieb
Abstract: Partial Differential Equation (PDE) problems often exhibit strong local spatial structures, and effectively capturing these structures is critical for approximating their solutions. Recently, the Fourier Neural Operator (FNO) has emerged as an efficient approach for solving these PDE problems. By using parametrization in the frequency domain, FNOs can efficiently capture global patterns. However, this approach inherently overlooks the critical role of local spatial features, as frequency-domain parameterized convolutions primarily emphasize global interactions without encoding comprehensive localized spatial dependencies. Although several studies have attempted to address this limitation, their extracted Local Spatial Features (LSFs) remain insufficient, and computational efficiency is often compromised. To address this limitation, we introduce a convolutional neural network (CNN) preprocessor to extract LSFs directly from input data, resulting in a hybrid architecture termed \textit{Conv-FNO}. Furthermore, we introduce two novel resizing schemes to make our Conv-FNO resolution invariant. In this work, we focus on demonstrating the effectiveness of incorporating LSFs into FNOs by conducting both a theoretical analysis and extensive numerical experiments. Our findings show that this simple yet impactful modification enhances the representational capacity of FNOs and significantly improves performance on challenging PDE benchmarks.

Paper number 94:
Title: Elevating Robust Multi-Talker ASR by Decoupling Speaker Separation and Speech Recognition
Authors: Yufeng Yang, Hassan Taherian, Vahid Ahmadi Kalkhorani, DeLiang Wang
Abstract: Despite the tremendous success of automatic speech recognition (ASR) with the introduction of deep learning, its performance is still unsatisfactory in many real-world multi-talker scenarios. Speaker separation excels in separating individual talkers but, as a frontend, it introduces processing artifacts that degrade the ASR backend trained on clean speech. As a result, mainstream robust ASR systems train the backend on noisy speech to avoid processing artifacts. In this work, we propose to decouple the training of the speaker separation frontend and the ASR backend, with the latter trained on clean speech only. Our decoupled system achieves 5.1% word error rates (WER) on the Libri2Mix dev/test sets, significantly outperforming other multi-talker ASR baselines. Its effectiveness is also demonstrated with the state-of-the-art 7.60%/5.74% WERs on 1-ch and 6-ch SMS-WSJ. Furthermore, on recorded LibriCSS, we achieve the speaker-attributed WER of 2.92%. These state-of-the-art results suggest that decoupling speaker separation and recognition is an effective approach to elevate robust multi-talker ASR.

Paper number 95:
Title: Guided Diffusion for the Extension of Machine Vision to Human Visual Perception
Authors: Takahiro Shindo, Yui Tatsumi, Taiju Watanabe, Hiroshi Watanabe
Abstract: Image compression technology eliminates redundant information to enable efficient transmission and storage of images, serving both machine vision and human visual perception. For years, image coding focused on human perception has been well-studied, leading to the development of various image compression standards. On the other hand, with the rapid advancements in image recognition models, image compression for AI tasks, known as Image Coding for Machines (ICM), has gained significant importance. Therefore, scalable image coding techniques that address the needs of both machines and humans have become a key area of interest. Additionally, there is increasing demand for research applying the diffusion model, which can generate human-viewable images from a small amount of data to image compression methods for human vision. Image compression methods that use diffusion models can partially reconstruct the target image by guiding the generation process with a small amount of conditioning information. Inspired by the diffusion model's potential, we propose a method for extending machine vision to human visual perception using guided diffusion. Utilizing the diffusion model guided by the output of the ICM method, we generate images for human perception from random noise. Guided diffusion acts as a bridge between machine vision and human vision, enabling transitions between them without any additional bitrate overhead. The generated images then evaluated based on bitrate and image quality, and we compare their compression performance with other scalable image coding methods for humans and machines.

Paper number 96:
Title: Cache-Aware Cooperative Multicast Beamforming in Dynamic Satellite-Terrestrial Networks
Authors: Shuo Yuan, Yaohua Sun, Mugen Peng
Abstract: With the burgeoning demand for data-intensive services, satellite-terrestrial networks (STNs) face increasing backhaul link congestion, deteriorating user quality of service (QoS), and escalating power consumption. Cache-aided STNs are acknowledged as a promising paradigm for accelerating content delivery to users and alleviating the load of backhaul links. However, the dynamic nature of low earth orbit (LEO) satellites and the complex interference among satellite beams and terrestrial base stations pose challenges in effectively managing limited edge resources. To address these issues, this paper proposes a method for dynamically scheduling caching and communication resources, aiming to reduce network costs in terms of transmission power consumption and backhaul traffic, while meeting user QoS demands and resource constraints. We formulate a mixed timescale problem to jointly optimize cache placement, LEO satellite beam direction, and cooperative multicast beamforming among satellite beams and base stations. To tackle this intricate problem, we propose a two-stage solution framework, where the primary problem is decoupled into a short-term content delivery subproblem and a long-term cache placement subproblem. The former subproblem is solved by designing an alternating optimization approach with whale optimization and successive convex approximation methods according to the cache placement state, while cache content in STNs is updated using an iterative algorithm that utilizes historical information. Simulation results demonstrate the effectiveness of our proposed algorithms, showcasing their convergence and significantly reducing transmission power consumption and backhaul traffic by up to 52%.

Paper number 97:
Title: Real-World Remote Sensing Image Dehazing: Benchmark and Baseline
Authors: Zeng-Hui Zhu, Wei Lu, Si-Bao Chen, Chris H. Q. Ding, Jin Tang, Bin Luo
Abstract: Remote Sensing Image Dehazing (RSID) poses significant challenges in real-world scenarios due to the complex atmospheric conditions and severe color distortions that degrade image quality. The scarcity of real-world remote sensing hazy image pairs has compelled existing methods to rely primarily on synthetic datasets. However, these methods struggle with real-world applications due to the inherent domain gap between synthetic and real data. To address this, we introduce Real-World Remote Sensing Hazy Image Dataset (RRSHID), the first large-scale dataset featuring real-world hazy and dehazed image pairs across diverse atmospheric conditions. Based on this, we propose MCAF-Net, a novel framework tailored for real-world RSID. Its effectiveness arises from three innovative components: Multi-branch Feature Integration Block Aggregator (MFIBA), which enables robust feature extraction through cascaded integration blocks and parallel multi-branch processing; Color-Calibrated Self-Supervised Attention Module (CSAM), which mitigates complex color distortions via self-supervised learning and attention-guided refinement; and Multi-Scale Feature Adaptive Fusion Module (MFAFM), which integrates features effectively while preserving local details and global context. Extensive experiments validate that MCAF-Net demonstrates state-of-the-art performance in real-world RSID, while maintaining competitive performance on synthetic datasets. The introduction of RRSHID and MCAF-Net sets new benchmarks for real-world RSID research, advancing practical solutions for this complex task. The code and dataset are publicly available at \url{this https URL}.

Paper number 98:
Title: Geometric Constrained Non-Line-of-Sight Imaging
Authors: Xueying Liu, Lianfang Wang, Jun Liu, Yong Wang, Yuping Duan
Abstract: Normal reconstruction is crucial in non-line-of-sight (NLOS) imaging, as it provides key geometric and lighting information about hidden objects, which significantly improves reconstruction accuracy and scene understanding. However, jointly estimating normals and albedo expands the problem from matrix-valued functions to tensor-valued functions that substantially increasing complexity and computational difficulty. In this paper, we propose a novel joint albedo-surface reconstruction method, which utilizes the Frobenius norm of the shape operator to control the variation rate of the normal field. It is the first attempt to apply regularization methods to the reconstruction of surface normals for hidden objects. By improving the accuracy of the normal field, it enhances detail representation and achieves high-precision reconstruction of hidden object geometry. The proposed method demonstrates robustness and effectiveness on both synthetic and experimental datasets. On transient data captured within 15 seconds, our surface normal-regularized reconstruction model produces more accurate surfaces than recently proposed methods and is 30 times faster than the existing surface reconstruction approach.

Paper number 99:
Title: Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms
Authors: Nachuan Ma, Zhengfei Song, Qiang Hu, Chuang-Wei Liu, Yu Han, Yanting Zhang, Rui Fan, Lihua Xie
Abstract: In the emerging field of urban digital twins (UDTs), advancing intelligent road inspection (IRI) vehicles with automatic road crack detection systems is essential for maintaining civil infrastructure. Over the past decade, deep learning-based road crack detection methods have been developed to detect cracks more efficiently, accurately, and objectively, with the goal of replacing manual visual inspection. Nonetheless, there is a lack of systematic reviews on state-of-the-art (SoTA) deep learning techniques, especially data-fusion and label-efficient algorithms for this task. This paper thoroughly reviews the SoTA deep learning-based algorithms, including (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed for road crack detection. Also, we create a dataset called UDTIRI-Crack, comprising $2,500$ high-quality images from seven public annotated sources, as the first extensive online benchmark in this field. Comprehensive experiments are conducted to compare the detection performance, computational efficiency, and generalizability of public SoTA deep learning-based algorithms for road crack detection. In addition, the feasibility of foundation models and large language models (LLMs) for road crack detection is explored. Afterwards, the existing challenges and future development trends of deep learning-based road crack detection algorithms are discussed. We believe this review can serve as practical guidance for developing intelligent road detection vehicles with the next-generation road condition assessment systems. The released benchmark UDTIRI-Crack is available at this https URL.

Paper number 100:
Title: Channel Capacity Saturation Point and Beamforming Acceleration for Near-Field XL-MIMO Multiuser Communications
Authors: Xiangyu Cui, Ki-Hong Park, Mohamed-Slim Alouini
Abstract: One of the most important technologies in the fifth generation (5G) and the sixth generation (6G) is massive multiple input multiple outputs (MIMO) or extremely large-scale MIMO (XL-MIMO). With the evolving high-frequency technologies in millimeter band or tereHz band, the communication scene is changing into near-field rather than the conventional far-field scenario. In this letter, instead of advertising the XL-MIMO in the near-field, we appeal that a limit should be set on the size of the antenna array, beyond which the channel capacity will not show a significant increase. We show capacity saturation point can be analytically determined. Moreover, we propose a new beamforming algorithm that relieve the heavy computation due to the large antenna size even around the saturation point. Numerical results are provided to validate our analysis and show the performance of our newly proposed beamforming scheme.

Paper number 101:
Title: SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation
Authors: Hui Xue, Sarah M. Hooper, Iain Pierce, Rhodri H. Davies, John Stairs, Joseph Naegele, Adrienne E. Campbell-Washburn, Charlotte Manisty, James C. Moon, Thomas A. Treibel, Peter Kellman, Michael S. Hansen
Abstract: To develop and evaluate a new deep learning MR denoising method that leverages quantitative noise distribution information from the reconstruction process to improve denoising performance and generalization. This retrospective study trained 14 different transformer and convolutional models with two backbone architectures on a large dataset of 2,885,236 images from 96,605 cardiac retro-gated cine complex series acquired at 3T. The proposed training scheme, termed SNRAware, leverages knowledge of the MRI reconstruction process to improve denoising performance by simulating large, high quality, and diverse synthetic datasets, and providing quantitative information about the noise distribution to the model. In-distribution testing was performed on a hold-out dataset of 3000 samples with performance measured using PSNR and SSIM, with ablation comparison without the noise augmentation. Out-of-distribution tests were conducted on cardiac real-time cine, first-pass cardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model generalization across imaging sequences, dynamically changing contrast, different anatomies, and field strengths. The best model found in the in-distribution test generalized well to out-of-distribution samples, delivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion imaging, respectively. Further, a model trained with 100% cardiac cine data generalized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.

Paper number 102:
Title: Ordering and refining path-complete Lyapunov functions through composition lifts
Authors: Wouter Jongeneel, Raphaël M. Jungers
Abstract: A fruitful approach to study stability of switched systems is to look for multiple Lyapunov functions. However, in general, we do not yet understand the interplay between the desired stability certificate, the template of the Lyapunov functions and their mutual relationships to accommodate switching. In this work we elaborate on path-complete Lyapunov functions: a graphical framework that aims to elucidate this interplay. In particular, previously, several preorders were introduced to compare multiple Lyapunov functions. These preorders are initially algorithmically intractable due to the algebraic nature of Lyapunov inequalities, yet, lifting techniques were proposed to turn some preorders purely combinatorial and thereby eventually tractable. In this note we show that a conjecture in this area regarding the so-called composition lift, that was believed to be true, is false. This refutal, however, points us to a beneficial structural feature of the composition lift that we exploit to iteratively refine path-complete graphs, plus, it points us to a favourable adaptation of the composition lift.

Paper number 103:
Title: A Tutorial on Six-Dimensional Movable Antenna Enhanced Wireless Networks: Synergizing Positionable and Rotatable Antennas
Authors: Xiaodan Shao, Weidong Mei, Changsheng You, Qingqing Wu, Beixiong Zheng, Cheng-Xiang Wang, Junling Li, Rui Zhang, Robert Schober, Lipeng Zhu, Weihua Zhuang, Xuemin (Sherman)Shen
Abstract: Six-dimensional movable antenna (6DMA) is a new and revolutionary technique that fully exploits the wireless channel spatial variations at the transmitter/receiver by flexibly adjusting the three-dimensional (3D) positions and 3D rotations of antennas/antenna surfaces (sub-arrays), thereby improving the performance of wireless networks cost-effectively without the need to deploy additional antennas. It is thus expected that the integration of new 6DMAs into future sixth-generation (6G) wireless networks will fundamentally enhance antenna agility and adaptability, and introduce new degrees of freedom (DoFs) for system design. Despite its great potential, 6DMA faces new challenges to be efficiently implemented in wireless networks, including corresponding architectures, antenna position and rotation optimization, channel estimation, and system design from both communication and sensing perspectives. In this paper, we provide a tutorial on 6DMA-enhanced wireless networks to address the above issues by unveiling associated new channel models, hardware implementations and practical position/rotation constraints, as well as various appealing applications in wireless networks. Moreover, we discuss two special cases of 6DMA, namely, rotatable 6DMA with fixed antenna position and positionable 6DMA with fixed antenna rotation, and highlight their respective design challenges and applications. We further present prototypes developed for 6DMA-enhanced communication along with experimental results obtained with these prototypes. Finally, we outline promising directions for further investigation.

Paper number 104:
Title: NMPC-based Unified Posture Manipulation and Thrust Vectoring for Fault Recovery
Authors: Adarsh Salagame, Shashwat Pandya, Ioannis Mandralis, Eric Sihite, Alireza Ramezani, Morteza Gharib
Abstract: Multi-rotors face significant risks, as actuator failures at high altitudes can easily result in a crash and the robot's destruction. Therefore, rapid fault recovery in the event of an actuator failure is necessary for the fault-tolerant and safe operation of unmanned aerial robots. In this work, we present a fault recovery approach based on the unification of posture manipulation and thrust vectoring. The key contributions of this work are: 1) Derivation of two flight dynamics models (high-fidelity and reduced-order) that capture posture control and thrust vectoring. 2) Design of a controller based on Nonlinear Model Predictive Control (NMPC) and demonstration of fault recovery in simulation using a high-fidelity model of the Multi-Modal Mobility Morphobot (M4) in Simscape.

Paper number 105:
Title: Efficient Transformed Gaussian Process State-Space Models for Non-Stationary High-Dimensional Dynamical Systems
Authors: Zhidi Lin, Ying Li, Feng Yin, Juan Maroñas, Alexandre H. Thiéry
Abstract: Gaussian process state-space models (GPSSMs) have emerged as a powerful framework for modeling dynamical systems, offering interpretable uncertainty quantification and inherent regularization. However, existing GPSSMs face significant challenges in handling high-dimensional, non-stationary systems due to computational inefficiencies, limited scalability, and restrictive stationarity assumptions. In this paper, we propose an efficient transformed Gaussian process state-space model (ETGPSSM) to address these limitations. Our approach leverages a single shared Gaussian process (GP) combined with normalizing flows and Bayesian neural networks, enabling efficient modeling of complex, high-dimensional state transitions while preserving scalability. To address the lack of closed-form expressions for the implicit process in the transformed GP, we follow its generative process and introduce an efficient variational inference algorithm, aided by the ensemble Kalman filter (EnKF), to enable computationally tractable learning and inference. Extensive empirical evaluations on synthetic and real-world datasets demonstrate the superior performance of our ETGPSSM in system dynamics learning, high-dimensional state estimation, and time-series forecasting, outperforming existing GPSSMs and neural network-based methods in both accuracy and computational efficiency.

Paper number 106:
Title: Optimizing Influence Campaigns: Nudging under Bounded Confidence
Authors: Yen-Shao Chen, Tauhid Zaman
Abstract: Influence campaigns in online social networks are often run by organizations, political parties, and nation states to influence large audiences. These campaigns are employed through the use of agents in the network that share persuasive content. Yet, their impact might be minimal if the audiences remain unswayed, often due to the bounded confidence phenomenon, where only a narrow spectrum of viewpoints can influence them. Here we show that to persuade under bounded confidence, an agent must nudge its targets to gradually shift their opinions. Using a control theory approach, we show how to construct an agent's nudging policy under the bounded confidence opinion dynamics model and also how to select targets for multiple agents in an influence campaign on a social network. Simulations on real Twitter networks show that a multi-agent nudging policy can shift the mean opinion, decrease opinion polarization, or even increase it. We find that our nudging based policies outperform other common techniques that do not consider the bounded confidence effect. Finally, we show how to craft prompts for large language models, such as ChatGPT, to generate text-based content for real nudging policies. This illustrates the practical feasibility of our approach, allowing one to go from mathematical nudging policies to real social media content.

Paper number 107:
Title: Limited-angle SPECT image reconstruction using deep image prior
Authors: Kensuke Hori, Fumio Hashimoto, Kazuya Koyama, Takeyuki Hashimoto
Abstract: In SPECT image reconstruction, limited-angle (LA) conditions lead to a loss of frequency components, which distort the reconstructed tomographic image along directions corresponding to the non-collected projection angle range. Although conventional iterative image reconstruction methods have been used to improve the reconstructed images in LA conditions, the image quality is still unsuitable for clinical use. We propose a LA SPECT image reconstruction method that uses an end-to-end deep image prior (DIP) framework to improve reconstructed image quality. The proposed LA SPECT image reconstruction is an end-to-end DIP framework which incorporates a forward projection model into the loss function to optimise the neural network. By also incorporating a binary mask that indicates whether each data point in the measured projection data has been collected, the proposed method restores the non-collected projection data and reconstructs a less distorted image. The proposed method was evaluated using 20 numerical phantoms and clinical patient data. In numerical simulations, the proposed method outperformed existing back-projection-based methods in terms of PSNR and SSIM. We analysed the reconstructed tomographic images in the frequency domain using an object-specific modulation transfer function, in simulations and on clinical patient data, to evaluate the response of the reconstruction method to different frequencies of the object. The proposed method significantly improved the response to almost all spatial frequencies, even in the non-collected projection angle range. The results demonstrate that the proposed method reconstructs a less distorted tomographic image. The proposed end-to-end DIP-based reconstruction method restores lost frequency components and mitigates image distortion under LA conditions by incorporating a binary mask into the loss function.

Paper number 108:
Title: ALWNN Empowered Automatic Modulation Classification: Conquering Complexity and Scarce Sample Conditions
Authors: Yunhao Quan, Chuang Gao, Nan Cheng, Zhijie Zhang, Zhisheng Yin, Wenchao Xu, Danyang Wang
Abstract: In Automatic Modulation Classification (AMC), deep learning methods have shown remarkable performance, offering significant advantages over traditional approaches and demonstrating their vast potential. Nevertheless, notable drawbacks, particularly in their high demands for storage, computational resources, and large-scale labeled data, which limit their practical application in real-world scenarios. To tackle this issue, this paper innovatively proposes an automatic modulation classification model based on the Adaptive Lightweight Wavelet Neural Network (ALWNN) and the few-shot framework (MALWNN). The ALWNN model, by integrating the adaptive wavelet neural network and depth separable convolution, reduces the number of model parameters and computational complexity. The MALWNN framework, using ALWNN as an encoder and incorporating prototype network technology, decreases the model's dependence on the quantity of samples. Simulation results indicate that this model performs remarkably well on mainstream datasets. Moreover, in terms of Floating Point Operations Per Second (FLOPS) and Normalized Multiply - Accumulate Complexity (NMACC), ALWNN significantly reduces computational complexity compared to existing methods. This is further validated by real-world system tests on USRP and Raspberry Pi platforms. Experiments with MALWNN show its superior performance in few-shot learning scenarios compared to other algorithms.

Paper number 109:
Title: Finite-Time Bounds for Two-Time-Scale Stochastic Approximation with Arbitrary Norm Contractions and Markovian Noise
Authors: Siddharth Chandak, Shaan Ul Haque, Nicholas Bambos
Abstract: Two-time-scale Stochastic Approximation (SA) is an iterative algorithm with applications in reinforcement learning and optimization. Prior finite time analysis of such algorithms has focused on fixed point iterations with mappings contractive under Euclidean norm. Motivated by applications in reinforcement learning, we give the first mean square bound on non linear two-time-scale SA where the iterations have arbitrary norm contractive mappings and Markovian noise. We show that the mean square error decays at a rate of $O(1/n^{2/3})$ in the general case, and at a rate of $O(1/n)$ in a special case where the slower timescale is noiseless. Our analysis uses the generalized Moreau envelope to handle the arbitrary norm contractions and solutions of Poisson equation to deal with the Markovian noise. By analyzing the SSP Q-Learning algorithm, we give the first $O(1/n)$ bound for an algorithm for asynchronous control of MDPs under the average reward criterion. We also obtain a rate of $O(1/n)$ for Q-Learning with Polyak-averaging and provide an algorithm for learning Generalized Nash Equilibrium (GNE) for strongly monotone games which converges at a rate of $O(1/n^{2/3})$.

Paper number 110:
Title: 4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video
Authors: Qiang Hu, Zihan Zheng, Houqiang Zhong, Sihua Fu, Li Song, XiaoyunZhang, Guangtao Zhai, Yanfeng Wang
Abstract: 3D Gaussian Splatting (3DGS) has substantial potential for enabling photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number of Gaussians and their associated attributes poses significant challenges for storage and transmission. Existing methods typically handle dynamic 3DGS representation and compression separately, neglecting motion information and the rate-distortion (RD) trade-off during training, leading to performance degradation and increased model redundancy. To address this gap, we propose 4DGC, a novel rate-aware 4D Gaussian compression framework that significantly reduces storage size while maintaining superior RD performance for FVV. Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation that utilizes a compact motion grid combined with sparse compensated Gaussians to exploit inter-frame similarities. This representation effectively handles large motions, preserving quality and reducing temporal redundancy. Furthermore, we present an end-to-end compression scheme that employs differentiable quantization and a tiny implicit entropy model to compress the motion grid and compensated Gaussians efficiently. The entire framework is jointly optimized using a rate-distortion trade-off. Extensive experiments demonstrate that 4DGC supports variable bitrates and consistently outperforms existing methods in RD performance across multiple datasets.

Paper number 111:
Title: The On-Board Computer of the AcubeSAT Mission
Authors: Konstantinos Tsoupos, Stylianos Tzelepis, Georgios Sklavenitis, DimitriosStoupis, Grigorios Pavlakis, Panagiotis Bountzioukas, Christina Athanasiadou, Lily Ha, David Palma, Loris Franchi, Alkis Hatzopoulos
Abstract: AcubeSAT is an open-source CubeSat mission aiming to explore the effects of microgravity and radiation on eukaryotic cells using a compact microfluidic lab-on-a-chip platform. It is developed by SpaceDot, a volunteer, interdisciplinary student team at the Aristotle University of Thessaloniki and supported by the "Fly Your Satellite! 3" program of the European Space Agency (ESA) Education Office. The nanosatellite features an in-house designed on-board computer subsystem responsible for telecommand execution, telemetry fetching, onboard time synchronization, in-orbit patching, and fault recovery. The subsystem is designed on one PC/104 standard compatible Printed Circuit Board (PCB) that hosts the On-board Computer (OBC) on the one side and the Attitude and Orbit Control Subsystem (AOCS) on the other, and it is compatible with the LibreCube standard. The hosted subsystems are functionally isolated and feature an ARM Cortex-M7, radiation-tolerant microcontroller each. Before sending anything to space thorough testing is required and specifically the on-board computer board underwent vibration and thermal cycling tests to ensure nominal operation in all conditions. This paper aims to elucidate the decision-making process, design iterations, and development stages of the custom board and accompanying in-house software. Insights garnered from the initial partially successful environmental test campaign at the ESA CubeSat Support Facility will be shared, along with the ensuing preparations, results, and lessons learned from subsequent testing endeavors in April 2024. Furthermore, the current developmental status will be discussed alongside future electromagnetic compatibility testing, integration plan on a FlatSat, and prospects for the open-source design as a cost-effective, and modular solution that can be tailored with little effort for upcoming missions.

Paper number 112:
Title: Differentiable Simulator for Electrically Reconfigurable Electromagnetic Structures
Authors: Johannes Müller, Dennis Philipp, Matthias Günther
Abstract: This paper introduces a novel CUDA-enabled PyTorch-based framework designed for the gradient-based optimization of such reconfigurable electromagnetic structures with electrically tunable parameters. Traditional optimization techniques for these structures often rely on non-gradient-based methods, limiting efficiency and flexibility. Our framework leverages automatic differentiation, facilitating the application of gradient-based optimization methods. This approach is particularly advantageous for embedding within deep learning frameworks, enabling sophisticated optimization strategies. We demonstrate the framework's effectiveness through comprehensive simulations involving resonant structures with tunable parameters. Key contributions include the efficient solution of the inverse problem. The framework's performance is validated using three different resonant structures: a single-loop copper wire (Unit-Cell) as well as an 8x1 and an 8x8 array of resonant unit cells with multiple inductively coupled unit cells (1d and 2d Metasurfaces). Results show precise in-silico control over the magnetic field's component normal to the surface of each resonant structure, achieving desired field strengths with minimal error. The proposed framework is compatible with existing simulation software. This PyTorch-based framework sets the stage for advanced electromagnetic control strategies for resonant structures with application in e.g. MRI, providing a robust platform for further exploration and innovation in the design and optimization of resonant electromagnetic structures.

Paper number 113:
Title: Music Similarity Representation Learning Focusing on Individual Instruments with Source Separation and Human Preference
Authors: Takehiro Imamura, Yuka Hashizume, Wen-Chin Huang, Tomoki Toda
Abstract: This paper proposes music similarity representation learning (MSRL) based on individual instrument sounds (InMSRL) utilizing music source separation (MSS) and human preference without requiring clean instrument sounds during inference. We propose three methods that effectively improve performance. First, we introduce end-to-end fine-tuning (E2E-FT) for the Cascade approach that sequentially performs MSS and music similarity feature extraction. E2E-FT allows the model to minimize the adverse effects of a separation error on the feature extraction. Second, we propose multi-task learning for the Direct approach that directly extracts disentangled music similarity features using a single music similarity feature extractor. Multi-task learning, which is based on the disentangled music similarity feature extraction and MSS based on reconstruction with disentangled music similarity features, further enhances instrument feature disentanglement. Third, we employ perception-aware fine-tuning (PAFT). PAFT utilizes human preference, allowing the model to perform InMSRL aligned with human perceptual similarity. We conduct experimental evaluations and demonstrate that 1) E2E-FT for Cascade significantly improves InMSRL performance, 2) the multi-task learning for Direct is also helpful to improve disentanglement performance in the feature extraction, 3) PAFT significantly enhances the perceptual InMSRL performance, and 4) Cascade with E2E-FT and PAFT outperforms Direct with the multi-task learning and PAFT.

Paper number 114:
Title: Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and Power Interference in Multi-User OSaaS Networks
Authors: Agastya Raj, Devika Dass, Daniel C. Kilper, Marco Ruffini
Abstract: We present a framework to identify and mitigate rogue OOK signals and user-generated power interference in a multi-user Optical-Spectrum-as-a-Service network. Experimental tests on the OpenIreland-testbed achieve up to 89% detection rate within 10 seconds of an interference event.

Paper number 115:
Title: Learning a Class of Mixed Linear Regressions: Global Convergence under General Data Conditions
Authors: Yujing Liu, Zhixin Liu, Lei Guo
Abstract: Mixed linear regression (MLR) has attracted increasing attention because of its great theoretical and practical importance in capturing nonlinear relationships by utilizing a mixture of linear regression sub-models. Although considerable efforts have been devoted to the learning problem of such systems, i.e., estimating data labels and identifying model parameters, most existing investigations employ the offline algorithm, impose the strict independent and identically distributed (i.i.d.) or persistent excitation (PE) conditions on the regressor data, and provide local convergence results only. In this paper, we investigate the recursive estimation and data clustering problems for a class of stochastic MLRs with two components. To address this inherently nonconvex optimization problem, we propose a novel two-step recursive identification algorithm to estimate the true parameters, where the direction vector and the scaling coefficient of the unknown parameters are estimated by the least squares and the expectation-maximization (EM) principles, respectively. Under a general data condition, which is much weaker than the traditional i.i.d. and PE conditions, we establish the global convergence and the convergence rate of the proposed identification algorithm for the first time. Furthermore, we prove that, without any excitation condition on the regressor data, the data clustering performance including the cumulative mis-classification error and the within-cluster error can be optimal asymptotically. Finally, we provide a numerical example to illustrate the performance of the proposed learning algorithm.

Paper number 116:
Title: Uncertainty-guided Perturbation for Image Super-Resolution Diffusion Model
Authors: Leheng Zhang, Weiyi You, Kexuan Shi, Shuhang Gu
Abstract: Diffusion-based image super-resolution methods have demonstrated significant advantages over GAN-based approaches, particularly in terms of perceptual quality. Building upon a lengthy Markov chain, diffusion-based methods possess remarkable modeling capacity, enabling them to achieve outstanding performance in real-world scenarios. Unlike previous methods that focus on modifying the noise schedule or sampling process to enhance performance, our approach emphasizes the improved utilization of LR information. We find that different regions of the LR image can be viewed as corresponding to different timesteps in a diffusion process, where flat areas are closer to the target HR distribution but edge and texture regions are farther away. In these flat areas, applying a slight noise is more advantageous for the reconstruction. We associate this characteristic with uncertainty and propose to apply uncertainty estimate to guide region-specific noise level control, a technique we refer to as Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e., flat regions) receive reduced noise to preserve more LR information, therefore improving performance. Furthermore, we modify the network architecture of previous methods to develop our Uncertainty-guided Perturbation Super-Resolution (UPSR) model. Extensive experimental results demonstrate that, despite reduced model size and training overhead, the proposed UWSR method outperforms current state-of-the-art methods across various datasets, both quantitatively and qualitatively.

Paper number 117:
Title: Feasibility of multiple robust control barrier functions for bounding box constraints
Authors: Mark Spiller, Emilia Isbono, Philipp Schitz
Abstract: Enforcing multiple constraints based on the concept of control barrier functions (CBFs) is a remaining challenge because each of the CBFs requires a condition on the control inputs to be satisfied which may easily lead to infeasibility problems. The problem becomes even more challenging with input constraints and disturbances. In this paper, we consider enforcement of bounding box constraints for a second order system under limited control authority and input disturbances. To solve the constrained control problem, we apply multiple robust control barrier functions (RCBFs) which, in general, do not provide a feasible solution to the problem. However, we derive conditions on how to select the RCBF parameters to guarantee that a feasible solution always exists.

Paper number 118:
Title: UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach
Authors: Kangli Wang, Wei Gao
Abstract: Learning-based point cloud compression methods have made significant progress in terms of performance. However, these methods still encounter challenges including high complexity, limited compression modes, and a lack of support for variable rate, which restrict the practical application of these methods. In order to promote the development of practical point cloud compression, we propose an efficient unified point cloud geometry compression framework, dubbed as UniPCGC. It is a lightweight framework that supports lossy compression, lossless compression, variable rate and variable complexity. First, we introduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which allocates more computational complexity to groups with higher coding difficulty, and merges groups with lower coding difficulty. Second, Variable Rate and Complexity Module (VRCM) is achieved in the lossy mode through joint adoption of a rate modulation module and dynamic sparse convolution. Finally, through the dynamic combination of UELC and VRCM, we achieve lossy compression, lossless compression, variable rate and complexity within a unified framework. Compared to the previous state-of-the-art method, our method achieves a compression ratio (CR) gain of 8.1\% on lossless compression, and a Bjontegaard Delta Rate (BD-Rate) gain of 14.02\% on lossy compression, while also supporting variable rate and variable complexity.

Paper number 119:
Title: Wireless Hearables With Programmable Speech AI Accelerators
Authors: Malek Itani, Tuochao Chen, Arun Raghavan, Gavriel Kohlberg, Shyamnath Gollakota
Abstract: The conventional wisdom has been that designing ultra-compact, battery-constrained wireless hearables with on-device speech AI models is challenging due to the high computational demands of streaming deep learning models. Speech AI models require continuous, real-time audio processing, imposing strict computational and I/O constraints. We present NeuralAids, a fully on-device speech AI system for wireless hearables, enabling real-time speech enhancement and denoising on compact, battery-constrained devices. Our system bridges the gap between state-of-the-art deep learning for speech enhancement and low-power AI hardware by making three key technical contributions: 1) a wireless hearable platform integrating a speech AI accelerator for efficient on-device streaming inference, 2) an optimized dual-path neural network designed for low-latency, high-quality speech enhancement, and 3) a hardware-software co-design that uses mixed-precision quantization and quantization-aware training to achieve real-time performance under strict power constraints. Our system processes 6 ms audio chunks in real-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In real-world evaluations, including a user study with 28 participants, our system outperforms prior on-device models in speech quality and noise suppression, paving the way for next-generation intelligent wireless hearables that can enhance hearing entirely on-device.

Paper number 120:
Title: GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting
Authors: Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu
Abstract: In the Generative AI era, safeguarding 3D models has become increasingly urgent. While invisible watermarking is well-established for 2D images with encoder-decoder frameworks, generalizable and robust solutions for 3D remain elusive. The main difficulty arises from the renderer between the 3D encoder and 2D decoder, which disrupts direct gradient flow and complicates training. Existing 3D methods typically rely on per-scene iterative optimization, resulting in time inefficiency and limited generalization. In this work, we propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a well-known yet underexplored representation for watermarking. We identify two major challenges: (1) ensuring effective training generalized across diverse 3D models, and (2) reliably extracting watermarks from free-view renderings, even under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to embed messages, distortion layers to enhance resilience against various distortions, and a 2D decoder to extract watermarks from renderings. A key innovation is the Adaptive Marker Control mechanism that adaptively perturbs the initially optimized 3DGS, escaping local minima and improving both training stability and convergence. Extensive experiments show that GS-Marker outperforms per-scene training approaches in terms of decoding accuracy and model fidelity, while also significantly reducing computation time.

Paper number 121:
Title: On-chip calibration of Microscale-Thermocouples for Precise Temperature Measurement
Authors: Hassan Irshad Bhatti
Abstract: Precise temperature measurement at micro/nanoscale is crucial across various domains including physical sciences, chemical processes, industrial production, medical diagnosis, weather forecasting, electronics, and biology. Micro/nanoscale thermal mapping requires precise techniques such as thermocouples, resistance-based devices, infrared thermography, optical interferometry, Raman thermometry, and Time domain-thermoreflectance (TDTR) method. Each method has its advantages and limitations, emphasizing the importance of selecting the appropriate technique. Among these methods, micro-thin film thermocouples (TFTCs) offer a compelling solution due to their direct contact-based temperature measurements, minimal surface preparation requirements, lower cost, and robustness against environmental factors. Thermocouples work on the well-established Seebeck effect, where a voltage is generated proportional to the temperature difference between two points. However, at micro/nanoscale, the Seebeck coefficients of thermocouples differ from those in bulk materials, requiring experimental calibration for precise measurements. To address this, we introduce an on-chip characterization platform with a differential temperature measurement setup on a borosilicate glass substrate. This platform utilizes a microheater as a localized heat source to elevate the temperature at the hot junction of the TFTC while maintaining the cold junction at ambient conditions. Numerical simulations are employed to engineer both the microheater and TFTC junction for precise temperature control. The functionality of this platform is validated by fabricating TFTCs using standard fabrication processes and measuring the TFTC response to determine the differential Seebeck coefficient of a Platinum-Chromium TFTC Junction. The calculated sensitivity of Pt/Cr TFTCs using this calibration method is 19.23 +- 0.405 {\mu}V/C.

Paper number 122:
Title: Seeing Speech and Sound: Distinguishing and Locating Audios in Visual Scenes
Authors: Hyeonggon Ryu, Seongyu Kim, Joon Son Chung, Arda Senocak
Abstract: We present a unified model capable of simultaneously grounding both spoken language and non-speech sounds within a visual scene, addressing key limitations in current audio-visual grounding models. Existing approaches are typically limited to handling either speech or non-speech sounds independently, or at best, together but sequentially without mixing. This limitation prevents them from capturing the complexity of real-world audio sources that are often mixed. Our approach introduces a 'mix-and-separate' framework with audio-visual alignment objectives that jointly learn correspondence and disentanglement using mixed audio. Through these objectives, our model learns to produce distinct embeddings for each audio type, enabling effective disentanglement and grounding across mixed audio sources. Additionally, we created a new dataset to evaluate simultaneous grounding of mixed audio sources, demonstrating that our model outperforms prior methods. Our approach also achieves comparable or better performance in standard segmentation and cross-modal retrieval tasks, highlighting the benefits of our mix-and-separate approach.

Paper number 123:
Title: Signal Propagation in RIS-Aided 5G Systems
Authors: Adam Samorzewski, Adrian Kliks
Abstract: In this paper, we conduct an in-depth analysis of radio signal propagation characteristics within the urban environment of Poznan (Poland). The study specifically addresses the deployment of a 5th generation (5G NR - New Radio) Radio Access Network (RAN), which comprises 8 strategically positioned Base Stations (BSs). These base stations are configured with either Single Input Single Output (SISO) or Multiple Input Multiple Output (MIMO) antenna technologies, contingent upon the specific requirements of the network cells they serve. A key focus of our research is the integration of 15 reflecting arrays, known as Reconfigurable Intelligent Surfaces (RISs), which were installed throughout the study area. These RISs were deployed at various suspension heights to evaluate their impact on radio signal propagation and coverage. By exploring the influence of these RIS matrices, our research sheds light on their potential to significantly enhance signal quality, particularly in urban environments.

Paper number 124:
Title: Entropic Analysis of Time Series through Kernel Density Estimation
Authors: Audun Myers, Bill Kay, Iliana Alvarez, Michael Hughes, Cameron Mackenzie, Carlos Ortiz Marrero, Emily Ellwein, Erik Lentz
Abstract: This work presents a novel framework for time series analysis using entropic measures based on the kernel density estimate (KDE) of the time series' Takens' embeddings. Using this framework we introduce two distinct analytical tools: (1) a multi-scale KDE entropy metric, denoted as $\Delta\text{KE}$, which quantifies the evolution of time series complexity across different scales by measuring certain entropy changes, and (2) a sliding baseline method that employs the Kullback-Leibler (KL) divergence to detect changes in time series dynamics through changes in KDEs. The $\Delta{\rm KE}$ metric offers insights into the information content and ``unfolding'' properties of the time series' embedding related to dynamical systems, while the KL divergence-based approach provides a noise and outlier robust approach for identifying time series change points (injections in RF signals, e.g.). We demonstrate the versatility and effectiveness of these tools through a set of experiments encompassing diverse domains. In the space of radio frequency (RF) signal processing, we achieve accurate detection of signal injections under varying noise and interference conditions. Furthermore, we apply our methodology to electrocardiography (ECG) data, successfully identifying instances of ventricular fibrillation with high accuracy. Finally, we demonstrate the potential of our tools for dynamic state detection by accurately identifying chaotic regimes within an intermittent signal. These results show the broad applicability of our framework for extracting meaningful insights from complex time series data across various scientific disciplines.

Paper number 125:
Title: A Reliable and Efficient Detection Pipeline for Rodent Ultrasonic Vocalizations
Authors: Sabah Shahnoor Anis, Devin M. Kellis, Kris Ford Kaigler, Marlene A. Wilson, Christian O'Reilly
Abstract: Analyzing ultrasonic vocalizations (USVs) is crucial for understanding rodents' affective states and social behaviors, but the manual analysis is time-consuming and prone to errors. Automated USV detection systems have been developed to address these challenges. Yet, these systems often rely on machine learning and fail to generalize effectively to new datasets. To tackle these shortcomings, we introduce ContourUSV, an efficient automated system for detecting USVs from audio recordings. Our pipeline includes spectrogram generation, cleaning, pre-processing, contour detection, post-processing, and evaluation against manual annotations. To ensure robustness and reliability, we compared ContourUSV with three state-of-the-art systems using an existing open-access USV dataset (USVSEG) and a second dataset we are releasing publicly along with this paper. On average, across the two datasets, ContourUSV outperformed the other three systems with a 1.51x improvement in precision, 1.17x in recall, 1.80x in F1 score, and 1.49x in specificity while achieving an average speedup of 117.07x.

Paper number 126:
Title: Asymptotics of Proximity Operator for Squared Loss and Performance Prediction of Nonconvex Sparse Signal Recovery
Authors: Ryo Hayakawa
Abstract: Proximal splitting-based convex optimization is a promising approach to linear inverse problems because we can use some prior knowledge of the unknown variables explicitly. An understanding of the behavior of the optimization algorithms would be important for the tuning of the parameters and the development of new algorithms. In this paper, we first analyze the asymptotic property of the proximity operator for the squared loss function, which appears in the update equations of some proximal splitting methods for linear inverse problems. Our analysis shows that the output of the proximity operator can be characterized with a scalar random variable in the large system limit. Moreover, we apply the asymptotic result to the prediction of optimization algorithms for compressed sensing. Simulation results demonstrate that the MSE performance of the Douglas-Rachford algorithm can be well predicted in compressed sensing with the $\ell_{1}$ optimization. We also examine the behavior of the prediction for the case with nonconvex smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP) regularization.

Paper number 127:
Title: Recursive Identification of Binary-Valued Systems under Uniform Persistent Excitations
Authors: Jieming Ke, Ying Wang, Yanlong Zhao, Ji-Feng Zhang
Abstract: This paper studies the control-oriented identification problem of set-valued moving average systems with uniform persistent excitations and observation noises. A stochastic approximation-based (SA-based) algorithm without projections or truncations is proposed. The algorithm overcomes the limitations of the existing empirical measurement method and the recursive projection method, where the former requires periodic inputs, and the latter requires projections to restrict the search region in a compact this http URL analyze the convergence property of the algorithm, the distribution tail of the estimation error is proved to be exponentially convergent through an auxiliary stochastic process. Based on this key technique, the SA-based algorithm appears to be the first to reach the almost sure convergence rate of $ O(\sqrt{\ln\ln k/k}) $ theoretically in the non-periodic input case. Meanwhile, the mean square convergence is proved to have a rate of $ O(1/k) $, which is the best one even under accurate observations. A numerical example is given to demonstrate the effectiveness of the proposed algorithm and theoretical results.

Paper number 128:
Title: Distributed Safe Control Design and Probabilistic Safety Verification for Multi-Agent Systems
Authors: Han Wang, Antonis Papachristodoulou, Kostas Margellos
Abstract: We propose distributed iterative algorithms for safe control design and safety verification for networked multi-agent systems. These algorithms rely on distributing a control barrier function (CBF) related quadratic programming (QP) problem assuming the existence of CBFs. The proposed distributed algorithm addresses infeasibility issues of existing schemes via a cooperation mechanism between agents. The resulting control input is guaranteed to be optimal, and satisfies CBF constraints of all agents. Furthermore, a truncated algorithm is proposed to facilitate computational implementation. The performance of the truncated algorithm is evaluated using a distributed safety verification algorithm. The algorithm quantifies safety for multi-agent systems probabilistically by means of CBFs. Both upper and lower bounds on the probability of safety are obtained using the so called scenario approach. Both the scenario sampling and safety verification procedures are fully distributed. The efficacy of our algorithms is demonstrated by an example on multi-robot collision avoidance.

Paper number 129:
Title: Confidence Intervals for Performance Estimates in Brain MRI Segmentation
Authors: R. El Jurdi, G. Varoquaux, O. Colliot
Abstract: Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in the context of segmentation in 3D brain magnetic resonance imaging (MRI). We carry experiments on using the standard nnU-net framework, two datasets from the Medical Decathlon challenge that concern brain MRI (hippocampus and brain tumor segmentation) and two performance measures: the Dice Similarity Coefficient and the Hausdorff distance. We show that the parametric confidence intervals are reasonable approximations of the bootstrap estimates for varying test set sizes and spread of the performance metric. Importantly, we show that the test size needed to achieve a given precision is often much lower than for classification tasks. Typically, a 1\% wide confidence interval requires about 100-200 test samples when the spread is low (standard-deviation around 3\%). More difficult segmentation tasks may lead to higher spreads and require over 1000 samples.

Paper number 130:
Title: Optimal Control of Grid-Interfacing Inverters With Current Magnitude Limits
Authors: Trager Joswig-Jones, Baosen Zhang
Abstract: Grid-interfacing inverters act as the interface between renewable resources and the electric grid, and have the potential to offer fast and programmable controls compared to synchronous generators. With this flexibility there has been significant research efforts into determining the best way to control these inverters. Inverters are limited in their maximum current output in order to protect semiconductor devices, presenting a nonlinear constraint that needs to be accounted for in their control algorithms. Existing approaches either simply saturate a controller that is designed for unconstrained systems, or assume small perturbations and linearize a saturated system. These approaches can lead to stability issues or limiting the control actions to be too conservative. In this paper, we directly focus on a nonlinear system that explicitly accounts for the saturation of the current magnitude. We use a Lyapunov stability approach to determine a stability condition for the system, guaranteeing that a class of controllers would be stabilizing if they satisfy a simple SDP condition. With this condition we fit a linear-feedback controller by sampling the output (offline) model predictive control problems. This learned controller has improved performances with existing designs.

Paper number 131:
Title: Learned, uncertainty-driven adaptive acquisition for photon-efficient scanning microscopy
Authors: Cassandra Tong Ye, Jiashu Han, Kunzan Liu, Anastasios Angelopoulos, Linda Griffith, Kristina Monakhova, Sixian You
Abstract: Scanning microscopy systems, such as confocal and multiphoton microscopy, are powerful imaging tools for probing deep into biological tissue. However, scanning systems have an inherent trade-off between acquisition time, field of view, phototoxicity, and image quality, often resulting in noisy measurements when fast, large field of view, and/or gentle imaging is needed. Deep learning could be used to denoise noisy microscopy measurements, but these algorithms can be prone to hallucination, which can be disastrous for medical and scientific applications. We propose a method to simultaneously denoise and predict pixel-wise uncertainty for scanning microscopy systems, improving algorithm trustworthiness and providing statistical guarantees for deep learning predictions. Furthermore, we propose to leverage this learned, pixel-wise uncertainty to drive an adaptive acquisition technique that rescans only the most uncertain regions of a sample, saving time and reducing the total light dose to the sample. We demonstrate our method on experimental confocal and multiphoton microscopy systems, showing that our uncertainty maps can pinpoint hallucinations in the deep learned predictions. Finally, with our adaptive acquisition technique, we demonstrate up to 16X reduction in acquisition time and total light dose while successfully recovering fine features in the sample and reducing hallucinations. We are the first to demonstrate distribution-free uncertainty quantification for a denoising task with real experimental data and the first to propose adaptive acquisition based on reconstruction uncertainty.

Paper number 132:
Title: A Physics-informed Machine Learning-based Control Method for Nonlinear Dynamic Systems with Highly Noisy Measurements
Authors: Mason Ma, Jiajie Wu, Chase Post, Tony Shi, Jingang Yi, Tony Schmitz, Hong Wang
Abstract: This study presents a physics-informed machine learning-based control method for nonlinear dynamic systems with highly noisy measurements. Existing data-driven control methods that use machine learning for system identification cannot effectively cope with highly noisy measurements, resulting in unstable control performance. To address this challenge, the present study extends current physics-informed machine learning capabilities for modeling nonlinear dynamics with control and integrates them into a model predictive control framework. To demonstrate the capability of the proposed method we test and validate with two noisy nonlinear dynamic systems: the chaotic Lorenz 3 system, and turning machine tool. Analysis of the results illustrate that the proposed method outperforms state-of-the-art benchmarks as measured by both modeling accuracy and control performance for nonlinear dynamic systems under high-noise conditions.

Paper number 133:
Title: Implicit Image-to-Image Schrodinger Bridge for Image Restoration
Authors: Yuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Sifan Song, Zhennong Chen, Rui Hu, Li Zhang, Quanzheng Li, Zhiqiang Chen, Dufan Wu
Abstract: Diffusion-based models have demonstrated remarkable effectiveness in image restoration tasks; however, their iterative denoising process, which starts from Gaussian noise, often leads to slow inference speeds. The Image-to-Image Schrödinger Bridge (I$^2$SB) offers a promising alternative by initializing the generative process from corrupted images while leveraging training techniques from score-based diffusion models. In this paper, we introduce the Implicit Image-to-Image Schrödinger Bridge (I$^3$SB) to further accelerate the generative process of I$^2$SB. I$^3$SB restructures the generative process into a non-Markovian framework by incorporating the initial corrupted image at each generative step, effectively preserving and utilizing its information. To enable direct use of pretrained I$^2$SB models without additional training, we ensure consistency in marginal distributions. Extensive experiments across many image corruptions, including noise, low resolution, JPEG compression, and sparse sampling, and multiple image modalities, such as natural, human face, and medical images, demonstrate the acceleration benefits of I$^3$SB. Compared to I$^2$SB, I$^3$SB achieves the same perceptual quality with fewer generative steps, while maintaining or improving fidelity to the ground truth.

Paper number 134:
Title: SuperM2M: Supervised and Mixture-to-Mixture Co-Learning for Speech Enhancement and Noise-Robust ASR
Authors: Zhong-Qiu Wang
Abstract: The current dominant approach for neural speech enhancement is based on supervised learning by using simulated training data. The trained models, however, often exhibit limited generalizability to real-recorded data. To address this, this paper investigates training enhancement models directly on real target-domain data. We propose to adapt mixture-to-mixture (M2M) training, originally designed for speaker separation, for speech enhancement, by modeling multi-source noise signals as a single, combined source. In addition, we propose a co-learning algorithm that improves M2M with the help of supervised algorithms. When paired close-talk and far-field mixtures are available for training, M2M realizes speech enhancement by training a deep neural network (DNN) to produce speech and noise estimates in a way such that they can be linearly filtered to reconstruct the close-talk and far-field mixtures. This way, the DNN can be trained directly on real mixtures, and can leverage close-talk and far-field mixtures as a weak supervision to enhance far-field mixtures. To improve M2M, we combine it with supervised approaches to co-train the DNN, where mini-batches of real close-talk and far-field mixture pairs and mini-batches of simulated mixture and clean speech pairs are alternately fed to the DNN, and the loss functions are respectively (a) the mixture reconstruction loss on the real close-talk and far-field mixtures and (b) the regular enhancement loss on the simulated clean speech and noise. We find that, this way, the DNN can learn from real and simulated data to achieve better generalization to real data. We name this algorithm SuperM2M (supervised and mixture-to-mixture co-learning). Evaluation results on the CHiME-4 dataset show its effectiveness and potential.

Paper number 135:
Title: Diffusion-Aided Joint Source Channel Coding For High Realism Wireless Image Transmission
Authors: Mingyu Yang, Bowen Liu, Boyang Wang, Hun-Seok Kim
Abstract: Deep learning-based joint source-channel coding (deep JSCC) has been demonstrated to be an effective approach for wireless image transmission. Nevertheless, most existing work adopts an autoencoder framework to optimize conventional criteria such as Mean Squared Error (MSE) and Structural Similarity Index (SSIM) which do not suffice to maintain the perceptual quality of reconstructed images. Such an issue is more prominent under stringent bandwidth constraints or low signal-to-noise ratio (SNR) conditions. To tackle this challenge, we propose DiffJSCC, a novel framework that leverages the prior knowledge of the pre-trained Statble Diffusion model to produce high-realism images via the conditional diffusion denoising process. Our DiffJSCC first extracts multimodal spatial and textual features from the noisy channel symbols in the generation phase. Then, it produces an initial reconstructed image as an intermediate representation to aid robust feature extraction and a stable training process. In the following diffusion step, DiffJSCC uses the derived multimodal features, together with channel state information such as the signal-to-noise ratio (SNR), as conditions to guide the denoising diffusion process, which converts the initial random noise to the final reconstruction. DiffJSCC employs a novel control module to fine-tune the Stable Diffusion model and adjust it to the multimodal conditions. Extensive experiments on diverse datasets reveal that our method significantly surpasses prior deep JSCC approaches on both perceptual metrics and downstream task performance, showcasing its ability to preserve the semantics of the original transmitted images. Notably, DiffJSCC can achieve highly realistic reconstructions for 768x512 pixel Kodak images with only 3072 symbols (<0.008 symbols per pixel) under 1dB SNR channels.

Paper number 136:
Title: Signal-Comparison-Based Distributed Estimation Under Decaying Average Data Rate Communications
Authors: Jieming Ke, Xiaodong Lu, Yanlong Zhao, Ji-Feng Zhang
Abstract: The paper investigates the distributed estimation problem under low bit rate communications. Based on the signal-comparison (SC) consensus protocol under binary-valued communications, a new consensus+innovations type distributed estimation algorithm is proposed. Firstly, the high-dimensional estimates are compressed into binary-valued messages by using a periodic compressive strategy, dithered noises and a sign function. Next, based on the dithered noises and expanding triggering thresholds, a new stochastic event-triggered mechanism is proposed to reduce the communication frequency. Then, a modified SC consensus protocol is applied to fuse the neighborhood information. Finally, a stochastic approximation estimation algorithm is used to process innovations. The proposed SC-based algorithm has the advantages of high effectiveness and low communication cost. For the effectiveness, the estimates of the SC-based algorithm converge to the true value in the almost sure and mean square sense. A polynomial almost sure convergence rate is also obtained. For the communication cost, the local and global average bit rates for communications decay to zero at a polynomial rate. The trade-off between the convergence rate and the communication cost is established through event-triggered coefficients. A better convergence rate can be achieved by decreasing event-triggered coefficients, while lower communication cost can be achieved by increasing event-triggered coefficients. A simulation example is given to demonstrate the theoretical results.

Paper number 137:
Title: Multi-Channel Multi-Step Spectrum Prediction Using Transformer and Stacked Bi-LSTM
Authors: Guangliang Pan, Jie Li, Minglei Li
Abstract: Spectrum prediction is considered as a key technology to assist spectrum decision. Despite the great efforts that have been put on the construction of spectrum prediction, achieving accurate spectrum prediction emphasizes the need for more advanced solutions. In this paper, we propose a new multichannel multi-step spectrum prediction method using Transformer and stacked bidirectional LSTM (Bi- LSTM), named TSB. Specifically, we use multi-head attention and stacked Bi-LSTM to build a new Transformer based on encoder-decoder architecture. The self-attention mechanism composed of multiple layers of multi-head attention can continuously attend to all positions of the multichannel spectrum sequences. The stacked Bi-LSTM can learn these focused coding features by multi-head attention layer by layer. The advantage of this fusion mode is that it can deeply capture the long-term dependence of multichannel spectrum data. We have conducted extensive experiments on a dataset generated by a real simulation platform. The results show that the proposed algorithm performs better than the baselines.

Paper number 138:
Title: A Deep Learning Model for Coronary Artery Segmentation and Quantitative Stenosis Detection in Angiographic Images
Authors: Baixiang Huang, Yu Luo, Guangyu Wei, Songyan He, Yushuang Shao, Xueying Zeng
Abstract: Coronary artery disease (CAD) is a leading cause of cardiovascular-related mortality, and accurate stenosis detection is crucial for effective clinical decision-making. Coronary angiography remains the gold standard for diagnosing CAD, but manual analysis of angiograms is prone to errors and subjectivity. This study aims to develop a deep learning-based approach for the automatic segmentation of coronary arteries from angiographic images and the quantitative detection of stenosis, thereby improving the accuracy and efficiency of CAD diagnosis. We propose a novel deep learning-based method for the automatic segmentation of coronary arteries in angiographic images, coupled with a dynamic cohort method for stenosis detection. The segmentation model combines the MedSAM and VM-UNet architectures to achieve high-performance results. After segmentation, the vascular centerline is extracted, vessel diameter is computed, and the degree of stenosis is measured with high precision, enabling accurate identification of arterial stenosis. On the mixed dataset (including the ARCADE, DCA1, and GH datasets), the model achieved an average IoU of 0.6308, with sensitivity and specificity of 0.9772 and 0.9903, respectively. On the ARCADE dataset, the average IoU was 0.6303, with sensitivity of 0.9832 and specificity of 0.9933. Additionally, the stenosis detection algorithm achieved a true positive rate (TPR) of 0.5867 and a positive predictive value (PPV) of 0.5911, demonstrating the effectiveness of our model in analyzing coronary angiography images. SAM-VMNet offers a promising tool for the automated segmentation and detection of coronary artery stenosis. The model's high accuracy and robustness provide significant clinical value for the early diagnosis and treatment planning of CAD. The code and examples are available at this https URL.

Paper number 139:
Title: Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques
Authors: Yuanchao Li, Peter Bell, Catherine Lai
Abstract: Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.

Paper number 140:
Title: Prediction and Reference Quality Adaptation for Learned Video Compression
Authors: Xihua Sheng, Li Li, Dong Liu, Houqiang Li
Abstract: Temporal prediction is one of the most important technologies for video compression. Various prediction coding modes are designed in traditional video codecs. Traditional video codecs will adaptively to decide the optimal coding mode according to the prediction quality and reference quality. Recently, learned video codecs have made great progress. However, they did not effectively address the problem of prediction and reference quality adaptation, which limits the effective utilization of temporal prediction and reduction of reconstruction error propagation. Therefore, in this paper, we first propose a confidence-based prediction quality adaptation (PQA) module to provide explicit discrimination for the spatial and channel-wise prediction quality difference. With this module, the prediction with low quality will be suppressed and that with high quality will be enhanced. The codec can adaptively decide which spatial or channel location of predictions to use. Then, we further propose a reference quality adaptation (RQA) module and an associated repeat-long training strategy to provide dynamic spatially variant filters for diverse reference qualities. With these filters, our codec can adapt to different reference qualities, making it easier to achieve the target reconstruction quality and reduce the reconstruction error propagation. Experimental results verify that our proposed modules can effectively help our codec achieve a higher compression performance.

Paper number 141:
Title: Latent Space Imaging
Authors: Matheus Souza, Yidan Zheng, Kaizhang Kang, Yogeshwar Nath Mishra, Qiang Fu, Wolfgang Heidrich
Abstract: Digital imaging systems have traditionally relied on brute-force measurement and processing of pixels arranged on regular grids. In contrast, the human visual system performs significant data reduction from the large number of photoreceptors to the optic nerve, effectively encoding visual information into a low-bandwidth latent space representation optimized for brain processing. Inspired by this, we propose a similar approach to advance artificial vision systems. Latent Space Imaging introduces a new paradigm that combines optics and software to encode image information directly into the semantically rich latent space of a generative model. This approach substantially reduces bandwidth and memory demands during image capture and enables a range of downstream tasks focused on the latent space. We validate this principle through an initial hardware prototype based on a single-pixel camera. By implementing an amplitude modulation scheme that encodes into the generative model's latent space, we achieve compression ratios ranging from 1:100 to 1:1000 during imaging, and up to 1:16384 for downstream applications. This approach leverages the model's intrinsic linear boundaries, demonstrating the potential of latent space imaging for highly efficient imaging hardware, adaptable future applications in high-speed imaging, and task-specific cameras with significantly reduced hardware complexity.

Paper number 142:
Title: Spatial Frequencies and Degrees of Freedom: Their Roles in Near-Field Communications
Authors: Alva Kosasih, Özlem Tuğfe Demir, Nikolaos Kolomvakis, Emil Björnson
Abstract: As wireless technology begins to utilize physically larger arrays and/or higher frequencies, the transmitter and receiver will reside in each other's radiative near field. This fact gives rise to unusual propagation phenomena such as spherical wavefronts and beamfocusing, creating the impression that new spatial dimensions -- called degrees-of-freedom (DoF) -- can be exploited in the near field. However, this is a fallacy because the theoretically maximum DoF are already achievable in the far field. This paper sheds light on these issues by providing a tutorial on spatial frequencies, which are the fundamental components of wireless channels, and by explaining their role in characterizing the DoF in the near and far fields. In particular, we demonstrate how a single propagation path utilizes one spatial frequency in the far field and an interval of spatial frequencies in the near field. We explain how the array geometry determines the number of distinguishable spatial frequency bins and, thereby, the spatial DoF. We also describe how to model near-field multipath channels and their spatial correlation matrices. Finally, we discuss the research challenges and future directions in this field.

Paper number 143:
Title: Exploiting Target Location Distribution in MIMO Radar: PCRB vs. PSBP for Waveform Design
Authors: Lingyun Xu, Bowen Wang, Huiyong Li, Ziyang Cheng
Abstract: This paper investigates the issue of how to exploit prior target location distribution for multiple input multiple output (MIMO) radar waveform design. We consider a MIMO radar aiming to estimate the random angular location parameters of a point target, whose prior distribution information can be exploited by the radar. First, we establish the models of the MIMO radar system and the target location distribution. Based on the considered models, we propose the first category of target location distribution exploitation methods by analyzing the radar direction-of-angle (DoA) estimation performance and deriving a general form of posterior Cramer-Rao bound (PCRB) as the lower bound of the mean square error of DoA estimation. Following this, to investigate the potential of leveraging prior information from two distinct perspectives, thereby providing deeper insights into the estimation process, we proposed the second category of target location distribution exploitation methods by introducing a novel radar metric, probability scaled beampattern (PSBP), from the perspective of radar beampattern. To compare the two methods, we formulate the PCRB and PSBP oriented radar waveform design problems and propose corresponding low-complexity and convergence-guaranteed algorithms to tackle them. Finally, numerical simulations are conducted in different scenarios to provide a comprehensive evaluation and comparison of the radar performance.

Paper number 144:
Title: Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-context Learning
Authors: Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Zhu Han, Charlie Zhang
Abstract: Generative artificial intelligence (GAI) is a promising technique towards 6G networks, and generative foundation models such as large language models (LLMs) have attracted considerable interest from academia and telecom industry. This work considers a novel edge-cloud deployment of foundation models in 6G networks. Specifically, it aims to minimize the service delay of foundation models by radio resource allocation and task offloading, i.e., offloading diverse content generation tasks to proper LLMs at the network edge or cloud. In particular, we first introduce the communication system model, i.e., allocating radio resources and calculating link capacity to support generated content transmission, and then we present the LLM inference model to calculate the delay of content generation. After that, we propose a novel in-context learning method to optimize the task offloading decisions. It utilizes LLM's inference capabilities, and avoids the difficulty of dedicated model training or fine-tuning as in conventional machine learning algorithms. Finally, the simulations demonstrate that the proposed edge-cloud deployment and in-context learning task offloading method can achieve satisfactory generation service quality without dedicated model training or fine-tuning.

Paper number 145:
Title: DCSK-based Waveform Design for Self-sustainable RIS-aided Noncoherent SWIPT
Authors: Priyadarshi Mukherjee, Constantinos Psomas, Ioannis Krikidis
Abstract: This paper investigates the problem of transmit waveform design in the context of a chaotic signal-based self-sustainable reconfigurable intelligent surface (RIS)-aided system for simultaneous wireless information and power transfer (SWIPT). Specifically, we propose a differential chaos shift keying (DCSK)-based RIS-aided point-to-point set-up, where the RIS is partitioned into two non-overlapping surfaces. The elements of the first sub-surface perform energy harvesting (EH), which in turn, provide the required power to the other sub-surface operating in the information transfer (IT) mode. In this framework, by considering a generalized frequency-selective Nakagami-m fading scenario as well as the nonlinearities of the EH process, we derive closed-form analytical expressions for both the bit error rate (BER) at the receiver and the harvested power at the RIS. Our analysis demonstrates, that both these performance metrics depend on the parameters of the wireless channel, the transmit waveform design, and the number of reflecting elements at the RIS, which switch between the IT and EH modes, depending on the application requirements. Moreover, we show that, having more reflecting elements in the IT mode is not always beneficial and also, for a given acceptable BER, we derive a lower bound on the number of RIS elements that need to be operated in the EH mode. Furthermore, for a fixed RIS configuration, we investigate a trade-off between the achievable BER and the harvested power at the RIS and accordingly, we propose appropriate transmit waveform designs. Finally, our numerical results illustrate the importance of our intelligent DCSK-based waveform design on the considered framework.

Paper number 146:
Title: Noisy Low Rank Column-wise Sensing
Authors: Ankit Pratap Singh, Namrata Vaswani
Abstract: This letter studies the AltGDmin algorithm for solving the noisy low rank column-wise sensing (LRCS) problem. Our sample complexity guarantee improves upon the best existing one by a factor $\max(r, \log(1/\epsilon))/r$ where $r$ is the rank of the unknown matrix and $\epsilon$ is the final desired accuracy. A second contribution of this work is a detailed comparison of guarantees from all work that studies the exact same mathematical problem as LRCS, but refers to it by different names.

Paper number 147:
Title: Safe Control of Grid-Interfacing Inverters with Current Magnitude Limits
Authors: Trager Joswig-Jones, Baosen Zhang
Abstract: Grid-interfacing inverters allow renewable resources to be connected to the electric grid and offer fast and programmable control responses. However, inverters are subject to significant physical constraints. One such constraint is a current magnitude limit required to protect semiconductor devices. While many current limiting methods are available, they can often unpredictably alter the behavior of the inverter control during overcurrent events leading to instability or poor performance. In this paper, we present a safety filter approach to limit the current magnitude of inverters controlled as voltage sources. The safety filter problem is formulated with a control barrier function constraint that encodes the current magnitude limit. To ensure feasibility of the problem, we prove the existence of a safe linear controller for a specified reference. This approach allows for the desired voltage source behavior to be minimally altered while safely limiting the current output.

Paper number 148:
Title: Point Data for Site-Specific Mid-band Radio Propagation Channel Statistics in the Indoor Hotspot (InH) Environment for 3GPP and Next Generation Alliance (NGA) Channel Modeling
Authors: Theodore S. Rappaport, Dipankar Shakya, Mingjun Ying
Abstract: Extensive work has been carried out in the past year by various organizations in an effort to determine standardized statistical channel impulse response (CIR) parameters for the newly-released FR3 mid-band spectrum (7.25 GHz -- 24.25 GHz). In this work, we show that the wireless community currently lacks a unified method for presenting key parameters required for transparency and utilization by several constituencies when presenting propagation data for use by standard bodies or third parties to create statistical CIR models. This paper aims to solve the existing problem by offering a standard method to provide key propagation parameters in a point-data format that supports both statistical and site-specific channel characterization. As shown here, the point-data format enables multiple contributors to create channel model standards or pool measurement data to create larger datasets for exploring ray-tracing (e.g. site-specific) channel modeling or training in AI/ML propagation work, and to ensure the most accurate model using a larger dataset that is continually expanded through measurement contributions. The point-data approach includes site-specific point-by-point propagation data while readily supporting the creation of commonly-used cumulative distribution function (CDF) plot. The indoor hotspot (InH) datasets collected in Spring 2024 at 6.75 GHz and 16.95 GHZ by NYU WIRELESS are provided for the first time in point-data form, to augment statistical models previously presented solely as CDFs, in order to demonstrate how a standardized approach to measurement data could allow others to utilize the site-specific locations and key channel parameters observed at each location, to better understand, vet, and build upon statistical or site-specific CIRs from the contributions of many different data sources.

Paper number 149:
Title: A Control Barrier Function Candidate for Quadrotors with Limited Field of View
Authors: Biagio Trimarchi, Fabrizio Schiano, Roberto Tron
Abstract: The problem of control based on vision measurements (bearings) has been amply studied in the literature; however, the problem of addressing the limits of the field of view of physical sensors has received relatively less attention (especially for agents with non-trivial dynamics). The technical challenge is that, as in most vision-based control approaches, a standard approach to the problem requires knowing the distance between cameras and observed features in the scene, which is not directly available. Instead, we present a solution based on a Control Barrier Function (CBF) approach that uses a splitting of the original differential constraint to effectively remove the dependence on the unknown measurement error. Compared to the current literature, our approach gives strong robustness guarantees against bounded distance estimation errors. We showcase the proposed solution with the numerical simulations of a double integrator and a quadrotor tracking a trajectory while keeping the corners of a rectangular gate in the camera field of view.

Paper number 150:
Title: Sensor-Based Safety-Critical Control Using an Incremental Control Barrier Function Formulation via Reduced-Order Approximate Models
Authors: Johannes Autenrieb, Hyo-Sang Shin
Abstract: The existing control barrier function literature generally relies on precise mathematical models to guarantee system safety, limiting their applicability in scenarios with parametric uncertainties. While incremental control techniques have shown promise in addressing model uncertainties in flight control applications, translating these approaches to safety-critical control presents significant challenges. This paper bridges this gap by introducing measurement-robust incremental control barrier functions (MRICBFs), which leverage sensor-based reduced-order models to provide formal safety guarantees for uncertain systems. By carefully addressing the challenges of sensor accuracy and approximation errors in the incremental formulation, our approach enables substituting specific model components with real-time sensor measurements while maintaining rigorous safety guarantees. This formulation overcomes the limitations of traditional adaptive control methods that adjust system parameters over time, enabling immediate and reliable safety measures for a class of model uncertainties. The efficacy of MRICBFs is demonstrated in two simulation case studies: a simple first-order system with time-varying sensor biases and a more complex overactuated hypersonic glide vehicle with multiple state constraints.

Paper number 151:
Title: Urban Outdoor Propagation Measurements and Channel Models at 6.75 GHz FR1(C) and 16.95 GHz FR3 Upper Mid-Band Spectrum for 5G and 6G
Authors: Dipankar Shakya, Mingjun Ying, Theodore S. Rappaport, Peijie Ma, Idris Al-Wazani, Yanze Wu, Yanbo Wang, Doru Calin, Hitesh Poddar, Ahmad Bazzi, Marwa Chafii, Yunchou Xing, Amitava Ghosh
Abstract: Global allocations in the upper mid-band spectrum (4--24 GHz) necessitate a comprehensive exploration of the propagation behavior to meet the promise of coverage and capacity. This paper presents an extensive Urban Microcell (UMi) outdoor propagation measurement campaign at 6.75 GHz and 16.95 GHz conducted in Downtown Brooklyn, USA, using a 1 GHz bandwidth sliding correlation channel sounder over 40--880 m propagation distance, encompassing seven Line of Sight (LOS) and 13 Non-Line of Sight (NLOS) locations. Analysis of the path loss (PL) reveals lower directional and omnidirectional PL exponents compared to mmWave and sub-THz frequencies in the UMi environment, using the close-in (CI) free space PL (FSPL) model with a 1 m reference distance. Additionally, a decreasing trend in root mean square (RMS) delay spread (DS) and angular spread (AS) with increasing frequency was observed. The measured NLOS RMS DS and RMS AS mean values (as computed by 3GPP methods) are found to be consistently lower compared to 3GPP model predictions. Point-data tables with corresponding site-specific environmental information for all measured statistics at each TX-RX location are provided to support the models and results. The spatio-temporal statistics presented here offer valuable insights for the design of next-generation wireless systems and networks.

Paper number 152:
Title: Quantum Reinforcement Learning-Based Two-Stage Unit Commitment Framework for Enhanced Power Systems Robustness
Authors: Xiang Wei, Ziqing Zhu, Linghua Zhu, Ze Hu, Xian Zhang, Guibin Wang, Siqi Bu, Ka Wing Chan
Abstract: Unit commitment (UC) optimizes the start-up and shutdown schedules of generating units to meet load demand while minimizing costs. However, the increasing integration of renewable energy introduces uncertainties for real-time scheduling. Existing solutions face limitations both in modeling and algorithmic design. At the modeling level, they fail to incorporate widely adopted virtual power plants (VPPs) as flexibility resources, missing the opportunity to proactively mitigate potential real-time imbalances or ramping constraints through foresight-seeing decision-making. At the algorithmic level, existing probabilistic optimization, multi-stage approaches, and machine learning, face challenges in computational complexity and adaptability. To address these challenges, this study proposes a novel two-stage UC framework that incorporates foresight-seeing sequential decision-making in both day-ahead and real-time scheduling, leveraging VPPs as flexibility resources to proactively reserve capacity and ramping flexibility for upcoming renewable energy uncertainties over several hours. In particular, we develop quantum reinforcement learning (QRL) algorithms that integrate the foresight-seeing sequential decision-making and scalable computation advantages of deep reinforcement learning (DRL) with the parallel and high-efficiency search capabilities of quantum computing. Experimental results demonstrate that the proposed QRL-based approach outperforms in computational efficiency, real-time responsiveness, and solution quality.

Paper number 153:
Title: Parameterized TDOA: Instantaneous TDOA Estimation and Localization for Mobile Targets in a Time-Division Broadcast Positioning System
Authors: Chenxin Tu, Xiaowei Cui, Gang Liu, Sihao Zhao, Mingquan Lu
Abstract: In a time-division broadcast positioning system (TDBPS), localizing mobile targets using classical time difference of arrival (TDOA) methods poses significant challenges. Concurrent TDOA measurements are infeasible because targets receive signals from different anchors and extract their transmission times at different reception times, as well as at varying positions. Traditional TDOA estimation schemes implicitly assume that the target remains stationary during the measurement period, which is impractical for mobile targets exhibiting high dynamics. Existing methods for mobile target localization are mostly specialized and rely on motion modeling and do not rely on the concurrent TDOA measurements. This issue limits their direct use of the well-established classical TDOA-based localization methods and complicating the entire localization process. In this paper, to obtain concurrent TDOA estimates at any instant out of the sequential measurements for direct use of existing TDOA-based localization methods, we propose a novel TDOA estimation method, termed parameterized TDOA (P-TDOA). By approximating the time-varying TDOA as a polynomial function over a short period, we transform the TDOA estimation problem into a model parameter estimation problem and derive the desired TDOA estimates thereafter. Theoretical analysis shows that, under certain conditions, the proposed P-TDOA method closely approaches the Cramer-Rao Lower Bound (CRLB) for TDOA estimation in concurrent measurement scenarios, despite measurements being obtained sequentially. Extensive numerical simulations validate our theoretical analysis and demonstrate the effectiveness of the proposed method, highlighting substantial improvements over existing approaches across various scenarios.

Paper number 154:
Title: Upper Mid-Band Channel Measurements and Characterization at 6.75 GHz FR1(C) and 16.95 GHz FR3 in an Indoor Factory Scenario
Authors: Mingjun Ying, Dipankar Shakya, Theodore S. Rappaport, Peijie Ma, Yanbo Wang, Idris Al-Wazani, Yanze Wu, Hitesh Poddar
Abstract: This paper presents detailed radio propagation measurements for an indoor factory (InF) environment at 6.75 GHz and 16.95 GHz using a 1 GHz bandwidth channel sounder. Conducted at the NYU MakerSpace in the NYU Tandon School of Engineering campus in Brooklyn, NY, USA, our measurement campaign characterizes the radio propagation in a representative small factory with diverse machinery and open workspaces across 12 locations, comprising 5 line-of-sight (LOS) and 7 non-line-of-sight (NLOS) scenarios. Analysis using the close-in (CI) free space path loss (FSPL) model with a 1 m reference distance reveals path loss exponents (PLE) below 2 in LOS at 6.75 GHz and 16.95 GHz, while in NLOS, PLE is similar to free-space propagation (e.g., PLE = 2). The RMS delay spread (DS) decreases at higher frequencies with a clear frequency dependence. Also, measurements show a wider RMS angular spread (AS) in NLOS compared to LOS at both frequency bands, with a decreasing trend as frequency increases. These observations in a dense-scatterer factory environment demonstrate frequency-dependent behavior that differs from existing industry-standard 3GPP models. Our findings provide crucial insights into complex propagation mechanisms in factory environments, essential for designing robust air interface and industrial wireless networks at the upper mid-band FR3 spectrum.

Paper number 155:
Title: Generalized Scattering Matrix of Antenna: Moment Solution, Compression Storage and Application
Authors: Chenbo Shi, Jin Pan, Xin Gu, Shichen Liang, Le Zuo
Abstract: This paper presents a computation method of generalized scattering matrix (GSM) based on integral equations and the method of moments (MoM), specifically designed for antennas excited through waveguide ports. By leveraging two distinct formulations -- magnetic-type and electric-type integral equations -- we establish concise algebraic relations linking the GSM directly to the impedance matrices obtained from MoM. To address practical challenges in storing GSM data across wide frequency bands and multiple antenna scenarios, we propose a efficient compression scheme. This approach alleviates memory demands by selectively storing the dominant eigencomponents that govern scattering behavior. Numerical validation examples confirm the accuracy of our method by comparisons with full-wave simulation results. Furthermore, we introduce an efficient iterative procedure to predict antenna array performance, highlighting remarkable improvements in computational speed compared to conventional numerical methods. These results collectively demonstrate the GSM framework's strong potential for antenna-array design processes.

Paper number 156:
Title: Two-Stage Robust Optimal Operation of Distribution Networks Considering Renewable Energy and Demand Asymmetric Uncertainties
Authors: Zhisheng Xiong, Bo Zeng, Peter Palensky, Pedro P. Vergara
Abstract: This paper presents a confidence level-based distributionally information gap decision theory (CL-DIGDT) framework for the two-stage robust optimal operation of distribution networks, aiming at deriving an optimal operational scheme capable of addressing asymmetric uncertainties related to renewable energy and load demands. Building on conventional IGDT, the proposed framework utilizes the confidence level to capture the asymmetric characteristics of uncertainties and maximize the risk-averse capability of the solution in a probabilistic manner. To account for the probabilistic consideration, the imprecise Dirichlet model is employed to construct the ambiguity sets of uncertainties, reducing reliance on precise probability distributions. Consequently, a two-stage robust optimal operation model for distribution networks using CL-DIGDT is developed. An iterative method is proposed to solve the model and determine the upper and lower bounds of the objective function. Case study demonstrates that the proposed approach yields a more robust and statistically optimized solution with required accuracy compared to existing method, contributing to a reduction in first-stage cost by 0.84%, second-stage average cost by 6.7%, and significantly increasing the reliability of the solution by 8%.

Paper number 157:
Title: RankByGene: Gene-Guided Histopathology Representation Learning Through Cross-Modal Ranking Consistency
Authors: Wentao Huang, Meilong Xu, Xiaoling Hu, Shahira Abousamra, Aniruddha Ganguly, Saarthak Kapse, Alisa Yurovsky, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Michael L. Miller, Chao Chen
Abstract: Spatial transcriptomics (ST) provides essential spatial context by mapping gene expression within tissue, enabling detailed study of cellular heterogeneity and tissue organization. However, aligning ST data with histology images poses challenges due to inherent spatial distortions and modality-specific variations. Existing methods largely rely on direct alignment, which often fails to capture complex cross-modal relationships. To address these limitations, we propose a novel framework that aligns gene and image features using a ranking-based alignment loss, preserving relative similarity across modalities and enabling robust multi-scale alignment. To further enhance the alignment's stability, we employ self-supervised knowledge distillation with a teacher-student network architecture, effectively mitigating disruptions from high dimensionality, sparsity, and noise in gene expression data. Extensive experiments on seven public datasets that encompass gene expression prediction, slide-level classification, and survival analysis demonstrate the efficacy of our method, showing improved alignment and predictive performance over existing methods.

Paper number 158:
Title: k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning
Authors: Yifan Yang, Jianheng Zhuo, Zengrui Jin, Ziyang Ma, Xiaoyu Yang, Zengwei Yao, Liyong Guo, Wei Kang, Fangjun Kuang, Long Lin, Daniel Povey, Xie Chen
Abstract: Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.

Paper number 159:
Title: CABLD: Contrast-Agnostic Brain Landmark Detection with Consistency-Based Regularization
Authors: Soorena Salari, Arash Harirpoush, Hassan Rivaz, Yiming Xiao
Abstract: Anatomical landmark detection in medical images is essential for various clinical and research applications, including disease diagnosis and surgical planning. However, manual landmark annotation is time-consuming and requires significant expertise. Existing deep learning (DL) methods often require large amounts of well-annotated data, which are costly to acquire. In this paper, we introduce CABLD, a novel self-supervised DL framework for 3D brain landmark detection in unlabeled scans with varying contrasts by using only a single reference example. To achieve this, we employed an inter-subject landmark consistency loss with an image registration loss while introducing a 3D convolution-based contrast augmentation strategy to promote model generalization to new contrasts. Additionally, we utilize an adaptive mixed loss function to schedule the contributions of different sub-tasks for optimal outcomes. We demonstrate the proposed method with the intricate task of MRI-based 3D brain landmark detection. With comprehensive experiments on four diverse clinical and public datasets, including both T1w and T2w MRI scans at different MRI field strengths, we demonstrate that CABLD outperforms the state-of-the-art methods in terms of mean radial errors (MREs) and success detection rates (SDRs). Our framework provides a robust and accurate solution for anatomical landmark detection, reducing the need for extensively annotated datasets and generalizing well across different imaging contrasts. Our code will be publicly available at: this https URL.

Paper number 160:
Title: Optimizing Coverage in Convex Quadrilateral Regions with a Single UAV
Authors: Alexander Vavoulas, Nicholas Vaiopoulos, Konstantinos K. Delibasis, Harilaos G. Sandalidis
Abstract: The integration of unmanned aerial vehicles (UAVs) into next-generation wireless networks is a promising solution for providing flexible, efficient coverage. This paper explores the optimal deployment of a single UAV to cover an arbitrary convex quadrilateral region, utilizing a directional antenna with a tiltable beam that produces an elliptical coverage footprint. We examine two distinct coverage scenarios: (i) the largest inscribed ellipse, which maximizes coverage within the quadrilateral while excluding the boundary, and (ii) the smallest circumscribed ellipse, ensuring complete coverage of the entire area. The study formulates an optimization framework that accounts for path loss, signal-to-noise ratio (SNR), and energy consumption to determine the optimal altitude of the UAV. By employing a simplified path loss model, we derive the altitude that minimizes maximum path loss, while also analyzing the impact of antenna directivity on maximizing the minimum SNR at the coverage boundary. Additionally, the UAV's energy consumption is evaluated, considering the power demands during hovering, forward flight, and vertical takeoff. Numerical simulations are presented to illustrate the trade-offs between coverage effectiveness, communication performance, and energy efficiency across various environmental conditions and antenna configurations.

Paper number 161:
Title: Merging synthetic and real embryo data for advanced AI predictions
Authors: Oriana Presacan, Alexandru Dorobantiu, Vajira Thambawita, Michael A. Riegler, Mette H. Stensen, Mario Iliceto, Alexandru C. Aldea, Akriti Sharma
Abstract: Accurate embryo morphology assessment is essential in assisted reproductive technology for selecting the most viable embryo. Artificial intelligence has the potential to enhance this process. However, the limited availability of embryo data presents challenges for training deep learning models. To address this, we trained two generative models using two datasets-one we created and made publicly available, and one existing public dataset-to generate synthetic embryo images at various cell stages, including 2-cell, 4-cell, 8-cell, morula, and blastocyst. These were combined with real images to train classification models for embryo cell stage prediction. Our results demonstrate that incorporating synthetic images alongside real data improved classification performance, with the model achieving 97% accuracy compared to 94.5% when trained solely on real data. This trend remained consistent when tested on an external Blastocyst dataset from a different clinic. Notably, even when trained exclusively on synthetic data and tested on real data, the model achieved a high accuracy of 92%. Furthermore, combining synthetic data from both generative models yielded better classification results than using data from a single generative model. Four embryologists evaluated the fidelity of the synthetic images through a Turing test, during which they annotated inaccuracies and offered feedback. The analysis showed the diffusion model outperformed the generative adversarial network, deceiving embryologists 66.6% versus 25.3% and achieving lower Frechet inception distance scores.

Paper number 162:
Title: Robust Optimal Safe and Stability Guaranteeing Reinforcement Learning Control for Quadcopter
Authors: Sanghyoup Gu, Ratnesh Kumar
Abstract: Recent advances in deep learning have provided new data-driven ways of controller design to replace the traditional manual synthesis and certification approaches. Employing neural network (NN) as controllers however, presents its own challenge: that of certifying stability due to their inherent complex nonlinearity, and while NN controllers have demonstrated high performance in complex systems, they often lack formal stability guarantees. This issue is further accentuated for critical nonlinear applications such as of unmanned aerial vehicles (UAVs), complicating their stability guarantees, whereas a lack of stability assurance raises the risk of critical damage or even complete failure under a loss of control. In this study, we improve a Robust, Optimal, Safe and Stability Guaranteed Training (ROSS-GT) method of [1] to design an NN controller for a quadcopter flight control. The approach ensures closed-loop system stability by finding a Lyapunov function, and providing a safe initial state domain that remains invariant under the control and guarantees stability to an equilibrium within it. Stability guaranteeing constraints are derived from the sector bound of the system nonlinearity and of its parameters and disturbance variations, in the form of a Lipschitz bound for a NN control. The control performance is further optimized by searching over the class of stability-guaranteeing controllers to minimize the reference tracking error and the control costs.

Paper number 163:
Title: Emergency-Brake Simplex: Toward A Verifiably Safe Control-CPS Architecture for Abrupt Runtime Reachability Constraint Changes
Authors: Henghua Shen, Qixin Wang
Abstract: When a system's constraints change abruptly, the system's reachability safety does no longer sustain. Thus, the system can reach a forbidden/dangerous value. Conventional remedy practically involves online controller redesign (OCR) to re-establish the reachability's compliance with the new constraints, which, however, is usually too slow. There is a need for an online strategy capable of managing runtime changes in reachability constraints. However, to the best of the authors' knowledge, this topic has not been addressed in the existing literature. In this paper, we propose a fast fault tolerance strategy to recover the system's reachability safety in runtime. Instead of redesigning the system's controller, we propose to change the system's reference state to modify the system's reachability to comply with the new constraints. We frame the reference state search as an optimization problem and employ the Karush-Kuhn-Tucker (KKT) method as well as the Interior Point Method (IPM) based Newton's method (as a fallback for the KKT method) for fast solution derivation. The optimization also allows more future fault tolerance. Numerical simulations demonstrate that our method outperforms the conventional OCR method in terms of computational efficiency and success rate. Specifically, the results show that the proposed method finds a solution $10^{2}$ (with the IPM based Newton's method) $\sim 10^{4}$ (with the KKT method) times faster than the OCR method. Additionally, the improvement rate of the success rate of our method over the OCR method is $40.81\%$ without considering the deadline of run time. The success rate remains at $49.44\%$ for the proposed method, while it becomes $0\%$ for the OCR method when a deadline of $1.5 \; seconds$ is imposed.

Paper number 164:
Title: Joint Transmit and Pinching Beamforming for Pinching Antenna Systems (PASS): Optimization-Based or Learning-Based?
Authors: Xiaoxia Xu, Xidong Mu, Yuanwei Liu, Arumugam Nallanathan
Abstract: A novel pinching antenna system (PASS)-enabled downlink multi-user multiple-input single-output (MISO) framework is proposed. PASS consists of multiple waveguides spanning over thousands of wavelength, which equip numerous low-cost dielectric particles, named pinching antennas (PAs), to radiate signals into free space. The positions of PAs can be reconfigured to change both the large-scale path losses and phases of signals, thus facilitating the novel pinching beamforming design. A sum rate maximization problem is formulated, which jointly optimizes the transmit and pinching beamforming to adaptively achieve constructive signal enhancement and destructive interference mitigation. To solve this highly coupled and nonconvex problem, both optimization-based and learning-based methods are proposed. 1) For the optimization-based method, a majorization-minimization and penalty dual decomposition (MM-PDD) algorithm is developed, which handles the nonconvex complex exponential component using a Lipschitz surrogate function and then invokes PDD for problem decoupling. 2) For the learning-based method, a novel Karush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which enables KKT solutions to be reconstructed in a data-driven manner by learning dual variables. Following this idea, a KDL-Tranformer algorithm is developed, which captures both inter-PA/inter-user dependencies and channel-state-information (CSI)-beamforming dependencies by attention mechanisms. Simulation results demonstrate that: i) The proposed PASS framework significantly outperforms conventional massive multiple input multiple output (MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve over 30% system performance than MM-PDD algorithm, while achieving a millisecond-level response on modern GPUs.

Paper number 165:
Title: Rotatable Antenna Enabled Wireless Communication System with Visual Recognition: A Prototype Implementation
Authors: Liang Dai, Beixiong Zheng, Yanhua Tan, Lipeng Zhu, Fangjiong Chen, Rui Zhang
Abstract: Rotatable antenna (RA) is an emerging technology that has great potential to exploit additional spatial degrees of freedom (DoFs) by flexibly altering the three-dimensional (3D) orientation/boresight of each antenna. In this demonstration, we present a prototype of the RA-enabled wireless communication system with a visual recognition module to evaluate the performance gains provided by the RA in practical environments. In particular, a mechanically-driven RA is developed by integrating a digital servo motor, a directional antenna, and a microcontroller, which enables the dynamic adjustment of the RA orientation. Moreover, the orientation adjustment of the RA is guided by the user's direction information provided by the visual recognition module, thereby significantly enhancing system response speed and self-orientation accuracy. The experimental results demonstrate that the RA-enabled communication system achieves significant improvement in communication coverage performance compared to the conventional fixed antenna system.

Paper number 166:
Title: Survey on Beyond Diagonal RIS Enabled 6G Wireless Networks: Fundamentals, Recent Advances, and Challenges
Authors: Wali Ullah Khan, Manzoor Ahmed, Chandan Kumar Sheemar, Marco Di Renzo, Eva Lagunas, Asad Mahmood, Syed Tariq Shah, Octavia A. Dobre, Jorge Querol, Symeon Chatzinotas
Abstract: Beyond Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) represent a groundbreaking innovation in sixth-generation (6G) wireless networks, enabling unprecedented control over wireless propagation environments compared to conventional diagonal RIS (D-RIS). This survey provides a comprehensive analysis of BD-RIS, detailing its architectures, operational principles, and mathematical modeling while highlighting its performance benefits. BD-RIS classifications, including single-connected, fully-connected, and group-connected architectures, and their reflective, transmissive, hybrid, and multi-sector operating modes are examined. Recent advances in BD-RIS-enabled 6G networks are reviewed, focusing on critical areas such as channel estimation, sum-rate and spectral efficiency optimization, energy efficiency enhancement, and security. The survey identifies fundamental challenges in BD-RIS research, including hardware design limitations, adaptive channel estimation, and the impact of non-ideal hardware effects. Future research directions for BD-RIS are proposed, emphasizing the integration of artificial intelligence and machine learning (AI/ML), joint optimization of communication and sensing, and enhanced physical layer security (PLS). This study concludes by underscoring BD-RIS's transformative potential to redefine 6G wireless networks, offering valuable insights and lessons for future research and development.

Paper number 167:
Title: Markerless Tracking-Based Registration for Medical Image Motion Correction
Authors: Luisa Neubig, Deirdre Larsen, Takeshi Ikuma, Markus Kopp, Melda Kunduk, Andreas M. Kist
Abstract: Our study focuses on isolating swallowing dynamics from interfering patient motion in videofluoroscopy, an X-ray technique that records patients swallowing a radiopaque bolus. These recordings capture multiple motion sources, including head movement, anatomical displacements, and bolus transit. To enable precise analysis of swallowing physiology, we aim to eliminate distracting motion, particularly head movement, while preserving essential swallowing-related dynamics. Optical flow methods fail due to artifacts like flickering and instability, making them unreliable for distinguishing different motion groups. We evaluated markerless tracking approaches (CoTracker, PIPs++, TAP-Net) and quantified tracking accuracy in key medical regions of interest. Our findings show that even sparse tracking points generate morphing displacement fields that outperform leading registration methods such as ANTs, LDDMM, and VoxelMorph. To compare all approaches, we assessed performance using MSE and SSIM metrics post-registration. We introduce a novel motion correction pipeline that effectively removes disruptive motion while preserving swallowing dynamics and surpassing competitive registration techniques.

Paper number 168:
Title: COVID 19 Diagnosis Analysis using Transfer Learning
Authors: Anjali Dharmik
Abstract: Coronaviruses, including SARS-CoV-2, are responsible for COVID-19, a highly transmissible disease that emerged in December 2019 in Wuhan, China. During the past five years, significant advancements have been made in understanding and mitigating the virus. Although the initial outbreak led to global health crises, improved vaccination strategies, antiviral treatments, and AI-driven diagnostic tools have contributed to better disease management. However, COVID-19 continues to pose risks, particularly for immuno-compromised individuals and those with pre-existing conditions. This study explores the use of deep learning for a rapid and accurate diagnosis of COVID-19, addressing ongoing challenges in healthcare infrastructure and testing accessibility. We propose an enhanced automated detection system leveraging state-of-the-art convolutional neural networks (CNNs), including updated versions of VGG16, VGG19, and ResNet50, to classify COVID-19 infections from chest radiographs and computerized tomography (CT) scans. Our results, based on an expanded dataset of over 6000 medical images, demonstrate that the optimized ResNet50 model achieves the highest classification performance, with 97.77% accuracy, 100% sensitivity, 93.33% specificity, and a 98.0% F1-score. These findings reinforce the potential of AI-assisted diagnostic tools in improving early detection and pandemic preparedness.

Paper number 169:
Title: Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life
Authors: Jingyuan Xue, Longfei Wei, Fang Sheng, Yuxin Gao, Jianfei Zhang
Abstract: The accurate prediction of RUL for lithium-ion batteries is crucial for enhancing the reliability and longevity of energy storage systems. Traditional methods for RUL prediction often struggle with issues such as data sparsity, varying battery chemistries, and the inability to capture complex degradation patterns over time. In this study, we propose a survival analysis-based framework combined with deep learning models to predict the RUL of lithium-ion batteries. Specifically, we utilize five advanced models: the Cox-type models (Cox, CoxPH, and CoxTime) and two machine-learning-based models (DeepHit and MTLR). These models address the challenges of accurate RUL estimation by transforming raw time-series battery data into survival data, including key degradation indicators such as voltage, current, and internal resistance. Advanced feature extraction techniques enhance the model's robustness in diverse real-world scenarios, including varying charging conditions and battery chemistries. Our models are tested using 10-fold cross-validation, ensuring generalizability and minimizing overfitting. Experimental results show that our survival-based framework significantly improves RUL prediction accuracy compared to traditional methods, providing a reliable tool for battery management and maintenance optimization. This study contributes to the advancement of predictive maintenance in battery technology, offering valuable insights for both researchers and industry practitioners aiming to enhance the operational lifespan of lithium-ion batteries.

Paper number 170:
Title: Removing Structured Noise with Diffusion Models
Authors: Tristan S.W. Stevens, Hans van Gorp, Faik C. Meral, Junseob Shin, Jason Yu, Jean-Luc Robert, Ruud J.G. van Sloun
Abstract: Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusion modeling for inverse problems in the context of non-Gaussian measurement models.

Paper number 171:
Title: Distributed Bayesian Estimation in Sensor Networks: Consensus on Marginal Densities
Authors: Parth Paritosh, Nikolay Atanasov, Sonia Martinez
Abstract: In this paper, we aim to design and analyze distributed Bayesian estimation algorithms for sensor networks. The challenges we address are to (i) derive a distributed provably-correct algorithm in the functional space of probability distributions over continuous variables, and (ii) leverage these results to obtain new distributed estimators restricted to subsets of variables observed by individual agents. This relates to applications such as cooperative localization and federated learning, where the data collected at any agent depends on a subset of all variables of interest. We present Bayesian density estimation algorithms using data from non-linear likelihoods at agents in centralized, distributed, and marginal distributed settings. After setting up a distributed estimation objective, we prove almost-sure convergence to the optimal set of pdfs at each agent. Then, we prove the same for a storage-aware algorithm estimating densities only over relevant variables at each agent. Finally, we present a Gaussian version of these algorithms and implement it in a mapping problem using variational inference to handle non-linear likelihood models associated with LiDAR sensing.

Paper number 172:
Title: Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems
Authors: Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues
Abstract: The networked nature of multi-robot systems presents challenges in the context of multi-agent reinforcement learning. Centralized control policies do not scale with increasing numbers of robots, whereas independent control policies do not exploit the information provided by other robots, exhibiting poor performance in cooperative-competitive tasks. In this work we propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, which accounts for the correlation among robots during training while overcoming the need of value function factorization. Extensive simulations in different multi-robot scenarios demonstrate the success of the proposed approach, surpassing previous multi-robot reinforcement learning solutions in scalability, while achieving similar or superior performance (with averaged cumulative reward up to x2 greater than the state-of-the-art with robot teams x6 larger than the number of robots at training time). We also validate our approach on multiple real robots in the Georgia Tech Robotarium under imperfect communication, demonstrating zero-shot sim-to-real transfer and scalability across number of robots.

Paper number 173:
Title: Ice-Filling: Near-Optimal Channel Estimation for Dense Array Systems
Authors: Mingyao Cui, Zijian Zhang, Linglong Dai, Kaibin Huang
Abstract: By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems (DASs) can fully unleash the multiplexing and diversity gains of limited apertures. To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers. To overcome this obstacle, this paper reveals that designing the observation matrix to exploit the high spatial correlation of DAS channels is crucial for realizing near-optimal Bayesian channel estimation. Specifically, we prove that the observation matrix design for channel estimation is equivalent to a time-domain duality of point-to-point multiple-input multiple-output precoding, except for the change in the total power constraint on the precoding matrix to the pilot-wise discrete power constraint on the observation matrix. Inspired by Bayesian regression, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case. Particularly, we prove that the ice-filling algorithm can be interpreted as a ``quantized" water-filling algorithm, wherein the latter's continuous power-allocation process is converted into the former's discrete pilot-assignment process. To support the near-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions. Finally, numerical results confirm that our proposed designs achieve the near-optimal channel estimation performance and outperform existing approaches significantly.

Paper number 174:
Title: A Greedy Quantum Route-Generation Algorithm
Authors: Jordan Makansi, David E. Bernal Neira
Abstract: Routing and scheduling problems with time windows have long been important optimization problems for logistics and planning. Many classical heuristics and exact methods exist for such problems. However, there are no satisfactory methods for generating routes using quantum computing (QC), for mainly two reasons: inequality constraints, and the trade-off of feasibility and solution quality. Inequality constraints are typically handled using slack variables; and feasible solutions are found by filtering samples. These challenges are amplified in the presence of noise inherent in QC. Here, we propose a greedy algorithm that generates routes by using information from all samples obtained from the quantum computer. By noticing the relationship between qubits in our formulation as a directed acyclic graph (DAG), we designed an algorithm that adaptively constructs a feasible solution. We prove its convergence to a feasible solution, and illustrate its efficacy by solving the Fleet Sizing Vehicle Routing Problem with Time Windows (FSVRPTW). Our computational results show that this method obtains a lower objective value than the current state-of-the-art annealing approaches, both classical and hybrid, for the same amount of time using D-Wave Hybrid Solvers. We also show its robustness to noise on D-Wave Advantage2 through computational results as compared to the filtering approach on DWaveSampler, even when the filtering approach is given a longer annealing time, and a larger sample size.

Paper number 175:
Title: Non-overshooting continuous in convergence sliding mode control of second-order systems
Authors: Michael Ruderman, Denis Efimov
Abstract: This paper proposes a novel nonlinear sliding mode state feedback controller for perturbed second-order systems. In analogy to a linear proportional-derivative (PD) feedback control, the proposed nonlinear scheme uses the output of interest and its time derivative. The control has only one free design parameter, and the closed-loop system is shown to possess uniform boundedness and finite-time convergence of trajectories in the presence of matched disturbances. We derive a strict Lyapunov function for the closed-loop control system with a bounded exogenous perturbation, and use it for both, the control parameter tuning and analysis of the finite-time convergence. The essential features of the proposed new control law is non-overshooting despite the unknown dynamic disturbances and the continuous control action during the convergence to zero equilibrium. Apart from the numerical results, a revealing experimental example is also shown in favor of the proposed control and in comparison with PD and sub-optimal nonlinear damping regulators.

Paper number 176:
Title: Certifiably Robust Policies for Uncertain Parametric Environments
Authors: Yannik Schnitzer, Alessandro Abate, David Parker
Abstract: We present a data-driven approach for producing policies that are provably robust across unknown stochastic environments. Existing approaches can learn models of a single environment as an interval Markov decision processes (IMDP) and produce a robust policy with a probably approximately correct (PAC) guarantee on its performance. However these are unable to reason about the impact of environmental parameters underlying the uncertainty. We propose a framework based on parametric Markov decision processes (MDPs) with unknown distributions over parameters. We learn and analyse IMDPs for a set of unknown sample environments induced by parameters. The key challenge is then to produce meaningful performance guarantees that combine the two layers of uncertainty: (1) multiple environments induced by parameters with an unknown distribution; (2) unknown induced environments which are approximated by IMDPs. We present a novel approach based on scenario optimisation that yields a single PAC guarantee quantifying the risk level for which a specified performance level can be assured in unseen environments, plus a means to trade-off risk and performance. We implement and evaluate our framework using multiple robust policy generation methods on a range of benchmarks. We show that our approach produces tight bounds on a policy's performance with high confidence.

Paper number 177:
Title: STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment
Authors: Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu, Rilin Chen, Dong Yu
Abstract: Visual and auditory perception are two crucial ways humans experience the world. Text-to-video generation has made remarkable progress over the past year, but the absence of harmonious audio in generated video limits its broader applications. In this paper, we propose Semantic and Temporal Aligned Video-to-Audio (STA-V2A), an approach that enhances audio generation from videos by extracting both local temporal and global semantic video features and combining these refined video features with text as cross-modal guidance. To address the issue of information redundancy in videos, we propose an onset prediction pretext task for local temporal feature extraction and an attentive pooling module for global semantic feature extraction. To supplement the insufficient semantic information in videos, we propose a Latent Diffusion Model with Text-to-Audio priors initialization and cross-modal guidance. We also introduce Audio-Audio Align, a new metric to assess audio-temporal alignment. Subjective and objective metrics demonstrate that our method surpasses existing Video-to-Audio models in generating audio with better quality, semantic consistency, and temporal alignment. The ablation experiment validated the effectiveness of each module. Audio samples are available at this https URL.

Paper number 178:
Title: Where are we in audio deepfake detection? A systematic analysis over generative and detection models
Authors: Xiang Li, Pin-Yu Chen, Wenqi Wei
Abstract: Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using generative Artificial Intelligence (AI) technology have made it possible to generate high-quality and realistic human-like audio. This poses growing challenges in distinguishing AI-synthesized speech from the genuine human voice and could raise concerns about misuse for impersonation, fraud, spreading misinformation, and scams. However, existing detection methods for AI-synthesized audio have not kept pace and often fail to generalize across diverse datasets. In this paper, we introduce SONAR, a synthetic AI-Audio Detection Framework and Benchmark, aiming to provide a comprehensive evaluation for distinguishing cutting-edge AI-synthesized auditory content. SONAR includes a novel evaluation dataset sourced from 9 diverse audio synthesis platforms, including leading TTS providers and state-of-the-art TTS models. It is the first framework to uniformly benchmark AI-audio detection across both traditional and foundation model-based detection systems. Through extensive experiments, (1) we reveal the limitations of existing detection methods and demonstrate that foundation models exhibit stronger generalization capabilities, likely due to their model size and the scale and quality of pretraining data. (2) Speech foundation models demonstrate robust cross-lingual generalization capabilities, maintaining strong performance across diverse languages despite being fine-tuned solely on English speech data. This finding also suggests that the primary challenges in audio deepfake detection are more closely tied to the realism and quality of synthetic audio rather than language-specific characteristics. (3) We explore the effectiveness and efficiency of few-shot fine-tuning in improving generalization, highlighting its potential for tailored applications, such as personalized detection systems for specific entities or individuals.

Paper number 179:
Title: Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility and Safety Barrier Functions
Authors: Minnan Zhou, Mustafa Shaikh, Vatsalya Chaubey, Patrick Haggerty, Shumon Koga, Dimitra Panagou, Nikolay Atanasov
Abstract: This paper develops a control strategy for pursuit-evasion problems in environments with occlusions. We address the challenge of a mobile pursuer keeping a mobile evader within its field of view (FoV) despite line-of-sight obstructions. The signed distance function (SDF) of the FoV is used to formulate visibility as a control barrier function (CBF) constraint on the pursuer's control inputs. Similarly, obstacle avoidance is formulated as a CBF constraint based on the SDF of the obstacle set. While the visibility and safety CBFs are Lipschitz continuous, they are not differentiable everywhere, necessitating the use of generalized gradients. To achieve non-myopic pursuit, we generate reference control trajectories leading to evader visibility using a sampling-based kinodynamic planner. The pursuer then tracks this reference via convex optimization under the CBF constraints. We validate our approach in CARLA simulations and real-world robot experiments, demonstrating successful visibility maintenance using only onboard sensing, even under severe occlusions and dynamic evader movements.

Paper number 180:
Title: Optical-Flow Guided Prompt Optimization for Coherent Video Generation
Authors: Hyelin Nam, Jaemin Kim, Dohun Lee, Jong Chul Ye
Abstract: While text-to-video diffusion models have made significant strides, many still face challenges in generating videos with temporal consistency. Within diffusion frameworks, guidance techniques have proven effective in enhancing output quality during inference; however, applying these methods to video diffusion models introduces additional complexity of handling computations across entire sequences. To address this, we propose a novel framework called MotionPrompt that guides the video generation process via optical flow. Specifically, we train a discriminator to distinguish optical flow between random pairs of frames from real videos and generated ones. Given that prompts can influence the entire video, we optimize learnable token embeddings during reverse sampling steps by using gradients from a trained discriminator applied to random frame pairs. This approach allows our method to generate visually coherent video sequences that closely reflect natural motion dynamics, without compromising the fidelity of the generated content. We demonstrate the effectiveness of our approach across various models.

Paper number 181:
Title: Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion
Authors: Jona Ballé, Luca Versari, Emilien Dupont, Hyunjik Kim, Matthias Bauer
Abstract: Inspired by the success of generative image models, recent work on learned image compression increasingly focuses on better probabilistic models of the natural image distribution, leading to excellent image quality. This, however, comes at the expense of a computational complexity that is several orders of magnitude higher than today's commercial codecs, and thus prohibitive for most practical applications. With this paper, we demonstrate that by focusing on modeling visual perception rather than the data distribution, we can achieve a very good trade-off between visual quality and bit rate similar to "generative" compression models such as HiFiC, while requiring less than 1% of the multiply-accumulate operations (MACs) for decompression. We do this by optimizing C3, an overfitted image codec, for Wasserstein Distortion (WD), and evaluating the image reconstructions with a human rater study, showing that WD clearly outperforms LPIPS as an optimization objective. The study also reveals that WD outperforms other perceptual metrics such as LPIPS, DISTS, and MS-SSIM as a predictor of human ratings, remarkably achieving over 94% Pearson correlation with Elo scores.

Paper number 182:
Title: QSM-RimDS: A detection and segmentation tool for paramagnetic rim lesions in multiple sclerosis
Authors: Ha Luu, Mert Sisman, Ilhami Kovanlikaya, Tam Vu, Pascal Spincemaille, Yi Wang, Francesca Bagnato, Susan Gauthier, Thanh Nguyen
Abstract: Paramagnetic rim lesions (PRLs) are an emerging biomarker in multiple sclerosis (MS). Manual identification and rim segmentation of PRLs on quantitative susceptibility mapping (QSM) images are time-consuming. Deep learning-based QSM-RimNet can provide automated PRL detection, but this method does not provide rim segmentation for microglial density quantification and requires precise QSM lesion masks. The purpose of this study is to develop a U-Net-based QSM-RimDS method for joint PRL detection and rim segmentation using readily available T2-weighted (T2W) fluid-attenuated inversion recovery (FLAIR) lesion masks. Two expert readers performed PRL classification and rim segmentation as the reference. Dice similarity coefficient (DSC) was used to assess the agreement between rim segmentation obtained by QSM-RimDS and the manual expert segmentation. The PRL detection performances of QSM-RimDS and QSM-RimNet were evaluated using receiver operating characteristic (ROC) and precision-recall (PR) plots in a five-fold cross validation. A total of 260 PRLs (3.3\%) and 7720 non-PRLs (96.7\%) were identified by the readers. Compared to the expert rim segmentation, QSM-RimDS provided a mean DSC of 0.57 \pm 0.02 with moderate to high agreement (DSC \leq 0.5) in 73.8pm 5.7\% of PRLs over five folds. QSM-RimDS produced better and more consistent detection performance with a mean area under curve (AUC) of 0.754 \pm 0.037 vs. 0.514 \pm 0.121 by QSM-RimNet (46.7\% improvement) on PR plots, and 0.956 \pm 0.034 vs. 0.908 \pm 0.073 (5.3\% improvement) on ROC plots. In conclusion, QSM-RimDS improves PRL detection accuracy compared to QSM-RimNet and unlike QSM-RimNet can provide reasonably accurate rim segmentation.

Paper number 183:
Title: Quantifying Climate Change Impacts on Renewable Energy Generation: A Super-Resolution Recurrent Diffusion Model
Authors: Xiaochong Dong, Jun Dan, Yingyun Sun, Yang Liu, Xuemin Zhang, Shengwei Mei
Abstract: Driven by global climate change and the ongoing energy transition, the coupling between power supply capabilities and meteorological factors has become increasingly significant. Over the long term, accurately quantifying the power generation of renewable energy under the influence of climate change is essential for the development of sustainable power systems. However, due to interdisciplinary differences in data requirements, climate data often lacks the necessary hourly resolution to capture the short-term variability and uncertainties of renewable energy resources. To address this limitation, a super-resolution recurrent diffusion model (SRDM) has been developed to enhance the temporal resolution of climate data and model the short-term uncertainty. The SRDM incorporates a pre-trained decoder and a denoising network, that generates long-term, high-resolution climate data through a recurrent coupling mechanism. The high-resolution climate data is then converted into power value using the mechanism model, enabling the simulation of wind and photovoltaic (PV) power generation on future long-term scales. Case studies were conducted in the Ejina region of Inner Mongolia, China, using fifth-generation reanalysis (ERA5) and coupled model intercomparison project (CMIP6) data under two climate pathways: SSP126 and SSP585. The results demonstrate that the SRDM outperforms existing generative models in generating super-resolution climate data. Furthermore, the research highlights the estimation biases introduced when low-resolution climate data is used for power conversion.

Paper number 184:
Title: CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval
Authors: Mohammad Mahdi Abootorabi, Ehsaneddin Asgari
Abstract: This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During training, we utilize our newly introduced speech-text dataset, which encompasses 15 diverse categories ranging from fiction to religion. CLASP's audio component integrates audio spectrograms with a pre-trained self-supervised speech model, while its language encoding counterpart employs a sentence encoder pre-trained on over 100 languages. This unified lightweight model bridges the gap between various modalities and languages, enhancing its effectiveness in handling and retrieving multilingual and multimodal data. Our evaluations across multiple languages demonstrate that CLASP establishes new benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional ASR-based retrieval methods that rely on transcribing speech into text for subsequent text retrieval, especially in specific scenarios.

Paper number 185:
Title: MotionMap: Representing Multimodality in Human Pose Forecasting
Authors: Reyhaneh Hosseininejad, Megh Shukla, Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi
Abstract: Human pose forecasting is inherently multimodal since multiple futures exist for an observed pose sequence. However, evaluating multimodality is challenging since the task is ill-posed. Therefore, we first propose an alternative paradigm to make the task well-posed. Next, while state-of-the-art methods predict multimodality, this requires oversampling a large volume of predictions. This raises key questions: (1) Can we capture multimodality by efficiently sampling a smaller number of predictions? (2) Subsequently, which of the predicted futures is more likely for an observed pose sequence? We address these questions with MotionMap, a simple yet effective heatmap based representation for multimodality. We extend heatmaps to represent a spatial distribution over the space of all possible motions, where different local maxima correspond to different forecasts for a given observation. MotionMap can capture a variable number of modes per observation and provide confidence measures for different modes. Further, MotionMap allows us to introduce the notion of uncertainty and controllability over the forecasted pose sequence. Finally, MotionMap captures rare modes that are non-trivial to evaluate yet critical for safety. We support our claims through multiple qualitative and quantitative experiments using popular 3D human pose datasets: Human3.6M and AMASS, highlighting the strengths and limitations of our proposed method. Project Page: this https URL

Paper number 186:
Title: Electrostatic Clutches Enable Simultaneous Mechanical Multiplexing
Authors: Timothy E. Amish, Jeffrey T. Auletta, Chad C. Kessens, Joshua R. Smith, Jeffrey I. Lipton
Abstract: Actuating robotic systems with multiple degrees of freedom (DoF) traditionally requires numerous motors, leading to increased size, weight, cost, and power consumption. Mechanical multiplexing offers a solution by enabling a single actuator to control multiple DoF. However, existing multiplexers have either been limited to electrically controlled time-based multiplexing that control one DoF at a time or have relied on mechanical switching to control multiple DoF simultaneously. There is a strong need for a system that can perform electrically controlled multiplexing for both time-based and simultaneous control of multiple DoF. This study introduces a novel electrostatic capstan clutch-based mechanical multiplexer that enables high-force, single-motor control of multiple DoF. Here, we show that our system achieves both single-input-single-output (SISO) and single-input-multipleoutput (SIMO) actuation, allowing bidirectional control and position holding with minimal power consumption. Each output can actuate a 22.24 N load, limited by clutch performance, up to 5 cm. The number of outputs and actuation length is currently limited by the length of the drive shaft. We demonstrate the integration of our system into a 4-DoF commercial robotic hand using a single motor. These findings show that electrostatic clutchbased multiplexing provides a scalable and energy-efficient design solution for high-DoF robotic platforms, opening new possibilities for lightweight and power-efficient actuation in robotics.

Paper number 187:
Title: MusicEval: A Generative Music Dataset with Expert Ratings for Automatic Text-to-Music Evaluation
Authors: Cheng Liu, Hui Wang, Jinghua Zhao, Shiwan Zhao, Hui Bu, Xin Xu, Jiaming Zhou, Haoqin Sun, Yong Qin
Abstract: The technology for generating music from textual descriptions has seen rapid advancements. However, evaluating text-to-music (TTM) systems remains a significant challenge, primarily due to the difficulty of balancing performance and cost with existing objective and subjective evaluation methods. In this paper, we propose an automatic assessment task for TTM models to align with human perception. To address the TTM evaluation challenges posed by the professional requirements of music evaluation and the complexity of the relationship between text and music, we collect MusicEval, the first generative music assessment dataset. This dataset contains 2,748 music clips generated by 31 advanced and widely used models in response to 384 text prompts, along with 13,740 ratings from 14 music experts. Furthermore, we design a CLAP-based assessment model built on this dataset, and our experimental results validate the feasibility of the proposed task, providing a valuable reference for future development in TTM evaluation. The dataset is available at this https URL.

Paper number 188:
Title: Safe Gradient Flow for Bilevel Optimization
Authors: Sina Sharifi, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, Mahyar Fazlyab
Abstract: Bilevel optimization is a key framework in hierarchical decision-making, where one problem is embedded within the constraints of another. In this work, we propose a control-theoretic approach to solving bilevel optimization problems. Our method consists of two components: a gradient flow mechanism to minimize the upper-level objective and a safety filter to enforce the constraints imposed by the lower-level problem. Together, these components form a safe gradient flow that solves the bilevel problem in a single loop. To improve scalability with respect to the lower-level problem's dimensions, we introduce a relaxed formulation and design a compact variant of the safe gradient flow. This variant minimizes the upper-level objective while ensuring the lower-level decision variable remains within a user-defined suboptimality. Using Lyapunov analysis, we establish convergence guarantees for the dynamics, proving that they converge to a neighborhood of the optimal solution. Numerical experiments further validate the effectiveness of the proposed approaches. Our contributions provide both theoretical insights and practical tools for efficiently solving bilevel optimization problems.

Paper number 189:
Title: S2CFormer: Revisiting the RD-Latency Trade-off in Transformer-based Learned Image Compression
Authors: Yunuo Chen, Qian Li, Bing He, Donghui Feng, Ronghua Wu, Qi Wang, Li Song, Guo Lu, Wenjun Zhang
Abstract: Transformer-based Learned Image Compression (LIC) suffers from a suboptimal trade-off between decoding latency and rate-distortion (R-D) performance. Moreover, the critical role of the FeedForward Network (FFN)-based channel aggregation module has been largely overlooked. Our research reveals that efficient channel aggregation-rather than complex and time-consuming spatial operations-is the key to achieving competitive LIC models. Based on this insight, we initiate the ``S2CFormer'' paradigm, a general architecture that simplifies spatial operations and enhances channel operations to overcome the previous trade-off. We present two instances of the S2CFormer: S2C-Conv, and S2C-Attention. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. Furthermore, we introduce S2C-Hybrid, an enhanced variant that maximizes the strengths of different S2CFormer instances to achieve a better performance-latency trade-off. This model outperforms all the existing methods on the Kodak, Tecnick, and CLIC Professional Validation datasets, setting a new benchmark for efficient and high-performance LIC. The code is at \href{this https URL}{this https URL}.

Paper number 190:
Title: Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment
Authors: Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David Mortensen
Abstract: Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features.

Paper number 191:
Title: Omnidirectional Multi-Object Tracking
Authors: Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
Abstract: Panoramic imagery, with its 360° field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in panoramic field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The established dataset and source code are available at this https URL.

Paper number 192:
Title: Bimodal Connection Attention Fusion for Speech Emotion Recognition
Authors: Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss
Abstract: Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intra- and inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines.

Paper number 193:
Title: Heterogeneous bimodal attention fusion for speech emotion recognition
Authors: Jiachen Luo, Huy Phan, Lin Wang, Joshua Reiss
Abstract: Multi-modal emotion recognition in conversations is a challenging problem due to the complex and complementary interactions between different modalities. Audio and textual cues are particularly important for understanding emotions from a human perspective. Most existing studies focus on exploring interactions between audio and text modalities at the same representation level. However, a critical issue is often overlooked: the heterogeneous modality gap between low-level audio representations and high-level text representations. To address this problem, we propose a novel framework called Heterogeneous Bimodal Attention Fusion (HBAF) for multi-level multi-modal interaction in conversational emotion recognition. The proposed method comprises three key modules: the uni-modal representation module, the multi-modal fusion module, and the inter-modal contrastive learning module. The uni-modal representation module incorporates contextual content into low-level audio representations to bridge the heterogeneous multi-modal gap, enabling more effective fusion. The multi-modal fusion module uses dynamic bimodal attention and a dynamic gating mechanism to filter incorrect cross-modal relationships and fully exploit both intra-modal and inter-modal interactions. Finally, the inter-modal contrastive learning module captures complex absolute and relative interactions between audio and text modalities. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed HBAF method outperforms existing state-of-the-art baselines.
    