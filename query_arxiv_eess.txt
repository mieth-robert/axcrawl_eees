
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations
Authors: Hanyang He, John Harlim, Daning Huang, Yan Li
Abstract: Model predictive control (MPC)-based energy management systems (EMS) are essential for ensuring optimal, secure, and stable operation in microgrids with high penetrations of distributed energy resources. However, due to the high computational cost for the decision-making, the conventional MPC-based EMS typically adopts a simplified integrated-bus power balance model. While this simplification is effective for small networks, large-scale systems require a more detailed branch flow model to account for the increased impact of grid power losses and security constraints. This work proposes an efficient and reliable MPC-based EMS that incorporates power-loss effects and grid-security constraints. %, while adaptively shaping the battery power profile in response to online renewable inputs, achieving reduced operational costs. It enhances system reliability, reduces operational costs, and shows strong potential for online implementation due to its reduced computational effort. Specifically, a second-order cone program (SOCP) branch flow relaxation is integrated into the constraint set, yielding a convex formulation that guarantees globally optimal solutions with high computational efficiency. Owing to the radial topology of the microgrid, this relaxation is practically tight, ensuring equivalence to the original problem. Building on this foundation, an online demand response (DR) module is designed to further reduce the operation cost through peak shaving. To the best of our knowledge, no prior MPC-EMS framework has simultaneously modeled losses and security constraints while coordinating flexible loads within a unified architecture. The developed framework enables secure operation with effective peak shaving and reduced total cost. The effectiveness of the proposed method is validated on 10-bus, 18-bus, and 33-bus systems.

Paper number 2:
Title: Adaptive Cruise Control in Autonomous Vehicles: Challenges, Gaps, Comprehensive Review, and, Future Directions
Authors: Shradha Bavalatti, Yash Kangralkar, Santosh Pattar, Veena P Badiger
Abstract: The development of Autonomous Vehicles (AVs) has redefined the way of transportation by eliminating the need for human intervention in driving. This revolution is fueled by rapid advancements in adaptive cruise control (ACC), which make AVs capable of interpreting their surroundings and responding intelligently. While AVs offer significant advantages, such as enhanced safety and improved traffic efficiency, they also face several challenges that need to be addressed. Existing survey papers often lack a comprehensive analysis of these challenges and their potential solutions. Our paper stands out by meticulously identifying these gaps in current ACC research and offering impactful future directions to guide researchers in designing next-generation ACC systems. Our survey provides a detailed and systematic review, addressing the limitations of previous studies and proposing innovative approaches to achieve sustainable and fault-resilient urban transportation.

Paper number 3:
Title: On Architectures for Combining Reinforcement Learning and Model Predictive Control with Runtime Improvements
Authors: Xiaolong Jia, Nikhil Bajaj
Abstract: Model Predictive Control (MPC) faces computational demands and performance degradation from model inaccuracies. We propose two architectures combining Neural Network-approximated MPC (NNMPC) with Reinforcement Learning (RL). The first, Warm Start RL, initializes the RL actor with pre-trained NNMPC weights. The second, RLMPC, uses RL to generate corrective residuals for NNMPC outputs. We introduce a downsampling method reducing NNMPC input dimensions while maintaining performance. Evaluated on a rotary inverted pendulum, both architectures demonstrate runtime reductions exceeding 99% compared to traditional MPC while improving tracking performance under model uncertainties, with RL+MPC achieving 11-40% cost reduction depending on reference amplitude.

Paper number 4:
Title: Viability-Preserving Passive Torque Control
Authors: Zizhe Zhang, Yicong Wang, Zhiquan Zhang, Tianyu Li, Nadia Figueroa
Abstract: Conventional passivity-based torque controllers for manipulators are typically unconstrained, which can lead to safety violations under external perturbations. In this paper, we employ viability theory to pre-compute safe sets in the state-space of joint positions and velocities. These viable sets, constructed via data-driven and analytical methods for self-collision avoidance, external object collision avoidance and joint-position and joint-velocity limits, provide constraints on joint accelerations and thus joint torques via the robot dynamics. A quadratic programming-based control framework enforces these constraints on a passive controller tracking a dynamical system, ensuring the robot states remain within the safe set in an infinite time horizon. We validate the proposed approach through simulations and hardware experiments on a 7-DoF Franka Emika manipulator. In comparison to a baseline constrained passive controller, our method operates at higher control-loop rates and yields smoother trajectories.

Paper number 5:
Title: Real-time nonlinear inversion of magnetic resonance elastography with operator learning
Authors: Juampablo E. Heras Rivera, Caitlin M. Neher, Mehmet Kurt
Abstract: $\textbf{Purpose:}$ To develop and evaluate an operator learning framework for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE) data, which enables real-time inversion of elastograms with comparable spatial accuracy to NLI. $\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data from 61 individuals (mean age, 37.4 years; 34 female) were used for development of the framework. A predictive deep operator learning framework (oNLI) was trained using 10-fold cross-validation, with the complex curl of the measured displacement field as inputs and NLI-derived reference elastograms as outputs. A structural prior mechanism, analogous to Soft Prior Regularization in the MRE literature, was incorporated to improve spatial accuracy. Subject-level evaluation metrics included Pearson's correlation coefficient, absolute relative error, and structural similarity index measure between predicted and reference elastograms across brain regions of different sizes to understand accuracy. Statistical analyses included paired t-tests comparing the proposed oNLI variants to the convolutional neural network baselines. $\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\pm$ 0.5 ($\mu'$) and 10.0 $\pm$ 0.7 ($\mu''$) for oNLI and 15.8 $\pm$ 0.8 ($\mu'$) and 26.1 $\pm$ 1.1 ($\mu''$) for CNNs. Additionally, oNLI outperformed convolutional architectures as per Pearson's correlation coefficient, $r$, in the whole brain and across all subregions for both the storage modulus and loss modulus (p < 0.05). $\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion (30,000x speedup), outperforming CNN-based approaches and maintaining the fine-grained spatial accuracy achievable with NLI in the brain.

Paper number 6:
Title: Machine Learning-Driven Prediction of Lithium-Ion Battery Power Capability for eVTOL Aircraft
Authors: Hao Tu, Yebin Wang, Shaoshuai Mou, Huazhen Fang
Abstract: Electric vertical take-off and landing (eVTOL) aircraft have emerged as a promising solution to transform urban transportation. They present a few technical challenges for battery management, a prominent one of which is the prediction of the power capability of their lithium-ion battery systems. The challenge originates from the high C-rate discharging conditions required during eVTOL flights as well as the complexity of lithium-ion batteries' electro-thermal dynamics. This paper, for the first time, formulates a power limit prediction problem for eVTOL which explicitly considers long prediction horizons and the possible occurrence of emergency landings. We then harness machine learning to solve this problem in two intertwined ways. First, we adopt a dynamic model that integrates physics with machine learning to predict a lithium-ion battery's voltage and temperature behaviors with high accuracy. Second, while performing search for the maximum power, we leverage machine learning to predict the remaining discharge time and use the prediction to accelerate the search with fast computation. Our validation results show the effectiveness of the proposed study for eVTOL operations.

Paper number 7:
Title: COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques
Authors: Boyang Chen, Mohd Tasleem Khan, George Goussetis, Mathini Sellathurai, Yuan Ding, JoÃ£o F. C. Mota
Abstract: Convolutional Neural Networks (CNNs) are highly effective for computer vision and pattern recognition tasks; however, their computational intensity and reliance on hardware such as FPGAs pose challenges for deployment on low-power edge devices. In this work, we present COMET, a framework of CNN designs that employ efficient hardware offset-binary coding (OBC) techniques to enable co-optimization of performance and resource utilization. The approach formulates CNN inference with OBC representations of inputs (Scheme A) and weights (Scheme B) separately, enabling exploitation of bit-width asymmetry. The shift-accumulate operation is modified by incorporating the offset term with the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we introduce four novel look-up table (LUT) techniques -- parallel, shared, split, and hybrid -- and analyze them to identify the most efficient options. Building on this foundation, we develop an OBC-based general matrix multiplication core using the im2col transformation, enabling efficient acceleration of a fixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the proposed co-optimization approach significantly reduces resource utilization compared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on accuracy.

Paper number 8:
Title: How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan African Population Using Segmentation-Aware Data Augmentation and Model Ensembling
Authors: Claudia Takyi Ankomah, Livingstone Eli Ayivor, Ireneaus Nyame, Leslie Wambo, Patrick Yeboah Bonsu, Aondona Moses Iorumbur, Raymond Confidence, Toufiq Musah
Abstract: Brain tumors, particularly gliomas, pose significant chall-enges due to their complex growth patterns, infiltrative nature, and the variability in brain structure across individuals, which makes accurate diagnosis and monitoring difficult. Deep learning models have been developed to accurately delineate these tumors. However, most of these models were trained on relatively homogenous high-resource datasets, limiting their robustness when deployed in underserved regions. In this study, we performed segmentation-aware offline data augmentation on the BraTS-Africa dataset to increase the data sample size and diversity to enhance generalization. We further constructed an ensemble of three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to leverage their complementary strengths. Our best-performing model, MedNeXt, was trained on 1000 epochs and achieved the highest average lesion-wise dice and normalized surface distance scores of 0.86 and 0.81 respectively. However, the ensemble model trained for 500 epochs produced the most balanced segmentation performance across the tumour subregions. This work demonstrates that a combination of advanced augmentation and model ensembling can improve segmentation accuracy and robustness on diverse and underrepresented datasets. Code available at: this https URL

Paper number 9:
Title: Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems
Authors: Tuo Wu, Kwai-Man Luk, Jie Tang, Kai-Kit Wong, Jianchao Zheng, Baiyang Liu, David Morales-Jimenez, Maged Elkashlan, Kin-Fai Tong, Chan-Byoung Chae, Fumiyuki Adachi, George K. Karagiannidis
Abstract: Fluid antenna systems (FAS) are emerging as a transformative enabler for sixth-generation (6G) wireless communications, providing unprecedented spatial diversity through dynamic reconfiguration of antenna ports. However, the inherent spatial correlation among ports poses significant challenges for accurate analysis. Conventional models such as Jakes are analytically intractable, while oversimplified constant-correlation models fail to capture the true behavior. In this work, we address these challenges by applying the variable block-correlation model (VBCM) -- originally proposed by RamÃ­rez-Espinosa \textit{et al.} in 2024 -- to FAS security analysis, and by developing comprehensive optimization methods to enhance analytical accuracy. We derive new closed-form expressions for average secrecy capacity (ASC) and secrecy outage probability (SOP), demonstrating that the VBCM framework achieves simulation-aligned accuracy, with relative errors consistently below $5\%$ (compared to $10$--$15\%$ for constant-correlation models). To maximize ASC, we further design two algorithms: a grid search (GS) method and a gradient descent (GD) method. Numerical results reveal that the VBCM-based approach not only provides reliable insights into FAS security performance, but also yields substantial gains -- ASC improvements exceeding $120\%$ in high-threat scenarios and $18$--$19\%$ performance enhancements for compact antenna configurations. These findings underscore the practical value of integrating VBCM into FAS security analysis and optimization, establishing it as a powerful tool for advancing 6G communication systems.

Paper number 10:
Title: Learning Safety-Compatible Observers for Unknown Systems
Authors: Juho Bae, Daegyeong Roh, Han-Lim Choi
Abstract: This paper presents a data-driven approach for jointly learning a robust full-state observer and its robustness certificate for systems with unknown dynamics. Leveraging incremental input-to-state stability (delta ISS) notions, we jointly learn a delta ISS Lyapunov function that serves as the robustness certificate and prove practical convergence of the estimation error under standard fidelity assumptions on the learned models. This renders the observer safety-compatible: they can be consumed by certificate-based safe controllers so that, when the controller tolerates bounded estimation error, the controller's certificate remains valid under output feedback. We further extend the approach to interconnected systems via the small-gain theorem, yielding a distributed observer design framework. We validate the approach on a variety of nonlinear systems.

Paper number 11:
Title: On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models
Authors: Jun Tong
Abstract: Significant studies on communications over doubly selective channels have utilized on-grid DD channel models, which are previously investigated in Bello's seminar paper in 1963. The DD grid is typically specified by the bandwidth and time duration of the transmission frames. However, the physical channels are determined by the propagation environments and they are typically off-grid. Hence, there is often a gap between an actual physical channel and the on-grid model. This paper revisits the on-grid modeling of practical physical channels. We study the associated on-grid DD-domain representations for continuous-time, doubly selective channels with off-grid delay and Doppler shifts, accounting for practical time/frequency-domain windowing at the transceivers. The universal models obtained are applicable under the mild assumption that the windows have finite supports, and they extend Bello's classical results to account for more general windows. We also discuss the features and implications of the equivalent on-grid models.

Paper number 12:
Title: Pinching Antenna Systems (PASS) for Cell-Free Communications
Authors: Haochen Li
Abstract: A pinching antenna system (PASS) assisted cell-free communication system is proposed. A sum rate maximization problem under the BS power budget constraint and PA deployment constraint is formulated. To tackle the proposed non-convex optimization problem, an alternating optimization (AO) algorithm is developed. In particular, the digital beamforming sub-problem is solved using the weighted minimum mean square error (WMMSE) method, whereas the pinching beamforming sub-problem is handled via a penalty based approach combined with element-wise optimization. Simulation results demonstrate that: 1) the PASS assisted cell-free systems achieve superior performance over benchmark schemes; 2) increasing the number of PAs per waveguides can improve the advantage of PASS assisted cell-free systems; and 3) the cell-free architecture mitigates the average user rate degradation as the number of users increases.

Paper number 13:
Title: Scaling Multi-Talker ASR with Speaker-Agnostic Activity Streams
Authors: Xiluo He, Alexander Polok, JesÃºs Villalba, Thomas Thebaud, Matthew Maciejewski
Abstract: An increasingly common training paradigm for multi-talker automatic speech recognition (ASR) is to use speaker activity signals to adapt single-speaker ASR models for overlapping speech. Although effective, these systems require running the ASR model once per speaker, resulting in inference costs that scale with the number of speakers and limiting their practicality. In this work, we propose a method that decouples the inference cost of activity-conditioned ASR systems from the number of speakers by converting speaker-specific activity outputs into two speaker-agnostic streams. A central challenge is that naÃ¯vely merging speaker activities into streams significantly degrades recognition, since pretrained ASR models assume contiguous, single-speaker inputs. To address this, we design new heuristics aimed at preserving conversational continuity and maintaining compatibility with existing systems. We show that our approach is compatible with Diarization-Conditioned Whisper (DiCoW) to greatly reduce runtimes on the AMI and ICSI meeting datasets while retaining competitive performance.

Paper number 14:
Title: Cyber Resilience of Three-phase Unbalanced Distribution System Restoration under Sparse Adversarial Attack on Load Forecasting
Authors: Chen Chao, Zixiao Ma, Ziang Zhang
Abstract: System restoration is critical for power system resilience, nonetheless, its growing reliance on artificial intelligence (AI)-based load forecasting introduces significant cybersecurity risks. Inaccurate forecasts can lead to infeasible planning, voltage and frequency violations, and unsuccessful recovery of de-energized segments, yet the resilience of restoration processes to such attacks remains largely unexplored. This paper addresses this gap by quantifying how adversarially manipulated forecasts impact restoration feasibility and grid security. We develop a gradient-based sparse adversarial attack that strategically perturbs the most influential spatiotemporal inputs, exposing vulnerabilities in forecasting models while maintaining stealth. We further create a restoration-aware validation framework that embeds these compromised forecasts into a sequential restoration model and evaluates operational feasibility using an unbalanced three-phase optimal power flow formulation. Simulation results show that the proposed approach is more efficient and stealthier than baseline attacks. It reveals system-level failures, such as voltage and power ramping violations that prevent the restoration of critical loads. These findings provide actionable insights for designing cybersecurity-aware restoration planning frameworks.

Paper number 15:
Title: Optimal Energy Management in Indoor Farming Using Lighting Flexibility and Intelligent Model Predictive Control
Authors: Mohammadjavad Abbaspour, Mukund R. Shukla, Praveen K. Saxena, Shivam Saxena
Abstract: Indoor farming enables year-round food production but its reliance on artificial lighting significantly increases energy consumption, peak load charges, and energy costs for growers. Recent studies indicate that plants are able to tolerate interruptions in light, enabling the design of 24-hour lighting schedules (or "recipes") with strategic light modulation in alignment with day-ahead pricing. Thus, we propose an optimal lighting control strategy for indoor farming that modulates light intensity and photoperiod to reduce energy costs. The control strategy is implemented within a model predictive control framework and augmented with transformer-based neural networks to forecast 24-hour ahead solar radiation and electricity prices to improve energy cost reduction. The control strategy is informed by real-world experimentation on lettuce crops to discover minimum light exposure and appropriate dark-light intervals, which are mathematically formulated as constraints to maintain plant health. Simulations for a one-hectare greenhouse, based on real electricity market data from Ontario, demonstrate an annual cost reduction of $318,400 (20.9%), a peak load decrease of 1.6 MW (33.32%), and total energy savings of 1890 MWh (20.2%) against a baseline recipe. These findings highlight the potential of intelligent lighting control to improve the sustainability and economic feasibility of indoor farming.

Paper number 16:
Title: Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition
Authors: Martin Kocour, Martin Karafiat, Alexander Polok, Dominik Klement, LukÃ¡Å¡ Burget, Jan ÄernockÃ½
Abstract: We propose a speaker-attributed (SA) Whisper-based model for multi-talker speech recognition that combines target-speaker modeling with serialized output training (SOT). Our approach leverages a Diarization-Conditioned Whisper (DiCoW) encoder to extract target-speaker embeddings, which are concatenated into a single representation and passed to a shared decoder. This enables the model to transcribe overlapping speech as a serialized output stream with speaker tags and timestamps. In contrast to target-speaker ASR systems such as DiCoW, which decode each speaker separately, our approach performs joint decoding, allowing the decoder to condition on the context of all speakers simultaneously. Experiments show that the model outperforms existing SOT-based approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).

Paper number 17:
Title: Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?
Authors: Fanghao Xia, Zesong Fei, Xinyi Wang, Nanchi Su, Zhaolin Wang, Yuanwei Liu, Jie Xu
Abstract: In this paper, sensing-assisted secure communication in a multi-user multi-eavesdropper integrated sensing and communication (ISAC) system is investigated. Confidential communication signals and dedicated sensing signals are jointly transmitted by a base station (BS) to simultaneously serve users and sense aerial eavesdroppers (AEs). A sum rate maximization problem is formulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and sensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A fractional-programming-based alternating optimization algorithm is developed to solve this problem for fully digital arrays, where successive convex approximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle non-convex constraints. Furthermore, the minimum number of dedicated sensing beams is analyzed via a worst-case rank bound, upon which the proposed beamforming design is further extended to the hybrid analog-digital (HAD) array architecture, where the unit-modulus constraint is addressed by manifold optimization. Simulation results demonstrate that only a small number of sensing beams are sufficient for both sensing and jamming AEs, and the proposed designs consistently outperform strong baselines while also revealing the communication-sensing trade-off.

Paper number 18:
Title: A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification
Authors: Yiqiao Chen
Abstract: Cardiovascular disease (CVD) is a major pediatric health burden, and early screening is of critical importance. Electrocardiography (ECG), as a noninvasive and accessible tool, is well suited for this purpose. This paper presents the first benchmark study of deep learning for multi-label pediatric CVD classification on the recently released ZZU-pECG dataset, comprising 3716 recordings with 19 CVD categories. We systematically evaluate four representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under both 9-lead and 12-lead configurations. All models achieved strong results, with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings. ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and Transformer also showed competitive performance. Per-class analysis indicated challenges for rare conditions such as hypertrophic cardiomyopathy in the 9-lead subset, reflecting the effect of limited positive samples. This benchmark establishes reusable baselines and highlights complementary strengths across paradigms. It further points to the need for larger-scale, multi-center validation, age-stratified analysis, and broader disease coverage to support real-world pediatric ECG applications.

Paper number 19:
Title: On the Duality Between Quantized Time and States in Dynamic Simulation
Authors: Liya Huang, Georgios Tzounas
Abstract: This letter introduces a formal duality between discrete-time and quantized-state numerical methods. We interpret quantized state system (QSS) methods as integration schemes applied to a dual form of the system model, where time is seen as a state-dependent variable. This perspective enables the definition of novel QSS-based schemes inspired by classical time-integration techniques. As a proof of concept, we illustrate the idea by introducing a QSS Adams-Bashforth method applied to a test equation. We then move to demonstrate how the proposed approach can achieve notable performance improvements in realistic power system simulations.

Paper number 20:
Title: Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms
Authors: Jacopo Pegoraro, Gianmaria Ventura, Dario Tagliaferri, Marco Mezzavilla, Andrea Bedin, Michele Rossi, Joerg Widmer
Abstract: Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G wireless networks. The bandwidth availability and diversity of FR3 offer unprecedented opportunities for coherent multiband Integrated Sensing and Communications (ISAC), which aggregates the carrier phase information from multiple frequency bands to increase the sensing resolution to the cm-level. However, the frequency anisotropy of sensing targets over GHz-wide bands and the non-contiguity of the 6G spectrum, pose critical challenges to the application of existing multiband ISAC techniques. We present the first study on coherent multiband sensing in FR3. We experimentally characterize the frequency anisotropy of targets and propose new phase coherence metrics for multiband processing. Then, we analyze the impact of non-contiguous FR3 bands considered by 3GPP, and design a new algorithm to mitigate the resulting sensing artifacts, outperforming existing techniques. Our results represent a first step toward fully developing multiband ISAC for FR3.

Paper number 21:
Title: ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs
Authors: Changhong Li, ClÃ©ment Bled, Rosa Fernandez, Shreejith Shanker
Abstract: Denoising is a core operation in modern video pipelines. In codecs, in-loop filters suppress sensor noise and quantisation artefacts to improve rate-distortion performance; in cinema post-production, denoisers are used for restoration, grain management, and plate clean-up. However, state-of-the-art deep denoisers are computationally intensive and, at scale, are typically deployed on GPUs, incurring high power and cost for real-time, high-resolution streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated denoising system that serves inference on data-centre Field Programmable Gate Arrays (FPGAs). A compact convolutional model is quantised (post-training quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration offloads computation from the host CPU/GPU to a networked FPGA service, while remaining callable from existing workflows, e.g., NUKE, without disrupting artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\times$ Giga Operations Per Second (GOPS) throughput and 5.29$\times$ higher energy efficiency than prior FPGA denoising accelerators, with negligible degradation in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These results indicate that specialised accelerators can provide practical, scalable denoising for both encoding pipelines and post-production, reducing energy per frame without sacrificing quality or workflow compatibility. Code is available at this https URL.

Paper number 22:
Title: A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models
Authors: Yue wu
Abstract: There are limitations of traditional methods and deep learning methods in terms of interpretability, generalization, and quantification of uncertainty in industrial fault diagnosis, and there are core problems of insufficient credibility in industrial fault diagnosis. The architecture performs preliminary analysis through a Bayesian network-based diagnostic engine and features an LLM-driven cognitive quorum module with multimodal input capabilities. The module conducts expert-level arbitration of initial diagnoses by analyzing structured features and diagnostic charts, prioritizing final decisions after conflicts are identified. To ensure the reliability of the system output, the architecture integrates a confidence calibration module based on temperature calibration and a risk assessment module, which objectively quantifies the reliability of the system using metrics such as expected calibration error (ECE). Experimental results on a dataset containing multiple fault types showed that the proposed framework improved diagnostic accuracy by more than 28 percentage points compared to the baseline model, while the calibrated ECE was reduced by more than 75%. Case studies have confirmed that HCAA effectively corrects misjudgments caused by complex feature patterns or knowledge gaps in traditional models, providing novel and practical engineering solutions for building high-trust, explainable AI diagnostic systems for industrial applications.

Paper number 23:
Title: Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime
Authors: Lulu Song, Di Zhang, Tingting Zhang
Abstract: Source polar coding is a potential solution for short blocklength-based low-latency key generation with limited sources, which is a critical aspect of six generation (6G) Internet of things. However, existing source coding schemes still suffer from significant degradation in key generation rate and reconciliation reliability in short blocklength regime. To address this issue, we introduce a multilevel source polarization-adjusted convolutional (PAC) coding framework. Furthermore, we propose a novel code construction algorithm that jointly leverages polarization effects and the maximum likelihood (ML) decoding error coefficient. Simulations demonstrate that the multilevel source PAC scheme with the proposed code construction achieves superior key generation rate under key disagreement constraints compared to conventional and multilevel source polar coding methods even in short blocklength regimes.

Paper number 24:
Title: A MATLAB toolbox for Computation of Speech Transmission Index (STI)
Authors: Pavel Rajmic, JiÅÃ­ Schimmel, Å imon Cieslar
Abstract: The speech transmission index (STI) is a popular simple metric for the prediction of speech intelligibility when speech is passed through a transmission channel. Computation of STI from acoustic measurements is described in the IEC 60268-16:2020 standard. Though, reliable implementations of STI are not publicly accessible and are frequently limited to the use with a proprietary measurement hardware. We present a Matlab STI implementation of both the direct and indirect approaches according to the standard, including the shortened STIPA protocol. The suggested implementation meets prescribed requirements, as evidenced by tests on reference signals. Additionally, we conducted a verification measurement in comparison to a commercial measurement device. Our software comes with open source code.

Paper number 25:
Title: Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events
Authors: Shuoyan Wei, Feng Li, Shengeng Tang, Runmin Cong, Yao Zhao, Meng Wang, Huihui Bai
Abstract: Continuous space-time video super-resolution (C-STVSR) has garnered increasing interest for its capability to reconstruct high-resolution and high-frame-rate videos at arbitrary spatial and temporal scales. However, prevailing methods often generalize poorly, producing unsatisfactory results when applied to out-of-distribution (OOD) scales. To overcome this limitation, we present EvEnhancer, a novel approach that marries the unique properties of high temporal resolution and high dynamic range encapsulated in event streams to achieve robust and generalizable C-STVSR. Our approach incorporates event-adapted synthesis that capitalizes on the spatiotemporal correlations between frames and events to capture long-term motion trajectories, enabling adaptive interpolation and fusion across space and time. This is then coupled with a local implicit video transformer that integrates local implicit video neural function with cross-scale spatiotemporal attention to learn continuous video representations and generate plausible videos at arbitrary resolutions and frame rates. We further develop EvEnhancerPlus, which builds a controllable switching mechanism that dynamically determines the reconstruction difficulty for each spatiotemporal pixel based on local event statistics. This allows the model to adaptively route reconstruction along the most suitable pathways at a fine-grained pixel level, substantially reducing computational overhead while maintaining excellent performance. Furthermore, we devise a cross-derivative training strategy that stabilizes the convergence of such a multi-pathway framework through staged cross-optimization. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets, while maintaining superior generalizability at OOD scales. The code is available at this https URL.

Paper number 26:
Title: Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums
Authors: Jianyu Wang, Zhichao Li, Wenchi Cheng, Wei Zhang, Hailin Zhang
Abstract: Magnetic induction (MI) communication is an effective underground emergency communication technique after disasters such as landslides, mine collapses, and earthquakes, due to its advantages in mediums such as soil, concrete, and metals. However, the propagation mediums in practical MI based underground emergency communications are usually diverse and composed randomly due to the impact of disasters, which poses a challenge for MI communication in practical applications. In this paper, we formulate a statistical fading channel model, which reflects the random composition of diverse mediums and is shown to follow a lognormal distribution. To mitigate the impact of diverse medium fading, Multi-frequency Resonating Compensation (MuReC) based coils are used to achieve multiband transmission. Then, we analyze the performance of MuReC based multi-band MI communication with diverse medium fading and derive the expressions of signal-to-noise ratio (SNR) probability density functions, ergodic capacities, average bit error rates (BERs), and outage probabilities for both multiplexing and diversity cases. Numerical results show that MuReC based multiband transmission schemes can effectively reduce the impact of diverse medium fading and enhance the performance.

Paper number 27:
Title: On the Exact Sum PDF and CDF of Î±-Î¼ Variates
Authors: Fernando DarÃ­o Almeida GarcÃ­a, Francisco Raimundo Albuquerque Parente, Michel Daoud Yacoub, Jose CÃ¢ndido Silveira Santos Filho
Abstract: The sum of random variables (RVs) appears extensively in wireless communications, at large, both conventional and advanced, and has been subject of longstanding research. The statistical characterization of the referred sum is crucial to determine the performance of such communications systems. Although efforts have been undertaken to unveil these sum statistics, e.g., probability density function (PDF) and cumulative distribution function (CDF), no general efficient nor manageable solutions capable of evaluating the exact sum PDF and CDF are available to date. The only formulations are given in terms of either the multi-fold Brennan's integral or the multivariate Fox H-function. Unfortunately, these methods are only feasible up to a certain number of RVs, meaning that when the number of RVs in the sum increases, the computation of the sum PDF and CDF is subject to stability problems, convergence issues, or inaccurate results. In this paper, we derive new, simple, exact formulations for the PDF and CDF of the sum of L independent and identically distributed {\alpha}-{\mu} RVs. Unlike the available solutions, the computational complexity of our analytical expressions is independent of the number of summands. Capitalizing on our unprecedented findings, we analyze, in exact and asymptotic manners, the performance of L-branch pre-detection equal-gain combining and maximal-ratio combining receivers over {\alpha}-{\mu} fading environments. The coding and diversity gains of the system for both receivers are analyzed and quantified. Moreover, numerical simulations show that the computation time reduces drastically when using our expressions, which are arguably the most efficient and manageable formulations derived so far.

Paper number 28:
Title: Robust Beamforming for Magnetic Induction Based Underground Emergency Communications
Authors: Jianyu Wang, Tianrui Hou, Wenchi Cheng, Hailin Zhang
Abstract: Magnetic induction (MI) communication is an effective underground emergency communication technique after disasters such as landslides, mine collapses, and earthquakes, due to its advantages in mediums such as soil, concrete, and metals. Based on channel state information (CSI), magnetic beamforming can significantly improve the performance of MI communication. However, in post-disaster underground communication, channel estimation may suffer from errors due to factors such as complex environmental interferences. Taking channel estimation error into account, we formulate a beamforming optimization problem for multi-user MI underground emergency communications, which aims to minimize the power consumption under the constraints of sum rate and signal to interference plus noise ratio (SINR) of each user. Based on the worst-case optimization criterion and the S-procedure, the non-convex optimization problem is transformed into convex and solved. Numerical results show that the proposed robust beamforming scheme can effectively enhance communication reliability and effective throughput in the presence of channel estimation errors.

Paper number 29:
Title: AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images
Authors: Sanhita Basu, Tomas FrÃ¶ding, Ali Teymur Kahraman, Dimitris Toumpanakis, Tobias SjÃ¶blom
Abstract: Background: Pleural Effusions (PE) is a common finding in many different clinical conditions, but accurately measuring their volume from CT scans is challenging. Purpose: To improve PE segmentation and quantification for enhanced clinical management, we have developed and trained a semi-supervised deep learning framework on contrast-enhanced CT volumes. Materials and Methods: This retrospective study collected CT Pulmonary Angiogram (CTPA) data from internal and external datasets. A subset of 100 cases was manually annotated for model training, while the remaining cases were used for testing and validation. A novel semi-supervised deep learning framework, Teacher-Teaching Assistant-Student (TTAS), was developed and used to enable efficient training in non-segmented examinations. Segmentation performance was compared to that of state-of-the-art models. Results: 100 patients (mean age, 72 years, 28 [standard deviation]; 55 men) were included in the study. The TTAS model demonstrated superior segmentation performance compared to state-of-the-art models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73 for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80 - 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The developed TTAS framework offered superior PE segmentation, aiding accurate volume determination from CT scans.

Paper number 30:
Title: Enhancing Data Center Low-Voltage Ride-Through
Authors: Yiheng Xie, Wenqi Cui, Adam Wierman
Abstract: Data center loads have expanded significantly in recent years. Compared to traditional loads, data centers are highly sensitive to voltage deviations and thus their protection mechanisms trip more proactively during voltage fluctuations. During a grid fault, simultaneous tripping of large-scale data centers can further destabilize the transmission system and even lead to cascading failures. In response, transmission system operators are imposing voltage ride-through (VRT) requirements for data centers. In this work, we enhance the VRT capability of data centers by designing voltage controllers for their internal power distribution network. We first systematically analyze VRT standards and the controllable resources related to data centers. These resources enable the design of voltage control strategies to regulate voltages internal to the data center, thereby allowing loads to remain online during voltage disturbances from the external transmission grid. We study and contrast both centralized and decentralized controllers that unify the control of heterogeneous flexible resources. Additionally, we construct an integrated test system that simulates both the transient fault response of the transmission system and the data center distribution network. Case studies demonstrate that the proposed voltage control mechanisms provide effective yet simple solutions to enhance data center low-voltage ride-through capability.

Paper number 31:
Title: Electrical System Architecture for Aviation Electrification
Authors: Anoy Saha, Mona Ghassemi
Abstract: The electrification of aircraft is reshaping the foundations of aerospace design by positioning electrical systems at the center of propulsion, control, and onboard functionality. This chapter provides an overview of electrical system architectures for electric and hybrid electric aircraft, highlighting both established principles and emerging design strategies. The discussion begins with the motivations for electrification, including reducing environmental impact, improving operational efficiency, and replacing complex pneumatic and hydraulic subsystems with lighter and more reliable electrical alternatives. Aircraft electrical architectures are classified into four major categories: conventional, more electric, all electric, and hybrid electric. A range of system topologies is examined, including direct current (DC), alternating current (AC), hybrid, and distributed configurations. Each is considered in terms of its effectiveness in delivering power, enabling redundancy, supporting fault isolation, and managing thermal performance. Real world examples are presented to demonstrate practical applications, with case studies drawn from the Boeing 787 Dreamliner, the Eviation Alice commuter aircraft, and NASA X57 Maxwell demonstrator. These examples illustrate the ongoing transition from incremental subsystem electrification toward fully integrated architectures that promise higher efficiency and greater sustainability.

Paper number 32:
Title: On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications
Authors: Vincent Savaux, Steve Sawadogo, Hyeon Seok Rou, Giuseppe Thadeu Freitas de Abreu
Abstract: This paper investigates the robustness of affine frequency division multiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation schemes against non-white Gaussian noise, which can model various sources of additive disturbances to the received signal. The proposed approach demonstrates that the performance of these waveforms depends on the ability of the demodulation matrix to whiten the noise-a property that is, in turn, related to the sparsity of the matrix. AFDM is shown to outperform OTFS and orthogonal frequency division multiplexing (OFDM), as its demodulation matrix is generally less sparse than those of the other waveforms. Based on this analysis, several application examples and use cases are presented, such as the use of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals. Finally, simulation results confirm that AFDM achieves better performance than OTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in most application scenarios.

Paper number 33:
Title: Sliding Window Attention for Learned Video Compression
Authors: Alexander Kopte, AndrÃ© Kaup
Abstract: To manage the complexity of transformers in video compression, local attention mechanisms are a practical necessity. The common approach of partitioning frames into patches, however, creates architectural flaws like irregular receptive fields. When adapted for temporal autoregressive models, this paradigm, exemplified by the Video Compression Transformer (VCT), also necessitates computationally redundant overlapping windows. This work introduces 3D Sliding Window Attention (SWA), a patchless form of local attention. By enabling a decoder-only architecture that unifies spatial and temporal context processing, and by providing a uniform receptive field, our method significantly improves rate-distortion performance, achieving BjÃ¸rntegaard Delta-rate savings of up to 18.6 % against the VCT baseline. Simultaneously, by eliminating the need for overlapping windows, our method reduces overall decoder complexity by a factor of 2.8, while its entropy model is nearly 3.5 times more efficient. We further analyze our model's behavior and show that while it benefits from long-range temporal context, excessive context can degrade performance.

Paper number 34:
Title: 3D Electronic-Photonic Heterogenous Interconnect Platforms Enabling Energy-Efficient Scalable Architectures For Future HPC Systems
Authors: Anirban Samanta, Shun-Hung Lee, Chun-Yi Cheng, Samuel Palermo, S. J. Ben Yoo
Abstract: 3D interconnects have emerged as a solution to address the scaling issues of interconnect bandwidth and the memory wall problem in high-performance computing (HPC), such as High-Bandwidth Memory (HBM). However, the copper-based electrical interconnect retains fundamental limitations. Dense I/O for high-speed signals lead to degraded signal quality for end-to-end links, necessitating additional circuits to mitigate signal impairments and resulting in poor energy efficiency. We propose a 3D chiplet stacking electronic-photonic interconnect (EPIC) platform, which offers a solution by moving the high-speed data communication interface to the optical domain across the 3D stack by using Through Silicon Optical Vias (TSOV), while retaining the functionality of electrical TSVs and 2.5D interconnects for power delivery and short-reach low-latency communications. We then benchmark the proposed model against state-of-the-art 3D electrical interconnects to demonstrate our 3D EPIC platform beating the 3D electrical interconnects to $>$10 TB/s/$mm^2$ bandwidth density. We present a pathway to extend our demonstrated, industry-ready design to achieving $\leq$100 fJ/bit high-speed communication.

Paper number 35:
Title: Use of Quadcopter Wakes to Supplement Strawberry Pollination
Authors: Sadie Cutler, Ben DeFay, Scott McArt, Kirstin Petersen
Abstract: Pollinators are critical to the world's ecosystems and food supply, yet recent studies have found pollination shortfalls in several crops, including strawberry. This is troubling because wild and managed pollinators are currently experiencing declines. One possibility is to try and provide supplemental pollination solutions. These solutions should be affordable and simple for farmers to implement if their use is to be widespread; quadcopters are a great example, already used for monitoring on many farms. This paper investigates a new method for artificial pollination based on wind pollination that bears further investigation. After determining the height where the lateral flow is maximized, we performed field experiments with a quadcopter assisting natural pollinators. Although our results in the field were inconclusive, lab studies show that the idea shows promise and could be adapted for better field results.

Paper number 36:
Title: Data-driven Practical Stabilization of Nonlinear Systems via Chain Policies: Sample Complexity and Incremental Learning
Authors: Roy Siegelmann, Enrique Mallada
Abstract: We propose a method for data-driven practical stabilization of nonlinear systems with provable guarantees, based on the concept of Nonparametric Chain Policies (NCPs). The approach employs a normalized nearest-neighbor rule to assign, at each state, a finite-duration control signal derived from stored data, after which the process repeats. Unlike recent works that model the system as linear, polynomial, or polynomial fraction, we only assume the system to be locally Lipschitz. Our analysis builds on the framework of Recurrent Lyapunov Functions (RLFs), which enable data-driven certification of practical stability using standard norm functions instead of requiring the explicit construction of a classical Lyapunov function. To extend this framework, we introduce the concept of Recurrent Control Lyapunov Functions (R-CLFs), which can certify the existence of an NCP that practically stabilizes an arbitrarily small c-neighborhood of an equilibrium point. We also provide an explicit sample complexity guarantee of O((3/rho)^d log(R/c)) number of trajectories, where R is the domain radius, d the state dimension, and rho a system-dependent constant. The proposed Chain Policies are nonparametric, thus allowing new verified data to be readily incorporated into the policy to either improve convergence rate or enlarge the certified region. Numerical experiments illustrate and validate these properties.

Paper number 37:
Title: A Multilingual Framework for Dysarthria: Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation
Authors: Ananya Raghu, Anisha Raghu, Nithika Vivek, Sofie Budman, Omar Mansour
Abstract: Dysarthria is a motor speech disorder that results in slow and often incomprehensible speech. Speech intelligibility significantly impacts communication, leading to barriers in social interactions. Dysarthria is often a characteristic of neurological diseases including Parkinson's and ALS, yet current tools lack generalizability across languages and levels of severity. In this study, we present a unified AI-based multilingual framework that addresses six key components: (1) binary dysarthria detection, (2) severity classification, (3) clean speech generation, (4) speech-to-text conversion, (5) emotion detection, and (6) voice cloning. We analyze datasets in English, Russian, and German, using spectrogram-based visualizations and acoustic feature extraction to inform model training. Our binary detection model achieved 97% accuracy across all three languages, demonstrating strong generalization across languages. The severity classification model also reached 97% test accuracy, with interpretable results showing model attention focused on lower harmonics. Our translation pipeline, trained on paired Russian dysarthric and clean speech, reconstructed intelligible outputs with low training (0.03) and test (0.06) L1 losses. Given the limited availability of English dysarthric-clean pairs, we fine-tuned the Russian model on English data and achieved improved losses of 0.02 (train) and 0.03 (test), highlighting the promise of cross-lingual transfer learning for low-resource settings. Our speech-to-text pipeline achieved a Word Error Rate of 0.1367 after three epochs, indicating accurate transcription on dysarthric speech and enabling downstream emotion recognition and voice cloning from transcribed speech. Overall, the results and products of this study can be used to diagnose dysarthria and improve communication and understanding for patients across different languages.

Paper number 38:
Title: Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate
Authors: Mohammad Salman, Hadi Zayyani, Hasan Abu Hilal, Mostafa Rashdan
Abstract: This letter presents a novel method for estimating the position, velocity, and acceleration of a moving target using range-based measurements. Although most existing studies focus on position and velocity estimation, the framework of this letter is extended to include acceleration. To achieve this, we propose using the derivative of the range rate, in addition to the range and range rate measurements. The proposed method estimates the position at first using Time-of-Arrival (TOA)-based techniques; then, develops a reformulated least squares (LS) and weighted least squares (WLS) approaches for velocity estimation; and finally, employs the derivative of the range rate to estimate the acceleration using previous position and velocity estimates. On the other hand, closed-form LS and WLS solutions are derived for both velocity and acceleration. The simulation results show that the proposed approach provides improved performance in estimating moving target kinematics compared to existing methods.

Paper number 39:
Title: Distributed MPC-based Coordination of Traffic Perimeter and Signal Control: A Lexicographic Optimization Approach
Authors: Viet Hoang Pham, Hyo-Sung Ahn
Abstract: This paper introduces a comprehensive strategy that integrates traffic perimeter control with traffic signal control to alleviate congestion in an urban traffic network (UTN). The strategy is formulated as a lexicographic multi-objective optimization problem, starting with the regulation of traffic inflows at boundary junctions to maximize the capacity while ensuring a smooth operation of the UTN. Following this, the signal timings at internal junctions are collaboratively optimized to enhance overall traffic conditions under the regulated inflows. The use of a model predictive control (MPC) approach ensures that the control solution adheres to safety and capacity constraints within the network. To address the computational complexity of the problem, the UTN is divided into subnetworks, each managed by a local agent. A distributed solution method based on the alternating direction method of multipliers (ADMM) algorithm is employed, allowing each agent to determine its optimal control decisions using local information from its subnetwork and neighboring agents. Numerical simulations using VISSIM and MATLAB demonstrate the effectiveness of the proposed traffic control strategy.

Paper number 40:
Title: A Conformal Prediction-Based Chance-Constrained Programming Approach for 24/7 Carbon-Free Data Center Operation Scheduling
Authors: Yijie Yang, Jian Shi, Dan Wang, Chenye Wu, Zhu Han
Abstract: The rapid growth of AI applications is dramatically increasing data center energy demand, exacerbating carbon emissions, and necessitating a shift towards 24/7 carbon-free energy (CFE). Unlike traditional annual energy matching, 24/7 CFE requires matching real-time electricity consumption with clean energy generation every hour, presenting significant challenges due to the inherent variability and forecasting errors of renewable energy sources. Traditional robust and data-driven optimization methods often fail to leverage the features of the prediction model (also known as contextual or covariate information) when constructing the uncertainty set, leading to overly conservative operational decisions. This paper proposes a comprehensive approach for 24/7 CFE data center operation scheduling, focusing on robust decision-making under renewable generation uncertainty. This framework leverages covariate information through a multi-variable conformal prediction (CP) technique to construct statistically valid and adaptive uncertainty sets for renewable forecasts. The uncertainty sets directly inform the chance-constrained programming (CCP) problem, ensuring that chance constraints are met with a specified probability. We further establish theoretical underpinnings connecting the CP-generated uncertainty sets to the statistical feasibility guarantees of the CCP. Numerical results highlight the benefits of this covariate-aware approach, demonstrating up to 6.65% cost reduction and 6.96% decrease in carbon-based energy usage compared to conventional covariate-independent methods, thereby enabling data centers to progress toward 24/7 CEF.

Paper number 41:
Title: MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition
Authors: Umberto Cappellazzo, Minsu Kim, Pingchuan Ma, Honglie Chen, Xubo Liu, Stavros Petridis, Maja Pantic
Abstract: Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition.

Paper number 42:
Title: CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization
Authors: Mohammad Kazzazi, Mohammad Morsali, Rouhollah Amiri
Abstract: This paper presents CLEAR -- a closed-form localization estimator with a reduced sensor network. The proposed method is a computationally efficient, two-stage estimator that fuses time-difference-of-arrival (TDOA) and frequency-difference-of-arrival (FDOA) measurements with a minimal number of sensors. CLEAR localizes a moving source in N-dimensional space using only N+1 sensors, achieving the theoretical minimum sensor count. The first stage introduces auxiliary range and range-rate parameters to construct a set of pseudo-linear equations, solved via weighted least squares. An algebraic elimination using Sylvester's resultant then reduces the problem to a quartic equation, yielding closed-form estimates for the nuisance variables. A second, lightweight linear refinement stage is applied to mitigate residual bias. Under mild Gaussian noise assumptions, the estimator's position and velocity estimates are statistically efficient, closely approaching the Cramer-Rao lower bound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios demonstrate CRLB-level accuracy and consistent performance gains over representative two-stage and iterative baselines, confirming the method's high suitability for power-constrained, distributed Internet of Things (IoT) applications such as UAV tracking and smart transportation.

Paper number 43:
Title: Drax: Speech Recognition with Discrete Flow Matching
Authors: Aviv Navon, Aviv Shamsian, Neta Glazer, Yael Segal-Feldman, Gill Hetz, Joseph Keshet, Ethan Fetaya
Abstract: Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR.

Paper number 44:
Title: Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge Distillation guided Structured Pruning
Authors: Ze Li, Ming Cheng, Ming Li
Abstract: Large-scale self-supervised Pre-Trained Models (PTMs) have shown significant improvements in the speaker verification (SV) task by providing rich feature representations. In this paper, we utilize w2v-BERT 2.0, a model with approximately 600 million parameters trained on 450 million hours of unlabeled data across 143 languages, for the SV task. The MFA structure with Layer Adapter is employed to process the multi-layer feature outputs from the PTM and extract speaker embeddings. Additionally, we incorporate LoRA for efficient fine-tuning. Our model achieves state-of-the-art results with 0.12% and 0.55% EER on the Vox1-O and Vox1-H test sets, respectively. Furthermore, we apply knowledge distillation guided structured pruning, reducing the model size by 80% while achieving only a 0.04% EER degradation. Source code and models are released at this https URL.

Paper number 45:
Title: Probing Whisper for Dysarthric Speech in Detection and Assessment
Authors: Zhengjun Yue, Devendra Kayande, Zoran Cvetkovic, Erfan Loweimi
Abstract: Large-scale end-to-end models such as Whisper have shown strong performance on diverse speech tasks, but their internal behavior on pathological speech remains poorly understood. Understanding how dysarthric speech is represented across layers is critical for building reliable and explainable clinical assessment tools. This study probes the Whisper-Medium model encoder for dysarthric speech for detection and assessment (i.e., severity classification). We evaluate layer-wise embeddings with a linear classifier under both single-task and multi-task settings, and complement these results with Silhouette scores and mutual information to provide perspectives on layer informativeness. To examine adaptability, we repeat the analysis after fine-tuning Whisper on a dysarthric speech recognition task. Across metrics, the mid-level encoder layers (13-15) emerge as most informative, while fine-tuning induces only modest changes. The findings improve the interpretability of Whisper's embeddings and highlight the potential of probing analyses to guide the use of large-scale pretrained models for pathological speech.

Paper number 46:
Title: Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks
Authors: Dario Tagliaferri, Silvia Mura, Musa Furkan Keskin, Sauradeep Dey, Henk Wymeersch
Abstract: This paper addresses the challenge of integrating multistatic coherent imaging functionalities in the downlink (DL) of a phase-coherent distributed multiple input multiple output (D-MIMO) communication network. During DL, the D-MIMO access points (APs) jointly precode the transmitted signals to maximize the spectral efficiency (SE) at the users (UEs) locations. However, imaging requires that \textit{(i)} a fraction of the APs work as receivers for sensing and \textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals to illuminate the area to be imaged and allow multistatic operation. In these settings, our contribution is twofold. We propose a novel distributed integrated sensing and communication (D-ISAC) system that superposes a purposely designed AP-specific signal for imaging to the legacy UE-specific communication one, with a tunable trade-off factor. We detail both the imaging waveform design according to the \textit{extended orthogonality condition} and the space-frequency precoder design. Then, we propose an optimized selection strategy for the receiving APs, in order to maximize imaging performance under half-duplex constraints. Extensive numerical results prove the feasibility and benefits of our proposal, materializing the potential of joint multistatic imaging and communications in practical D-MIMO deployments.

Paper number 47:
Title: Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments
Authors: Ziang Zhao, Weixi Liang, Kai Hu, Qun Zhang, Xiongbin Yu, Qiang Li
Abstract: Accurate channel modeling is essential for realizing the potential of terahertz (THz) communications in 6G indoor networks, where existing models struggle with severe frequency selectivity and multipath effects. We propose a physically grounded Rician fading channel model that jointly incorporates deterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS) components, enhanced by frequency-dependent attenuation characterized by optimized exponents alpha and beta. Unlike conventional approaches, our model integrates a two-ray reflection framework to capture standing wave phenomena and employs wideband spectral averaging to mitigate frequency selectivity over bandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning 0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE) as low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and reducing RMSE by 73.3% as bandwidth increases. These findings underscore the importance of bandwidth in suppressing oscillatory artifacts and improving modeling accuracy. Our approach provides a robust foundation for THz system design, supporting reliable indoor wireless personal area networks (WPANs), device-to-device (D2D) communications, and precise localization in future 6G applications.

Paper number 48:
Title: A Hybrid GNN-IZR Framework for Fast and Empirically Robust AC Power Flow Analysis in Radial Distribution Systems
Authors: Mohamed Shamseldein
Abstract: The Alternating Current Power Flow (ACPF) problem forces a trade-off between the speed of data-driven models and the reliability of analytical solvers. This paper introduces a hybrid framework that synergizes a Graph Neural Network (GNN) with the Implicit Z-Bus Recursive (IZR) method, a robust, non-iterative solver for radial distribution networks. The framework employs a physics-informed GNN for rapid initial predictions and invokes the IZR solver as a failsafe for stressed cases identified by a two-stage trigger. A failure is defined as any solution with a maximum power mismatch exceeding 0.1 p.u., a significant operational deviation. On a challenging test set of 7,500 stressed scenarios for the IEEE 33-bus system, the GNN-only model failed on 13.11 % of cases. In contrast, the hybrid framework identified all potential failures, delegating them to the IZR solver to achieve a 0.00 % failure rate, empirically matching the 100 % success rate of the analytical solver on this specific test set. An expanded ablation study confirms that both physics-informed training and Z-bus sensitivity features are critical, collaboratively reducing the GNN's failure rate from 98.72 % (data-only) to 13.11 %. The hybrid approach demonstrates a pragmatic path to achieving the empirical reliability of an analytical solver while leveraging GNN speed, enabling a significant increase in the number of scenarios analyzable in near real-time.

Paper number 49:
Title: Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data
Authors: Minsu Kim, Walid Saad, Dour Calin
Abstract: In 6G wireless networks, multi-modal ML models can be leveraged to enable situation-aware network decisions in dynamic environments. However, trained ML models often fail to generalize under domain shifts when training and test data distributions are different because they often focus on modality-specific spurious features. In practical wireless systems, domain shifts occur frequently due to dynamic channel statistics, moving obstacles, or hardware configuration. Thus, there is a need for learning frameworks that can achieve robust generalization under scarce multi-modal data in wireless networks. In this paper, a novel and data-efficient two-phase learning framework is proposed to improve generalization performance in unseen and unfamiliar wireless environments with minimal amount of multi-modal data. In the first stage, a physics-based loss function is employed to enable each BS to learn the physics underlying its wireless environment captured by multi-modal data. The data-efficiency of the physics-based loss function is analytically investigated. In the second stage, collaborative domain adaptation is proposed to leverage the wireless environment knowledge of multiple BSs to guide under-performing BSs under domain shift. Specifically, domain-similarity-aware model aggregation is proposed to utilize the knowledge of BSs that experienced similar domains. To validate the proposed framework, a new dataset generation framework is developed by integrating CARLA and MATLAB-based mmWave channel modeling to predict mmWave RSS. Simulation results show that the proposed physics-based training requires only 13% of data samples to achieve the same performance as a state-of-the-art baseline that does not use physics-based training. Moreover, the proposed collaborative domain adaptation needs only 25% of data samples and 20% of FLOPs to achieve the convergence compared to baselines.

Paper number 50:
Title: The method of the approximate inverse for limited-angle CT
Authors: Bernadette Hahn, Gael Rigaud, Richard SchmÃ¤hl
Abstract: Limited-angle computerized tomography stands for one of the most difficult challenges in imaging. Although it opens the way to faster data acquisition in industry and less dangerous scans in medicine, standard approaches, such as the filtered backprojection (FBP) algorithm or the widely used total-variation functional, often produce various artefacts that hinder the diagnosis. With the rise of deep learning, many modern techniques have proven themselves successful in removing such artefacts but at the cost of large datasets. In this paper, we propose a new model-driven approach based on the method of the approximate inverse, which could serve as new starting point for learning strategies in the future. In contrast to FBP-type approaches, our reconstruction step consists in evaluating linear functionals on the measured data using reconstruction kernels that are precomputed as solution of an auxiliary problem. With this problem being uniquely solvable, the derived limited-angle reconstruction kernel (LARK) is able to fully reconstruct the object without the well-known streak artefacts, even for large limited angles. However, it inherits severe ill-conditioning which leads to a different kind of artefacts arising from the singular functions of the limited-angle Radon transform. The problem becomes particularly challenging when working on semi-discrete (real or analytical) measurements. We develop a general regularization strategy, named constrained limited-angle reconstruction kernel (CLARK), by combining spectral filter, the method of the approximate inverse and custom edge-preserving denoising in order to stabilize the whole process. We further derive and interpret error estimates for the application on real, i.e. semi-discrete, data and we validate our approach on synthetic and real data.

Paper number 51:
Title: Adaptive double-phase Rudin--Osher--Fatemi denoising model
Authors: Wojciech GÃ³rny, MichaÅ Åasica, Alexandros Matsoukas
Abstract: We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.

Paper number 52:
Title: Low-Rank-Based Approximate Computation with Memristors
Authors: Binyu Lu, Matthias Frey, Stark Draper, Jingge Zhu
Abstract: Memristor crossbars enable vector-matrix multiplication (VMM), and are promising for low-power applications. However, it can be difficult to write the memristor conductance values exactly. To improve the accuracy of VMM, we propose a scheme based on low-rank matrix approximation. Specifically, singular value decomposition (SVD) is first applied to obtain a low-rank approximation of the target matrix, which is then factored into a pair of smaller matrices. Subsequently, a two-step serial VMM is executed, where the stochastic write errors are mitigated through step-wise averaging. To evaluate the performance of the proposed scheme, we derive a general expression for the resulting computation error and provide an asymptotic analysis under a prescribed singular-value profile, which reveals how the error scales with matrix size and rank. Both analytical and numerical results confirm the superiority of the proposed scheme compared with the benchmark scheme.

Paper number 53:
Title: Effect of nearby Metals on Electro-Quasistatic Human Body Communication
Authors: Samyadip Sarkar, Arunashish Datta, David Yang, Mayukh Nath, Shovan Maity, Shreyas Sen
Abstract: In recent decades Human Body Communication has emerged as a promising alternative to traditional radio wave communication, utilizing the body's conductive properties for low-power connectivity among wearables. This method harnesses the human body as an energy-efficient channel for data transmission within the electro-quasistatic frequency range, enabling advancements in human-machine interaction. While prior work has noted the role of parasitic return paths in such capacitively coupled systems, the influence of surrounding metallic objects on these paths, which are critical for EQS wireless signaling, has not been fully explored. This paper fills that gap with a structured study of how various conducting objects, from non-grounded (floating) metals and grounded metals to enclosed metallic environments such as elevators and cars, affect the body-communication channel. We present a theoretical framework supported by finite element method simulations and experiments with wearable devices. Results show that metallic objects within 20 cm of devices can reduce transmission loss by about 10 dB. When a device ground connects to a grounded metallic object, channel gain can increase by at least 20 dB. Contact area during touch-based interactions with grounded metals produces contact-impedance dependent high-pass channel characteristics. Proximity to metallic objects introduces variability within a critical distance, with grounded metals producing a larger overall effect than floating metals. These findings improve understanding of body-centric communication links and inform design for healthcare, consumer electronics, defense, and industrial applications.

Paper number 54:
Title: The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems
Authors: Muhammad Umar Farooq Qaisar, Weijie Yuan, Onur GÃ¼nlÃ¼, Taneli Riihonen, Yuanhao Cui, Lin Zhang, Nuria Gonzalez-Prelcic, Marco Di Renzo, Zhu Han
Abstract: The commencement of the sixth-generation (6G) wireless networks represents a fundamental shift in the integration of communication and sensing technologies to support next-generation applications. Integrated sensing and communication (ISAC) is a key concept in this evolution, enabling end-to-end support for both communication and sensing within a unified framework. It enhances spectrum efficiency, reduces latency, and supports diverse use cases, including smart cities, autonomous systems, and perceptive environments. This tutorial provides a comprehensive overview of ISAC's role in 6G networks, beginning with its evolution since 5G and the technical drivers behind its adoption. Core principles and system variations of ISAC are introduced, followed by an in-depth discussion of the enabling technologies that facilitate its practical deployment. The paper further analyzes current research directions to highlight key challenges, open issues, and emerging trends. Design insights and recommendations are also presented to support future development and implementation. This work ultimately try to address three central questions: Why is ISAC essential for 6G? What innovations does it bring? How will it shape the future of wireless communication?

Paper number 55:
Title: Differentiable physics for sound field reconstruction
Authors: Samuel A. Verburg, Efren Fernandez-Grande, Peter Gerstoft
Abstract: Sound field reconstruction involves estimating sound fields from a limited number of spatially distributed observations. This work introduces a differentiable physics approach for sound field reconstruction, where the initial conditions of the wave equation are approximated with a neural network, and the differential operator is computed with a differentiable numerical solver. The use of a numerical solver enables a stable network training while enforcing the physics as a strong constraint, in contrast to conventional physics-informed neural networks, which include the physics as a constraint in the loss function. We introduce an additional sparsity-promoting constraint to achieve meaningful solutions even under severe undersampling conditions. Experiments demonstrate that the proposed approach can reconstruct sound fields under extreme data scarcity, achieving higher accuracy and better convergence compared to physics-informed neural networks.

Paper number 56:
Title: A Diffusion-based Generative Machine Learning Paradigm for Contingency Screening
Authors: Quan Tran, Suresh S. Muknahallipatna, Dongliang Duan, Nga Nguyen
Abstract: Contingency screening is a crucial part of electric power systems all the time. Power systems frequently encounter multiple challenging operational dilemmas that could lead to the instability of power systems. Contingency analysis is effort-consuming by utilizing traditional numerical analysis methods. It is commonly addressed by generating a whopping number of possible contingencies or manipulating network parameters to determine the worst scenarios. This paper proposes a novel approach that diverts the nature of contingency analysis from pre-defined scenario screening to proactive-unsupervised screening. The potentially risky scenarios of power systems are generated from learning how the previous ones occurred. In other words, the internal perturbation that initiates contingencies is learned prior to being self-replicated for rendering the worst scenarios. By leveraging the perturbation diffusion technique, a proposed model is built to point out the worst scenarios instead of repeatedly simulating one-by-one scenarios to define the highest-risk ones. Empirical experiments are implemented on the IEEE systems to test and validate the proposed solution.

Paper number 57:
Title: Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks
Authors: Zhou Zhang, Yizhu Wang, Saman Atapattu, Sumei Sun
Abstract: Caching is crucial in hybrid satellite-terrestrial networks to reduce latency, optimize throughput, and improve data availability by storing frequently accessed content closer to users, especially in bandwidth-limited satellite systems, requiring strategic Medium Access Control (MAC) layer. This paper addresses throughput optimization in satellite-terrestrial integrated networks through opportunistic cooperative caching. We propose a joint probing and scheduling strategy to enhance content retrieval efficiency. The strategy leverages the LEO satellite to probe satellite-to-ground links and cache states of multiple cooperative terrestrial stations, enabling dynamic user scheduling for content delivery. Using an optimal stopping theoretic approach with two levels of incomplete information, we make real-time decisions on satellite-terrestrial hybrid links and caching probing. Our threshold-based strategy optimizes probing and scheduling, significantly improving average system throughput by exploiting cooperative caching, satellite-terrestrial link transmission, and time diversity from dynamic user requests. Simulation results validate the effectiveness and practicality of the proposed strategies.

Paper number 58:
Title: On properties of hydraulic equilibria in district heating networks
Authors: Ask HÃ¤llstrÃ¶m, Felix Agner, Richard Pates
Abstract: District heating networks are an integral part of the energy system in many countries. In future smart energy systems, they are expected to enhance energy flexibility and support the integration of renewable and waste energy sources. An important aspect of these networks is the control of flow rates, which dictates the heat delivered to consumers. This paper concerns the properties of flow rates in tree-structured district heating networks. We show that under mild assumptions of monotonicity in the hydraulic network components, statements regarding the stationary flow rate distribution can be made. In particular, when all consumers in a network incrementally open their valves, an increase in total flow rate throughput is guaranteed, while if one consumer does not open their valve when others do, they will receive a reduced flow rate. These properties are illustrated numerically on a small 2-consumer network as well as on a larger 22-consumer network. Previous works have shown that these properties allow the design and use of efficient control strategies for optimal heat distribution.

Paper number 59:
Title: Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding
Authors: Gayathri Shekar, Saman Atapattu, Prathapasinghe Dharmawansa, Kandeepan Sithamparanathan
Abstract: Holographic MIMO (HMIMO) has emerged as a promising solution for future wireless systems by enabling ultra-dense, spatially continuous antenna deployments. While prior studies have primarily focused on electromagnetic (EM) modeling or simulation-based performance analysis, a rigorous communication-theoretic framework remains largely unexplored. This paper presents the first analytical performance study of a multi-user HMIMO downlink system with matched filter (MF) precoding - a low-complexity baseline scheme. By incorporating multipath propagation, mutual coupling, and element excitation, we derive a novel closed-form expression for the MF signal-to-interference-plus-noise ratio (SINR) using an equivalent random variable model. Leveraging bivariate gamma distributions, we then develop tractable throughput approximations under full, partial, and no channel state information (CSI) scenarios. Additionally, we formulate a max-min beamforming problem to benchmark optimal user fairness performance. Numerical results validate the accuracy of the proposed framework and reveal that MF precoding achieves competitive performance with strong robustness to low SINR and CSI uncertainty.

Paper number 60:
Title: Data-Driven Adaptive PID Control Based on Physics-Informed Neural Networks
Authors: Junsei Ito, Yasuaki Wasa
Abstract: This article proposes a data-driven PID controller design based on the principle of adaptive gain optimization, leveraging Physics-Informed Neural Networks (PINNs) generated for predictive modeling purposes. The proposed control design method utilizes gradients of the PID gain optimization, achieved through the automatic differentiation of PINNs, to apply model predictive control using a cost function based on tracking error and control inputs. By optimizing PINNs-based PID gains, the method achieves adaptive gain tuning that ensures stability while accounting for system nonlinearities. The proposed method features a systematic framework for integrating PINNs-based models of dynamical control systems into closed-loop control systems, enabling direct application to PID control design. A series of numerical experiments is conducted to demonstrate the effectiveness of the proposed method from the control perspectives based on both time and frequency domains.

Paper number 61:
Title: UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models
Authors: Wenhao Guan, Zhikang Niu, Ziyue Jiang, Kaidi Wang, Peijie Chen, Qingyang Hong, Lin Li, Xie Chen
Abstract: Large language models (LLMs) have demonstrated promising performance in both automatic speech recognition (ASR) and text-to-speech (TTS) systems, gradually becoming the mainstream approach. However, most current approaches address these tasks separately rather than through a unified framework. This work aims to integrate these two tasks into one unified model. Although discrete speech tokenization enables joint modeling, its inherent information loss limits performance in both recognition and generation. In this work, we present UniVoice, a unified LLM framework through continuous representations that seamlessly integrates speech recognition and synthesis within a single model. Our approach combines the strengths of autoregressive modeling for speech recognition with flow matching for high-quality generation. To mitigate the inherent divergence between autoregressive and flow-matching models, we further design a dual attention mechanism, which switches between a causal mask for recognition and a bidirectional attention mask for synthesis. Furthermore, the proposed text-prefix-conditioned speech infilling method enables high-fidelity zero-shot voice cloning. Experimental results demonstrate that our method can achieve or exceed current single-task modeling methods in both ASR and zero-shot TTS tasks. This work explores new possibilities for end-to-end speech understanding and generation.

Paper number 62:
Title: Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization
Authors: Meidong Xia, Zhenyao He, Wei Xu, Yongming Huang, Derrick Wing Kwan Ng, Naofal Al-Dhahir
Abstract: Networked integrated sensing and communication (ISAC) has gained significant attention as a promising technology for enabling next-generation wireless systems. To further enhance networked ISAC, delegating the reception of sensing signals to dedicated target monitoring terminals (TMTs) instead of base stations (BSs) offers significant advantages in terms of sensing capability and deployment flexibility. Despite its potential, the coordinated beamforming design for networked integrated communication and time-of-arrival (ToA)-based multi-TMT localization remains largely unexplored. In this paper, we present a comprehensive study to fill this gap. Specifically, we first establish signal models for both communication and localization, and, for the first time, derive a closed-form CramÃ©r-Rao lower bound (CRLB) to characterize the localization performance. Subsequently, we exploit this CRLB to formulate two optimization problems, focusing on sensing-centric and communication-centric criteria, respectively. For the sensing-centric problem, we develop a globally optimal algorithm based on semidefinite relaxation (SDR) when each BS is equipped with more antennas than the total number of communication users. While for the communication-centric problem, we design a globally optimal algorithm for the single-BS case using bisection search. For the general case of both problems, we propose a unified successive convex approximation (SCA)-based algorithm, which is suboptimal yet efficient, and further extend it from single-target scenarios to more practical multi-target scenarios. Finally, simulation results demonstrate the effectiveness of our proposed algorithms, reveal the intrinsic performance trade-offs between communication and localization, and further show that deploying more TMTs is always preferable to deploying more BSs in networked ISAC systems.

Paper number 63:
Title: Design Process of a Self Adaptive Smart Serious Games Ecosystem
Authors: X. Tao, P. Chen, M. Tsami, F. Khayati, M. Eckert
Abstract: This paper outlines the design vision and planned evolution of Blexer v3, a modular and AI-driven rehabilitation ecosystem based on serious games. Building on insights from previous versions of the system, we propose a new architecture that aims to integrate multimodal sensing, real-time reasoning, and intelligent control. The envisioned system will include distinct modules for data collection, user state inference, and gameplay adaptation. Key features such as dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are also considered to support personalized interventions. We present the complete conceptual framework of Blexer v3, which defines the modular structure and data flow of the system. This serves as the foundation for the next phase: the development of a functional prototype and its integration into clinical rehabilitation scenarios.

Paper number 64:
Title: On Prediction-Based Properties of Discrete-Event Systems: Notions, Applications and Supervisor Synthesis
Authors: Bohan Cui, Yu Chen, Alessandro Giua, Xiang Yin
Abstract: In this work, we investigate the problem of synthesizing property-enforcing supervisors for partially-observed discrete-event systems (DES). Unlike most existing approaches, where the enforced property depends solely on the executed behavior of the system, here we consider a more challenging scenario in which the property relies on predicted future behaviors that have not yet occurred. This problem arises naturally in applications involving future information, such as active prediction or intention protection. To formalize the problem, we introduce the notion of prediction-based properties, a new class of observational properties tied to the system's future information. We demonstrate that this notion is very generic and can model various practical properties, including predictability in fault prognosis and pre-opacity in intention security. We then present an effective approach for synthesizing supervisors that enforce prediction-based properties. Our method relies on a novel information structure that addresses the fundamental challenge arising from the dependency between current predictions and the control policy. The key idea is to first borrow information from future instants and then ensure information consistency. This reduces the supervisor synthesis problem to a safety game in the information space. We prove that the proposed algorithm is both sound and complete, and the resulting supervisor is maximally permissive.

Paper number 65:
Title: Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from Therapist-informed Input
Authors: Zhimin Hou, Jiacheng Hou, Xiao Chen, Hamid Sadeghian, Tianyu Ren, Sami Haddadin
Abstract: Therapist-in-the-loop robotic rehabilitation has shown great promise in enhancing rehabilitation outcomes by integrating the strengths of therapists and robotic systems. However, its broader adoption remains limited due to insufficient safe interaction and limited adaptation capability. This article proposes a novel telerobotics-mediated framework that enables therapists to intuitively and safely deliver assist-as-needed~(AAN) therapy based on two primary contributions. First, our framework encodes the therapist-informed corrective force into via-points in a latent space, allowing the therapist to provide only minimal assistance while encouraging patient maintaining own motion preferences. Second, a shape-adaptive ANN rehabilitation policy is learned to partially and progressively deform the reference trajectory for movement therapy based on encoded patient motion preferences and therapist-informed via-points. The effectiveness of the proposed shape-adaptive AAN strategy was validated on a telerobotic rehabilitation system using two representative tasks. The results demonstrate its practicality for remote AAN therapy and its superiority over two state-of-the-art methods in reducing corrective force and improving movement smoothness.

Paper number 66:
Title: Dimensionally-Efficient Transmission and Storage of Unitary Matrices
Authors: Juan Vidal AlegrÃ­a
Abstract: Unitary matrices are the basis of a large number of signal processing applications. In many of these applications, finding ways to efficiently store, and even transmit these matrices, can significantly reduce memory and throughput requirements. In this work, we study the problem of efficient transmission and storage of unitary matrices. Specifically, we explicitly derive a dimensionally-efficient parametrization (DEP) for unitary matrices that allows identifying them with sequences of real numbers, where the dimension coincides with the dimension of the unitary group where they lie. We also characterize its inverse map that allows retrieving the original unitary matrices from their DEP. The proposed approach effectively allows halving the dimension with respect to naively considering all the entries of each unitary matrix, thus reducing the resources required to store and transmit these matrices. Furthermore, we show that the sequence of real numbers associated to the proposed DEP is bounded, and we delimit the interval where these numbers are contained, facilitating the implementation of quantization approaches with limited distortion. On the other hand, we outline ways to further reduce the dimension of the DEP when considering more restrictive constraints for matrices that show up in certain applications. The numerical results showcase the potential of the proposed approach in general settings, as well as in three specific applications of current interest for wireless communications research.

Paper number 67:
Title: Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS
Authors: Wali Ullah Khan, Chandan Kumar Sheemar, Eva Lagunas, Xingwang Li, Symeon Chatzinotas, Petar Popovski, Zhu Han
Abstract: In this work, we study a multi-user NTN in which a satellite serves as the primary network and a high-altitude platform station (HAPS) operates as the secondary network, acting as a cognitive radio. To reduce the cost, complexity, and power consumption of conventional antenna arrays, we equip the HAPS with a transmissive BD-RIS antenna front end. We then formulate a joint optimization problem for the BD-RIS phase response and the HAPS transmit power allocation under strict per-user interference temperature constraints. To tackle the resulting highly nonconvex problem, we propose an alternating-optimization framework: the power-allocation subproblem admits a closed-form, water-filling-type solution derived from the Karush-Kuhn-Tucker (KKT) conditions, while the BD-RIS configuration is refined via Riemannian manifold optimization. Simulation results show significant gains in data rate and interference suppression over diagonal RIS-assisted benchmarks, establishing BD-RIS as a promising enabler for future multilayer NTNs.

Paper number 68:
Title: Interference Alignment for Multi-cluster Over-the-Air Computation
Authors: Lucas SempÃ©rÃ©, Yue Bi, Yue Wu, Pengwenlong Gu, Selma Boumerdassi
Abstract: One of the main challenges facing Internet of Things (IoT) networks is managing interference caused by the large number of devices communicating simultaneously, particularly in multi-cluster networks where multiple devices simultaneously transmit to their respective receiver. Over-the-Air Computation (AirComp) has emerged as a promising solution for efficient real-time data aggregation, yet its performance suffers in dense, interference-limited environments. To address this, we propose a novel Interference Alignment (IA) scheme tailored for up-link AirComp systems. Unlike previous approaches, the proposed method scales to an arbitrary number $\sf K$ of clusters and enables each cluster to exploit half of the available channels, instead of only $\tfrac{1}{\sf K}$ as in time-sharing. In addition, we develop schemes tailored to scenarios where users are shared between adjacent clusters.

Paper number 69:
Title: MPC strategies for density profile control with pellet fueling in nuclear fusion tokamaks under uncertainty
Authors: Christopher A. Orrico, Hari Prasad Varadarajan, Matthijs van Berkel, Lennard Ceelen, Thomas O. S. J. Bosman, W. P. M. H. Heemels, Dinesh Krishnamoorthy
Abstract: Control of the density profile based on pellet fueling for the ITER nuclear fusion tokamak involves a multi-rate nonlinear system with safety-critical constraints, input delays, and discrete actuators with parametric uncertainty. To address this challenging problem, we propose a multi-stage MPC (msMPC) approach to handle uncertainty in the presence of mixed-integer inputs. While the scenario tree of msMPC accounts for uncertainty, it also adds complexity to an already computationally intensive mixed-integer MPC (MI-MPC) problem. To achieve real-time density profile controller with discrete pellets and uncertainty handling, we systematically reduce the problem complexity by (1) reducing the identified prediction model size through dynamic mode decomposition with control, (2) applying principal component analysis to reduce the number of scenarios needed to capture the parametric uncertainty in msMPC, and (3) utilizing the penalty term homotopy for MPC (PTH-MPC) algorithm to reduce the computational burden caused by the presence of mixed-integer inputs. We compare the performance and safety of the msMPC strategy against a nominal MI-MPC in plant simulations, demonstrating the first predictive density control strategy with uncertainty handling, viable for real-time pellet fueling in ITER.

Paper number 70:
Title: Efficient Probabilistic Planning with Maximum-Coverage Distributionally Robust Backward Reachable Trees
Authors: Alex Rose, Naman Aggarwal, Christopher Jewison, Jonathan P. How
Abstract: This paper presents a new multi-query motion planning algorithm for linear Gaussian systems with the goal of reaching a Euclidean ball with high probability. We develop a new formulation for ball-shaped ambiguity sets of Gaussian distributions and leverage it to develop a distributionally robust belief roadmap construction algorithm. This algorithm synthe- sizes robust controllers which are certified to be safe for maximal size ball-shaped ambiguity sets of Gaussian distributions. Our algorithm achieves better coverage than the maximal coverage algorithm for planning over Gaussian distributions [1], and we identify mild conditions under which our algorithm achieves strictly better coverage. For the special case of no process noise or state constraints, we formally prove that our algorithm achieves maximal coverage. In addition, we present a second multi-query motion planning algorithm for linear Gaussian systems with the goal of reaching a region parameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with high probability. This algorithm plans over ellipsoidal sets of maximal size ball-shaped ambiguity sets of Gaussian distributions, and provably achieves equal or better coverage than the best-known algorithm for planning over ellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the efficacy of both methods in a wide range of conditions via extensive simulation experiments.

Paper number 71:
Title: Robust stability of event-triggered nonlinear moving horizon estimation
Authors: Isabelle Krauss, Victor G. Lopez, Matthias A. MÃ¼ller
Abstract: In this work, we propose an event-triggered moving horizon estimation (ET-MHE) scheme for the remote state estimation of general nonlinear systems. In the presented method, whenever an event is triggered, a single measurement is transmitted and the nonlinear MHE optimization problem is subsequently solved. If no event is triggered, the current state estimate is updated using an open-loop prediction based on the system dynamics. Moreover, we introduce a novel event-triggering rule under which we demonstrate robust global exponential stability of the ET-MHE scheme, assuming a suitable detectability condition is met. In addition, we show that with the adoption of a varying horizon length, a tighter bound on the estimation error can be achieved. Finally, we validate the effectiveness of the proposed method through two illustrative examples.

Paper number 72:
Title: Power Reserve Capacity from Virtual Power Plants with Reliability and Cost Guarantees
Authors: Lorenzo Zapparoli, Blazhe Gjorgiev, Giovanni Sansavini
Abstract: The growing penetration of renewable energy sources is expected to drive higher demand for power reserve ancillary services (AS). One solution is to increase the supply by integrating distributed energy resources (DERs) into the AS market through virtual power plants (VPPs). Several methods have been developed to assess the potential of VPPs to provide services. However, the existing approaches fail to account for AS products' requirements (reliability and technical specifications) and to provide accurate cost estimations. Here, we propose a new method to assess VPPs' potential to deliver power reserve capacity products under forecasting uncertainty. First, the maximum feasible reserve quantity is determined using a novel formulation of subset simulation for efficient uncertainty quantification. Second, the supply curve is characterized by considering explicit and opportunity costs. The method is applied to a VPP based on a representative Swiss low-voltage network with a diversified DER portfolio. We find that VPPs can reliably offer reserve products and that opportunity costs drive product pricing. Additionally, we show that the product's requirements strongly impact the reserve capacity provision capability. This approach aims to support VPP managers in developing market strategies and policymakers in designing DER-focused AS products.

Paper number 73:
Title: An Active Fault-Tolerant Online Control Allocation Scheme for a Dual-System UAV in Transition Flight
Authors: Junfeng Cai, Marco Lovera
Abstract: A novel active fault-tolerant control (AFTC) scheme for a dual-system vertical takeoff and landing (VTOL) unmanned aerial vehicle (UAV) during transition flight is proposed in this paper. The AFTC scheme is composed of a baseline control law and an online control reallocation module. First, the structured $H_{\infty}$ baseline control law is able to guarantee the stability of closed-loop systems without being reconfigured under simultaneous actuator fault conditions. Second, compared to the existing mainstream method of sliding mode control that is a discontinuous control strategy, the AFTC scheme can effectively avoid control chattering problem by adopting the structured $H_{\infty}$ baseline control law. Third, an online control allocation (CA) module is implemented to carry out a unified CA for all the available actuators. When actuator faults/failures occur, the CA matrix is updated according to fault information and real-time airspeed, which is able to redistribute the virtual control signals to the remaining healthy actuators, avoiding significant performance degradation. Based on the developed AFTC scheme, symmetric and non-symmetric actuator fault scenarios are simulated on a nonlinear six-degree-of-freedom simulator, where the cases of merely structured $H_{\infty}$ control and structured $H_{\infty}$ based AFTC are compared and analyzed. The results show that the proposed structured $H_{\infty}$ based AFTC system is capable of handling more complicated fault scenarios and model uncertainties with no need to reconfigure the baseline control law. The proposed AFTC scheme significantly improves the safety and reliability of the transition flight of dual-system VTOL UAVs.

Paper number 74:
Title: Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing
Authors: Seyed Soroush Karimi Madahi, Kenneth Bruninx, Bert Claessens, Chris Develder
Abstract: In Europe, profit-seeking balance responsible parties can deviate in real time from their day-ahead nominations to assist transmission system operators in maintaining the supply-demand balance. Model predictive control (MPC) strategies to exploit these implicit balancing strategies capture arbitrage opportunities, but fail to accurately capture the price-formation process in the European imbalance markets and face high computational costs. Model-free reinforcement learning (RL) methods are fast to execute, but require data-intensive training and usually rely on real-time and historical data for decision-making. This paper proposes an MPC-guided RL method that combines the complementary strengths of both MPC and RL. The proposed method can effectively incorporate forecasts into the decision-making process (as in MPC), while maintaining the fast inference capability of RL. The performance of the proposed method is evaluated on the implicit balancing battery control problem using Belgian balancing data from 2023. First, we analyze the performance of the standalone state-of-the-art RL and MPC methods from various angles, to highlight their individual strengths and limitations. Next, we show an arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and 54.36%, compared to standalone RL and MPC.

Paper number 75:
Title: The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II
Authors: Andreas Bathelt, Benjamin Deutschmann, Hyeon Seok Rou, Kuranage Roche Rayan Ranasinghe, Giuseppe Thadeu Freitas de Abreu, Peter Vouras
Abstract: In every imaging or sensing application, the physical hardware creates constraints that must be overcome or they limit system performance. Techniques that leverage additional degrees of freedom can effectively extend performance beyond the inherent physical capabilities of the hardware. An example includes synchronizing distributed sensors so as to synthesize a larger aperture for remote sensing applications. An additional example is integrating the communication and sensing functions in a wireless system through the clever design of waveforms and optimized resource management. As these technologies mature beyond the conceptual and prototype phase they will ultimately transition to the commercial market. Here, standards play a critical role in ensuring success. Standards ensure interoperability between systems manufactured by different vendors and define industry best practices for vendors and customers alike. The Signal Processing Society of the Institute for Electrical and Electronics Engineers (IEEE) plays a leading role in developing high-quality standards for computational sensing technologies through the working groups of the Synthetic Aperture Standards Committee (SASC). In this column we highlight the standards activities of the P3383 Performance Metrics for Integrated Sensing and Communication (ISAC) Systems Working Group and the P3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed Sensors Working Group.

Paper number 76:
Title: Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation
Authors: Ardavan Rahimian
Abstract: We study how far a diffusion process on a graph can drift from a designed starting pattern when that pattern is produced using Laplacian regularisation. Under standard stability conditions for undirected, entrywise nonnegative graphs, we give a closed-form, instance-specific upper bound on the steady-state spread, measured as the relative change between the final and initial profiles. The bound separates two effects: (i) an irreducible term determined by the graph's maximum node degree, and (ii) a design-controlled term that shrinks as the regularisation strength increases (following an inverse square-root law). This leads to a simple design rule: given any target limit on spread, one can choose a sufficient regularisation strength in closed form. Although one motivating application is array beamforming, where the initial pattern is the squared magnitude of the beamformer weights, the result applies to any scenario that first enforces Laplacian smoothness and then evolves by linear diffusion on a graph. Overall, the guarantee is non-asymptotic, easy to compute, and certifies how much steady-state deviation can occur.

Paper number 77:
Title: AURA Score: A Metric For Holistic Audio Question Answering Evaluation
Authors: Satvik Dixit, Soham Deshmukh, Bhiksha Raj
Abstract: Audio Question Answering (AQA) is a key task for evaluating Audio-Language Models (ALMs), yet assessing open-ended responses remains challenging. Existing metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from NLP and audio captioning, rely on surface similarity and fail to account for question context, reasoning, and partial correctness. To address the gap in literature, we make three contributions in this work. First, we introduce AQEval to enable systematic benchmarking of AQA metrics. It is the first benchmark of its kind, consisting of 10k model responses annotated by multiple humans for their correctness and relevance. Second, we conduct a comprehensive analysis of existing AQA metrics on AQEval, highlighting weak correlation with human judgment, especially for longer answers. Third, we propose a new metric - AURA score, to better evaluate open-ended model responses. On AQEval, AURA achieves state-of-the-art correlation with human ratings, significantly outperforming all baselines. Through this work, we aim to highlight the limitations of current AQA evaluation methods and motivate better metrics. We release both the AQEval benchmark and the AURA metric to support future research in holistic AQA evaluation.

Paper number 78:
Title: Perceptual Evaluation of Extrapolated Spatial Room Impulse Responses From a Mono Source
Authors: Ben Heritage, Fiona Ryder, Michael McLoughlin, Karolina Prawda
Abstract: Immersion in virtual and augmented reality solutions is reliant on plausible spatial audio. However, plausibly representing a space for immersive audio often requires many individual acoustic measurements of source-microphone pairs with specialist spatial microphones, making the procedure time-consuming and expensive. In this study, we evaluate the plausibility of extrapolated and spatialised Room Impulse Responses (RIRs) by using a 3-Alternative Forced Choice (3AFC) listening test. The stimuli comprised of RIRs from three spaces convolved with speech, orchestral, and instrumental music. When asked to select which stimuli was artificial out of one extrapolated and two real stimuli, an overall accuracy of 38% was achieved from 20 participants (5 percentage points above the expected guessing rate). Given the listening test result, this study shows that it is possible to extrapolate plausible spatial RIRs from mono measurements, decreasing the need for time and specialist equipment in acoustic measurements.

Paper number 79:
Title: Robust Cislunar Navigation via LFT-Based $\mathcal{H}_\infty$ Filtering with Bearing-Only Measurements
Authors: Raktim Bhattacharya
Abstract: This paper develops a robust estimation framework for cislunar navigation that embeds the Circular Restricted Three-Body Problem (CR3BP) dynamics and bearing-only optical measurements within a Linear Fractional Transformation (LFT) representation. A full-order $\mathcal{H}_\infty$ observer is synthesized with explicit $\mathcal{L}_2$ performance bounds. The formulation yields a nonlinear estimator that operates directly on the governing equations and avoids reliance on local linearizations. Dominant nonlinearities are expressed as structured real uncertainties, while measurement fidelity is represented through range-dependent weighting with Earth-Moon distances reconstructed from line-of-sight geometry. The sensing architecture assumes passive star-tracker-class optical instruments, eliminating the need for time-of-flight ranging or precision clocks. Simulations demonstrate bounded estimation errors and smooth position tracking over multiple orbital periods, with the largest deviations observed in the out-of-plane states, consistent with the stiffness of the vertical dynamics and the limitations of angle-only observability. Application to a Near Rectilinear Halo Orbit (NRHO) illustrates that the framework can achieve robust onboard navigation with bounded estimation errors with flight-representative sensors.

Paper number 80:
Title: MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling
Authors: Bi-Cheng Yan, Ming-Kang Tsai, Berlin Chen
Abstract: Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.

Paper number 81:
Title: My First Five Years of Faculty Career at the University of Delaware
Authors: Xiang-Gen Xia
Abstract: In this short article, I would like to briefly summarize my research in the first 5 years in my university academia life in USA. I think that my research results obtained in these 5 years are the best in my career, at least which I like the most by myself. I wish that my experience in my junior academia career could be of some help to young researchers.

Paper number 82:
Title: Multi-Loop Design of Virtual Synchronous Machine Control for DFIG-Based Wind Farms
Authors: Javier Garcia-Aguilar, Aurelio Garcia-Cerrada, Juan L. Zamora, Emilio Bueno, Elena Saiz, Almudena MuÃ±oz-Babiano, Mohammad E. Zarei
Abstract: The displacement of synchronous generators by converter-interfaced renewable energy sources obliges wind farms to provide inertia, damping, and voltage support, above all in increasingly weak grid conditions. This paper presents a co-ordinated frequency-domain methodology for tuning all control layers of doubly-fed induction generators (DFIGs) within a wind farm operated as a Virtual Synchronous Machine (VSM). Starting from a full small-signal linearisation that preserves loop-to-loop and machine-to-machine couplings, the procedure reshapes every local open loop to explicit phase-margin targets through a single, prioritised iteration. The resulting controllers provide a step response and stability margins close to those programmed at the design stage, in spite of the cross coupling between control loops. Since controller synthesis relies exclusively on classical loop-shaping tools available in commercial simulation suites, it is readily applicable to industrial-scale projects.

Paper number 83:
Title: PowerPlots: An Open Source Power Grid Visualization and Data Analysis Framework for Academic Research
Authors: Noah Rhodes
Abstract: Data visualization is important for developing an understanding of a complex system. this http URL is a data visualization tool for power grids, one of the most complex systems in the world. The design of this http URL is intended to facilitate exploration of power grid data while performing research and to facilitate communication of research findings to an audience. Several tools created to support this software also facilitate analysis of power grid data by transforming the data into graph topology or data-frame data formats that are more compatible for some applications. The high level of flexibility in this http URL enables researchers who are developing and analyzing methods for solving novel power grid problems to better understand and communicate the complexities of their research.

Paper number 84:
Title: A Modular Conditional Diffusion Framework for Image Reconstruction
Authors: Magauiya Zhussip, Iaroslav Koshelev, Stamatis Lefkimmiatis
Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality. However, the task-specific nature of existing solutions and the excessive computational costs related to their training, make such models impractical and challenging to use for different IR tasks than those that were initially trained for. This hinders their wider adoption, especially by those who lack access to powerful computational resources and vast amount of training data. In this work we aim to address the above issues and enable the successful adoption of DPMs in practical IR-related applications. Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a relatively small module (0.7M params) related to the particular IR task of interest. Moreover, the architecture of the proposed framework allows for a sampling strategy that leads to at least four times reduction of neural function evaluations without suffering any performance loss, while it can also be combined with existing acceleration techniques such as DDIM. We evaluate our model on four benchmarks for the tasks of burst JDD-SR, dynamic scene deblurring, and super-resolution. Our method outperforms existing approaches in terms of perceptual quality while it retains a competitive performance with respect to fidelity metrics.

Paper number 85:
Title: Textured Gaussians for Enhanced 3D Scene Appearance Modeling
Authors: Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim
Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.

Paper number 86:
Title: Atlas-free Brain Network Transformer
Authors: Shuai Huang, Xuan Kan, James J. Lah, Deqiang Qiu
Abstract: Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predefined regions, and atlas-selection biases, collectively undermining the reliability and interpretability of the derived brain networks. To address these challenges, we propose a novel atlas-free brain network transformer (atlas-free BNT) that leverages individualized brain parcellations derived directly from subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel connectivity features in a standardized voxel-based feature space, which are subsequently processed using the BNT architecture to produce comparable subject-level embeddings. Experimental evaluations on sex classification and brain-connectome age prediction tasks demonstrate that our atlas-free BNT consistently outperforms state-of-the-art atlas-based methods, including elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach significantly improves the precision, robustness, and generalizability of brain network analyses. This advancement holds great potential to enhance neuroimaging biomarkers and clinical diagnostic tools for personalized precision medicine.

Paper number 87:
Title: Universal Beta Splatting
Authors: Rong Liu, Zhongpai Gao, Benjamin Planche, Meida Chen, Van Nguyen Nguyen, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Yue Wang, Andrew Feng, Ziyan Wu
Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at this https URL.

Paper number 88:
Title: Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment
Authors: Ameya Daigavane, YuQing Xie, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Tess Smidt
Abstract: Diffusion models are a popular class of generative models trained to reverse a noising process starting from a target data distribution. Training a diffusion model consists of learning how to denoise noisy samples at different noise levels. When training diffusion models for point clouds such as molecules and proteins, there is often no canonical orientation that can be assigned. To capture this symmetry, the true data samples are often augmented by transforming them with random rotations sampled uniformly over $SO(3)$. Then, the denoised predictions are often rotationally aligned via the Kabsch-Umeyama algorithm to the ground truth samples before computing the loss. However, the effect of this alignment step has not been well studied. Here, we show that the optimal denoiser can be expressed in terms of a matrix Fisher distribution over $SO(3)$. Alignment corresponds to sampling the mode of this distribution, and turns out to be the zeroth order approximation for small noise levels, explaining its effectiveness. We build on this perspective to derive better approximators to the optimal denoiser in the limit of small noise. Our experiments highlight that alignment is often a `good enough' approximation for the noise levels that matter most for training diffusion models.

Paper number 89:
Title: Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks
Authors: Song Wang, Zhenyu Lei, Zhen Tan, Jundong Li, Javier Rasero, Aiying Zhang, Chirag Agarwal
Abstract: Nearly one in five adolescents currently live with a diagnosed mental or behavioral health condition, such as anxiety, depression, or conduct disorder, underscoring the urgency of developing accurate and interpretable diagnostic tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a powerful lens into large-scale functional connectivity, where brain regions are modeled as nodes and inter-regional synchrony as edges, offering clinically relevant biomarkers for psychiatric disorders. While prior works use graph neural network (GNN) approaches for disorder prediction, they remain complex black-boxes, limiting their reliability and clinical translation. In this work, we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages large language models (LLMs) and neurobiological domain knowledge to automatically generate, filter, and encode interpretable functional connectivity concepts. Each concept is represented as a structured subgraph linking specific brain regions, which are then passed through a concept classifier. Our design ensures predictions through clinically meaningful connectivity patterns, enabling both interpretability and strong predictive performance. Extensive experiments across multiple psychiatric disorder datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform their vanilla counterparts, improving accuracy while providing transparent, clinically aligned explanations. Furthermore, concept analyses highlight disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses for future investigation, establishing CONCEPTNEURO as an interpretable, domain-informed framework for psychiatric disorder diagnosis.

Paper number 90:
Title: Unified Unsupervised Anomaly Detection via Matching Cost Filtering
Authors: Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu
Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level anomalies using only normal training data, with wide applications such as industrial inspection and medical analysis, where anomalies are scarce due to privacy concerns and cold-start constraints. Existing methods, whether reconstruction-based (restoring normal counterparts) or embedding-based (pretrained representations), fundamentally conduct image- or feature-level matching to generate anomaly maps. Nonetheless, matching noise has been largely overlooked, limiting their detection ability. Beyond earlier focus on unimodal RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D and RGB--Text, enabled by point cloud sensing and vision--language models. Despite shared challenges, these lines remain largely isolated, hindering a comprehensive understanding and knowledge transfer. In this paper, we advocate unified UAD for both unimodal and multimodal settings in the matching perspective. Under this insight, we present Unified Cost Filtering (UCF), a generic post-hoc refinement framework for refining anomaly cost volume of any UAD model. The cost volume is constructed by matching a test sample against normal samples from the same or different modalities, followed by a learnable filtering module with multi-layer attention guidance from the test sample, mitigating matching noise and highlighting subtle anomalies. Comprehensive experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in enhancing a variety of UAD methods, consistently achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD scenarios. Code and models will be released at this https URL.

Paper number 91:
Title: Visual Language Model as a Judge for Object Detection in Industrial Diagrams
Authors: Sanjukta Ghosh
Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are essential for the design, operation, and maintenance of industrial plants. Converting these diagrams into digital form is an important step toward building digital twins and enabling intelligent industrial automation. A central challenge in this digitalization process is accurate object detection. Although recent advances have significantly improved object detection algorithms, there remains a lack of methods to automatically evaluate the quality of their outputs. This paper addresses this gap by introducing a framework that employs Visual Language Models (VLMs) to assess object detection results and guide their refinement. The approach exploits the multimodal capabilities of VLMs to identify missing or inconsistent detections, thereby enabling automated quality assessment and improving overall detection performance on complex industrial diagrams.

Paper number 92:
Title: Audio Forensics Evaluation (SAFE) Challenge
Authors: Kirill Trapeznikov, Paul Cummer, Pranay Pherwani, Jai Aslam, Michael S. Davinroy, Peter Bautista, Laura Cassani, Matthew Stamm, Jill Crisman
Abstract: The increasing realism of synthetic speech generated by advanced text-to-speech (TTS) models, coupled with post-processing and laundering techniques, presents a significant challenge for audio forensic detection. In this paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation) Challenge, a fully blind evaluation framework designed to benchmark detection models across progressively harder scenarios: raw synthetic speech, processed audio (e.g., compression, resampling), and laundered audio intended to evade forensic analysis. The SAFE challenge consisted of a total of 90 hours of audio and 21,000 audio samples split across 21 different real sources and 17 different TTS models and 3 tasks. We present the challenge, evaluation design and tasks, dataset details, and initial insights into the strengths and limitations of current approaches, offering a foundation for advancing synthetic audio detection research. More information is available at \href{this https URL}{this https URL}.

Paper number 93:
Title: Efficient Input-Constrained Impulsive Optimal Control of Linear Systems with Application to Spacecraft Relative Motion
Authors: Ethan Foss, Simone D'Amico
Abstract: This work presents a novel algorithm for impulsive optimal control of linear time-varying systems with the inclusion of input magnitude constraints. Impulsive optimal control problems, where the optimal input solution is a sum of delta functions, are typically formulated as an optimization over a normed function space subject to integral equality constraints and can be efficiently solved for linear time-varying systems in their dual formulation. In this dual setting, the problem takes the form of a semi-infinite program which is readily solvable in online scenarios for constructing maneuver plans. This work augments the approach with the inclusion of magnitude constraints on the input over time windows of interest, which is shown to preserve the impulsive nature of the optimal solution and enable efficient solution procedures via semi-infinite programming. The resulting algorithm is demonstrated on the highly relevant problem of relative motion control of spacecraft in Low Earth Orbit (LEO) and compared to several other proposed solutions from the literature.

Paper number 94:
Title: Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography
Authors: Refik Mert Cam, Seonyeong Park, Umberto Villa, Mark A. Anastasio
Abstract: Quantitative photoacoustic computed tomography (qPACT) is a promising imaging modality for estimating physiological parameters such as blood oxygen saturation. However, developing robust qPACT reconstruction methods remains challenging due to computational demands, modeling difficulties, and experimental uncertainties. Learning-based methods have been proposed to address these issues but remain largely unvalidated. Virtual imaging (VI) studies are essential for validating such methods early in development, before proceeding to less-controlled phantom or in vivo studies. Effective VI studies must employ ensembles of stochastically generated numerical phantoms that accurately reflect relevant anatomy and physiology. Yet, most prior VI studies for qPACT relied on overly simplified phantoms. In this work, a realistic VI testbed is employed for the first time to assess a representative 3D learning-based qPACT reconstruction method for breast imaging. The method is evaluated across subject variability and physical factors such as measurement noise and acoustic aberrations, offering insights into its strengths and limitations.

Paper number 95:
Title: Scalable Ground Station Selection for Large LEO Constellations
Authors: Grace Ra Kim, Duncan Eddy, Vedant Srinivas, Mykel J. Kochenderfer
Abstract: Effective ground station selection is critical for low Earth orbiting (LEO) satellite constellations to minimize operational costs, maximize data downlink volume, and reduce communication gaps between access windows. Traditional ground station selection typically begins by choosing from a fixed set of locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps reduce the problem scope to optimizing locations over existing infrastructure. However, finding a globally optimal solution for stations using existing mixed-integer programming methods quickly becomes intractable at scale, especially when considering multiple providers and large satellite constellations. To address this issue, we introduce a scalable, hierarchical framework that decomposes the global selection problem into single-satellite, short time-window subproblems. Optimal station choices from each subproblem are clustered to identify consistently high-value locations across all decomposed cases. Cluster-level sets are then matched back to the closest GSaaS candidate sites to produce a globally feasible solution. This approach enables scalable coordination while maintaining near-optimal performance. We evaluate our method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10 stations), achieving solutions within 95% of the global IP optimum for all test cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and Planet's Flock (96) show that while exact IP solutions fail to scale, our framework continues to deliver high-quality site selections.

Paper number 96:
Title: Cooling Under Convexity: An Inventory Control Perspective on Industrial Refrigeration
Authors: Vade Shah, Yohan John, Ethan Freifeld, Lily Y. Chen, Jason R. Marden
Abstract: Industrial refrigeration systems have substantial energy needs, but optimizing their operation remains challenging due to the tension between minimizing energy costs and meeting strict cooling requirements. Load shifting--strategic overcooling in anticipation of future demands--offers substantial efficiency gains. This work seeks to rigorously quantify these potential savings through the derivation of optimal load shifting policies. Our first contribution establishes a novel connection between industrial refrigeration and inventory control problems with convex ordering costs, where the convexity arises from the relationship between energy consumption and cooling capacity. Leveraging this formulation, we derive three main theoretical results: (1) an optimal algorithm for deterministic demand scenarios, along with proof that optimal trajectories are non-increasing (a valuable structural insight for practical control); (2) performance bounds that quantify the value of load shifting as a function of cost convexity, demand variability, and temporal patterns; (3) a computationally tractable load shifting heuristic with provable near-optimal performance under uncertainty. Numerical simulations validate our theoretical findings, and a case study using real industrial refrigeration data demonstrates an opportunity for improved load shifting.

Paper number 97:
Title: A Sequential Quadratic Programming Perspective on Optimal Control
Authors: Abhijeet, Suman Chakravorty
Abstract: This paper offers a unified perspective on different approaches to the solution of optimal control problems through the lens of constrained sequential quadratic programming. In particular, it allows us to find the relationships between Newton's method, the iterative LQR (iLQR), and Differential Dynamic Programming (DDP) approaches to solve the problem. It is shown that the iLQR is a principled SQP approach, rather than simply an approximation of DDP by neglecting the Hessian terms, to solve optimal control problems that can be guaranteed to always produce a cost-descent direction and converge to an optimum; while Newton's approach or DDP do not have similar guarantees, especially far from an optimum. Our empirical evaluations on the pendulum and cart-pole swing-up tasks serve to corroborate the SQP-based analysis proposed in this paper.

Paper number 98:
Title: Robust Permissive Controller Synthesis for Interval MDPs
Authors: Khang Vo Huynh, David Parker, Lu Feng
Abstract: We address the problem of robust permissive controller synthesis for robots operating under uncertain dynamics, modeled as Interval Markov Decision Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition probabilities to vary within intervals, capturing epistemic uncertainty from sensing noise, actuation imprecision, and coarse system abstractions-common in robotics. Traditional controller synthesis typically yields a single deterministic strategy, limiting adaptability. In contrast, permissive controllers (multi-strategies) allow multiple actions per state, enabling runtime flexibility and resilience. However, prior work on permissive controller synthesis generally assumes exact transition probabilities, which is unrealistic in many robotic applications. We present the first framework for robust permissive controller synthesis on IMDPs, guaranteeing that all strategies compliant with the synthesized multi-strategy satisfy reachability or reward-based specifications under all admissible transitions. We formulate the problem as mixed-integer linear programs (MILPs) and propose two encodings: a baseline vertex-enumeration method and a scalable duality-based method that avoids explicit enumeration. Experiments on four benchmark domains show that both methods synthesize robust, maximally permissive controllers and scale to large IMDPs with up to hundreds of thousands of states.

Paper number 99:
Title: CANOPI: Contingency-Aware Nodal Optimal Power Investments with High Temporal Resolution
Authors: Thomas Lee, Andy Sun
Abstract: We present CANOPI, a novel algorithmic framework, for solving the Contingency-Aware Nodal Power Investments problem, a large-scale nonlinear optimization problem that jointly optimizes generation, storage, and transmission expansion. The underlying problem is nonlinear due to the impact of transmission upgrades on impedances, and the problem's large scale arises from the confluence of spatial and temporal resolutions. We propose algorithmic approaches to address these computational challenges. We pose a linear approximation of the overall nonlinear model, and develop a fixed-point algorithm to adjust for the nonlinear impedance feedback effect. We solve the large-scale linear expansion model with a specialized level-bundle method leveraging a novel interleaved approach to contingency constraint generation. We introduce a minimal cycle basis algorithm that improves the numerical sparsity of cycle-based DC power flow formulations, accelerating solve times for the operational subproblems. CANOPI is demonstrated on a 1493-bus Western Interconnection test system built from realistic-geography network data, with hourly operations spanning 52 week-long scenarios and a total possible set of 20 billion individual transmission contingency constraints. Numerical results quantify the reliability and economic benefits of fully incorporating transmission contingencies in integrated planning models and highlight the computational advantages of the proposed methods.

Paper number 100:
Title: Platonic Transformers: A Solid Choice For Equivariance
Authors: Mohammad Mohaiminul Islam, Rishabh Anand, David R. Wessels, Friso de Kruiff, Thijs P. Kuipers, Rex Ying, Clara I. SÃ¡nchez, Sharvaree Vadgama, Georg BÃ¶kman, Erik J. Bekkers
Abstract: While widespread, Transformers lack inductive biases for geometric symmetries common in science and computer vision. Existing equivariant methods often sacrifice the efficiency and flexibility that make Transformers so effective through complex, computationally intensive designs. We introduce the Platonic Transformer to resolve this trade-off. By defining attention relative to reference frames from the Platonic solid symmetry groups, our method induces a principled weight-sharing scheme. This enables combined equivariance to continuous translations and Platonic symmetries, while preserving the exact architecture and computational cost of a standard Transformer. Furthermore, we show that this attention is formally equivalent to a dynamic group convolution, which reveals that the model learns adaptive geometric filters and enables a highly scalable, linear-time convolutional variant. Across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25), the Platonic Transformer achieves competitive performance by leveraging these geometric constraints at no additional cost.

Paper number 101:
Title: Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models
Authors: Kartik Pandit, Sourav Ganguly, Arnesh Banerjee, Shaahin Angizi, Arnob Ghosh
Abstract: Ensuring safety is a foundational requirement for large language models (LLMs). Achieving an appropriate balance between enhancing the utility of model outputs and mitigating their potential for harm is a complex and persistent challenge. Contemporary approaches frequently formalize this problem within the framework of Constrained Markov Decision Processes (CMDPs) and employ established CMDP optimization techniques. However, these methods exhibit two notable limitations. First, their reliance on reward and cost functions renders performance highly sensitive to the underlying scoring mechanism, which must capture semantic meaning rather than being triggered by superficial keywords. Second, CMDP-based training entails tuning dual-variable, a process that is both computationally expensive and does not provide any provable safety guarantee for a fixed dual variable that can be exploitable through adversarial jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF (CS-RLHF) that introduces a cost model trained on a large-scale corpus to assign semantically grounded safety scores. In contrast to the lagrangian-based approach, CS-RLHF adopts a rectified penalty-based formulation. This design draws on the theory of exact penalty functions in constrained optimization, wherein constraint satisfaction is enforced directly through a suitably chosen penalty term. With an appropriately scaled penalty, feasibility of the safety constraints can be guaranteed at the optimizer, eliminating the need for dual-variable updates. Empirical evaluation demonstrates that CS-RLHF outperforms state-of-the-art LLM model responses rendering at-least 5 times efficient against nominal and jail-breaking prompts

Paper number 102:
Title: Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning
Authors: NicolÃ² Dal Fabbro, Milad Mesbahi, Renato Mendes, JoÃ£o Borges de Sousa, George J. Pappas
Abstract: We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.

Paper number 103:
Title: Generalization of Graph Neural Network Models for Distribution Grid Fault Detection
Authors: Burak Karabulut, Carlo Manna, Chris Develder
Abstract: Fault detection in power distribution grids is critical for ensuring system reliability and preventing costly outages. Moreover, fault detection methodologies should remain robust to evolving grid topologies caused by factors such as reconfigurations, equipment failures, and Distributed Energy Resource (DER) integration. Current data-driven state-of-the-art methods use Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in short). Specifically, for power system fault diagnosis, Graph Convolutional Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures have been proposed and adopted in domains outside of power systems. In this paper, we set out to systematically and consistently benchmark various GNN architectures in an RNN+GNN pipeline model. Specifically, to the best of our knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention (GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring their generalization potential for deployment in different settings than those used for training them. Our experimental results on the IEEE 123-node distribution network show that RGATv2 has superior generalization capabilities, maintaining high performance with an F1-score reduction of $\sim$12% across different topology settings. In contrast, pure RNN models largely fail, experiencing an F1-score reduction of up to $\sim$60%, while other RGNN variants also exhibit significant performance degradation, i.e., up to $\sim$25% lower F1-scores.

Paper number 104:
Title: MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation
Authors: Wei-Lung Mao, Chun-Chi Wang, Po-Heng Chou, Kai-Chun Liu, Yu Tsao
Abstract: The rising aging population has increased the importance of fall detection (FD) systems as an assistive technology, where deep learning techniques are widely applied to enhance accuracy. FD systems typically use edge devices (EDs) worn by individuals to collect real-time data, which are transmitted to a cloud center (CC) or processed locally. However, this architecture faces challenges such as a limited ED model size and data transmission latency to the CC. Mobile edge computing (MEC), which allows computations at MEC servers deployed between EDs and CC, has been explored to address these challenges. We propose a multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC splits the architecture into stations, each with a neural network model. If front-end equipment cannot detect falls reliably, data are transmitted to a station with more robust back-end computing. The knowledge distillation (KD) approach was employed to improve front-end detection accuracy by allowing high-power back-end stations to provide additional learning experiences, enhancing precision while reducing latency and processing loads. Simulation results demonstrate that the KD approach improved accuracy by 11.65% on the SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD system exhibits improved accuracy and reduced latency.

Paper number 105:
Title: Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops
Authors: Mattia Scardecchia
Abstract: Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.

Paper number 106:
Title: Safety-Oriented Dynamic Path Planning for Automated Vehicles
Authors: Mostafa Emam, Matthias Gerdts
Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning and obstacle avoidance capabilities, particularly in dynamic environments. This paper introduces a bi-level control framework that efficiently augments road boundaries by incorporating time-dependent grid projections of obstacle movements, thus enabling precise and adaptive path planning. The main control loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path optimization, wherein homotopy-based constraint relaxation is employed to improve the solvability of the optimal control problem (OCP). Furthermore, an independent backup loop runs concurrently to provide safe fallback trajectories when an optimal trajectory cannot be computed by the main loop within a critical time frame, thus enhancing safety and real-time performance. Our evaluation showcases the benefits of the proposed methods in various driving scenarios, highlighting the real-time applicability and robustness of our approach. Overall, the framework represents a significant step towards safer and more reliable autonomous driving in complex and dynamic environments.

Paper number 107:
Title: Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast
Authors: Aymeric Fabre
Abstract: In electricity markets around the world, the ability to anticipate price movements with precision can be the difference between profit and loss, especially for fast-acting assets like battery energy storage systems (BESS). As grid volatility increases due to renewables and market decentralisation, operators and forecasters alike face growing pressure to transform prediction into strategy. Yet while forecast data is abundant, especially in advanced markets like Australia's National Electricity Market (NEM), its practical value in driving real-world BESS trading decisions remains largely unexplored. This thesis dives into that gap. This work addresses a key research question: Can the accuracy of the Australian Energy Market Operator (AEMO) energy price forecasts be systematically leveraged to develop a reliable and profitable battery energy storage system trading algorithm? Despite the availability of AEMO price forecasts, no existing framework evaluates their reliability or incorporates them into practical BESS trading strategies. By analysing patterns in forecast accuracy based on time of day, forecast horizon, and regional variations, this project creates a novel, forecast-informed BESS trading model to optimise arbitrage financial returns. The performance of this forecast-driven algorithm is benchmarked against a basic trading algorithm with no knowledge of forecast data. The study further explores the potential of machine learning techniques to predict future energy prices by enhancing AEMO forecasts to govern a more advanced trading strategy. The research outcomes will inform future improvements in energy market trading models and promote more efficient BESS integration into market operations.

Paper number 108:
Title: Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents
Authors: Raaghav Malik, Satpreet H. Singh, Sonja Johnson-Yu, Nathan Wu, Roy Harpaz, Florian Engert, Kanaka Rajan
Abstract: Larval zebrafish hunting provides a tractable setting to study how ecological and energetic constraints shape adaptive behavior in both biological brains and artificial agents. Here we develop a minimal agent-based model, training recurrent policies with deep reinforcement learning in a bout-based zebrafish simulator. Despite its simplicity, the model reproduces hallmark hunting behaviors -- including eye vergence-linked pursuit, speed modulation, and stereotyped approach trajectories -- that closely match real larval zebrafish. Quantitative trajectory analyses show that pursuit bouts systematically reduce prey angle by roughly half before strike, consistent with measurements. Virtual experiments and parameter sweeps vary ecological and energetic constraints, bout kinematics (coupled vs. uncoupled turns and forward motion), and environmental factors such as food density, food speed, and vergence limits. These manipulations reveal how constraints and environments shape pursuit dynamics, strike success, and abort rates, yielding falsifiable predictions for neuroscience experiments. These sweeps identify a compact set of constraints -- binocular sensing, the coupling of forward speed and turning in bout kinematics, and modest energetic costs on locomotion and vergence -- that are sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors arise in minimal agents without detailed biomechanics, fluid dynamics, circuit realism, or imitation learning from real zebrafish data. Taken together, this work provides a normative account of zebrafish hunting as the optimal balance between energetic cost and sensory benefit, highlighting the trade-offs that structure vergence and trajectory dynamics. We establish a virtual lab that narrows the experimental search space and generates falsifiable predictions about behavior and neural coding.

Paper number 109:
Title: Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation
Authors: Kuang Yuan, Yang Gao, Xilin Li, Xinhao Mei, Syavosh Zadissa, Tarun Pruthi, Saeed Bagheri Sereshki
Abstract: Acoustic scene classification (ASC) models on edge devices typically operate under fixed class assumptions, lacking the transferability needed for real-world applications that require adaptation to new or refined acoustic categories. We propose ContrastASC, which learns generalizable acoustic scene representations by structuring the embedding space to preserve semantic relationships between scenes, enabling adaptation to unseen categories without retraining. Our approach combines supervised contrastive fine-tuning of pre-trained models with contrastive representation distillation to transfer this structured knowledge to compact student models. Our evaluation shows that ContrastASC demonstrates improved few-shot adaptation to unseen categories while maintaining strong closed-set performance.

Paper number 110:
Title: DÃ©sentrelacement FrÃ©quentiel Doux pour les Codecs Audio Neuronaux
Authors: BenoÃ®t GiniÃ¨s, Xiaoyu Bie, Olivier Fercoq, GaÃ«l Richard
Abstract: While neural-based models have led to significant advancements in audio feature extraction, the interpretability of the learned representations remains a critical challenge. To address this, disentanglement techniques have been integrated into discrete neural audio codecs to impose structure on the extracted tokens. However, these approaches often exhibit strong dependencies on specific datasets or task formulations. In this work, we propose a disentangled neural audio codec that leverages spectral decomposition of time-domain signals to enhance representation interpretability. Experimental evaluations demonstrate that our method surpasses a state-of-the-art baseline in both reconstruction fidelity and perceptual quality.

Paper number 111:
Title: Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics
Authors: Hanwen Zhang, Kun Fang, Ziyu Wang, Ichiro Fujinaga
Abstract: Evaluation for continuous piano pedal depth estimation tasks remains incomplete when relying only on conventional frame-level metrics, which overlook musically important features such as direction-change boundaries and pedal curve contours. To provide more interpretable and musically meaningful insights, we propose an evaluation framework that augments standard frame-level metrics with an action-level assessment measuring direction and timing using segments of press/hold/release states and a gesture-level analysis that evaluates contour similarity of each press-release cycle. We apply this framework to compare an audio-only baseline with two variants: one incorporating symbolic information from MIDI, and another trained in a binary-valued setting, all within a unified architecture. Results show that the MIDI-informed model significantly outperforms the others at action and gesture levels, despite modest frame-level gains. These findings demonstrate that our framework captures musically relevant improvements indiscernible by traditional metrics, offering a more practical and effective approach to evaluating pedal depth estimation models.

Paper number 112:
Title: Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech
Authors: Ilias Tougui, Mehdi Zakroum, Mounir Ghogho
Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (/a/, /e/, /o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) at syllable level, and /pataka/ sequences at word level. Source code will be available at this https URL.

Paper number 113:
Title: Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation
Authors: Shimaa Elbana, Ahmad Kamal, Shahd Ahmed Ali, Ahmad Al-Kabbany
Abstract: The increasing size and complexity of medical imaging datasets, particularly in 3D formats, present significant barriers to collaborative research and transferability. This study investigates whether the ZFP compression technique can mitigate these challenges without compromising the performance of automated cerebrovascular segmentation, a critical first step in intracranial aneurysm detection. We apply ZFP in both its error tolerance and fixed-rate modes to a large scale, and one of the most recent, datasets in the literature, 3D medical dataset containing ground-truth vascular segmentations. The segmentation quality on the compressed volumes is rigorously compared to the uncompressed baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance mode--while maintaining a high degree of fidelity, with the mean Dice coefficient remaining high at 0.87656. These results demonstrate that ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration across the community.

Paper number 114:
Title: HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control
Authors: Alex Durkin, Jasper Stolte, Mehmet MercangÃ¶z
Abstract: Start-ups and product grade-changes are critical steps in continuous-process plant operation, because any misstep immediately affects product quality and drives operational losses. These transitions have long relied on manual operation by a handful of expert operators, but the progressive retirement of that workforce is leaving plant owners without the tacit know-how needed to execute them consistently. In the absence of a process model, offline reinforcement learning (RL) promises to capture and even surpass human expertise by mining historical start-up and grade-change logs, yet standard offline RL struggles with distribution shift and value-overestimation whenever a learned policy ventures outside the data envelope. We introduce HOFLON (Hybrid Offline Learning + Online Optimization) to overcome those limitations. Offline, HOFLON learns (i) a latent data manifold that represents the feasible region spanned by past transitions and (ii) a long-horizon Q-critic that predicts the cumulative reward from state-action pairs. Online, it solves a one-step optimization problem that maximizes the Q-critic while penalizing deviations from the learned manifold and excessive rates of change in the manipulated variables. We test HOFLON on two industrial case studies: a polymerization reactor start-up and a paper-machine grade-change problem, and benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm. In both plants HOFLON not only surpasses IQL but also delivers, on average, better cumulative rewards than the best start-up or grade-change observed in the historical data, demonstrating its potential to automate transition operations beyond current expert capability.

Paper number 115:
Title: Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO
Authors: Pedro Ivo da Cruz, Dimitri Silva, Tito Spadini, Ricardo Suyama, Murilo Bellezoni Loiola
Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern wireless communication systems, like 5G and 6G, but it is vulnerable to active eavesdropping attacks. One type of such attack is the pilot contamination attack (PCA), where a malicious user copies pilot signals from an authentic user during uplink, intentionally interfering with the base station's (BS) channel estimation accuracy. In this work, we propose to use a Decision Tree (DT) algorithm for PCA detection at the BS in a multi-user system. We present a methodology to generate training data for the DT classifier and select the best DT according to their depth. Then, we simulate different scenarios that could be encountered in practice and compare the DT to a classical technique based on likelihood ratio testing (LRT) submitted to the same scenarios. The results revealed that a DT with only one level of depth is sufficient to outperform the LRT. The DT shows a good performance regarding the probability of detection in noisy scenarios and when the malicious user transmits with low power, in which case the LRT fails to detect the PCA. We also show that the reason for the good performance of the DT is its ability to compute a threshold that separates PCA data from non-PCA data better than the LRT's threshold. Moreover, the DT does not necessitate prior knowledge of noise power or assumptions regarding the signal power of malicious users, prerequisites typically essential for LRT and other hypothesis testing methodologies.

Paper number 116:
Title: From Qubits to Rhythm: Exploring Quantum Random Walks in Rhythmspaces
Authors: MarÃ­a Aguado-YÃ¡Ã±ez, Karl Jansen, Daniel GÃ³mez-MarÃ­n, Sergi JordÃ 
Abstract: A quantum computing algorithm for rhythm generation is presented, which aims to expand and explore quantum computing applications in the arts, particularly in music. The algorithm maps quantum random walk trajectories onto a rhythmspace -- a 2D interface that interpolates rhythmic patterns. The methodology consists of three stages. The first stage involves designing quantum computing algorithms and establishing a mapping between the qubit space and the rhythmspace. To minimize circuit depth, a decomposition of a 2D quantum random walk into two 1D quantum random walks is applied. The second stage focuses on biasing the directionality of quantum random walks by introducing classical potential fields, adjusting the probability distribution of the wave function based on the position gradient within these fields. Four potential fields are implemented: a null potential, a linear field, a Gaussian potential, and a Gaussian potential under inertial dynamics. The third stage addresses the sonification of these paths by generating MIDI drum pattern messages and transmitting them to a Digital Audio Workstation (DAW). This work builds upon existing literature that applies quantum computing to simpler qubit spaces with a few positions, extending the formalism to a 2D x-y plane. It serves as a proof of concept for scalable quantum computing-based generative random walk algorithms in music and audio applications. Furthermore, the approach is applicable to generic multidimensional sound spaces, as the algorithms are not strictly constrained to rhythm generation and can be adapted to different musical structures.

Paper number 117:
Title: Privacy Enhancement in Over-the-Air Federated Learning via Adaptive Receive Scaling
Authors: Faeze Moradi Kalarde, Ben Liang, Min Dong, Yahia A. Eldemerdash Ahmed, Ho Ting Cheng
Abstract: In Federated Learning (FL) with over-the-air aggregation, the quality of the signal received at the server critically depends on the receive scaling factors. While a larger scaling factor can reduce the effective noise power and improve training performance, it also compromises the privacy of devices by reducing uncertainty. In this work, we aim to adaptively design the receive scaling factors across training rounds to balance the trade-off between training convergence and privacy in an FL system under dynamic channel conditions. We formulate a stochastic optimization problem that minimizes the overall RÃ©nyi differential privacy (RDP) leakage over the entire training process, subject to a long-term constraint that ensures convergence of the global loss function. Our problem depends on unknown future information, and we observe that standard Lyapunov optimization is not applicable. Thus, we develop a new online algorithm, termed AdaScale, based on a sequence of novel per-round problems that can be solved efficiently. We further derive upper bounds on the dynamic regret and constraint violation of AdaSacle, establishing that it achieves diminishing dynamic regret in terms of time-averaged RDP leakage while ensuring convergence of FL training to a stationary point. Numerical experiments on canonical classification tasks show that our approach effectively reduces RDP and DP leakages compared with state-of-the-art benchmarks without compromising learning performance.

Paper number 118:
Title: Convex Pollution Control of Wastewater Treatment Systems
Authors: Joshua Taylor
Abstract: We design a model-predictive controller for managing the actuators in sewer networks. It minimizes flooding and combined-sewer overflow during rain and pollution at other times. To make the problem tractable, we use a convex relaxation of the microbial growth kinetics and a physically motivated linearization of the mass flow bilinearities. With these approximations, the trajectory optimization in each control period is a second-order cone program. In simulation, the controller releases roughly 15% less pollutant mass than a conventional controller while treating nearly the same volume of flow. It does so by better balancing the flow over the treatment plants and over time.

Paper number 119:
Title: A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM
Authors: Otobong Jerome, Geesara Prathap Kulathunga, Devitt Dmitry, Eugene Murawjow, Alexandr Klimchik
Abstract: Off-road environments present unique challenges for autonomous navigation due to their complex and unstructured nature. Traditional global path-planning methods, which typically aim to minimize path length and travel time, perform poorly on large-scale maps and fail to account for critical factors such as real-time performance, kinematic feasibility, and memory efficiency. This paper introduces a novel global path-planning method specifically designed for off-road environments, addressing these essential factors. The method begins by constructing an intermediate map within the pixel coordinate system, incorporating geographical features like off-road trails, waterways, restricted and passable areas, and trees. The planning problem is then divided into three sub-problems: graph-based path planning, kinematic feasibility checking, and path smoothing. This approach effectively meets real-time performance requirements while ensuring kinematic feasibility and efficient memory use. The method was tested in various off-road environments with large-scale maps up to several square kilometers in size, successfully identifying feasible paths in an average of 1.5 seconds and utilizing approximately 1.5GB of memory under extreme conditions. The proposed framework is versatile and applicable to a wide range of off-road autonomous navigation tasks, including search and rescue missions and agricultural operations.

Paper number 120:
Title: From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents
Authors: Amin Vahidi-Moghaddam, Sayed Pedram Haeri Boroujeni, Iman Jebellat, Ehsan Jebellat, Niloufar Mehrabi, Zhaojian Li
Abstract: One of the main challenges in modern control applications, particularly in robot and vehicle motion control, is achieving accurate, fast, and safe movement. To address this, optimal control policies have been developed to enforce safety while ensuring high performance. Since basic first-principles models of real systems are often available, model-based controllers are widely used. Model predictive control (MPC) is a leading approach that optimizes performance while explicitly handling safety constraints. However, obtaining accurate models for complex systems is difficult, which motivates data-driven alternatives. ML-based MPC leverages learned models to reduce reliance on hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal policies directly from interaction data. Data-enabled predictive control (DeePC) goes further by bypassing modeling altogether, directly learning safe policies from raw input-output data. Recently, large language model (LLM) agents have also emerged, translating natural language instructions into structured formulations of optimal control problems. Despite these advances, data-driven policies face significant limitations. They often suffer from slow response times, high computational demands, and large memory needs, making them less practical for real-world systems with fast dynamics, limited onboard computing, or strict memory constraints. To address this, various technique, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, have been proposed to reduce computational complexity. In this paper, we present eight such approaches and demonstrate their effectiveness across real-world applications, including robotic arms, soft robots, and vehicle motion control.

Paper number 121:
Title: DADS Under Unknown Input Coefficients
Authors: Iasson Karafyllis, Miroslav Krstic
Abstract: This short note shows that the Deadzone-Adapted Disturbance Suppression (DADS) adaptive control scheme is applicable to systems with unknown input coefficients. We study time-invariant, control-affine systems that satisfy the matching condition for which no bounds for the disturbance and the unknown parameters are known. The input coefficients can be time-varying as well as the unknown parameters. The only thing assumed for the input coefficients is their sign. The adaptive control design is Lyapunov-based and can be accomplished for every system for which a smooth globally stabilizing feedback exists when the disturbances are absent and all unknown parameters are known. The design is given by simple, explicit formulas. The proposed controllers guarantee an attenuation of the plant state to an assignable small level, despite unknown bounds on the parameters and disturbance, without a drift of the gain, state, and input.

Paper number 122:
Title: GDiffuSE: Diffusion-based speech enhancement with noise model guidance
Authors: Efrayim Yanir, David Burshtein, Sharon Gannot
Abstract: This paper introduces a novel speech enhancement (SE) approach based on a denoising diffusion probabilistic model (DDPM), termed Guided diffusion for speech enhancement (GDiffuSE). In contrast to conventional methods that directly map noisy speech to clean speech, our method employs a lightweight helper model to estimate the noise distribution, which is then incorporated into the diffusion denoising process via a guidance mechanism. This design improves robustness by enabling seamless adaptation to unseen noise types and by leveraging large-scale DDPMs originally trained for speech generation in the context of SE. We evaluate our approach on noisy signals obtained by adding noise samples from the BBC sound effects database to LibriSpeech utterances, showing consistent improvements over state-of-the-art baselines under mismatched noise conditions. Examples are available at our project webpage.

Paper number 123:
Title: Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation
Authors: Amirmasoud Molaei, Reza Ghabcheloo
Abstract: Rock capturing with standard excavator buckets is a challenging task typically requiring the expertise of skilled operators. Unlike soil digging, it involves manipulating large, irregular rocks in unstructured environments where complex contact interactions with granular material make model-based control impractical. Existing autonomous excavation methods focus mainly on continuous media or rely on specialized grippers, limiting their applicability to real-world construction sites. This paper introduces a fully data-driven control framework for rock capturing that eliminates the need for explicit modeling of rock or soil properties. A model-free reinforcement learning agent is trained in the AGX Dynamics simulator using the Proximal Policy Optimization (PPO) algorithm and a guiding reward formulation. The learned policy outputs joint velocity commands directly to the boom, arm, and bucket of a CAT365 excavator model. Robustness is enhanced through extensive domain randomization of rock geometry, density, and mass, as well as the initial configurations of the bucket, rock, and goal position. To the best of our knowledge, this is the first study to develop and evaluate an RL-based controller for the rock capturing task. Experimental results show that the policy generalizes well to unseen rocks and varying soil conditions, achieving high success rates comparable to those of human participants while maintaining machine stability. These findings demonstrate the feasibility of learning-based excavation strategies for discrete object manipulation without requiring specialized hardware or detailed material models.

Paper number 124:
Title: Adaptive Federated Learning via Dynamical System Model
Authors: Aayushya Agarwal, Larry Pileggi, Gauri Joshi
Abstract: Hyperparameter selection is critical for stable and efficient convergence of heterogeneous federated learning, where clients differ in computational capabilities, and data distributions are non-IID. Tuning hyperparameters is a manual and computationally expensive process as the hyperparameter space grows combinatorially with the number of clients. To address this, we introduce an end-to-end adaptive federated learning method in which both clients and central agents adaptively select their local learning rates and momentum parameters. Our approach models federated learning as a dynamical system, allowing us to draw on principles from numerical simulation and physical design. Through this perspective, selecting momentum parameters equates to critically damping the system for fast, stable convergence, while learning rates for clients and central servers are adaptively selected to satisfy accuracy properties from numerical simulation. The result is an adaptive, momentum-based federated learning algorithm in which the learning rates for clients and servers are dynamically adjusted and controlled by a single, global hyperparameter. By designing a fully integrated solution for both adaptive client updates and central agent aggregation, our method is capable of handling key challenges of heterogeneous federated learning, including objective inconsistency and client drift. Importantly, our approach achieves fast convergence while being insensitive to the choice of the global hyperparameter, making it well-suited for rapid prototyping and scalable deployment. Compared to state-of-the-art adaptive methods, our framework is shown to deliver superior convergence for heterogeneous federated learning while eliminating the need for hyperparameter tuning both client and server updates.

Paper number 125:
Title: Machine Unlearning in Speech Emotion Recognition via Forget Set Alone
Authors: Zhao Ren, Rathi Adarshi Rammohan, Kevin Scheck, Tanja Schultz
Abstract: Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in human-computer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition.

Paper number 126:
Title: Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space
Authors: Christian Limberg, Fares Schulz, Zhe Zhang, Stefan Weinzierl
Abstract: This paper presents a novel approach to neural instrument sound synthesis using a two-stage semi-supervised learning framework capable of generating pitch-accurate, high-quality music samples from an expressive timbre latent space. Existing approaches that achieve sufficient quality for music production often rely on high-dimensional latent representations that are difficult to navigate and provide unintuitive user experiences. We address this limitation through a two-stage training paradigm: first, we train a pitch-timbre disentangled 2D representation of audio samples using a Variational Autoencoder; second, we use this representation as conditioning input for a Transformer-based generative model. The learned 2D latent space serves as an intuitive interface for navigating and exploring the sound landscape. We demonstrate that the proposed method effectively learns a disentangled timbre space, enabling expressive and controllable audio generation with reliable pitch conditioning. Experimental results show the model's ability to capture subtle variations in timbre while maintaining a high degree of pitch accuracy. The usability of our method is demonstrated in an interactive web application, highlighting its potential as a step towards future music production environments that are both intuitive and creatively empowering: this https URL

Paper number 127:
Title: Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins
Authors: Nahshon Mokua Obiri, Kristof Van Laerhoven
Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context factors, which challenge log-distance models and the assumption of log-normal shadowing. We present an environment-aware, statistically disciplined path loss framework evaluated using leakage-safe cross-validation on a 12-month campaign in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is augmented with environmental covariates (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure), as well as the signal-to-noise ratio. We compare multiple linear regression with regularized variants, Bayesian linear regression, and a selective second-order polynomial applied to continuous drivers. Predictor relevance is established using heteroscedasticity-robust Type II and III analysis of variance and nested partial F tests. Shadow fading is profiled with kernel density estimation and non-parametric families, including Normal, Skew-Normal, Student's t, and Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07 to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are non-Gaussian; a 3-component mixture captures a sharp core with a light, broad tail. We convert accuracy into reliability by prescribing the fade margin as the upper-tail quantile of cross-validated residuals, quantifying uncertainty via a moving-block bootstrap, and validating on a held-out set. At 99% packet delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7 to 27.9 dB for linear baselines. This result presents a deployment-ready, interpretable workflow with calibrated reliability control for indoor Internet of Things planning, aligned with 6G targets.

Paper number 128:
Title: Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators
Authors: Apurva Badithela, David Snyder, Lihan Zha, Joseph Mikhail, Matthew O'Kelly, Anushri Dixit, Anirudha Majumdar
Abstract: Rapid progress in imitation learning, foundation models, and large-scale datasets has led to robot manipulation policies that generalize to a wide-range of tasks and environments. However, rigorous evaluation of these policies remains a challenge. Typically in practice, robot policies are often evaluated on a small number of hardware trials without any statistical assurances. We present SureSim, a framework to augment large-scale simulation with relatively small-scale real-world testing to provide reliable inferences on the real-world performance of a policy. Our key idea is to formalize the problem of combining real and simulation evaluations as a prediction-powered inference problem, in which a small number of paired real and simulation evaluations are used to rectify bias in large-scale simulation. We then leverage non-asymptotic mean estimation algorithms to provide confidence intervals on mean policy performance. Using physics-based simulation, we evaluate both diffusion policy and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and initial conditions, and find that our approach saves over \(20-25\%\) of hardware evaluation effort to achieve similar bounds on policy performance.

Paper number 129:
Title: Geometry of Distance Protection
Authors: Josh A. Taylor, Alejandro D. DomÃ­nguez-GarcÃ­a
Abstract: Distance relays detect faults on transmission lines. They face uncertainty from the fault's location and resistance, as well as the current from the line's remote terminal. In this paper, we aggregate this uncertainty with the Minkowski sum. This allows us to explicitly model the power grid surrounding the relay's line, and in turn accommodate any mix of synchronous machines and inverter-based resources. To make the relay's task easier, inverters can inject perturbations, or auxiliary signals, such as negative-sequence current. We use Farkas' lemma to construct an optimization for designing inverter auxiliary signals.

Paper number 130:
Title: PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization
Authors: Jushan Chen, Santiago Paternain
Abstract: Recently, diffusion models have gained popularity and attention in trajectory optimization due to their capability of modeling multi-modal probability distributions. However, addressing nonlinear equality constraints, i.e, dynamic feasi- bility, remains a great challenge in diffusion-based trajectory optimization. Recent diffusion-based trajectory optimization frameworks rely on a single-shooting style approach where the denoised control sequence is applied to forward propagate the dynamical system, which cannot explicitly enforce constraints on the states and frequently leads to sub-optimal solutions. In this work, we propose a novel direct trajectory optimization approach via model-based diffusion, which directly generates a sequence of states. To ensure dynamic feasibility, we propose a gradient-free projection mechanism that is incorporated into the reverse diffusion process. Our results show that, compared to a recent state-of-the-art baseline, our approach leads to zero dynamic feasibility error and approximately 4x higher success rate in a quadrotor waypoint navigation scenario involving dense static obstacles.

Paper number 131:
Title: Evaluating Self-Supervised Speech Models via Text-Based LLMS
Authors: Takashi Maekaku, Keita Goto, Jinchuan Tian, Yusuke Shinohara, Shinji Watanabe
Abstract: Self-Supervised Learning (SSL) has gained traction for its ability to learn rich representations with low labeling costs, applicable across diverse downstream tasks. However, assessing the downstream-task performance remains challenging due to the cost of extra training and evaluation. Existing methods for task-agnostic evaluation also require extra training or hyperparameter tuning. We propose a novel evaluation metric using large language models (LLMs). By inputting discrete token sequences and minimal domain cues derived from SSL models into LLMs, we obtain the mean log-likelihood; these cues guide in-context learning, rendering the score more reliable without extra training or hyperparameter tuning. Experimental results show a correlation between LLM-based scores and automatic speech recognition task. Additionally, our findings reveal that LLMs not only functions as an SSL evaluation tools but also provides inference-time embeddings that are useful for speaker verification task.

Paper number 132:
Title: SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection
Authors: Baber Jan, Saeed Anwar, Aiman H. El-Maleh, Abdul Jabbar Siddiqui, Abdul Bais
Abstract: Camouflaged object detection segments objects with intrinsic similarity and edge disruption. Current detection methods rely on accumulated complex components. Each approach adds components such as boundary modules, attention mechanisms, and multi-scale processors independently. This accumulation creates a computational burden without proportional gains. To manage this complexity, they process at reduced resolutions, eliminating fine details essential for camouflage. We present SPEGNet, addressing fragmentation through a unified design. The architecture integrates multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations, maintaining semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions. This design strikes a balance between boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$ on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed. Our approach excels across scales, from tiny, intricate objects to large, pattern-similar ones, while handling occlusion and ambiguous boundaries. Code, model weights, and results are available on \href{this https URL}{this https URL}.

Paper number 133:
Title: Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads
Authors: Huanqing Wang, Kaixiang Zhang, Kyungjoon Lee, Yu Mei, Vaibhav Srivastava, Jun Sheng, Ziyou Song, Zhaojian Li
Abstract: Data-driven control methods such as data-enabled predictive control (DeePC) have shown strong potential in efficient control of soft robots without explicit parametric models. However, in object manipulation tasks, unknown external payloads and disturbances can significantly alter the system dynamics and behavior, leading to offset error and degraded control performance. In this paper, we present a novel velocity-form DeePC framework that achieves robust and optimal control of soft robots under unknown payloads. The proposed framework leverages input-output data in an incremental representation to mitigate performance degradation induced by unknown payloads, eliminating the need for weighted datasets or disturbance estimators. We validate the method experimentally on a planar soft robot and demonstrate its superior performance compared to standard DeePC in scenarios involving unknown payloads.

Paper number 134:
Title: Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers
Authors: Juncheng Wang, Chao Xu, Cheng Yu, Zhe Hu, Haoyu Xie, Guoqi Yu, Lei Shang, Shujun Wang
Abstract: While language models (LMs) paired with residual vector quantization (RVQ) tokenizers have shown promise in text-to-audio (T2A) generation, they still lag behind diffusion-based models by a non-trivial margin. We identify a critical dilemma underpinning this gap: incorporating more RVQ layers improves audio reconstruction fidelity but exceeds the generation capacity of conventional LMs. To address this, we first analyze RVQ dynamics and uncover two key limitations: 1) orthogonality of features across RVQ layers hinders effective LMs training, and 2) descending semantic richness in tokens from deeper RVQ layers exacerbates exposure bias during autoregressive decoding. Based on these insights, we propose Siren, a novel LM-based framework that employs multiple isolated transformers with causal conditioning and anti-causal alignment via reinforcement learning. Extensive experiments demonstrate that Siren outperforms both existing LM-based and diffusion-based T2A systems, achieving state-of-the-art results. By bridging the representational strengths of LMs with the fidelity demands of audio synthesis, our approach repositions LMs as competitive contenders against diffusion models in T2A tasks. Moreover, by aligning audio representations with linguistic structures, Siren facilitates a promising pathway toward unified multi-modal generation frameworks.

Paper number 135:
Title: Robustness assessment of large audio language models in multiple-choice evaluation
Authors: Fernando LÃ³pez, Santosh Kesiraju, Jordi Luque
Abstract: Recent advances in large audio language models (LALMs) have primarily been assessed using a multiple-choice question answering (MCQA) framework. However, subtle changes, such as shifting the order of choices, result in substantially different results. Existing MCQA frameworks do not account for this variability and report a single accuracy number per benchmark or category. We dive into the MCQA evaluation framework and conduct a systematic study spanning three benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings indicate that models are sensitive not only to the ordering of choices, but also to the paraphrasing of the question and the choices. Finally, we propose a simpler evaluation protocol and metric that account for subtle variations and provide a more detailed evaluation report of LALMs within the MCQA framework.

Paper number 136:
Title: Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI
Authors: Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, Joonhyuk Kang
Abstract: The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. We propose a framework for synthetic biomedical time-series data generation based on advanced forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets preserve essential temporal and spectral properties of real data, which enables robust analysis while effectively addressing data scarcity and privacy challenges. Our evaluations across multiple subjects demonstrate that the generated synthetic data can serve as an effective substitute for real data and also significantly boost AI model performance. The approach maintains critical biomedical features while provides high scalability for various applications and integrates seamlessly into open-source repositories, substantially expanding resources for AI-driven biomedical research.

Paper number 137:
Title: Modeling and Managing Temporal Obligations in GUCON Using SPARQL-star and RDF-star
Authors: Ines Akaichi, Giorgos Flouris, Irini Fundulaki, Sabrina Kirrane
Abstract: In the digital age, data frequently crosses organizational and jurisdictional boundaries, making effective governance essential. Usage control policies have emerged as a key paradigm for regulating data usage, safeguarding privacy, protecting intellectual property, and ensuring compliance with regulations. A central mechanism for usage control is the handling of obligations, which arise as a side effect of using and sharing data. Effective monitoring of obligations requires capturing usage traces and accounting for temporal aspects such as start times and deadlines, as obligations may evolve over times into different states, such as fulfilled, violated, or expired. While several solutions have been proposed for obligation monitoring, they often lack formal semantics or provide limited support for reasoning over obligation states. To address these limitations, we extend GUCON, a policy framework grounded in the formal semantics of SPAQRL graph patterns, to explicitly model the temporal aspects of an obligation. This extension enables the expressing of temporal obligations and supports continuous monitoring of their evolving states based on usage traces stored in temporal knowledge graphs. We demonstrate how this extended model can be represented using RDF-star and SPARQL-star and propose an Obligation State Manager that monitors obligation states and assess their compliance with respect to usage traces. Finally, we evaluate both the extended model and its prototype implementation.

Paper number 138:
Title: Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba
Authors: Baher Mohammad, Magauiya Zhussip, Stamatios Lefkimmiatis
Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE - edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires ~6x less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.

Paper number 139:
Title: Rapid stabilization for a wave equation with boundary disturbance
Authors: Patricio GuzmÃ¡n, AgustÃ­n Huerta, Hugo Parada
Abstract: In this paper, we study the rapid stabilization of an unstable wave equation, in which an unknown disturbance is located at the boundary condition. We address two different boundary conditions: Dirichlet- Dirichlet and Dirichlet-Neumann. In both cases, we design a feedback law, located at the same place as the unknown disturbance, that forces the exponential decay of the energy for any desired decay rate while suppressing the effects of the unknown disturbance. For the feedback design, we employ the backstepping method, Lyapunov techniques and the sign multivalued operator. The well-posedness of the closed-loop system, which is a differential inclusion, is shown with the maximal monotone operator theory.

Paper number 140:
Title: Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models
Authors: Nick JanÃen, Melanie Schaller, Bodo Rosenhahn
Abstract: Understanding the robustness of deep learning models for multivariate long-term time series forecasting (M-LTSF) remains challenging, as evaluations typically rely on real-world datasets with unknown noise properties. We propose a simulation-based evaluation framework that generates parameterizable synthetic datasets, where each dataset instance corresponds to a different configuration of signal components, noise types, signal-to-noise ratios, and frequency characteristics. These configurable components aim to model real-world multivariate time series data without the ambiguity of unknown noise. This framework enables fine-grained, systematic evaluation of M-LTSF models under controlled and diverse scenarios. We benchmark four representative architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based). Our analysis reveals that all models degrade severely when lookback windows cannot capture complete periods of seasonal patters in the data. S-Mamba and Autoformer perform best on sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals. White and Brownian noise universally degrade performance with lower signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer shows seasonal-noise vulnerability. Further spectral analysis shows that S-Mamba and iTransformer achieve superior frequency reconstruction. This controlled approach, based on our synthetic and principle-driven testbed, offers deeper insights into model-specific strengths and limitations through the aggregation of MSE scores and provides concrete guidance for model selection based on signal characteristics and noise conditions.

Paper number 141:
Title: A Fixed Point Framework for the Existence of EFX Allocations
Authors: S. Rasoul Etesami
Abstract: We consider the problem of the existence of an envy-free allocation up to any good (EFX) for linear valuations and establish new results by connecting this problem to a fixed point framework. Specifically, we first use randomized rounding to extend the discrete EFX constraints into a continuous space and show that an EFX allocation exists if and only if the optimal value of the continuously extended objective function is nonpositive. In particular, we demonstrate that this optimization problem can be formulated as an unconstrained difference of convex (DC) program, which can be further simplified to the minimization of a piecewise linear concave function over a polytope. Leveraging this connection, we show that the proposed DC program has a nonpositive optimal objective value if and only if a well-defined continuous vector map admits a fixed point. Crucially, we prove that the reformulated fixed point problem satisfies all the conditions of Brouwer's fixed point theorem, except that self-containedness is violated by an arbitrarily small positive constant. To address this, we propose a slightly perturbed continuous map that always admits a fixed point. This fixed point serves as a proxy for the fixed point (if it exists) of the original map, and hence for an EFX allocation through an appropriate transformation. Our results offer a new approach to establishing the existence of EFX allocations through fixed point theorems. Moreover, the equivalence with DC programming enables a more efficient and systematic method for computing such allocations (if one exists) using tools from nonlinear optimization. Our findings bridge the discrete problem of finding an EFX allocation with two continuous frameworks: solving an unconstrained DC program and identifying a fixed point of a continuous vector map.

Paper number 142:
Title: Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data
Authors: Usman Akram, Yiyue Chen, Haris Vikalo
Abstract: Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

Paper number 143:
Title: Optimal participation of energy communities in electricity markets under uncertainty. A multi-stage stochastic programming approach
Authors: Albert SolÃ  Vilalta, Ignasi MaÃ±Ã©, F.- Javier Heredia
Abstract: We propose a multi-stage stochastic programming model for the optimal participation of energy communities in electricity markets. The multi-stage aspect captures the different times at which variable renewable generation and electricity prices are observed. This results in large-scale optimization problem instances containing large scenario trees with 34 stages, to which scenario reduction techniques are applied. Case studies with real data are discussed to analyse proposed regulatory frameworks in Europe. The added value of considering stochasticity is also analysed.

Paper number 144:
Title: Multi-Agent Distributed Optimization With Feasible Set Privacy
Authors: Shreya Meel, Sennur Ulukus
Abstract: We consider the problem of decentralized constrained optimization with multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$ private from each other. We assume that the objective function $f$ is known to all agents and each feasible set is a collection of points from a universal alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the communication with the remaining (non-leader) agents, and is the first to retrieve the solution set. The leader searches for the solution by sending queries to and receiving answers from the non-leaders, such that the information on the individual feasible sets revealed to the leader should be no more than nominal, i.e., what is revealed from learning the solution set alone. We develop achievable schemes for obtaining the solution set at nominal information leakage, and characterize their communication costs under two communication setups between agents. In this work, we focus on two kinds of network setups: i) ring, where each agent communicates with two adjacent agents, and ii) star, where only the leader communicates with the remaining agents. We show that, if the leader first learns the joint feasible set through an existing private set intersection (PSI) protocol and then deduces the solution set, the information leaked to the leader is greater than nominal. Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is a PSI-variant where the intersection is revealed only when its cardinality is larger than a threshold value. Finally, for various realizations of $f$ mapped uniformly at random to a fixed range of values, our schemes are more communication-efficient with a high probability compared to retrieving the entire feasible set through PSI.

Paper number 145:
Title: Majorization-Minimization based Hybrid Localization Method for High Precision Localization in Wireless Sensor Networks
Authors: Kuntal Panwar, Prabhu Babu, R. Jyothi
Abstract: This paper investigates the hybrid source localization problem using the four radio measurements - time of arrival (TOA), time difference of arrival (TDOA), received signal strength (RSS), and angle of arrival (AOA). First, after invoking tractable approximations in the RSS and AOA models, the maximum likelihood estimation (MLE) problem for the hybrid TOA-TDOA-RSS-AOA data model is derived. Then a weighted least-squares problem is formulated from the MLE, which is solved using the principle of the majorization-minimization (MM), resulting in an iterative algorithm with guaranteed convergence. The key feature of the proposed method is that it provides a unified framework where localization using any possible merger out of these four measurements can be implemented as per the requirement/application. Extensive numerical simulations are conducted to study the performance of the proposed method. The obtained results indicate that the hybrid localization model improves the localization accuracy compared to the heterogeneous measurements under different network scenarios, which also includes the presence of non-line of sight (NLOS) errors.

Paper number 146:
Title: Adaptive Disturbance Observer-based Full-Order Integral-Terminal Sliding Mode Control with Unknown A Priori Bound on Uncertainty
Authors: Jit Koley, Binoy Krishna Roy
Abstract: This study presents a novel, continuous finite-time control strategy for a class of nonlinear systems subject to matched uncertainties with unknown bounds. We propose an Adaptive Disturbance Observer-based Full-order Integral-Terminal Sliding Mode Control (ADO-FOITSMC) to stabilize a chain of integrators in presence of exogenous disturbances whose time derivative is bounded by a constant that is not known a priori. Key features of this approach include a significant reduction in control input chattering and a non-monotonic adaptive law for the observer gains, which prevents overestimation while ensuring the global boundedness of system states. The effectiveness and practical viability of the proposed algorithm are demonstrated through its application to the attitude stabilization of a rigid spacecraft.

Paper number 147:
Title: Leveraging Self-Supervised Audio-Visual Pretrained Models to Improve Vocoded Speech Intelligibility in Cochlear Implant Simulation
Authors: Richard Lee Lai, Jen-Cheng Hou, I-Chun Chern, Kuo-Hsuan Hung, Yi-Ting Chen, Mandar Gogate, Tughrul Arslan, Amir Hussain, Yu Tsao
Abstract: Individuals with hearing impairments face challenges in their ability to comprehend speech, particularly in noisy environments. The aim of this study is to explore the effectiveness of audio-visual speech enhancement (AVSE) in enhancing the intelligibility of vocoded speech in cochlear implant (CI) simulations. Notably, the study focuses on a challenged scenario where there is limited availability of training data for the AVSE task. To address this problem, we propose a novel deep neural network framework termed Self-Supervised Learning-based AVSE (SSL-AVSE). The proposed SSL-AVSE combines visual cues, such as lip and mouth movements, from the target speakers with corresponding audio signals. The contextually combined audio and visual data are then fed into a Transformer-based SSL AV-HuBERT model to extract features, which are further processed using a BLSTM-based SE model. The results demonstrate several key findings. Firstly, SSL-AVSE successfully overcomes the issue of limited data by leveraging the AV-HuBERT model. Secondly, by fine-tuning the AV-HuBERT model parameters for the target SE task, significant performance improvements are achieved. Specifically, there is a notable enhancement in PESQ (Perceptual Evaluation of Speech Quality) from 1.43 to 1.67 and in STOI (Short-Time Objective Intelligibility) from 0.70 to 0.74. Furthermore, the performance of the SSL-AVSE was evaluated using CI vocoded speech to assess the intelligibility for CI users. Comparative experimental outcomes reveal that in the presence of dynamic noises encountered during human conversations, SSL-AVSE exhibits a substantial improvement. The NCM (Normal Correlation Matrix) values indicate an increase of 26.5% to 87.2% compared to the noisy baseline.

Paper number 148:
Title: Robust MRI Reconstruction by Smoothed Unrolling (SMUG)
Authors: Shijun Liang, Van Hoang Minh Nguyen, Jinghan Jia, Ismail Alkhouri, Sijia Liu, Saiprasad Ravishankar
Abstract: As the popularity of deep learning (DL) in the field of magnetic resonance imaging (MRI) continues to rise, recent research has indicated that DL-based MRI reconstruction models might be excessively sensitive to minor input disturbances, including worst-case additive perturbations. This sensitivity often leads to unstable, aliased images. This raises the question of how to devise DL techniques for MRI reconstruction that can be robust to train-test variations. To address this problem, we propose a novel image reconstruction framework, termed Smoothed Unrolling (SMUG), which advances a deep unrolling-based MRI reconstruction model using a randomized smoothing (RS)-based robust learning approach. RS, which improves the tolerance of a model against input noises, has been widely used in the design of adversarial defense approaches for image classification tasks. Yet, we find that the conventional design that applies RS to the entire DL-based MRI model is ineffective. In this paper, we show that SMUG and its variants address the above issue by customizing the RS process based on the unrolling architecture of a DL-based MRI reconstruction model. Compared to the vanilla RS approach, we show that SMUG improves the robustness of MRI reconstruction with respect to a diverse set of instability sources, including worst-case and random noise perturbations to input measurements, varying measurement sampling rates, and different numbers of unrolling steps. Furthermore, we theoretically analyze the robustness of our method in the presence of perturbations.

Paper number 149:
Title: Sampling-based Stochastic Data-driven Predictive Control under Data Uncertainty - Extended Version
Authors: Johannes Teutsch, Sebastian Kerz, Dirk Wollherr, Marion Leibold
Abstract: We present a stochastic constrained output-feedback data-driven predictive control scheme for linear time-invariant systems subject to bounded additive disturbances. The approach uses data-driven predictors based on an extension of Willems' fundamental lemma and requires only a single persistently exciting input-output data trajectory. Compared to current state-of-the-art approaches, we do not rely on availability of exact disturbance data. Instead, we leverage a novel parameterization of the unknown disturbance data considering consistency with the measured data and the system class. This allows for deterministic approximation of the chance constraints in a sampling-based fashion. A robust constraint on the first predicted step enables recursive feasibility, closed-loop constraint satisfaction, and robust asymptotic stability in expectation under standard assumptions. A numerical example demonstrates the efficiency of the proposed control scheme.

Paper number 150:
Title: ResSR: A Computationally Efficient Residual Approach to Super-Resolving Multispectral Images
Authors: Haley Duba-Sullivan, Emma J. Reid, Sophie Voisin, Charles A. Bouman, Gregery T. Buzzard
Abstract: Multispectral imaging sensors typically have wavelength-dependent resolution, which limits downstream processing. Consequently, researchers have proposed multispectral image super-resolution (MSI-SR) methods which upsample low-resolution bands to achieve a common resolution across all wavelengths. However, existing MSI-SR methods are computationally expensive because they require spatially regularized deconvolution and/or training-based methods. In this paper, we introduce ResSR, a computationally efficient MSI-SR method that achieves high-quality reconstructions by using spectral decomposition along with spatial residual correction. ResSR applies singular value decomposition to identify correlations across spectral bands, uses pixel-wise computation to upsample the MSI, and then applies a residual correction process to correct the high-spatial frequency components of the upsampled bands. While ResSR is formulated as the solution to a spatially-coupled optimization problem, we use pixel-wise regularization and derive an approximate non-iterative solution, resulting in a computationally efficient, non-iterative algorithm. Results on a combination of simulated and measured data show that ResSR is 2$\times$ to 10$\times$ faster than alternative MSI-SR algorithms, while producing comparable or better image quality. Code is available at this https URL.

Paper number 151:
Title: Adversarial Attacks and Robust Defenses in Speaker Embedding based Zero-Shot Text-to-Speech System
Authors: Ze Li, Yao Shi, Yunfei Xu, Ming Li
Abstract: Speaker embedding based zero-shot Text-to-Speech (TTS) systems enable high-quality speech synthesis for unseen speakers using minimal data. However, these systems are vulnerable to adversarial attacks, where an attacker introduces imperceptible perturbations to the original speaker's audio waveform, leading to synthesized speech sounds like another person. This vulnerability poses significant security risks, including speaker identity spoofing and unauthorized voice manipulation. This paper investigates two primary defense strategies to address these threats: adversarial training and adversarial purification. Adversarial training enhances the model's robustness by integrating adversarial examples during the training process, thereby improving resistance to such attacks. Adversarial purification, on the other hand, employs diffusion probabilistic models to revert adversarially perturbed audio to its clean form. Experimental results demonstrate that these defense mechanisms can significantly reduce the impact of adversarial perturbations, enhancing the security and reliability of speaker embedding based zero-shot TTS systems in adversarial environments.

Paper number 152:
Title: A Physics-Informed Context-Aware Approach for Anomaly Detection in Tele-driving Operations Under False Data Injection Attacks
Authors: Subhadip Ghosh, Aydin Zaboli, Junho Hong, Jaerock Kwon
Abstract: Tele-operated driving (ToD) systems are special types of cyber-physical systems (CPSs) where the operator remotely controls the steering, acceleration, and braking actions of the vehicle. Malicious actors may inject false data in communication channels to manipulate the tele-operators driving commands to cause harm. Hence, protection of this communication is necessary for the safe operation of the target vehicle. However, according to the National Institute of Standards and Technology (NIST) cybersecurity framework, protection merely is not enough and the detection of an attack is necessary. Moreover, UN R155 mandates that security incidents across vehicle fleets be detected and logged. Thus, cyber-physical threats of ToD are modeled with an attack-centric approach in this paper. Then, an attack model with false data injection (FDI) on steering control commands is created from real vehicle data. The risk of this attack model is assessed for a last-mile delivery (LMD) application. Finally, a physics-informed context-aware anomaly detection system (PCADS) is proposed to detect such false injection attacks, and preliminary experimental results are presented to validate the model.

Paper number 153:
Title: Segmenting Bi-Atrial Structures Using ResNext Based Framework
Authors: Malitha Gunawardhana, Mark L Trew, Gregory B Sands, Jichao Zhao
Abstract: Atrial Fibrillation (AF), the most common sustained cardiac arrhythmia worldwide, increasingly requires accurate bi-atrial structural assessment to guide ablation strategies, particularly in persistent AF. Late gadolinium-enhanced magnetic resonance imaging (LGE-MRI) enables visualisation of atrial fibrosis, but precise manual segmentation remains time-consuming, operator-dependent, and prone to variability. We propose TASSNet, a novel two-stage deep learning framework for fully automated segmentation of both left atrium (LA) and right atrium (RA), including atrial walls and cavities, from 3D LGE-MRI. TASSNet introduces two main innovations: (i) a ResNeXt-based encoder to enhance feature extraction from limited medical datasets, and (ii) a cyclical learning rate schedule to address convergence instability in highly imbalanced, small-batch 3D segmentation tasks. We evaluated our method on two datasets, one of which was completely out-of-distribution, without any additional training. In both cases, TASSNet successfully segmented atrial structures with high accuracy. These results highlight TASSNet's potential for robust and reproducible bi-atrial segmentation, enabling advanced fibrosis quantification and personalised ablation planning in clinical AF management.

Paper number 154:
Title: DALNet: A Denoising Diffusion Probabilistic Model for High-Fidelity Day-Ahead Load Forecasting
Authors: Han Guo, Ding Lin
Abstract: Accurate probabilistic load forecasting is crucial for maintaining the safety and stability of power systems. However, the mainstream approach, multi-step prediction, is hindered by cumulative errors and forecasting lags, which limits its effectiveness in probabilistic day-ahead load forecasting (PDALF). To overcome these challenges, we introduce DALNet, a novel denoising diffusion model designed to generate load curves rather than relying on direct prediction. By shifting the focus to curve generation, DALNet captures the complex distribution of actual load time-series data under specific conditions with greater fidelity. To further enhance DALNet, we propose the temporal multi-scale attention block (TMSAB), a mechanism designed to integrate both positional and temporal information for improved forecasting precision. Furthermore, we utilize kernel density estimation (KDE) to reconstruct the distribution of generated load curves and employ Kullback-Leibler (KL) divergence to compare them with the actual data distribution. Experimental results demonstrate that DALNet excels in load forecasting accuracy and offers a novel perspective for other predictive tasks within power systems.

Paper number 155:
Title: MAD: A Magnitude And Direction Policy Parametrization for Stability Constrained Reinforcement Learning
Authors: Luca Furieri, Sucheth Shenoy, Danilo Saccani, Andrea Martin, Giancarlo Ferrari-Trecate
Abstract: We introduce magnitude and direction (MAD) policies, a policy parameterization for reinforcement learning (RL) that preserves Lp closed-loop stability for nonlinear dynamical systems. Despite their completeness in describing all stabilizing controllers, methods based on nonlinear Youla and system-level synthesis are significantly impacted by the difficulty of parametrizing Lp-stable operators. In contrast, MAD policies introduce explicit feedback on state-dependent features - a key element behind the success of reinforcement learning pipelines - without jeopardizing closed-loop stability. This is achieved by letting the magnitude of the control input be described by a disturbance-feedback Lp-stable operator, while selecting its direction based on state-dependent features through a universal function approximator. We further characterize the robust stability properties of MAD policies under model mismatch. Unlike existing disturbance-feedback policy parametrizations, MAD policies introduce state-feedback components compatible with model-free RL pipelines, ensuring closed-loop stability with no model information beyond assuming open-loop stability. Numerical experiments show that MAD policies trained with deep deterministic policy gradient (DDPG) methods generalize to unseen scenarios - matching the performance of standard neural network policies while guaranteeing closed-loop stability by design.

Paper number 156:
Title: Conformalized Generative Bayesian Imaging: An Uncertainty Quantification Framework for Computational Imaging
Authors: Canberk Ekmekci, Mujdat Cetin
Abstract: Uncertainty quantification plays an important role in achieving trustworthy and reliable learning-based computational imaging. Recent advances in generative modeling and Bayesian neural networks have enabled the development of uncertainty-aware image reconstruction methods. Current generative model-based methods seek to quantify the inherent (aleatoric) uncertainty on the underlying image for given measurements by learning to sample from the posterior distribution of the underlying image. On the other hand, Bayesian neural network-based approaches aim to quantify the model (epistemic) uncertainty on the parameters of a deep neural network-based reconstruction method by approximating the posterior distribution of those parameters. Unfortunately, an ongoing need for an inversion method that can jointly quantify complex aleatoric uncertainty and epistemic uncertainty patterns still persists. In this paper, we present a scalable framework that can quantify both aleatoric and epistemic uncertainties. The proposed framework accepts an existing generative model-based posterior sampling method as an input and introduces an epistemic uncertainty quantification capability through Bayesian neural networks with latent variables and deep ensembling. Furthermore, by leveraging the conformal prediction methodology, the proposed framework can be easily calibrated to ensure rigorous uncertainty quantification. We evaluated the proposed framework on magnetic resonance imaging, computed tomography, and image inpainting problems and showed that the epistemic and aleatoric uncertainty estimates produced by the proposed framework display the characteristic features of true epistemic and aleatoric uncertainties. Furthermore, our results demonstrated that the use of conformal prediction on top of the proposed framework enables marginal coverage guarantees consistent with frequentist principles.

Paper number 157:
Title: Poisson multi-Bernoulli mixture filter for trajectory measurements
Authors: Marco Fontana, Ãngel F. GarcÃ­a-FernÃ¡ndez, Simon Maskell
Abstract: This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of target states. In prediction, the filter obtains the PMBM density on the set of trajectories over the last two time steps. This density is then updated with the set of trajectory measurements. After the update step, the PMBM posterior on the set of two-step trajectories is marginalised to obtain a PMBM density on the set of target states. The filter provides a closed-form solution for multi-target filtering based on sets of trajectory measurements, estimating the set of target states at the end of each time window. Additionally, the paper proposes computationally lighter alternatives to the TM-PMBM filter by deriving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler divergence minimisation in an augmented space with auxiliary variables. The performance of the proposed filters are evaluated in a simulation study.

Paper number 158:
Title: From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification
Authors: Abdulhady Abas Abdullah, Soran Badawi, Dana A. Abdullah, Dana Rasul Hamad
Abstract: The complexity and difficulties of Kurdish speaker detection among its several dialects are investigated in this work. Because of its great phonetic and lexical differences, Kurdish with several dialects including Kurmanji, Sorani, and Hawrami offers special challenges for speaker recognition systems. The main difficulties in building a strong speaker identification system capable of precisely identifying speakers across several dialects are investigated in this work. To raise the accuracy and dependability of these systems, it also suggests solutions like sophisticated machine learning approaches, data augmentation tactics, and the building of thorough dialect-specific corpus. The results show that customized strategies for every dialect together with cross-dialect training greatly enhance recognition performance.

Paper number 159:
Title: A Hybrid Strategy for Probabilistic Forecasting and Trading of Aggregated Wind-Solar Power: Design and Analysis in HEFTCom2024
Authors: Chuanqing Pu, Feilong Fan, Nengling Tai, Songyuan Liu, Jinming Yu
Abstract: Obtaining accurate probabilistic energy forecasts and making effective decisions amid diverse uncertainties are routine challenges in future energy systems. This paper presents the winning solution of team GEB, which ranked 3rd in trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution provides accurate probabilistic forecasts for a wind-solar hybrid system, and achieves substantial trading revenue in the day-ahead electricity market. Key components include: (1) a stacking-based approach combining sister forecasts from various Numerical Weather Predictions (NWPs) to provide wind power forecasts, (2) an online solar post-processing model to address the distribution shift in the online test set caused by increased solar capacity, (3) a probabilistic aggregation method for accurate quantile forecasts of hybrid generation, and (4) a stochastic trading strategy to maximize expected trading revenue considering uncertainties in electricity prices. This paper also explores the potential of end-to-end learning to further enhance the trading revenue by shifting the distribution of forecast errors. Detailed case studies are provided to validate the effectiveness of these proposed methods. Code for all mentioned methods is available for reproduction and further research in both industry and academia.

Paper number 160:
Title: Robust Stability Analysis of Positive Lure System with Neural Network Feedback
Authors: Hamidreza Montazeri Hedesh, Moh. Kamalul Wafi, Bahram Shafai, Milad Siami
Abstract: This paper investigates the robustness of the Lur'e problem under positivity constraints, drawing on results from the positive Aizerman conjecture and robustness properties of Metzler matrices. Specifically, we consider a control system of Lur'e type in which not only the linear part includes parametric uncertainty but also the nonlinear sector bound is unknown. We investigate tools from positive linear systems to effectively solve the problems in complicated and uncertain nonlinear systems. By leveraging the positivity characteristic of the system, we derive an explicit formula for the stability radius of Lur'e systems. Furthermore, we extend our analysis to systems with neural network (NN) feedback loops. Building on this approach, we also propose a refinement method for sector bounds of NNs. This study introduces a scalable and efficient approach for robustness analysis of both Lur'e and NN-controlled systems. Finally, the proposed results are supported by illustrative examples.

Paper number 161:
Title: Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints
Authors: Hamidreza Montazeri Hedesh, Moh Kamalul Wafi, Milad Siami
Abstract: We study the local stability of nonlinear systems in the Lur'e form with static nonlinear feedback realized by feedforward neural networks (FFNNs). By leveraging positivity system constraints, we employ a localized variant of the Aizerman conjecture, which provides sufficient conditions for exponential stability of trajectories confined to a compact set. Using this foundation, we develop two distinct methods for estimating the Region of Attraction (ROA): (i) a less conservative Lyapunov-based approach that constructs invariant sublevel sets of a quadratic function satisfying a linear matrix inequality (LMI), and (ii) a novel technique for computing tight local sector bounds for FFNNs via layer-wise propagation of linear relaxations. These bounds are integrated into the localized Aizerman framework to certify local exponential stability. Numerical results demonstrate substantial improvements over existing integral quadratic constraint-based approaches in both ROA size and scalability.

Paper number 162:
Title: Chaotic Noncoherent SWIPT in Multi-Functional RIS-Aided Systems
Authors: Priyadarshi Mukherjee, Constantinos Psomas, Himal A. Suraweera, Ioannis Krikidis
Abstract: In this letter, we investigate the design of chaotic signal-based transmit waveforms in a multi-functional reconfigurable intelligent surface (MF-RIS)-aided set-up for simultaneous wireless information and power transfer. We propose a differential chaos shift keying-based MF-RIS-aided set-up, where the MF-RIS is partitioned into three non-overlapping surfaces. The elements of the first sub-surface perform energy harvesting (EH), which in turn, provide the required power to the other two sub-surfaces responsible for transmission and reflection of the incident signal. By considering a frequency selective scenario and a realistic EH model, we characterize the chaotic MF-RIS-aided system in terms of its EH performance and the associated bit error rate. Thereafter, we characterize the harvested energy-bit error rate trade-off and derive a lower bound on the number of elements required to operate in the EH mode. Accordingly, we propose novel transmit waveform designs to demonstrate the importance of the choice of appropriate system parameters in the context of achieving self-sustainability.

Paper number 163:
Title: Achieving 70 Gb/s Over A VCSEL-Based Optical Wireless Link Using A Multi-Mode Fiber-Coupled Receiver
Authors: Hossein Kazemi, Isaac N. O. Osahon, Nikolay Ledentsov Jr., Ilya Titkov, Nikolay Ledentsov, Harald Haas
Abstract: In this paper, we demonstrate a laser-based optical wireless communication (OWC) system employing a 940 nm single-mode (SM) vertical cavity surface emitting laser (VCSEL) and a multi-mode (MM) fiber-coupled receiver, achieving a record data rate beyond 70 Gb/s, while the optical transmit power is below 5 mW. The use of a high speed fiber-optic photoreceiver avoids limiting the communication bandwidth by the receiver, enabling ultra-high capacity and energy-efficient light fidelity (LiFi) links to unlock new applications. This work experimentally validates the feasibility of ultra-high speed indoor OWC systems using a single low-power and low-cost VCSEL for next-generation LiFi connectivity.

Paper number 164:
Title: Filling of incomplete sinograms from sparse PET detector configurations using a residual U-Net
Authors: Klara Leffler, Luigi Tommaso Luppino, Samuel Kuttner, Karin SÃ¶derkvist, Jan Axelsson
Abstract: Long axial field-of-view PET scanners offer increased field-of-view and sensitivity compared to traditional PET scanners. However, a significant cost is associated with the densely packed photodetectors required for the extended-coverage systems, limiting clinical utilisation. To mitigate the cost limitations, alternative sparse system configurations have been proposed, allowing an extended field-of-view PET design with detector costs similar to a standard PET system, albeit at the expense of image quality. In this work, we propose a deep sinogram restoration network to fill in the missing sinogram data. Our method utilises a modified Residual U-Net, trained on clinical PET scans from a GE Signa PET/MR, simulating the removal of 50% of the detectors in a chessboard pattern (retaining only 25% of all lines of response). The model successfully recovers missing counts, with a mean absolute error below two events per pixel, outperforming 2D interpolation in both sinogram and reconstructed image domain. Notably, the predicted sinograms exhibit a smoothing effect, leading to reconstructed images lacking sharpness in finer details. Despite these limitations, the model demonstrates a substantial capacity for compensating for the undersampling caused by the sparse detector configuration. This proof-of-concept study suggests that sparse detector configurations, combined with deep learning techniques, offer a viable alternative to conventional PET scanner designs. This approach supports the development of cost-effective, total body PET scanners, allowing a significant step forward in medical imaging technology.

Paper number 165:
Title: Implicit Neural Representation of Beamforming for Continuous Aperture Array (CAPA) System
Authors: Shiyong Chen, Jia Guo, Shengqian Han
Abstract: In this paper, a learning-based approach for optimizing downlink beamforming in continuous aperture array (CAPA) systems is proposed, where a MIMO scenario that both the base station (BS) and the user are equipped with CAPA is considered. As the beamforming in the CAPA system is a function that maps a coordinate on the aperture to the beamforming weight at the coordinate, a DNN called BeaINR is proposed to parameterize this function, which is called implicit neural representation (INR). We further find that the optimal beamforming function lies in the subspace of channel function, i.e., it can be expressed as a weighted integral of channel function. Based on this finding, we propose another DNN called CoefINR to learn the weighting coefficient with INR, which has lower complexity than learning the beamforming function with BeaINR. Simulation results show that the proposed INR-based methods outperform numerical baselines in both spectral efficiency (SE) and inference time, with CoefINR offering additional training efficiency.

Paper number 166:
Title: RAM-W600: A Multi-Task Wrist Dataset and Benchmark for Rheumatoid Arthritis
Authors: Songxiao Yang, Haolin Wang, Yao Fu, Ye Tian, Tamotsu Kamishima, Masayuki Ikebe, Yafei Ou, Masatoshi Okutomi
Abstract: Rheumatoid arthritis (RA) is a common autoimmune disease that has been the focus of research in computer-aided diagnosis (CAD) and disease monitoring. In clinical settings, conventional radiography (CR) is widely used for the screening and evaluation of RA due to its low cost and accessibility. The wrist is a critical region for the diagnosis of RA. However, CAD research in this area remains limited, primarily due to the challenges in acquiring high-quality instance-level annotations. (i) The wrist comprises numerous small bones with narrow joint spaces, complex structures, and frequent overlaps, requiring detailed anatomical knowledge for accurate annotation. (ii) Disease progression in RA often leads to osteophyte, bone erosion (BE), and even bony ankylosis, which alter bone morphology and increase annotation difficulty, necessitating expertise in rheumatology. This work presents a multi-task dataset for wrist bone in CR, including two tasks: (i) wrist bone instance segmentation and (ii) Sharp/van der Heijde (SvdH) BE scoring, which is the first public resource for wrist bone instance segmentation. This dataset comprises 1048 wrist conventional radiographs of 388 patients from six medical centers, with pixel-level instance segmentation annotations for 618 images and SvdH BE scores for 800 images. This dataset can potentially support a wide range of research tasks related to RA, including joint space narrowing (JSN) progression quantification, BE detection, bone deformity evaluation, and osteophyte detection. It may also be applied to other wrist-related tasks, such as carpal bone fracture localization. We hope this dataset will significantly lower the barrier to research on wrist RA and accelerate progress in CAD research within the RA-related domain.

Paper number 167:
Title: Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification Mapping on Non-Contrast CT
Authors: Xiangjian Hou, Ebru Yaman Akcicek, Xin Wang, Kazem Hashemizadeh, Scott Mcnally, Chun Yuan, Xiaodong Ma
Abstract: While total intracranial carotid artery calcification (ICAC) volume is an established stroke biomarker, growing evidence shows this aggregate metric ignores the critical influence of plaque location, since calcification in different segments carries distinct prognostic and procedural risks. However, a finer-grained, segment-specific quantification has remained technically infeasible. Conventional 3D models are forced to process downsampled volumes or isolated patches, sacrificing the global context required to resolve anatomical ambiguity and render reliable landmark localization. To overcome this, we reformulate the 3D challenge as a \textbf{Parallel Probabilistic Landmark Localization} task along the 1D axial dimension. We propose the \textbf{Depth-Sequence Transformer (DST)}, a framework that processes full-resolution CT volumes as sequences of 2D slices, learning to predict $N=6$ independent probability distributions that pinpoint key anatomical landmarks. Our DST framework demonstrates exceptional accuracy and robustness. Evaluated on a 100-patient clinical cohort with rigorous 5-fold cross-validation, it achieves a Mean Absolute Error (MAE) of \textbf{0.1 slices}, with \textbf{96\%} of predictions falling within a $\pm1$ slice tolerance. Furthermore, to validate its architectural power, the DST backbone establishes the best result on the public Clean-CC-CCII classification benchmark under an end-to-end evaluation protocol. Our work delivers the first practical tool for automated segment-specific ICAC analysis. The proposed framework provides a foundation for further studies on the role of location-specific biomarkers in diagnosis, prognosis, and procedural planning.

Paper number 168:
Title: Sensing Accuracy Optimization for Multi-UAV SAR Interferometry with Data Offloading
Authors: Mohamed-Amine Lahmeri, Pouya Fakharizadeh, VÃ­ctor Mustieles-PÃ©rez, Martin Vossiek, Gerhard Krieger, Robert Schober
Abstract: The integration of unmanned aerial vehicles (UAVs) with radar imaging sensors has revolutionized the monitoring of dynamic and local Earth surface processes by enabling high-resolution and cost-effective remote sensing. This paper investigates the optimization of the sensing accuracy of a UAV swarm deployed to perform multi-baseline interferometric synthetic aperture radar (InSAR) sensing. In conventional single-baseline InSAR systems, only one synthetic aperture radar (SAR) antenna pair acquires two SAR images from two distinct angles to generate a digital elevation model (DEM) of the target area. However, multi-baseline InSAR extends this concept by aggregating multiple acquisitions from different angles, thus, significantly enhancing the vertical accuracy of the DEM. The heavy computations required for this process are performed on the ground and, therefore, the radar data is transmitted in real time to a ground station (GS) via a frequency-division multiple access (FDMA) air-to-ground backhaul link. This work focuses on improving the sensing precision by minimizing the height error of the averaged DEM while simultaneously ensuring sensing and communication quality-of-service (QoS). To this end, the UAV formation, velocity, and communication power allocation are jointly optimized using evolutionary algorithms (EAs). Our approach is benchmarked against established optimization methods, including genetic algorithms (GAs), simulated annealing (SA), and deep reinforcement learning (DRL) techniques. Numerical results show that the proposed solution outperforms these baseline schemes and achieves sub-decimeter vertical accuracy in several scenarios. These findings underline the potential of coordinated UAV swarms for delivering high-precision and real-time Earth observations through radar interferometry.

Paper number 169:
Title: Multisession Longitudinal Dynamic MRI Incorporating Patient-Specific Prior Image Information Across Time
Authors: Jingjia Chen, Hersh Chandarana, Daniel K. Sodickson, Li Feng
Abstract: Serial Magnetic Resonance Imaging (MRI) exams are often performed in clinical practice, offering shared anatomical and motion information across imaging sessions. However, existing reconstruction methods process each session independently without leveraging this valuable longitudinal information. In this work, we propose a novel concept of longitudinal dynamic MRI, which incorporates patient-specific prior images to exploit temporal correlations across sessions. This framework enables progressive acceleration of data acquisition and reduction of scan time as more imaging sessions become available. The concept is demonstrated using the 4D Golden-angle RAdial Sparse Parallel (GRASP) MRI, a state-of-the-art dynamic imaging technique. Longitudinal reconstruction is performed by concatenating multi-session time-resolved 4D GRASP datasets into an extended dynamic series, followed by a low-rank subspace-based reconstruction algorithm. A series of experiments were conducted to evaluate the feasibility and performance of the proposed method. Results show that longitudinal 4D GRASP reconstruction consistently outperforms standard single-session reconstruction in image quality, while preserving inter-session variations. The approach demonstrated robustness to changes in anatomy, imaging intervals, and body contour, highlighting its potential for improving imaging efficiency and consistency in longitudinal MRI applications. More generally, this work suggests a new context-aware imaging paradigm in which the more we see a patient, the faster we can image.

Paper number 170:
Title: EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan Africa using MedNeXt V2 with Deep Supervision
Authors: Ahmed Jaheen, Abdelrahman Elsayed, Damir Kim, Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Mostafa Salem, Hu Wang, Sarim Hashmi, Mohammad Yaqub
Abstract: Brain cancer affects millions worldwide, and in nearly every clinical setting, doctors rely on magnetic resonance imaging (MRI) to diagnose and monitor gliomas. However, the current standard for tumor quantification through manual segmentation of multi-parametric MRI is time-consuming, requires expert radiologists, and is often infeasible in under-resourced healthcare systems. This problem is especially pronounced in low-income regions, where MRI scanners are of lower quality and radiology expertise is scarce, leading to incorrect segmentation and quantification. In addition, the number of acquired MRI scans in Africa is typically small. To address these challenges, the BraTS-Lighthouse 2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa (SSA), where resource constraints and image quality degradation introduce significant shifts. In this study, we present EMedNeXt -- an enhanced brain tumor segmentation framework based on MedNeXt V2 with deep supervision and optimized post-processing pipelines tailored for SSA. EMedNeXt introduces three key contributions: a larger region of interest, an improved nnU-Net v2-based architectural skeleton, and a robust model ensembling system. Evaluated on the hidden validation set, our solution achieved an average LesionWise DSC of 0.897 with an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and 1.0 mm, respectively.

Paper number 171:
Title: KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network
Authors: Mohammad Alikhani
Abstract: Human Activity Recognition (HAR) plays a critical role in numerous applications, including healthcare monitoring, fitness tracking, and smart environments. Traditional deep learning (DL) approaches, while effective, often require extensive parameter tuning and may lack interpretability. In this work, we investigate the use of a single three-axis accelerometer and the Kolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model complex nonlinear relationships with improved interpretability and parameter efficiency. The MotionSense dataset, containing smartphone-based motion sensor signals across various physical activities, is employed to evaluate the proposed approach. Our methodology involves preprocessing and normalization of accelerometer and gyroscope data, followed by KAN-based feature learning and classification. Experimental results demonstrate that the KAN achieves competitive or superior classification performance compared to conventional deep neural networks, while maintaining a significantly reduced parameter count. This highlights the potential of KAN architectures as an efficient and interpretable alternative for real-world HAR systems. The open-source implementation of the proposed framework is available at the Project's GitHub Repository.

Paper number 172:
Title: EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical Vectors and Discrete Speech Tokens
Authors: Joonyong Park, Kenichi Nakamura
Abstract: This paper introduces EmoSSLSphere, a novel framework for multilingual emotional text-to-speech (TTS) synthesis that combines spherical emotion vectors with discrete token features derived from self-supervised learning (SSL). By encoding emotions in a continuous spherical coordinate space and leveraging SSL-based representations for semantic and acoustic modeling, EmoSSLSphere enables fine-grained emotional control, effective cross-lingual emotion transfer, and robust preservation of speaker identity. We evaluate EmoSSLSphere on English and Japanese corpora, demonstrating significant improvements in speech intelligibility, spectral fidelity, prosodic consistency, and overall synthesis quality. Subjective evaluations further confirm that our method outperforms baseline models in terms of naturalness and emotional expressiveness, underscoring its potential as a scalable solution for multilingual emotional TTS.

Paper number 173:
Title: Notes on Deterministic and Stochastic Approaches in Electromagnetic Information Theory
Authors: Marco Donald Migliore
Abstract: This paper investigates the relationship between the Number of Degrees of Freedom ($N_{\rm DoF}$) of the field in deterministic and stochastic source models within Electromagnetic Information Theory (EIT). Our findings demonstrate a fundamental connection between these two approaches. Specifically, we show that a deterministic model and a stochastic model with a spatially incoherent and homogeneous source yield not only the same $N_{\rm DoF}$ but also identical eigenvalues and basis functions for field representation. This key equivalence not only explains the effectiveness of deterministic approaches in EIT but also corroborates the use of classical electromagnetic methods within this new discipline.

Paper number 174:
Title: Hierarchical Multi-Agent MCTS for Safety-Critical Coordination in Mixed-Autonomy Roundabouts
Authors: Zhihao Lin, Shuo Liu, Zhen Tian, Dezong Zhao, Jianglin Lan, Chongfeng Wei
Abstract: Navigating unsignalized roundabouts in mixed-autonomy traffic presents significant challenges due to dense vehicle interactions, lane-changing complexities, and behavioral uncertainties of human-driven vehicles (HDVs). This paper proposes a safety-critical decision-making framework for connected and automated vehicles (CAVs) navigating dual-lane roundabouts alongside HDVs. We formulate the problem as a multi-agent Markov Decision Process and develop a hierarchical safety assessment mechanism that evaluates three critical interaction types: CAV-to-CAV (C2C), CAV-to-HDV (C2H), and CAV-to-Boundary (C2B). A key contribution is our lane-specific uncertainty model for HDVs, which captures distinct behavioral patterns between inner and outer lanes, with outer-lane vehicles exhibiting $2.3\times$ higher uncertainty due to less constrained movements. We integrate this safety framework with a multi-agent Monte Carlo Tree Search (MCTS) algorithm that employs safety-aware pruning to eliminate high-risk trajectories while maintaining computational efficiency. The reward function incorporates Shapley value-based credit assignment to balance individual performance with group coordination. Extensive simulation results validate the effectiveness of the proposed approach under both fully autonomous (100% AVs) and mixed traffic (50% AVs + 50% HDVs) conditions. Compared to benchmark methods, our framework consistently reduces trajectory deviations across all AVs and significantly lowers the rate of Post-Encroachment Time (PET) violations, achieving only 1.0% in the fully autonomous scenario and 3.2% in the mixed traffic setting.

Paper number 175:
Title: RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment
Authors: Xiangying Li, Jiankuan Li, Yong Tang
Abstract: This paper proposes a transformer-based lossless crossover method, termed Resonant Transformer Router (RTR), which achieves frequency separation while ensuring perfect phase alignment between low-frequency (LF) and high-frequency (HF) channels at the crossover frequency. The core property of RTR is that its frequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so that the original signal can be perfectly reconstructed by linear summation of the two channels. Theoretical derivation and circuit simulations demonstrate that RTR provides superior energy efficiency, phase consistency, and robustness against component tolerances. Compared with conventional LC crossovers and digital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted filtering solution suitable for high-fidelity audio and communication front-ends. The core theory behind this paper's work, lossless crossover, is based on a Chinese patent [CN116318117A] developed from the previous research of one of the authors, Jiankuan Li. We provide a comprehensive experimental validation of this theory and propose a new extension.

Paper number 176:
Title: Stacked Intelligent Metasurface for End-to-End OFDM System
Authors: Yida Zhang, Qiuyan Liu, Hongtao Luo, Yuqi Xia, Qiang Wang, Fuchang Li, Xiaofeng Tao, Yuanwei Liu
Abstract: Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabled wave-domain signal processing have emerged as promising research directions for offloading baseband digital processing tasks and efficiently simplifying transceiver design. However, existing architectures are limited to employing SIM (DPSIM) for a single communication function, such as precoding or combining. To further enhance the overall performance of SIM (DPSIM)-assisted systems and achieve end-to-end (E2E) joint optimization from the transmitted bitstream to the received bitstream, we propose an SIM (DPSIM)-assisted E2E orthogonal frequency division multiplexing (OFDM) system, in which traditional communication tasks such as channel coding, modulation, precoding, combining, demodulation, and channel decoding are performed synchronously within the electromagnetic (EM) forward propagation. Furthermore, inspired by the idea of abstracting real metasurfaces as hidden layers of a neural network, we propose the EM neural network (EMNN) to enable the control of the E2E OFDM communication system. In addition, transfer learning is introduced into the model training, and a training and deployment framework for the EMNN is designed. Simulation results demonstrate that both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems can achieve robust bitstream transmission under complex channel conditions. Our study highlights the application potential of EMNN and SIM (DPSIM)-assisted E2E OFDM systems in the design of next-generation transceivers.

Paper number 177:
Title: Functional WMMSE Algorithm for Continuous Aperture Array Systems
Authors: Shiyong Chen
Abstract: In this paper, a functional weighted minimum mean-squared error (WMMSE) algorithm is proposed for downlink beamforming in multiuser continuous aperture array (CAPA) systems where both the base station (BS) and users employ CAPAs. The beamforming optimization problem in such systems is inherently functional. To address this challenge, we first employ an orthonormal basis expansion to transform the functional problem into a discrete formulation that shares the same structure as the conventional discrete multi-user multiple-input multiple-output (MU-MIMO) problem. By deriving the first-order optimality conditions of the discrete formulation and mapping them back to the functional domain, we obtain the update equations of the proposed functional WMMSE algorithm. Simulation results show that the proposed method outperforms baselines in both spectral efficiency (SE) and computational complexity.

Paper number 178:
Title: On Fast Attitude Filtering Using Matrix Fisher Distributions with Stability Guarantee
Authors: Shijie Wang, Haichao Gui, Rui Zhong
Abstract: This paper addresses two interrelated problems of the nonlinear filtering mechanism and fast attitude filtering with the matrix Fisher distribution (MFD) on the special orthogonal group. By analyzing the distribution evolution along Bayes' rule, we reveal two essential properties that enhance the performance of Bayesian attitude filters with MFDs, particularly in challenging conditions, from a theoretical viewpoint. Benefiting from the new understanding of the filtering mechanism associated with MFDs, two closed-form filters with MFDs is then proposed. These filters avoid the burdensome computations in previous MFD-based filters by introducing linearized error systems with right-invariant errors but retaining the two advantageous properties. Moreover, we leverage the two properties and closed-form filtering iteration to prove the almost-global exponential stability of the proposed filter with right-invariant error for the single-axis rotation, which, to our knowledge, is not achieved by existing directional statistics-based filters. Numerical simulations demonstrate that the proposed filters are significantly more accurate than the classic invariant Kalman filter. Besides, they are also as accurate as recent MFD-based Bayesian filters in challenging circumstances with large initial error and measurement uncertainty but consumes far less computation time (about 1/5 to 1/100 of previous MFD-based attitude filters).

Paper number 179:
Title: Multi-Agent Guided Policy Search for Non-Cooperative Dynamic Games
Authors: Jingqi Li, Gechen Qu, Jason J. Choi, Somayeh Sojoudi, Claire Tomlin
Abstract: Multi-agent reinforcement learning (MARL) optimizes strategic interactions in non-cooperative dynamic games, where agents have misaligned objectives. However, data-driven methods such as multi-agent policy gradients (MA-PG) often suffer from instability and limit-cycle behaviors. Prior stabilization techniques typically rely on entropy-based exploration, which slows learning and increases variance. We propose a model-based approach that incorporates approximate priors into the reward function as regularization. In linear quadratic (LQ) games, we prove that such priors stabilize policy gradients and guarantee local exponential convergence to an approximate Nash equilibrium. We then extend this idea to infinite-horizon nonlinear games by introducing Multi-agent Guided Policy Search (MA-GPS), which constructs short-horizon local LQ approximations from trajectories of current policies to guide training. Experiments on nonlinear vehicle platooning and a six-player strategic basketball formation show that MA-GPS achieves faster convergence and more stable learning than existing MARL methods.

Paper number 180:
Title: Spectral Flow Learning Theory: Finite-Sample Guarantees for Vector-Field Identification
Authors: Chi Ho Leung, Philip E. ParÃ©
Abstract: We study the identification of continuous-time vector fields from irregularly sampled trajectories. We introduce Spectral Flow Learning (SFL), which learns in a windowed flow space using a lag-linear label operator that aggregates lagged Koopman actions. We provide finite-sample high-probability (FS-HP) guarantees for the class of variable-step linear multistep methods (vLLM). The FS-HP rates are constructed using spectral regularization with qualification-controlled filters for flow predictors under standard source and filter assumptions. A multistep observability inequality links flow error to vector-field error and yields two-term bounds that combine a statistical rate with an explicit discretization bias from vLMM theory. This preliminary preprint states the results and sketches proofs, with full proofs and extensions deferred to a journal version.

Paper number 181:
Title: NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications
Authors: Ying-Ren Chien, Po-Heng Chou, You-Jie Peng, Chun-Yuan Huang, Hen-Wai Tsao, Yu Tsao
Abstract: To effectively process impulse noise for narrowband powerline communications (NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic asynchronous impulsive noise (APIN) is a critical task. However, existing mathematical noise generative models only capture part of the characteristics of noise. In this study, we propose a novel generative adversarial network (GAN) called noise generation GAN (NGGAN) that learns the complicated characteristics of practically measured noise samples for data synthesis. To closely match the statistics of complicated noise over the NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. To train NGGAN, we adhere to the following principles: 1) we design the length of input signals that the NGGAN model can fit to facilitate cyclostationary noise generation; 2) the Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and training data; and 3) to measure the similarity performances of GAN-based models based on the mathematical and practically measured datasets, we conduct both quantitative and qualitative analyses. The training datasets include: 1) a piecewise spectral cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter; and 3) practical measurements from NB-PLC systems. Simulation results demonstrate that the generated noise samples from the proposed NGGAN are highly close to the real noise samples. The principal component analysis (PCA) scatter plots and FrÃ©chet inception distance (FID) analysis have shown that NGGAN outperforms other GAN-based models by generating noise samples with superior fidelity and higher diversity.

Paper number 182:
Title: Code Generation and Conic Constraints for Model-Predictive Control on Microcontrollers with Conic-TinyMPC
Authors: Ishaan Mahajan, Khai Nguyen, Sam Schoedel, Elakhya Nedumaran, Moises Mata, Brian Plancher, Zachary Manchester
Abstract: Model-predictive control (MPC) is a powerful framework for controlling dynamic systems under constraints, but it remains challenging to deploy on resource-constrained platforms, especially for problems involving conic constraints. To address this, we extend recent work developing fast, structure-exploiting, cached ADMM solvers for embedded applications, to provide support for second-order cones, as well as C++ code generation from Python, MATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our solver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to 142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and enables us to fit order-of-magnitude larger problems in memory. We validate our solver's deployed performance through simulation and hardware experiments, including conically-constrained trajectory tracking on a 27g Crazyflie quadrotor. To get started with Conic-TinyMPC, visit our documentation, examples, and the open-source codebase at this https URL.

Paper number 183:
Title: Wall-Street: Smart Surface-Enabled 5G mmWave for Roadside Networking
Authors: Kun Woo Cho, Prasanthi Maddala, Ivan Seskar, Kyle Jamieson
Abstract: 5G mmWave roadside networks promise high bandwidth but face significant challenges in maintaining reliable connections for users moving at high speed. Frequent handovers, complex beam alignment, and signal blockage from car bodies lead to service interruptions and degraded performance. We present Wall-Street, a vehicle-mounted smart surface that enhances 5G mmWave connectivity for in-vehicle users. Wall-Street improves mobility management by (1) steering outdoor mmWave signals into the vehicle for shared coverage, enabling a single, collective handover decision for all users; (2) enabling neighbor-cell search without interrupting data transfer, allowing for seamless handovers; and (3) connecting users to a new cell before disconnecting from the old cell for reliable handovers. We have implemented and integrated Wall-Street in the COSMOS testbed and evaluated its real-time performance with multiple gNBs and users inside a surface-mounted vehicle, driving on a nearby road. In multi-UE scenarios, Wall-Street improves TCP throughput by up to 78% and reduces RTT by up to 34% over the standard 5G Standalone handover protocol.

Paper number 184:
Title: Energy-Efficient Approximate Full Adders Applying Memristive Serial IMPLY Logic For Image Processing
Authors: Seyed Erfan Fatemieh, Mohammad Reza Reshadinezhad
Abstract: Researchers and designers are facing problems with memory and power walls, considering the pervasiveness of Von-Neumann architecture in the design of processors and the problems caused by reducing the dimensions of deep sub-micron transistors. Memristive Approximate Computing (AC) and In-Memory Processing (IMP) can be promising solutions to these problems. We have tried to solve the power and memory wall problems by presenting the implementation algorithm of four memristive approximate full adders applying the Material Implication (IMPLY) method. The proposed circuits reduce the number of computational steps by up to 40% compared to the state-of-the-art. The energy consumption of the proposed circuits improves over the previous exact ones by 49%-75% and over the approximate full adders by up to 41%. Multiple error evaluation criteria evaluate the computational accuracy of the proposed approximate full adders in three scenarios in the 8-bit approximate adder structure. The proposed approximate full adders are evaluated in three image processing applications in three scenarios. The results of application-level simulation indicate that the four proposed circuits can be applied in all three scenarios, considering the acceptable image quality metrics of the output images.

Paper number 185:
Title: On-Demand Growth of Semiconductor Heterostructures Guided by Physics-Informed Machine Learning
Authors: Chao Shen, Yuan Li, Wenkang Zhan, Shujie Pan, Fuxin Lin, Kaiyao Xin, Hui Cong, Chi Xu, Xiaotian Cheng, Ruixiang Liu, Zhibo Ni, Chaoyuan Jin, Bo Xu, Siming Chen, Zhongming Wei, Chunlai Xue, Zhanguo Wang, Chao Zhao
Abstract: Developing tailored semiconductor heterostructures on demand represents a critical capability for addressing the escalating performance demands in electronic and optoelectronic devices. However, traditional fabrication methods remain constrained by simulation-based design and iterative trial-and-error optimization. Here, we introduce SemiEpi, a self-driving platform designed for molecular beam epitaxy (MBE) to perform multi-step semiconductor heterostructure growth through in-situ monitoring and on-the-fly feedback control. By integrating standard MBE reactors, physics-informed machine learning (ML) models, and parameter initialization, SemiEpi identifies optimal initial conditions and proposes experiments for heterostructure growth, eliminating the need for extensive expertise in MBE processes. As a proof of concept, we demonstrate the optimization of high-density InAs quantum dot (QD) growth with a target emission wavelength of 1240 nm, showcasing the power of SemiEpi. We achieve a QD density of 5 x 10^10 cm^-2, a 1.6-fold increase in photoluminescence (PL) intensity, and a reduced full width at half maximum (FWHM) of 29.13 meV, leveraging in-situ reflective high-energy electron diffraction monitoring with feedback control for adjusting growth temperatures. Taken together, our results highlight the potential of ML-guided systems to address challenges in multi-step heterostructure growth, facilitate the development of a hardware-independent framework, and enhance process repeatability and stability, even without exhaustive knowledge of growth parameters.

Paper number 186:
Title: Virtual VNA 2.0: Ambiguity-Free Scattering Matrix Estimation by Terminating Not-Directly-Accessible Ports with Tunable and Coupled Loads
Authors: Philipp del Hougne
Abstract: We recently introduced the "Virtual VNA" concept which estimates the $N \times N$ scattering matrix characterizing an arbitrarily complex linear reciprocal system with $N$ monomodal lumped ports by inputting and outputting waves only via $N_\mathrm{A}<N$ ports while terminating the $N_\mathrm{S}=N-N_\mathrm{A}$ remaining ports with known tunable individual loads. However, vexing ambiguities about the signs of the off-diagonal scattering coefficients involving the $N_\mathrm{S}$ not-directly-accessible (NDA) ports remained. If only phase-insensitive measurements were used, an additional blockwise phase ambiguity ensued. Here, inspired by the emergence of "beyond-diagonal reconfigurable intelligent surfaces" in wireless communications, we lift all ambiguities with at most $N_\mathrm{S}$ additional measurements involving a known multi-port load network. We experimentally validate our approach based on an 8-port chaotic cavity, using a simple coaxial cable as two-port load network. Endowed with the multi-port load network, the "Virtual VNA 2.0" is now able to estimate the entire scattering matrix without any ambiguity, even without ever measuring phase information explicitly. Potential applications include the challenging characterization of large and/or embedded antenna arrays.

Paper number 187:
Title: Advanced Clustering Techniques for Speech Signal Enhancement: A Review and Metanalysis of Fuzzy C-Means, K-Means, and Kernel Fuzzy C-Means Methods
Authors: Abdulhady Abas Abdullah, Aram Mahmood Ahmed, Tarik Rashid, Hadi Veisi
Abstract: Speech signal processing is a cornerstone of modern communication technologies, tasked with improving the clarity and comprehensibility of audio data in noisy environments. The primary challenge in this field is the effective separation and recognition of speech from background noise, crucial for applications ranging from voice-activated assistants to automated transcription services. The quality of speech recognition directly impacts user experience and accessibility in technology-driven communication. This review paper explores advanced clustering techniques, particularly focusing on the Kernel Fuzzy C-Means (KFCM) method, to address these challenges. Our findings indicate that KFCM, compared to traditional methods like K-Means (KM) and Fuzzy C-Means (FCM), provides superior performance in handling non-linear and non-stationary noise conditions in speech signals. The most notable outcome of this review is the adaptability of KFCM to various noisy environments, making it a robust choice for speech enhancement applications. Additionally, the paper identifies gaps in current methodologies, such as the need for more dynamic clustering algorithms that can adapt in real time to changing noise conditions without compromising speech recognition quality. Key contributions include a detailed comparative analysis of current clustering algorithms and suggestions for further integrating hybrid models that combine KFCM with neural networks to enhance speech recognition accuracy. Through this review, we advocate for a shift towards more sophisticated, adaptive clustering techniques that can significantly improve speech enhancement and pave the way for more resilient speech processing systems.

Paper number 188:
Title: EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing Wheelchair System Using Deep Learning Approach
Authors: Biplov Paneru, Bishwash Paneru, Bipul Thapa, Khem Narayan Poudyal
Abstract: This study offers a revolutionary strategy to developing wheelchairs based on the Brain-Computer Interface (BCI) that incorporates Artificial Intelligence (AI) using a The device uses electroencephalogram (EEG) data to mimic wheelchair navigation. Five different models were trained on a pre-filtered dataset that was divided into fixed-length windows using a sliding window technique. Each window contained statistical measurements, FFT coefficients for different frequency bands, and a label identifying the activity carried out during that window that was taken from an open-source Kaggle repository. The XGBoost model outperformed the other models, CatBoost, GRU, SVC, and XGBoost, with an accuracy of 60%. The CatBoost model with a major difference between training and testing accuracy shows overfitting, and similarly, the best-performing model, with SVC, was implemented in a tkinter GUI. The wheelchair movement could be simulated in various directions, and a Raspberry Pi-powered wheelchair system for brain-computer interface is proposed here.

Paper number 189:
Title: ProcTex: Consistent and Interactive Text-to-texture Synthesis for Part-based Procedural Models
Authors: Ruiqi Xu, Zihan Zhu, Ben Ahlbrand, Srinath Sridhar, Daniel Ritchie
Abstract: Recent advances in generative modeling have driven significant progress in text-guided texture synthesis. However, current methods focus on synthesizing texture for single static 3D object, and struggle to handle entire families of shapes, such as those produced by procedural programs. Applying existing methods naively to each procedural shape is too slow to support exploring different parameter configurations at interactive rates, and also results in inconsistent textures across the procedural shapes. To this end, we introduce ProcTex, the first text-to-texture system designed for part-based procedural models. ProcTex enables consistent and real-time text-guided texture synthesis for families of shapes, which integrates seamlessly with the interactive design flow of procedural modeling. To ensure consistency, our core approach is to synthesize texture for a template shape from the procedural model, followed by a texture transfer stage to apply the texture to other procedural shapes via solving dense correspondence. To ensure interactiveness, we propose a novel correspondence network and show that dense correspondence can be effectively learned by a neural network for procedural models. We also develop several techniques, including a retexturing pipeline to support structural variation from procedural parameters, and part-level UV texture map generation for local appearance editing. Extensive experiments on a diverse set of procedural models validate ProcTex's ability to produce high-quality, visually consistent textures while supporting interactive applications.

Paper number 190:
Title: Rough Stochastic Pontryagin Maximum Principle and an Indirect Shooting Method
Authors: Thomas Lew
Abstract: We derive first-order Pontryagin optimality conditions for stochastic optimal control with deterministic controls for systems modeled by rough differential equations (RDE) driven by Gaussian rough paths. This Pontryagin Maximum Principle (PMP) applies to systems following stochastic differential equations (SDE) driven by Brownian motion, yet it does not rely on forward-backward SDEs and involves the same Hamiltonian as the deterministic PMP. The proof consists of first deriving various integrable error bounds for solutions to nonlinear and linear RDEs by leveraging recent results on Gaussian rough paths. The PMP then follows using standard techniques based on needle-like variations. As an application, we propose the first indirect shooting method for nonlinear stochastic optimal control and show that it converges 10x faster than a direct method on a stabilization task.

Paper number 191:
Title: QuIC: Quantum-Inspired Compound Adapters for Parameter Efficient Fine-Tuning
Authors: Snehal Raj, Brian Coyle
Abstract: Scaling full finetuning of large foundation models strains GPU memory and training time. Parameter Efficient Fine-Tuning (PEFT) methods address this issue via adapter modules which update only a small subset of model parameters. In this work, we introduce Quantum-Inspired Compound Adapters (QuIC Adapters), a PEFT approach inspired from Hamming-weight preserving quantum circuits that can effectively finetune a model using less than 0.02\% memory footprint of the base model. QuIC adapters preserve pretrained representations by enforcing orthogonality in weight parameters, and have native deployment mechanisms on quantum computers. We test QuIC adapters by finetuning large language models like LLaMA and vision transformers on language, math, reasoning and vision benchmarks. In its first-order configuration, QuIC recovers the performance of existing orthogonal methods, while higher-order configurations enable substantial parameter compression (over 40x smaller than LoRA) for a modest performance trade-off, unlocking applications in highly resource-constrained environments. Through ablation studies, we determine that combining multiple Hamming-weight orders with orthogonality and matrix compounding are essential for performant finetuning. Our findings suggest that QuIC adapters offers a promising direction for efficient finetuning of foundation models in resource-constrained environments.

Paper number 192:
Title: Sample Complexity of Linear Quadratic Regulator Without Initial Stability
Authors: Amirreza Neshaei Moghaddam, Alex Olshevsky, Bahman Gharesifard
Abstract: Inspired by REINFORCE, we introduce a novel receding-horizon algorithm for the Linear Quadratic Regulator (LQR) problem with unknown dynamics. Unlike prior methods, our algorithm avoids reliance on two-point gradient estimates while maintaining the same order of sample complexity. Furthermore, it eliminates the restrictive requirement of starting with a stable initial policy, broadening its applicability. Beyond these improvements, we introduce a refined analysis of error propagation through the contraction of the Riccati operator under the Riemannian distance. This refinement leads to a better sample complexity and ensures improved convergence guarantees.

Paper number 193:
Title: Nonparametric adaptive payload tracking for an offshore crane
Authors: TorbjÃ¸rn Smith, Olav Egeland
Abstract: A nonparametric adaptive controller is proposed for crane control where the payload tracks a desired trajectory with feedback from the payload position. The controller is based on a novel version of partial feedback linearization where the unactuated crane load dynamics are controlled with the position of the actuated crane dynamics instead of the acceleration. This is made possible by taking advantage of the gravity terms in a new Cartesian model that we propose for the load dynamics. This Cartesian model structure makes it possible to implement a nonparametric adaptive controller which cancels disturbances on the crane load by approximating the effects of unknown disturbance forces and structurally unknown dynamics in a reproducing kernel Hilbert space (RKHS). It is shown that the nonparametric adaptive controller leads to uniformly ultimately bounded errors in the presence of unknown forces and unmodeled dynamics. In addition, it is shown that the proposed partial feedback linearization based on the Cartesian model has certain advantages in payload tracking control also in the non-adaptive case. The performance of the nonparametric adaptive controller is validated in simulation and experiments with good results.

Paper number 194:
Title: A Corrector-aided Look-ahead Distance-based Guidance for Online Reference Path Following with an Efficient Mid-course Guidance Strategy
Authors: Reva Dhillon, Agni Ravi Deepa, Hrishav Das, Subham Basak, Satadal Ghosh
Abstract: Efficient path-following is crucial in most of the applications of autonomous vehicles (UxV). Among various guidance strategies presented in literature, the look-ahead distance ($L_1$)-based nonlinear guidance has received significant attention due to its ease in implementation and ability to maintain a low cross-track error while following simpler reference paths and generating bounded lateral acceleration commands. However, the constant value of $L_1$ becomes problematic when the UxV is far away from the reference path and also produces higher cross-track error while following complex reference paths having high variation in radius of curvature. To address these challenges, the notion of look-ahead distance is leveraged in a novel way to develop a two-phase guidance strategy. Initially, when the UxV is far from the reference path, an optimized $L_1$ selection strategy is developed to guide the UxV towards the vicinity of the start point of the reference path, while maintaining minimal lateral acceleration command. Once the vehicle reaches a close neighborhood of the reference path, a novel notion of corrector point is incorporated in the constant $L_1$-based guidance scheme to generate the guidance command that effectively reduces the root mean square of the cross-track error and lateral acceleration requirement thereafter. Simulation results validate satisfactory performance of this proposed corrector point and look-ahead point pair-based guidance strategy, along with the developed mid-course guidance scheme. Also, its superiority over the conventional constant $L_1$ guidance scheme is established by simulation studies over different initial condition scenarios.

Paper number 195:
Title: Digital-physical testbed for ship autonomy studies in the Marine Cybernetics Laboratory basin
Authors: Emir Cem Gezer, Mael Korentin Ivan Moreau, Anders Sandneseng HÃ¸gden, Dong Trong Nguyen, Roger Skjetne, Asgeir SÃ¸rensen
Abstract: The algorithms developed for Maritime Autonomous Surface Ships (MASS) are often challenging to test on actual vessels due to high operational costs and safety considerations. Simulations offer a cost-effective alternative and eliminate risks, but they may not accurately represent real-world dynamics for the given tasks. Utilizing small-scale model ships and robotic vessels in conjunction with a laboratory basin provides an accessible testing environment for the early stages of validation processes. However, designing and developing a model vessel for a single test can be costly and cumbersome, and researchers often lack access to such infrastructure. To address these challenges and enable streamlined testing, we have developed an in-house testbed that facilitates the development, testing, verification, and validation of MASS algorithms in a digital-physical laboratory. This infrastructure includes a set of small-scale model vessels, a simulation environment for each vessel, a comprehensive testbed environment, and a digital twin in Unity. With this, we aim to establish a full design and verification pipeline that starts with high-fidelity simulation models of each model vessel, to the model-scale testing in the laboratory basin, allowing possibilities for moving towards semi-fullscale validation with R/V milliAmpere1 and full-scale validation with R/V Gunnerus. In this work, we present our progress on the development of this testbed environment and its components, demonstrating its effectiveness in enabling ship guidance, navigation, and control (GNC), including autonomy.

Paper number 196:
Title: VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion
Authors: Joon-Seung Choi, Dong-Min Byun, Hyung-Seok Oh, Seong-Whan Lee
Abstract: Controlling singing style is crucial for achieving an expressive and natural singing voice. Among the various style factors, vibrato plays a key role in conveying emotions and enhancing musical depth. However, modeling vibrato remains challenging due to its dynamic nature, making it difficult to control in singing voice conversion. To address this, we propose VibESVC, a controllable singing voice conversion model that explicitly extracts and manipulates vibrato using discrete wavelet transform. Unlike previous methods that model vibrato implicitly, our approach decomposes the F0 contour into frequency components, enabling precise transfer. This allows vibrato control for enhanced flexibility. Experimental results show that VibE-SVC effectively transforms singing styles while preserving speaker similarity. Both subjective and objective evaluations confirm high-quality conversion.

Paper number 197:
Title: StressTest: Can YOUR Speech LM Handle the Stress?
Authors: Iddo Yosha, Gallil Maimon, Yossi Adi
Abstract: Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - this http URL.

Paper number 198:
Title: HoToPy: A toolbox for X-ray holo-tomography in Python
Authors: Jens Lucht, Paul Meyer, Leon Merten Lohse, Tim Salditt
Abstract: We present a Python toolbox for holographic and tomographic X-ray imaging. It comprises a collection of phase retrieval algorithms for the deeply holographic and direct contrast imaging regimes, including non-linear approaches and extended choices of regularization, constraint sets, and optimizers, all implemented with a unified and intuitive interface. Moreover, it features auxiliary functions for (tomographic) alignment, image processing, and simulation of imaging experiments. The capability of the toolbox is illustrated by the example of a catalytic nanoparticle, imaged in the deeply holographic regime at the 'GINIX' instrument of the P10 beamline at the PETRA III storage ring (DESY, Hamburg). Due to its modular design, the toolbox can be used for algorithmic development and benchmarking in a lean and flexible manner, or be interfaced and integrated in the reconstruction pipeline of other synchrotron or XFEL instruments for phase imaging based on propagation.

Paper number 199:
Title: TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography
Authors: Yuqin Dai, Wanlu Zhu, Ronghui Li, Xiu Li, Zhenyu Zhang, Jun Li, Jian Yang
Abstract: Music-driven dance generation has garnered significant attention due to its wide range of industrial applications, particularly in the creation of group choreography. During the group dance generation process, however, most existing methods still face three primary issues: multi-dancer collisions, single-dancer foot sliding and abrupt swapping in the generation of long group dance. In this paper, we propose TCDiff++, a music-driven end-to-end framework designed to generate harmonious group dance. Specifically, to mitigate multi-dancer collisions, we utilize a dancer positioning embedding to encode temporal and identity information. Additionally, we incorporate a distance-consistency loss to ensure that inter-dancer distances remain within plausible ranges. To address the issue of single-dancer foot sliding, we introduce a swap mode embedding to indicate dancer swapping patterns and design a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For long group dance generation, we present a long group diffusion sampling strategy that reduces abrupt position shifts by injecting positional information into the noisy input. Furthermore, we integrate a Sequence Decoder layer to enhance the model's ability to selectively process long sequences. Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art performance, particularly in long-duration scenarios, ensuring high-quality and coherent group dance generation.

Paper number 200:
Title: TolerantECG: A Foundation Model for Imperfect Electrocardiogram
Authors: Huynh Dang Nguyen, Trong-Thang Pham, Ngan Le, Van Nguyen
Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.

Paper number 201:
Title: Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data
Authors: Mohammad Alikhani, Reza Kazemi
Abstract: In the era of the Fourth Industrial Revolution, cybersecurity and intrusion detection systems are vital for the secure and reliable operation of IoT and IIoT environments. A key challenge in this domain is the scarcity of labeled cyberattack data, as most industrial systems operate under normal conditions. This data imbalance, combined with the high cost of annotation, hinders the effective training of machine learning models. Moreover, the rapid detection of attacks is essential, especially in critical infrastructure, to prevent large-scale disruptions. To address these challenges, we propose a real-time intrusion detection system based on a semi-supervised contrastive learning framework using the Kolmogorov-Arnold Network (KAN). Our method leverages abundant unlabeled data to effectively distinguish between normal and attack behaviors. We validate our approach on three benchmark datasets, UNSW-NB15, BoT-IoT, and Gas Pipeline, using only 2.20%, 1.28%, and 8% of labeled samples, respectively, to simulate real-world conditions. Experimental results show that our method outperforms existing contrastive learning-based approaches. We further compare KAN with a traditional multilayer perceptron (MLP), demonstrating KAN's superior performance in both detection accuracy and robustness under limited supervision. KAN's ability to model complex relationships, along with its learnable activation functions, is also explored and visualized, offering interpretability and the potential for rule extraction. The method supports multi-class classification and proves effective in safety, critical environments where reliability is paramount.

Paper number 202:
Title: SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering
Authors: Jan Melechovsky, Ambuj Mehrish, Abhinaba Roy, Dorien Herremans
Abstract: Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.

Paper number 203:
Title: TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation
Authors: Keunwoo Choi, Seungheon Doh, Juhan Nam
Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational music recommendation generated by an agentic data pipeline. In the proposed pipeline, multiple large language model (LLM) agents are created under various roles with specialized prompts and access to different parts of information, and the chat data is acquired by logging the conversation between the Listener LLM and the Recsys LLM. To cover various conversation scenarios, for each conversation, the Listener LLM is conditioned on a finetuned conversation goal. Finally, all the LLMs are multimodal with audio and images, allowing a simulation of multimodal recommendation and conversation. In the LLM-as-a-judge and subjective evaluation experiments, TalkPlayData 2 achieved the proposed goal in various aspects related to training a generative recommendation model for music. TalkPlayData 2 and its generation code are open-sourced at this https URL.

Paper number 204:
Title: Fun-ASR Technical Report
Authors: Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou
Abstract: In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings.

Paper number 205:
Title: StereoFoley: Object-Aware Stereo Audio Generation from Video
Authors: Tornike Karchkhadze, Kuan-Lin Chen, Mojtaba Heydari, Robert Henzel, Alessandro Toso, Mehrez Souden, Joshua Atkins
Abstract: We present StereoFoley, a video-to-audio generation framework that produces semantically aligned, temporally synchronized, and spatially accurate stereo sound at 48 kHz. While recent generative video-to-audio models achieve strong semantic and temporal fidelity, they largely remain limited to mono or fail to deliver object-aware stereo imaging, constrained by the lack of professionally mixed, spatially accurate video-to-audio datasets. First, we develop and train a base model that generates stereo audio from video, achieving state-of-the-art in both semantic accuracy and synchronization. Next, to overcome dataset limitations, we introduce a synthetic data generation pipeline that combines video analysis, object tracking, and audio synthesis with dynamic panning and distance-based loudness controls, enabling spatially accurate object-aware sound. Finally, we fine-tune the base model on this synthetic dataset, yielding clear object-audio correspondence. Since no established metrics exist, we introduce stereo object-awareness measures and validate it through a human listening study, showing strong correlation with perception. This work establishes the first end-to-end framework for stereo object-aware video-to-audio generation, addressing a critical gap and setting a new benchmark in the field.

Paper number 206:
Title: Prompt-aware classifier free guidance for diffusion models
Authors: Xuanhao Zhang, Chang Li
Abstract: Diffusion models have achieved remarkable progress in image and audio generation, largely due to Classifier-Free Guidance. However, the choice of guidance scale remains underexplored: a fixed scale often fails to generalize across prompts of varying complexity, leading to oversaturation or weak alignment. We address this gap by introducing a prompt-aware framework that predicts scale-dependent quality and selects the optimal guidance at inference. Specifically, we construct a large synthetic dataset by generating samples under multiple scales and scoring them with reliable evaluation metrics. A lightweight predictor, conditioned on semantic embeddings and linguistic complexity, estimates multi-metric quality curves and determines the best scale via a utility function with regularization. Experiments on MSCOCO~2014 and AudioCaps show consistent improvements over vanilla CFG, enhancing fidelity, alignment, and perceptual preference. This work demonstrates that prompt-aware scale selection provides an effective, training-free enhancement for pretrained diffusion backbones.

Paper number 207:
Title: A Hierarchical Control Architecture for Space Robots in On-Orbit Servicing Operations
Authors: Pietro Bruschi
Abstract: In-Orbit Servicing and Active Debris Removal require advanced robotic capabilities for capturing and detumbling uncooperative targets. This work presents a hierarchical control framework for autonomous robotic capture of tumbling objects in space. A simulation environment is developed, incorporating sloshing dynamics of the chaser, a rarely studied effect in space robotics. The proposed controller combines an inner Lyapunov-based robust control loop for multi-body dynamics with an outer loop addressing an extended inverse kinematics problem. Simulation results show improved robustness and adaptability compared to existing control schemes.

Paper number 208:
Title: Distributionally robust LMI synthesis for LTI systems
Authors: Dennis Gramlich, Shuhao Yan, Carsten W. Scherer, Christian Ebenbauer%
Abstract: This article shows that distributionally robust controller synthesis as investigated in \cite{taskesen2024distributionally} can be formulated as a convex linear matrix inequality (LMI) synthesis problem. To this end, we rely on well-established convexification techniques from robust control. The LMI synthesis problem we propose has the advantage that it can be solved efficiently using off-the-shelf semi-definite programming (SDP) solvers. In addition, our formulation exposes the studied distributionally robust controller synthesis problem as an instance of robust $H_2$ synthesis.

Paper number 209:
Title: HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition
Authors: Gio Paik, Yongbeom Kim, Soungmin Lee, Sangmin Ahn, Chanwoo Kim
Abstract: Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at this https URL

Paper number 210:
Title: Learning Passive Continuous-Time Dynamics with Multistep Port-Hamiltonian Gaussian Processes
Authors: Chi Ho Leung, Philip E. ParÃ©
Abstract: We propose the multistep port-Hamiltonian Gaussian process (MS-PHS GP) to learn physically consistent continuous-time dynamics and a posterior over the Hamiltonian from noisy, irregularly-sampled trajectories. By placing a GP prior on the Hamiltonian surface $H$ and encoding variable-step multistep integrator constraints as finite linear functionals, MS-PHS GP enables closed-form conditioning of both the vector field and the Hamiltonian surface without latent states, while enforcing energy balance and passivity by design. We state a finite-sample vector-field bound that separates the estimation and variable-step discretization terms. Lastly, we demonstrate improved vector-field recovery and well-calibrated Hamiltonian uncertainty on mass-spring, Van der Pol, and Duffing benchmarks.

Paper number 211:
Title: TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling
Authors: Seungheon Doh, Keunwoo Choi, Juhan Nam
Abstract: While the recent developments in large language models (LLMs) have successfully enabled generative recommenders with natural language interactions, their recommendation behavior is limited, leaving other simpler yet crucial components such as metadata or attribute filtering underutilized in the system. We propose an LLM-based music recommendation system with tool calling to serve as a unified retrieval-reranking pipeline. Our system positions an LLM as an end-to-end recommendation system that interprets user intent, plans tool invocations, and orchestrates specialized components: boolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding similarity), and generative retrieval (semantic IDs). Through tool planning, the system predicts which types of tools to use, their execution order, and the arguments needed to find music matching user preferences, supporting diverse modalities while seamlessly integrating multiple database filtering methods. We demonstrate that this unified tool-calling framework achieves competitive performance across diverse recommendation scenarios by selectively employing appropriate retrieval methods based on user queries, envisioning a new paradigm for conversational music recommendation systems.

Paper number 212:
Title: Robust MPC for Large-scale Linear Systems
Authors: Georg Schildbach
Abstract: State-of-the-art approaches of Robust Model Predictive Control (MPC) are restricted to linear systems of relatively small scale, i.e., with no more than about 5 states. The main reason is the computational burden of determining a robust positively invariant (RPI) set, whose complexity suffers from the curse of dimensionality. The recently proposed approach of Deadbeat Robust Model Predictive Control (DRMPC) is the first that does not rely on an RPI set. Yet it comes with the full set of essential system theoretic guarantees. DRMPC is hence a viable option, in particular, for large-scale systems. This paper introduces a detailed design procedure for DRMPC. It is shown that the optimal control problem generated for DRMPC has exactly the same computational complexity as Nominal MPC. A numerical study validates its applicability to randomly generated large-scale linear systems of various dimensions.

Paper number 213:
Title: Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models
Authors: Wei-Lung Mao, Chun-Chi Wang, Po-Heng Chou, Yen-Ting Liu
Abstract: Since the defect detection of conventional industry components is time-consuming and labor-intensive, it leads to a significant burden on quality inspection personnel and makes it difficult to manage product quality. In this paper, we propose an automated defect detection system for the dual in-line package (DIP) that is widely used in industry, using digital camera optics and a deep learning (DL)-based model. The two most common defect categories of DIP are examined: (1) surface defects, and (2) pin-leg defects. However, the lack of defective component images leads to a challenge for detection tasks. To solve this problem, the ConSinGAN is used to generate a suitable-sized dataset for training and testing. Four varieties of the YOLO model are investigated (v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation. The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in accuracy of 95.50\%, detection time of 285 ms, and is far superior to threshold-based approaches. In addition, the supervisory control and data acquisition (SCADA) system is developed, and the associated sensor architecture is described. The proposed automated defect detection can be easily established with numerous types of defects or insufficient defect data.
    