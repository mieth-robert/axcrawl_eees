
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Deep Learning for Ophthalmology: The State-of-the-Art and Future Trends
Authors: Duy M. H. Nguyen, Hasan Md Tusfiqur Alam, Tai Nguyen, Devansh Srivastav, Hans-Juergen Profitlich, Ngan Le, Daniel Sonntag
Abstract: The emergence of artificial intelligence (AI), particularly deep learning (DL), has marked a new era in the realm of ophthalmology, offering transformative potential for the diagnosis and treatment of posterior segment eye diseases. This review explores the cutting-edge applications of DL across a range of ocular conditions, including diabetic retinopathy, glaucoma, age-related macular degeneration, and retinal vessel segmentation. We provide a comprehensive overview of foundational ML techniques and advanced DL architectures, such as CNNs, attention mechanisms, and transformer-based models, highlighting the evolving role of AI in enhancing diagnostic accuracy, optimizing treatment strategies, and improving overall patient care. Additionally, we present key challenges in integrating AI solutions into clinical practice, including ensuring data diversity, improving algorithm transparency, and effectively leveraging multimodal data. This review emphasizes AI's potential to improve disease diagnosis and enhance patient care while stressing the importance of collaborative efforts to overcome these barriers and fully harness AI's impact in advancing eye care.

Paper number 2:
Title: Security by Design Issues in Autonomous Vehicles
Authors: Martin Higgins, Devki Jha, David Blundell, David Wallom
Abstract: As autonomous vehicle (AV) technology advances towards maturity, it becomes imperative to examine the security vulnerabilities within these cyber-physical systems. While conventional cyber-security concerns are often at the forefront of discussions, it is essential to get deeper into the various layers of vulnerability that are often overlooked within mainstream frameworks. Our goal is to spotlight imminent challenges faced by AV operators and explore emerging technologies for comprehensive solutions. This research outlines the diverse security layers, spanning physical, cyber, coding, and communication aspects, in the context of AVs. Furthermore, we provide insights into potential solutions for each potential attack vector, ensuring that autonomous vehicles remain secure and resilient in an evolving threat landscape.

Paper number 3:
Title: Constrained and Regularized Quantitative Ultrasound Parameter Estimation using ADMM
Authors: Ali K. Z. Tehrani, Hassan Rivaz, Ivan M. Rosado-Mendez
Abstract: Regularized estimation of quantitative ultrasound (QUS) parameters, such as attenuation and backscatter coefficients, has gained research interest. Recently, the alternating direction method of multipliers (ADMM) has been applied successfully to estimate these parameters, by utilizing L2 and L1 norms for attenuation and backscatter coefficient regularization, respectively. While this method improves upon previous approaches, it does not fully leverage the prior knowledge of minimum physically feasible parameter values, sometimes yielding values outside the realistic range. This work addresses this limitation by incorporating minimum QUS parameter values as constraints to enhance ADMM estimation. The proposed method is validated using experimental phantom data.

Paper number 4:
Title: Artifact-free Sound Quality in DNN-based Closed-loop Systems for Audio Processing
Authors: chuan Wen, Guy Torfs, Sarah Verhulst
Abstract: Recent advances in deep neural networks (DNNs) have significantly improved various audio processing applications, including speech enhancement, synthesis, and hearing aid algorithms. DNN-based closed-loop systems have gained popularity in these applications due to their robust performance and ability to adapt to diverse conditions. Despite their effectiveness, current DNN-based closed-loop systems often suffer from sound quality degradation caused by artifacts introduced by suboptimal sampling methods. To address this challenge, we introduce dCoNNear, a novel DNN architecture designed for seamless integration into closed-loop frameworks. This architecture specifically aims to prevent the generation of spurious artifacts. We demonstrate the effectiveness of dCoNNear through a proof-of-principle example within a closed-loop framework that employs biophysically realistic models of auditory processing for both normal and hearing-impaired profiles to design personalized hearing aid algorithms. Our results show that dCoNNear not only accurately simulates all processing stages of existing non-DNN biophysical models but also eliminates audible artifacts, thereby enhancing the sound quality of the resulting hearing aid algorithms. This study presents a novel, artifact-free closed-loop framework that improves the sound quality of audio processing systems, offering a promising solution for high-fidelity applications in audio and hearing technologies.

Paper number 5:
Title: Collaborative Spacecraft Servicing under Partial Feedback using Lyapunov-based Deep Neural Networks
Authors: Cristian F. Nino, Omkar Sudhir Patil, Christopher D. Petersen, Sean Phillips, Warren E. Dixon
Abstract: Multi-agent systems are increasingly applied in space missions, including distributed space systems, resilient constellations, and autonomous rendezvous and docking operations. A critical emerging application is collaborative spacecraft servicing, which encompasses on-orbit maintenance, space debris removal, and swarm-based satellite repositioning. These missions involve servicing spacecraft interacting with malfunctioning or defunct spacecraft under challenging conditions, such as limited state information, measurement inaccuracies, and erratic target behaviors. Existing approaches often rely on assumptions of full state knowledge or single-integrator dynamics, which are impractical for real-world applications involving second-order spacecraft dynamics. This work addresses these challenges by developing a distributed state estimation and tracking framework that requires only relative position measurements and operates under partial state information. A novel $\rho$-filter is introduced to reconstruct unknown states using locally available information, and a Lyapunov-based deep neural network adaptive controller is developed that adaptively compensates for uncertainties stemming from unknown spacecraft dynamics. To ensure the collaborative spacecraft regulation problem is well-posed, a trackability condition is defined. A Lyapunov-based stability analysis is provided to ensure exponential convergence of errors in state estimation and spacecraft regulation to a neighborhood of the origin under the trackability condition. The developed method eliminates the need for expensive velocity sensors or extensive pre-training, offering a practical and robust solution for spacecraft servicing in complex, dynamic environments.

Paper number 6:
Title: Comparison of Neural Models for X-ray Image Classification in COVID-19 Detection
Authors: Jimi Togni, Romis Attux
Abstract: This study presents a comparative analysis of methods for detecting COVID-19 infection in radiographic images. The images, sourced from publicly available datasets, were categorized into three classes: 'normal,' 'pneumonia,' and 'COVID.' For the experiments, transfer learning was employed using eight pre-trained networks: SqueezeNet, DenseNet, ResNet, AlexNet, VGG, GoogleNet, ShuffleNet, and MobileNet. DenseNet achieved the highest accuracy of 97.64% using the ADAM optimization function in the multiclass approach. In the binary classification approach, the highest precision was 99.98%, obtained by the VGG, ResNet, and MobileNet networks. A comparative evaluation was also conducted using heat maps.

Paper number 7:
Title: PhaseMO: Future-Proof, Energy-efficient, Adaptive Massive MIMO
Authors: Adel Heidari, Agrim Gupta, Ish Kumar Jain, Dinesh Bharadia
Abstract: The rapid proliferation of devices and increasing data traffic in cellular networks necessitate advanced solutions to meet these escalating demands. Massive MIMO (Multiple Input Multiple Output) technology offers a promising approach, significantly enhancing throughput, coverage, and spatial multi-plexing. Despite its advantages, massive MIMO systems often lack flexible software controls over hardware, limiting their ability to optimize operational expenditure (OpEx) by reducing power consumption while maintaining performance. Current software-controlled methods, such as antenna muting combined with digital beamforming and hybrid beamforming, have notable limitations. Antenna muting struggles to maintain throughput and coverage, while hybrid beamforming faces hardware constraints that restrict scalability and future-proofing. This work presents PhaseMO, a versatile approach that adapts to varying network loads. PhaseMO effectively reduces power consumption in low-load scenarios without sacrificing coverage and overcomes the hardware limitations of hybrid beamforming, offering a scalable and future-proof solution. We will show that PhaseMO can achieve up to 30% improvement in energy efficiency while avoiding about 10% coverage reduction and 5dB increase in UE transmit power.

Paper number 8:
Title: GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology
Authors: Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering
Abstract: Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITE's potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists' diagnostic reasoning and support precision medicine.

Paper number 9:
Title: Privacy-Preserving Distributed Online Mirror Descent for Nonconvex Optimization
Authors: Yingjie Zhou, Tao Li
Abstract: We investigate the distributed online nonconvex optimization problem with differential privacy over time-varying networks. Each node minimizes the sum of several nonconvex functions while preserving the node's differential privacy. We propose a privacy-preserving distributed online mirror descent algorithm for nonconvex optimization, which uses the mirror descent to update decision variables and the Laplace differential privacy mechanism to protect privacy. Unlike the existing works, the proposed algorithm allows the cost functions to be nonconvex, which is more applicable. Based upon these, we prove that if the communication network is $B$-strongly connected and the constraint set is compact, then by choosing the step size properly, the algorithm guarantees $\epsilon$-differential privacy at each time. Furthermore, we prove that if the local cost functions are $\beta$-smooth, then the regret over time horizon $T$ grows sublinearly while preserving differential privacy, with an upper bound $O(\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated through numerical simulations.

Paper number 10:
Title: A Support Vector Approach in Segmented Regression for Map-assisted Non-cooperative Source Localization
Authors: Hao Sun, Weiming Huang, Junting Chen
Abstract: This paper presents a non-cooperative source localization approach based on received signal strength (RSS) and 2D environment map, considering both line-of-sight (LOS) and non-line-of-sight (NLOS) conditions. Conventional localization methods, e.g., weighted centroid localization (WCL), may perform bad. This paper proposes a segmented regression approach using 2D maps to estimate source location and propagation environment jointly. By leveraging topological information from the 2D maps, a support vector-assisted algorithm is developed to solve the segmented regression problem, separate the LOS and NLOS measurements, and estimate the location of source. The proposed method demonstrates a good localization performance with an improvement of over 30% in localization rooted mean squared error (RMSE) compared to the baseline methods.

Paper number 11:
Title: A Quasi-deterministic Channel Model for Underwater Acoustic Communication Systems
Authors: Yuxuan Yang, Yilin Ma, Hengtai Chang, Cheng-Xiang Wang
Abstract: In this paper, a quasi-deterministic (Q-D) model for non-stationary underwater acoustic (UWA) channels is proposed. This model combines the BELLHOP deterministic model and geometry-based stochastic model (GBSM), which provides higher accuracy and flexibility. Different propagation components in shallow water are classified as D-rays, R-rays and F-rays in the proposed model, where D-rays are modeled by BELLHOP while both R-rays and F-rays are modeled by GBSM. Some important channel statistical properties, including time-frequency correlation function (TF-CF), Doppler power spectrum density (PSD), average Doppler shift, and RMS Doppler spread are derived and simulated. Finally, simulation results illustrate the correctness of the proposed model.

Paper number 12:
Title: A Novel Non-Stationary Channel Emulator for 6G MIMO Wireless Channels
Authors: Yuan Zong, Lijian Xin, Jie Huang, Cheng-Xiang Wang
Abstract: The performance evaluation of sixth generation (6G) communication systems is anticipated to be a controlled and repeatable process in the lab, which brings up the demand for wireless channel emulators. However, channel emulation for 6G space-time-frequency (STF) non-stationary channels is missing currently. In this paper, a non-stationary multiple-input multiple-output (MIMO) geometry-based stochastic model (GBSM) that accurately characterizes the channel STF properties is introduced firstly. Then, a subspace-based method is proposed for reconstructing the channel fading obtained from the GBSM and a channel emulator architecture with frequency domain processing is presented for 6G MIMO systems. Moreover, the spatial time-varying channel transfer functions (CTFs) of the channel simulation and the channel emulation are compared and analyzed. The Doppler power spectral density (PSD) and delay PSD are further derived and compared between the channel model simulation and subspace-based emulation. The results demonstrate that the proposed channel emulator is capable of reproducing the non-stationary channel characteristics.

Paper number 13:
Title: Beam Domain Channel Estimation for Spatial Non-Stationary Massive MIMO Systems
Authors: Lin Hou, Hengtai Chang, Cheng-Xiang Wang, Jie Huang, Songjiang Yang
Abstract: In massive multiple-input multiple-output (MIMO) systems, the channel estimation scheme is subject to the spatial non-stationarity and inevitably power leakage in the beam domain. In this paper, a beam domain channel estimation scheme is investigated for spatial non-stationary (SNS) massive MIMO systems considering power leakage. %a novel beam domain channel estimation scheme is proposed for spatial non-stationary (SNS) massive MIMO systems. Specifically, a realistic massive MIMO beam domain channel model (BDCM) is introduced to capture the spatial non-stationarity considering power leakage by introducing the illustration of visibility region (VR). Then, a beam domain structure-based sparsity adaptive matching pursuit (BDS-SAMP) scheme is proposed based on the cross-block sparse structure and power ratio threshold of beam domain channel. Finally, the simulation results validate the accuracy of proposed BDS-SAMP scheme with low pilot overhead and reasonable complexity by comparing with conventional schemes.

Paper number 14:
Title: Target Tracking Using the Invariant Extended Kalman Filter with Numerical Differentiation for Estimating Curvature and Torsion
Authors: Shashank Verma, Dennis S. Bernstein
Abstract: The goal of target tracking is to estimate target position, velocity, and acceleration in real time using position data. This paper introduces a novel target-tracking technique that uses adaptive input and state estimation (AISE) for real-time numerical differentiation to estimate velocity, acceleration, and jerk from position data. These estimates are used to model the target motion within the Frenet-Serret (FS) frame. By representing the model in SE(3), the position and velocity are estimated using the invariant extended Kalman filter (IEKF). The proposed method, called FS-IEKF-AISE, is illustrated by numerical examples and compared to prior techniques.

Paper number 15:
Title: Frenet-Serret-Based Trajectory Prediction
Authors: Shashank Verma, Dennis S. Bernstein
Abstract: Trajectory prediction is a crucial element of guidance, navigation, and control systems. This paper presents two novel trajectory-prediction methods based on real-time position measurements and adaptive input and state estimation (AISE). The first method, called AISE/va, uses position measurements to estimate the target velocity and acceleration. The second method, called AISE/FS, models the target trajectory as a 3D curve using the Frenet-Serret formulas, which require estimates of velocity, acceleration, and jerk. To estimate velocity, acceleration, and jerk in real time, AISE computes first, second, and third derivatives of the position measurements. AISE does not rely on assumptions about the target maneuver, measurement noise, or disturbances. For trajectory prediction, both methods use measurements of the target position and estimates of its derivatives to extrapolate from the current position. The performance of AISE/va and AISE/FS is compared numerically with the $\alpha$-$\beta$-$\gamma$ filter, which shows that AISE/FS provides more accurate trajectory prediction than AISE/va and traditional methods, especially for complex target maneuvers.

Paper number 16:
Title: Adaptive Numerical Differentiation for Extremum Seeking with Sensor Noise
Authors: Shashank Verma, Juan Augusto Paredes Salazar, Jhon Manuel Portella Delgado, Ankit Goel, Dennis S. Bernstein
Abstract: Extremum-seeking control (ESC) is widely used to optimize performance when the system dynamics are uncertain. However, sensitivity to sensor noise is an important issue in ESC implementation due to the use of high-pass filters or gradient estimators. To reduce the sensitivity of ESC to noise, this paper investigates the use of adaptive input and state estimation (AISE) for numerical differentiation. In particular, this paper develops extremum-seeking control with adaptive input and state estimation (ESC/AISE), where the high-pass filter of ESC is replaced by AISE to improve performance under sensor noise. The effectiveness of ESC/AISE is illustrated via numerical examples.

Paper number 17:
Title: FSC-loss: A Frequency-domain Structure Consistency Learning Approach for Signal Data Recovery and Reconstruction
Authors: Liwen Zhang, Zhaoji Miao, Fan Yang, Gen Shi, Jie He, Yu An, Hui Hui, Jie Tian
Abstract: A core challenge for signal data recovery is to model the distribution of signal matrix (SM) data based on measured low-quality data in biomedical engineering of magnetic particle imaging (MPI). For acquiring the high-resolution (high-quality) SM, the number of meticulous measurements at numerous positions in the field-of-view proves time-consuming (measurement of a 37x37x37 SM takes about 32 hours). To improve reconstructed signal quality and shorten SM measurement time, existing methods explore to generating high-resolution SM based on time-saving measured low-resolution SM (a 9x9x9 SM just takes about 0.5 hours). However, previous methods show poor performance for high-frequency signal recovery in SM. To achieve a high-resolution SM recovery and shorten its acquisition time, we propose a frequency-domain structure consistency loss function and data component embedding strategy to model global and local structural information of SM. We adopt a transformer-based network to evaluate this function and the strategy. We evaluate our methods and state-of-the-art (SOTA) methods on the two simulation datasets and four public measured SMs in Open MPI Data. The results show that our method outperforms the SOTA methods in high-frequency structural signal recovery. Additionally, our method can recover a high-resolution SM with clear high-frequency structure based on a down-sampling factor of 16 less than 15 seconds, which accelerates the acquisition time over 60 times faster than the measurement-based HR SM with the minimum error (nRMSE=0.041). Moreover, our method is applied in our three in-house MPI systems, and boost their performance for signal reconstruction.

Paper number 18:
Title: Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation
Authors: Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, Kateryna Shapovalenko
Abstract: Decoding speech from non-invasive brain signals, such as electroencephalography (EEG), has the potential to advance brain-computer interfaces (BCIs), with applications in silent communication and assistive technologies for individuals with speech impairments. However, EEG-based speech decoding faces major challenges, such as noisy data, limited datasets, and poor performance on complex tasks like speech perception. This study attempts to address these challenges by employing variational autoencoders (VAEs) for EEG data augmentation to improve data quality and applying a state-of-the-art (SOTA) sequence-to-sequence deep learning architecture, originally successful in electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we adapt this architecture for word classification tasks. Using the Brennan dataset, which contains EEG recordings of subjects listening to narrated speech, we preprocess the data and evaluate both classification and sequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments show that VAEs have the potential to reconstruct artificial EEG data for augmentation. Meanwhile, our sequence-to-sequence model achieves more promising performance in generating sentences compared to our classification model, though both remain challenging tasks. These findings lay the groundwork for future research on EEG speech perception decoding, with possible extensions to speech production tasks such as silent or imagined speech.

Paper number 19:
Title: A Unified Framework for Foreground and Anonymization Area Segmentation in CT and MRI Data
Authors: Michal Nohel, Constantin Ulrich, Jonathan Suprijadi, Tassilo Wald, Klaus Maier-Hein
Abstract: This study presents an open-source toolkit to address critical challenges in preprocessing data for self-supervised learning (SSL) for 3D medical imaging, focusing on data privacy and computational efficiency. The toolkit comprises two main components: a segmentation network that delineates foreground regions to optimize data sampling and thus reduce training time, and a segmentation network that identifies anonymized regions, preventing erroneous supervision in reconstruction-based SSL methods. Experimental results demonstrate high robustness, with mean Dice scores exceeding 98.5 across all anonymization methods and surpassing 99.5 for foreground segmentation tasks, highlighting the efficacy of the toolkit in supporting SSL applications in 3D medical imaging for both CT and MRI images. The weights and code is available at this https URL.

Paper number 20:
Title: ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent Diffusion Models and Adversarial Training
Authors: Xinfa Zhu, Lei He, Yujia Xiao, Xi Wang, Xu Tan, Sheng Zhao, Lei Xie
Abstract: Style voice conversion aims to transform the speaking style of source speech into a desired style while keeping the original speaker's identity. However, previous style voice conversion approaches primarily focus on well-defined domains such as emotional aspects, limiting their practical applications. In this study, we present ZSVC, a novel Zero-shot Style Voice Conversion approach that utilizes a speech codec and a latent diffusion model with speech prompting mechanism to facilitate in-context learning for speaking style conversion. To disentangle speaking style and speaker timbre, we introduce information bottleneck to filter speaking style in the source speech and employ Uncertainty Modeling Adaptive Instance Normalization (UMAdaIN) to perturb the speaker timbre in the style prompt. Moreover, we propose a novel adversarial training strategy to enhance in-context learning and improve style similarity. Experiments conducted on 44,000 hours of speech data demonstrate the superior performance of ZSVC in generating speech with diverse speaking styles in zero-shot scenarios.

Paper number 21:
Title: A new methodology for the optimization of bolt tightening sequences for ring type joints
Authors: Ibai Coria, Mikel Abasolo, Imanol Olaskoaga, Arkaitz Etxezarreta, Josu Aguirrebeitia
Abstract: Achieving uniform bolt load distribution is critical to obtain leak-free service in pressure vessel gasketed joints used in offshore pipelines. This is a difficult task due to bolt load variations during the assembly process. In this sense, the Elastic Interaction Coefficients Method has been developed in previous works to define tightening sequences that provide the target load at the end of the sequence in one or two passes. The method is very costly because a complete sequence must be simulated and the load of every bolt must be measured after each tightening operation. The present work validates this method for Ring Type Joints and further develops a numerically and experimentally validated new methodology that provides highly satisfactory results with a significantly lower cost.

Paper number 22:
Title: Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions
Authors: Doaa Mahmud, Hadeel Hajmohamed, Shamma Almentheri, Shamma Alqaydi, Lameya Aldhaheri, Ruhul Amin Khalil, Nasir Saeed
Abstract: Intelligent Transportation Systems (ITS) are crucial for the development and operation of smart cities, addressing key challenges in efficiency, productivity, and environmental sustainability. This paper comprehensively reviews the transformative potential of Large Language Models (LLMs) in optimizing ITS. Initially, we provide an extensive overview of ITS, highlighting its components, operational principles, and overall effectiveness. We then delve into the theoretical background of various LLM techniques, such as GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications. Following this, we examine the wide-ranging applications of LLMs within ITS, including traffic flow prediction, vehicle detection and classification, autonomous driving, traffic sign recognition, and pedestrian detection. Our analysis reveals how these advanced models can significantly enhance traffic management and safety. Finally, we explore the challenges and limitations LLMs face in ITS, such as data availability, computational constraints, and ethical considerations. We also present several future research directions and potential innovations to address these challenges. This paper aims to guide researchers and practitioners through the complexities and opportunities of integrating LLMs in ITS, offering a roadmap to create more efficient, sustainable, and responsive next-generation transportation systems.

Paper number 23:
Title: Rotatable and Movable Antenna-Enabled Near-Field Integrated Sensing and Communication
Authors: Yunan Sun
Abstract: Integrated sensing and communication (ISAC) is regarded as a promising technology for next-generation communication networks. As the demand for communication performance significantly increases, extremely large-scale antenna arrays and tremendously high-frequency bands get widely applied in communication systems, leading to the expansion of the near-field region. On a parallel track, movable antennas (MAs) and six-dimensional MAs (6DMAs) are proposed as emerging technologies to improve the performance of communication and sensing. Based on such a background, this paper investigates the performance of ISAC systems in the near-field region, focusing on a novel system architecture that employs rotatable MAs (RMAs). Additionally, a spherical wave near-field channel model with respect to RMAs' rotations and positions is derived by considering the effective aperture loss. Two designs are explored: a sensing-centric design that minimizes the Cramér-Rao bound (CRB) with signal-to-interference-plus-noise ratio (SINR) constraints, and a communication-centric design that maximizes the sum-rate with a CRB constraint. To solve the formulated optimization problems, the paper proposes two alternating optimization (AO) based algorithms composed of the semidefinite relaxation (SDR) method and the particle swarm optimization (PSO) method. Numerical results demonstrate the convergence and effectiveness of the proposed algorithms and the superiority of the proposed setups for both sensing and communication performance compared to traditional fixed antenna systems, highlighting the potential of RMAs to enhance ISAC systems in near-field scenarios.

Paper number 24:
Title: The Role of Machine Learning in Congenital Heart Disease Diagnosis: Datasets, Algorithms, and Insights
Authors: Khalil Khan, Farhan Ullah, Ikram Syed, Irfan Ullah
Abstract: Congenital heart disease is among the most common fetal abnormalities and birth defects. Despite identifying numerous risk factors influencing its onset, a comprehensive understanding of its genesis and management across diverse populations remains limited. Recent advancements in machine learning have demonstrated the potential for leveraging patient data to enable early congenital heart disease detection. Over the past seven years, researchers have proposed various data-driven and algorithmic solutions to address this challenge. This paper presents a systematic review of congential heart disease recognition using machine learning, conducting a meta-analysis of 432 references from leading journals published between 2018 and 2024. A detailed investigation of 74 scholarly works highlights key factors, including databases, algorithms, applications, and solutions. Additionally, the survey outlines reported datasets used by machine learning experts for congenital heart disease recognition. Using a systematic literature review methodology, this study identifies critical challenges and opportunities in applying machine learning to congenital heart disease.

Paper number 25:
Title: New Linear Model of a Composite Energy Storage System with Realizable Dispatch Guarantees
Authors: Mazen Elsaadany, Mads R. Almassalkhi, Simon H. Tindemans
Abstract: To optimize battery dispatch, a model is required that can predict the state of charge (SOC) trajectory and ensure dispatch is admissible (i.e., does not lead to unexpected SOC saturation). But battery dispatch optimization is inherently challenging since batteries cannot simultaneously charge and discharge, which begets a non-convex complementarity constraint. In this paper, we consider a composition of energy storage elements that can charge or discharge independently and provide a sufficient linear energy storage model of the composite battery. This permits convex optimization of the composite battery SOC trajectory while ensuring admissibility of the resulting (aggregated) power schedule and disaggregation to the individual energy storage elements.

Paper number 26:
Title: SplineFormer: An Explainable Transformer-Based Approach for Autonomous Endovascular Navigation
Authors: Tudor Jianu, Shayan Doust, Mengyun Li, Baoru Huang, Tuong Do, Hoan Nguyen, Karl Bates, Tung D. Ta, Sebastiano Fichera, Pierre Berthet-Rayne, Anh Nguyen
Abstract: Endovascular navigation is a crucial aspect of minimally invasive procedures, where precise control of curvilinear instruments like guidewires is critical for successful interventions. A key challenge in this task is accurately predicting the evolving shape of the guidewire as it navigates through the vasculature, which presents complex deformations due to interactions with the vessel walls. Traditional segmentation methods often fail to provide accurate real-time shape predictions, limiting their effectiveness in highly dynamic environments. To address this, we propose SplineFormer, a new transformer-based architecture, designed specifically to predict the continuous, smooth shape of the guidewire in an explainable way. By leveraging the transformer's ability, our network effectively captures the intricate bending and twisting of the guidewire, representing it as a spline for greater accuracy and smoothness. We integrate our SplineFormer into an end-to-end robot navigation system by leveraging the condensed information. The experimental results demonstrate that our SplineFormer is able to perform endovascular navigation autonomously and achieves a 50% success rate when cannulating the brachiocephalic artery on the real robot.

Paper number 27:
Title: Recursive Least Squares with Fading Regularization for Finite-Time Convergence without Persistent Excitation
Authors: Brian Lai, Dimitra Panagou, Dennis S. Bernstein
Abstract: This paper extends recursive least squares (RLS) to include time-varying regularization. This extension provides flexibility for updating the least squares regularization term in real time. Existing results with constant regularization imply that the parameter-estimation error dynamics of RLS are globally attractive to zero if and only the regressor is weakly persistently exciting. This work shows that, by extending classical RLS to include a time-varying (fading) regularization term that converges to zero, the parameter-estimation error dynamics are globally attractive to zero without weakly persistent excitation. Moreover, if the fading regularization term converges to zero in finite time, then the parameter estimation error also converges to zero in finite time. Finally, we propose rank-1 fading regularization (R1FR) RLS, a time-varying regularization algorithm with fading regularization that converges to zero, and which runs in the same computational complexity as classical RLS. Numerical examples are presented to validate theoretical guarantees and to show how R1FR-RLS can protect against over-regularization.

Paper number 28:
Title: Regret Analysis: a control perspective
Authors: Travis E. Gibson, Sawal Acharya
Abstract: Online learning and model reference adaptive control have many interesting intersections. One area where they differ however is in how the algorithms are analyzed and what objective or metric is used to discriminate "good" algorithms from "bad" algorithms. In adaptive control there are usually two objectives: 1) prove that all time varying parameters/states of the system are bounded, and 2) that the instantaneous error between the adaptively controlled system and a reference system converges to zero over time (or at least a compact set). For online learning the performance of algorithms is often characterized by the regret the algorithm incurs. Regret is defined as the cumulative loss (cost) over time from the online algorithm minus the cumulative loss (cost) of the single optimal fixed parameter choice in hindsight. Another significant difference between the two areas of research is with regard to the assumptions made in order to obtain said results. Adaptive control makes assumptions about the input-output properties of the control problem and derives solutions for a fixed error model or optimization task. In the online learning literature results are derived for classes of loss functions (i.e. convex) while a priori assuming that all time varying parameters are bounded, which for many optimization tasks is not unrealistic, but is a non starter in control applications. In this work we discuss these differences in detail through the regret based analysis of gradient descent for convex functions and the control based analysis of a streaming regression problem. We close with a discussion about the newly defined paradigm of online adaptive control and ask the following question "Are regret optimal control strategies deployable?"

Paper number 29:
Title: Comprehensive Examination of Unrolled Networks for Linear Inverse Problems
Authors: Eric Chen, Xi Chen, Arian Maleki, Shirin Jalali
Abstract: Unrolled networks have become prevalent in various computer vision and imaging tasks. Although they have demonstrated remarkable efficacy in solving specific computer vision and computational imaging tasks, their adaptation to other applications presents considerable challenges. This is primarily due to the multitude of design decisions that practitioners working on new applications must navigate, each potentially affecting the network's overall performance. These decisions include selecting the optimization algorithm, defining the loss function, and determining the number of convolutional layers, among others. Compounding the issue, evaluating each design choice requires time-consuming simulations to train, fine-tune the neural network, and optimize for its performance. As a result, the process of exploring multiple options and identifying the optimal configuration becomes time-consuming and computationally demanding. The main objectives of this paper are (1) to unify some ideas and methodologies used in unrolled networks to reduce the number of design choices a user has to make, and (2) to report a comprehensive ablation study to discuss the impact of each of the choices involved in designing unrolled networks and present practical recommendations based on our findings. We anticipate that this study will help scientists and engineers design unrolled networks for their applications and diagnose problems within their networks efficiently.

Paper number 30:
Title: Large-scale Grid Optimization: The Workhorse of Future Grid Computations
Authors: Amritanshu Pandey, Mads Almassalkhi, Sam Chevalier
Abstract: Purpose: The computation methods for modeling, controlling and optimizing the transforming grid are evolving rapidly. We review and systemize knowledge for a special class of computation methods that solve large-scale power grid optimization problems. Summary: Large-scale grid optimizations are pertinent for, amongst other things, hedging against risk due to resource stochasticity, evaluating aggregated DERs' impact on grid operation and design, and improving the overall efficiency of grid operation in terms of cost, reliability, and carbon footprint. We attribute the continual growth in scale and complexity of grid optimizations to a large influx of new spatial and temporal features in both transmission (T) and distribution (D) networks. Therefore, to systemize knowledge in the field, we discuss the recent advancements in T and D systems from the viewpoint of mechanistic physics-based and emerging data-driven methods. Findings: We find that while mechanistic physics-based methods are leading the science in solving large-scale grid optimizations, data-driven techniques, especially physics-constrained ones, are emerging as an alternative to solve otherwise intractable problems. We also find observable gaps in the field and ascertain these gaps from the paper's literature review and by collecting and synthesizing feedback from industry experts.

Paper number 31:
Title: FleSpeech: Flexibly Controllable Speech Generation with Various Prompts
Authors: Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie
Abstract: Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at this https URL

Paper number 32:
Title: HyFusion: Enhanced Reception Field Transformer for Hyperspectral Image Fusion
Authors: Chia-Ming Lee, Yu-Fan Lin, Yu-Hao Ho, Li-Wei Kang, Chih-Chung Hsu
Abstract: Hyperspectral image (HSI) fusion addresses the challenge of reconstructing High-Resolution HSIs (HR-HSIs) from High-Resolution Multispectral images (HR-MSIs) and Low-Resolution HSIs (LR-HSIs), a critical task given the high costs and hardware limitations associated with acquiring high-quality HSIs. While existing methods leverage spatial and spectral relationships, they often suffer from limited receptive fields and insufficient feature utilization, leading to suboptimal performance. Furthermore, the scarcity of high-quality HSI data highlights the importance of efficient data utilization to maximize reconstruction quality. To address these issues, we propose HyFusion, a novel framework designed to enhance the receptive field and enable effective feature map reusing, thereby maximizing data utilization. First, HR-MSI and LR-HSI inputs are concatenated to form a quasi-fused draft, preserving complementary spatial and spectral details. Next, the Enhanced Reception Field Block (ERFB) is introduced, combining shifting-window attention and dense connections to expand the receptive field, effectively capturing long-range dependencies and reusing features to reduce information loss, thereby boosting data efficiency. Finally, the Dual-Coupled Network (DCN) dynamically extracts high-frequency spectral and spatial features from LR-HSI and HR-MSI, ensuring efficient cross-domain fusion. Extensive experiments demonstrate that HyFusion achieves state-of-the-art performance in HR-MSI/LR-HSI fusion, significantly improving reconstruction quality while maintaining a compact model size and computational efficiency. By integrating enhanced receptive fields and feature map reusing, HyFusion provides a practical and effective solution for HSI fusion in resource-constrained scenarios, setting a new benchmark in hyperspectral imaging. Our code will be publicly available.

Paper number 33:
Title: RadGPT: Constructing 3D Image-Text Tumor Datasets
Authors: Pedro R. A. S. Bassi, Mehmet Can Yavuz, Kang Wang, Xiaoxi Chen, Wenxuan Li, Sergio Decherchi, Andrea Cavalli, Yang Yang, Alan Yuille, Zongwei Zhou
Abstract: With over 85 million CT scans performed annually in the United States, creating tumor-related reports is a challenging and time-consuming task for radiologists. To address this need, we present RadGPT, an Anatomy-Aware Vision-Language AI Agent for generating detailed reports from CT scans. RadGPT first segments tumors, including benign cysts and malignant tumors, and their surrounding anatomical structures, then transforms this information into both structured reports and narrative reports. These reports provide tumor size, shape, location, attenuation, volume, and interactions with surrounding blood vessels and organs. Extensive evaluation on unseen hospitals shows that RadGPT can produce accurate reports, with high sensitivity/specificity for small tumor (<2 cm) detection: 80/73% for liver tumors, 92/78% for kidney tumors, and 77/77% for pancreatic tumors. For large tumors, sensitivity ranges from 89% to 97%. The results significantly surpass the state-of-the-art in abdominal CT report generation. RadGPT generated reports for 17 public datasets. Through radiologist review and refinement, we have ensured the reports' accuracy, and created the first publicly available image-text 3D medical dataset, comprising over 1.8 million text tokens and 2.7 million images from 9,262 CT scans, including 2,947 tumor scans/reports of 8,562 tumor instances. Our reports can: (1) localize tumors in eight liver sub-segments and three pancreatic sub-segments annotated per-voxel; (2) determine pancreatic tumor stage (T1-T4) in 260 reports; and (3) present individual analyses of multiple tumors--rare in human-made reports. Importantly, 948 of the reports are for early-stage tumors.

Paper number 34:
Title: Listening and Seeing Again: Generative Error Correction for Audio-Visual Speech Recognition
Authors: Rui Liu, Hongyu Yuan, Haizhou Li
Abstract: Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech Recognition (AVSR) takes audio and visual signals simultaneously to infer the transcription. Recent studies have shown that Large Language Models (LLMs) can be effectively used for Generative Error Correction (GER) in ASR by predicting the best transcription from ASR-generated N-best hypotheses. However, these LLMs lack the ability to simultaneously understand audio and visual, making the GER approach challenging to apply in AVSR. In this work, we propose a novel GER paradigm for AVSR, termed AVGER, that follows the concept of ``listening and seeing again''. Specifically, we first use the powerful AVSR system to read the audio and visual signals to get the N-Best hypotheses, and then use the Q-former-based Multimodal Synchronous Encoder to read the audio and visual information again and convert them into an audio and video compression representation respectively that can be understood by LLM. Afterward, the audio-visual compression representation and the N-Best hypothesis together constitute a Cross-modal Prompt to guide the LLM in producing the best transcription. In addition, we also proposed a Multi-Level Consistency Constraint training criterion, including logits-level, utterance-level and representations-level, to improve the correction accuracy while enhancing the interpretability of audio and visual compression representations. The experimental results on the LRS3 dataset show that our method outperforms current mainstream AVSR systems. The proposed AVGER can reduce the Word Error Rate (WER) by 24% compared to them. Code and models can be found at: this https URL.

Paper number 35:
Title: Bridging Impulse Control of Piecewise Deterministic Markov Processes and Markov Decision Processes: Frameworks, Extensions, and Open Challenges
Authors: Alice Cleynen, Benoîte de Saporta, Orlane Rossini, Régis Sabbadin, Amélie Vernay
Abstract: Control theory plays a pivotal role in understanding and optimizing the behavior of complex dynamical systems across various scientific and engineering disciplines. Two key frameworks that have emerged for modeling and solving control problems in stochastic systems are piecewise deterministic Markov processes (PDMPs) and Markov decision processes (MDPs). Each framework has its unique strengths, and their intersection offers promising opportunities for tackling a broad class of problems, particularly in the context of impulse controls and decision-making in complex systems. The relationship between PDMPs and MDPs is a natural subject of exploration, as embedding impulse control problems for PDMPs into the MDP framework could open new avenues for their analysis and resolution. Specifically, this integration would allow leveraging the computational and theoretical tools developed for MDPs to address the challenges inherent in PDMPs. On the other hand, PDMPs can offer a versatile and simple paradigm to model continuous time problems that are often described as discrete-time MDPs parametrized by complex transition kernels. This transformation has the potential to bridge the gap between the two frameworks, enabling solutions to previously intractable problems and expanding the scope of both fields. This paper presents a comprehensive review of two research domains, illustrated through a recurring medical example. The example is revisited and progressively formalized within the framework of thevarious concepts and objects introduced

Paper number 36:
Title: Multiple testing in multi-stream sequential change detection
Authors: Sanjit Dandapanthula, Aaditya Ramdas
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), and per-family error rate (PFER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL. One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the Type I error.

Paper number 37:
Title: Spatiotemporal Gaussian Optimization for 4D Cone Beam CT Reconstruction from Sparse Projections
Authors: Yabo Fu, Hao Zhang, Weixing Cai, Huiqiao Xie, Licheng Kuo, Laura Cervino, Jean Moran, Xiang Li, Tianfang Li
Abstract: In image-guided radiotherapy (IGRT), four-dimensional cone-beam computed tomography (4D-CBCT) is critical for assessing tumor motion during a patients breathing cycle prior to beam delivery. However, generating 4D-CBCT images with sufficient quality requires significantly more projection images than a standard 3D-CBCT scan, leading to extended scanning times and increased imaging dose to the patient. To address these limitations, there is a strong demand for methods capable of reconstructing high-quality 4D-CBCT images from a 1-minute 3D-CBCT acquisition. The challenge lies in the sparse sampling of projections, which introduces severe streaking artifacts and compromises image quality. This paper introduces a novel framework leveraging spatiotemporal Gaussian representation for 4D-CBCT reconstruction from sparse projections, achieving a balance between streak artifact reduction, dynamic motion preservation, and fine detail restoration. Each Gaussian is characterized by its 3D position, covariance, rotation, and density. Two-dimensional X-ray projection images can be rendered from the Gaussian point cloud representation via X-ray rasterization. The properties of each Gaussian were optimized by minimizing the discrepancy between the measured projections and the rendered X-ray projections. A Gaussian deformation network is jointly optimized to deform these Gaussian properties to obtain a 4D Gaussian representation for dynamic CBCT scene modeling. The final 4D-CBCT images are reconstructed by voxelizing the 4D Gaussians, achieving a high-quality representation that preserves both motion dynamics and spatial detail. The code and reconstruction results can be found at this https URL

Paper number 38:
Title: Holographic Metasurface-Based Beamforming for Multi-Altitude LEO Satellite Networks
Authors: Qingchao Li, Mohammed El-Hajjar, Kaijun Cao, Chao Xu, Harald Haas, Lajos Hanzo
Abstract: Low Earth Orbit (LEO) satellite networks are capable of improving the global Internet service coverage. In this context, we propose a hybrid beamforming design for holographic metasurface based terrestrial users in multi-altitude LEO satellite networks. Firstly, the holographic beamformer is optimized by maximizing the downlink channel gain from the serving satellite to the terrestrial user. Then, the digital beamformer is designed by conceiving a minimum mean square error (MMSE) based detection algorithm for mitigating the interference arriving from other satellites. To dispense with excessive overhead of full channel state information (CSI) acquisition of all satellites, we propose a low-complexity MMSE beamforming algorithm that only relies on the distribution of the LEO satellite constellation harnessing stochastic geometry, which can achieve comparable throughput to that of the algorithm based on the full CSI in the case of a dense LEO satellite deployment. Furthermore, it outperforms the maximum ratio combining (MRC) algorithm, thanks to its inter-satellite interference mitigation capacity. The simulation results show that our proposed holographic metasurface based hybrid beamforming architecture is capable of outperforming the state-of-the-art antenna array architecture in terms of its throughput, given the same physical size of the transceivers. Moreover, we demonstrate that the beamforming performance attained can be substantially improved by taking into account the mutual coupling effect, imposed by the dense placement of the holographic metasurface elements.

Paper number 39:
Title: Machine Learning for Identifying Grain Boundaries in Scanning Electron Microscopy (SEM) Images of Nanoparticle Superlattices
Authors: Aanish Paruchuri, Carl Thrasher, A. J. Hart, Robert Macfarlane, Arthi Jayaraman
Abstract: Nanoparticle superlattices consisting of ordered arrangements of nanoparticles exhibit unique optical, magnetic, and electronic properties arising from nanoparticle characteristics as well as their collective behaviors. Understanding how processing conditions influence the nanoscale arrangement and microstructure is critical for engineering materials with desired macroscopic properties. Microstructural features such as grain boundaries, lattice defects, and pores significantly affect these properties but are challenging to quantify using traditional manual analyses as they are labor-intensive and prone to errors. In this work, we present a machine learning workflow for automating grain segmentation in scanning electron microscopy (SEM) images of nanoparticle superlattices. This workflow integrates signal processing techniques, such as Radon transforms, with unsupervised learning methods like agglomerative hierarchical clustering to identify and segment grains without requiring manually annotated data. In the workflow we transform the raw pixel data into explainable numerical representation of superlattice orientations for clustering. Benchmarking results demonstrate the workflow's robustness against noisy images and edge cases, with a processing speed of four images per minute on standard computational hardware. This efficiency makes the workflow scalable to large datasets and makes it a valuable tool for integrating data-driven models into decision-making processes for material design and analysis. For example, one can use this workflow to quantify grain size distributions at varying processing conditions like temperature and pressure and using that knowledge adjust processing conditions to achieve desired superlattice orientations and grain sizes.

Paper number 40:
Title: Recognition-Oriented Low-Light Image Enhancement based on Global and Pixelwise Optimization
Authors: Seitaro Ono, Yuka Ogino, Takahiro Toizumi, Atsushi Ito, Masato Tsukada
Abstract: In this paper, we propose a novel low-light image enhancement method aimed at improving the performance of recognition models. Despite recent advances in deep learning, the recognition of images under low-light conditions remains a challenge. Although existing low-light image enhancement methods have been developed to improve image visibility for human vision, they do not specifically focus on enhancing recognition model performance. Our proposed low-light image enhancement method consists of two key modules: the Global Enhance Module, which adjusts the overall brightness and color balance of the input image, and the Pixelwise Adjustment Module, which refines image features at the pixel level. These modules are trained to enhance input images to improve downstream recognition model performance effectively. Notably, the proposed method can be applied as a frontend filter to improve low-light recognition performance without requiring retraining of downstream recognition models. Experimental results demonstrate that our method improves the performance of pretrained recognition models under low-light conditions and its effectiveness.

Paper number 41:
Title: DrawSpeech: Expressive Speech Synthesis Using Prosodic Sketches as Control Conditions
Authors: Weidong Chen, Shan Yang, Guangzhi Li, Xixin Wu
Abstract: Controlling text-to-speech (TTS) systems to synthesize speech with the prosodic characteristics expected by users has attracted much attention. To achieve controllability, current studies focus on two main directions: (1) using reference speech as prosody prompt to guide speech synthesis, and (2) using natural language descriptions to control the generation process. However, finding reference speech that exactly contains the prosody that users want to synthesize takes a lot of effort. Description-based guidance in TTS systems can only determine the overall prosody, which has difficulty in achieving fine-grained prosody control over the synthesized speech. In this paper, we propose DrawSpeech, a sketch-conditioned diffusion model capable of generating speech based on any prosody sketches drawn by users. Specifically, the prosody sketches are fed to DrawSpeech to provide a rough indication of the expected prosody trends. DrawSpeech then recovers the detailed pitch and energy contours based on the coarse sketches and synthesizes the desired speech. Experimental results show that DrawSpeech can generate speech with a wide variety of prosody and can precisely control the fine-grained prosody in a user-friendly manner. Our implementation and audio samples are publicly available.

Paper number 42:
Title: KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry
Authors: Zhong Wang, Lele Ren, Yue Wen, Hesheng Wang
Abstract: Recent advancements in LiDAR-Inertial Odometry (LIO) have boosted a large amount of applications. However, traditional LIO systems tend to focus more on localization rather than mapping, with maps consisting mostly of sparse geometric elements, which is not ideal for downstream tasks. Recent emerging neural field technology has great potential in dense mapping, but pure LiDAR mapping is difficult to work on high-dynamic vehicles. To mitigate this challenge, we present a new solution that tightly couples geometric kinematics with neural fields to enhance simultaneous state estimation and dense mapping capabilities. We propose both semi-coupled and tightly coupled Kinematic-Neural LIO (KN-LIO) systems that leverage online SDF decoding and iterated error-state Kalman filtering to fuse laser and inertial data. Our KN-LIO minimizes information loss and improves accuracy in state estimation, while also accommodating asynchronous multi-LiDAR inputs. Evaluations on diverse high-dynamic datasets demonstrate that our KN-LIO achieves performance on par with or superior to existing state-of-the-art solutions in pose estimation and offers improved dense mapping accuracy over pure LiDAR-based methods. The relevant code and datasets will be made available at https://**.

Paper number 43:
Title: Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images
Authors: Yuze Wang, Rong Xiao, Haifeng Li, Mariana Belgiu, Chao Tao
Abstract: In remote sensing scene classification, leveraging the transfer methods with well-trained optical models is an efficient way to overcome label scarcity. However, cloud contamination leads to optical information loss and significant impacts on feature distribution, challenging the reliability and stability of transferred target models. Common solutions include cloud removal for optical data or directly using Synthetic aperture radar (SAR) data in the target domain. However, cloud removal requires substantial auxiliary data for support and pre-training, while directly using SAR disregards the unobstructed portions of optical data. This study presents a scene classification transfer method that synergistically combines multi-modality data, which aims to transfer the source domain model trained on cloudfree optical data to the target domain that includes both cloudy optical and SAR data at low cost. Specifically, the framework incorporates two parts: (1) the collaborative transfer strategy, based on knowledge distillation, enables the efficient prior knowledge transfer across heterogeneous data; (2) the information regulation mechanism (IRM) is proposed to address the modality imbalance issue during transfer. It employs auxiliary models to measure the contribution discrepancy of each modality, and automatically balances the information utilization of modalities during the target model learning process at the sample-level. The transfer experiments were conducted on simulated and real cloud datasets, demonstrating the superior performance of the proposed method compared to other solutions in cloud-covered scenarios. We also verified the importance and limitations of IRM, and further discussed and visualized the modality imbalance problem during the model transfer. Codes are available at this https URL

Paper number 44:
Title: Separate Source Channel Coding Is Still What You Need: An LLM-based Rethinking
Authors: Tianqi Ren, Rongpeng Li, Ming-min Zhao, Xianfu Chen, Guangyi Liu, Yang Yang, Zhifeng Zhao, Honggang Zhang
Abstract: Along with the proliferating research interest in Semantic Communication (SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to the widely assumed existence in efficiently delivering information semantics. %has emerged as a pivotal area of research, aiming to enhance the efficiency and reliability of information transmission through deep learning-based methods. Nevertheless, this paper challenges the conventional JSCC paradigm, and advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy the underlying more degree of freedom for optimization. We demonstrate that SSCC, after leveraging the strengths of Large Language Model (LLM) for source coding and Error Correction Code Transformer (ECCT) complemented for channel decoding, offers superior performance over JSCC. Our proposed framework also effectively highlights the compatibility challenges between SemCom approaches and digital communication systems, particularly concerning the resource costs associated with the transmission of high precision floating point numbers. Through comprehensive evaluations, we establish that empowered by LLM-based compression and ECCT-enhanced error correction, SSCC remains a viable and effective solution for modern communication systems. In other words, separate source and channel coding is still what we need!

Paper number 45:
Title: MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge
Authors: Zijiang Yang, Meishu Song, Xin Jing, Haojie Zhang, Kun Qian, Bin Hu, Kota Tamada, Toru Takumi, Björn W. Schuller, Yoshiharu Yamamoto
Abstract: The Mice Autism Detection via Ultrasound Vocalization (MAD-UV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection.

Paper number 46:
Title: Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition
Authors: Huimeng Wang, Xurong Xie, Mengzhe Geng, Shujie Hu, Haoning Xu, Youjun Chen, Zhaoqing Li, Jiajun Deng, Xunying Liu
Abstract: Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\% and 1.77\% absolute (3.21\% and 4.82\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.

Paper number 47:
Title: Rapid Automated Mapping of Clouds on Titan With Instance Segmentation
Authors: Zachary Yahn, Douglas M Trent, Ethan Duncan, Benoît Seignovert, John Santerre, Conor Nixon
Abstract: Despite widespread adoption of deep learning models to address a variety of computer vision tasks, planetary science has yet to see extensive utilization of such tools to address its unique problems. On Titan, the largest moon of Saturn, tracking seasonal trends and weather patterns of clouds provides crucial insights into one of the most complex climates in the Solar System, yet much of the available image data are still analyzed in a conventional way. In this work, we apply a Mask R-CNN trained via transfer learning to perform instance segmentation of clouds in Titan images acquired by the Cassini spacecraft - a previously unexplored approach to a big data problem in planetary science. We demonstrate that an automated technique can provide quantitative measures for clouds, such as areas and centroids, that may otherwise be prohibitively time-intensive to produce by human mapping. Furthermore, despite Titan specific challenges, our approach yields accuracy comparable to contemporary cloud identification studies on Earth and other worlds. We compare the efficiencies of human-driven versus algorithmic approaches, showing that transfer learning provides speed-ups that may open new horizons for data investigation for Titan. Moreover, we suggest that such approaches have broad potential for application to similar problems in planetary science where they are currently under-utilized. Future planned missions to the planets and remote sensing initiatives for the Earth promise to provide a deluge of image data in the coming years that will benefit strongly from leveraging machine learning approaches to perform the analysis.

Paper number 48:
Title: Safe Reinforcement Learning with Minimal Supervision
Authors: Alexander Quessy, Thomas Richardson, Sebastian East
Abstract: Reinforcement learning (RL) in the real world necessitates the development of procedures that enable agents to explore without causing harm to themselves or others. The most successful solutions to the problem of safe RL leverage offline data to learn a safe-set, enabling safe online exploration. However, this approach to safe-learning is often constrained by the demonstrations that are available for learning. In this paper we investigate the influence of the quantity and quality of data used to train the initial safe learning problem offline on the ability to learn safe-RL policies online. Specifically, we focus on tasks with spatially extended goal states where we have few or no demonstrations available. Classically this problem is addressed either by using hand-designed controllers to generate data or by collecting user-generated demonstrations. However, these methods are often expensive and do not scale to more complex tasks and environments. To address this limitation we propose an unsupervised RL-based offline data collection procedure, to learn complex and scalable policies without the need for hand-designed controllers or user demonstrations. Our research demonstrates the significance of providing sufficient demonstrations for agents to learn optimal safe-RL policies online, and as a result, we propose optimistic forgetting, a novel online safe-RL approach that is practical for scenarios with limited data. Further, our unsupervised data collection approach highlights the need to balance diversity and optimality for safe online exploration.

Paper number 49:
Title: Right Label Context in End-to-End Training of Time-Synchronous ASR Models
Authors: Tina Raissi, Ralf Schlter, Hermann Ney
Abstract: Current time-synchronous sequence-to-sequence automatic speech recognition (ASR) models are trained by using sequence level cross-entropy that sums over all alignments. Due to the discriminative formulation, incorporating the right label context into the training criterion's gradient causes normalization problems and is not mathematically well-defined. The classic hybrid neural network hidden Markov model (NN-HMM) with its inherent generative formulation enables conditioning on the right label context. However, due to the HMM state-tying the identity of the right label context is never modeled explicitly. In this work, we propose a factored loss with auxiliary left and right label contexts that sums over all alignments. We show that the inclusion of the right label context is particularly beneficial when training data resources are limited. Moreover, we also show that it is possible to build a factored hybrid HMM system by relying exclusively on the full-sum criterion. Experiments were conducted on Switchboard 300h and LibriSpeech 960h.

Paper number 50:
Title: Evaluating Interval-based Tokenization for Pitch Representation in Symbolic Music Analysis
Authors: Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller
Abstract: Symbolic music analysis tasks are often performed by models originally developed for Natural Language Processing, such as Transformers. Such models require the input data to be represented as sequences, which is achieved through a process of tokenization. Tokenization strategies for symbolic music often rely on absolute MIDI values to represent pitch information. However, music research largely promotes the benefit of higher-level representations such as melodic contour and harmonic relations for which pitch intervals turn out to be more expressive than absolute pitches. In this work, we introduce a general framework for building interval-based tokenizations. By evaluating these tokenizations on three music analysis tasks, we show that such interval-based tokenizations improve model performances and facilitate their explainability.

Paper number 51:
Title: Unlocking the diagnostic potential of electrocardiograms through information transfer from cardiac magnetic resonance imaging
Authors: Özgün Turgut, Philip Müller, Paul Hager, Suprosanna Shit, Sophie Starck, Martin J. Menten, Eimo Martens, Daniel Rueckert
Abstract: Cardiovascular diseases (CVD) can be diagnosed using various diagnostic modalities. The electrocardiogram (ECG) is a cost-effective and widely available diagnostic aid that provides functional information of the heart. However, its ability to classify and spatially localise CVD is limited. In contrast, cardiac magnetic resonance (CMR) imaging provides detailed structural information of the heart and thus enables evidence-based diagnosis of CVD, but long scan times and high costs limit its use in clinical routine. In this work, we present a deep learning strategy for cost-effective and comprehensive cardiac screening solely from ECG. Our approach combines multimodal contrastive learning with masked data modelling to transfer domain-specific information from CMR imaging to ECG representations. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalisability of our method for subject-specific risk prediction of CVD and the prediction of cardiac phenotypes using only ECG data. Specifically, our novel multimodal pre-training paradigm improves performance by up to 12.19 % for risk prediction and 27.59 % for phenotype prediction. In a qualitative analysis, we demonstrate that our learned ECG representations incorporate information from CMR image regions of interest. Our entire pipeline is publicly available at this https URL.

Paper number 52:
Title: AutoFuse: Automatic Fusion Networks for Deformable Medical Image Registration
Authors: Mingyuan Meng, Michael Fulham, Dagan Feng, Lei Bi, Jinman Kim
Abstract: Deformable image registration aims to find a dense non-linear spatial correspondence between a pair of images, which is a crucial step for many medical tasks such as tumor growth monitoring and population analysis. Recently, Deep Neural Networks (DNNs) have been widely recognized for their ability to perform fast end-to-end registration. However, DNN-based registration needs to explore the spatial information of each image and fuse this information to characterize spatial correspondence. This raises an essential question: what is the optimal fusion strategy to characterize spatial correspondence? Existing fusion strategies (e.g., early fusion, late fusion) were empirically designed to fuse information by manually defined prior knowledge, which inevitably constrains the registration performance within the limits of empirical designs. In this study, we depart from existing empirically-designed fusion strategies and develop a data-driven fusion strategy for deformable image registration. To achieve this, we propose an Automatic Fusion network (AutoFuse) that provides flexibility to fuse information at many potential locations within the network. A Fusion Gate (FG) module is also proposed to control how to fuse information at each potential network location based on training data. Our AutoFuse can automatically optimize its fusion strategy during training and can be generalizable to both unsupervised registration (without any labels) and semi-supervised registration (with weak labels provided for partial training data). Extensive experiments on two well-benchmarked medical registration tasks (inter- and intra-patient registration) with eight public datasets show that our AutoFuse outperforms state-of-the-art unsupervised and semi-supervised registration methods.

Paper number 53:
Title: Explainable Severity ranking via pairwise n-hidden comparison: a case study of glaucoma
Authors: Hong Nguyen, Cuong V. Nguyen, Shrikanth Narayanan, Benjamin Y. Xu, Michael Pazzani
Abstract: Primary open-angle glaucoma (POAG) is a chronic and progressive optic nerve condition that results in an acquired loss of optic nerve fibers and potential blindness. The gradual onset of glaucoma results in patients progressively losing their vision without being consciously aware of the changes. To diagnose POAG and determine its severity, patients must undergo a comprehensive dilated eye examination. In this work, we build a framework to rank, compare, and interpret the severity of glaucoma using fundus images. We introduce a siamese-based severity ranking using pairwise n-hidden comparisons. We additionally have a novel approach to explaining why a specific image is deemed more severe than others. Our findings indicate that the proposed severity ranking model surpasses traditional ones in terms of diagnostic accuracy and delivers improved saliency explanations.

Paper number 54:
Title: Deep Unfolding Network with Spatial Alignment for multi-modal MRI reconstruction
Authors: Hao Zhang, Qi Wang, Jun Shi, Shihui Ying, Zhijie Wen
Abstract: Multi-modal Magnetic Resonance Imaging (MRI) offers complementary diagnostic information, but some modalities are limited by the long scanning time. To accelerate the whole acquisition process, MRI reconstruction of one modality from highly undersampled k-space data with another fully-sampled reference modality is an efficient solution. However, the misalignment between modalities, which is common in clinic practice, can negatively affect reconstruction quality. Existing deep learning-based methods that account for inter-modality misalignment perform better, but still share two main common limitations: (1) The spatial alignment task is not adaptively integrated with the reconstruction process, resulting in insufficient complementarity between the two tasks; (2) the entire framework has weak interpretability. In this paper, we construct a novel Deep Unfolding Network with Spatial Alignment, termed DUN-SA, to appropriately embed the spatial alignment task into the reconstruction process. Concretely, we derive a novel joint alignment-reconstruction model with a specially designed cross-modal spatial alignment term. By relaxing the model into cross-modal spatial alignment and multi-modal reconstruction tasks, we propose an effective algorithm to solve this model alternatively. Then, we unfold the iterative steps of the proposed algorithm and design corresponding network modules to build DUN-SA with interpretability. Through end-to-end training, we effectively compensate for spatial misalignment using only reconstruction loss, and utilize the progressively aligned reference modality to provide inter-modality prior to improve the reconstruction of the target modality. Comprehensive experiments on three real datasets demonstrate that our method exhibits superior reconstruction performance compared to state-of-the-art methods.

Paper number 55:
Title: Unconditional Latent Diffusion Models Memorize Patient Imaging Data: Implications for Openly Sharing Synthetic Data
Authors: Salman Ul Hassan Dar, Marvin Seyfarth, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Robert Malte Siepmann, Fabian Christopher Laqua, Jannik Kahmann, Norbert Frey, Bettina Baeßler, Sebastian Foersch, Daniel Truhn, Jakob Nikolas Kather, Sandy Engelhardt
Abstract: AI models present a wide range of applications in the field of medicine. However, achieving optimal performance requires access to extensive healthcare data, which is often not readily available. Furthermore, the imperative to preserve patient privacy restricts patient data sharing with third parties and even within institutes. Recently, generative AI models have been gaining traction for facilitating open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, some of these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in unconditional latent diffusion models. We train latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. We then detect the amount of training data memorized utilizing our novel self-supervised copy detection approach and further investigate various factors that can influence memorization. Our findings show a surprisingly high degree of patient data memorization across all datasets. Comparison with non-diffusion generative models, such as autoencoders and generative adversarial networks, indicates that while latent diffusion models are more susceptible to memorization, overall they outperform non-diffusion models in synthesis quality. Further analyses reveal that using augmentation strategies, small architecture, and increasing dataset can reduce memorization while over-training the models can enhance it. Collectively, our results emphasize the importance of carefully training generative models on private medical imaging datasets, and examining the synthetic data to ensure patient privacy before sharing it for medical research and applications.

Paper number 56:
Title: DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer Interaction Module
Authors: Xinyu Wang, Haotian Jiang, Haolin Huang, Yu Fang, Mengjie Xu, Qian Wang
Abstract: Speech recognition is the technology that enables machines to interpret and process human speech, converting spoken language into text or commands. This technology is essential for applications such as virtual assistants, transcription services, and communication tools. The Audio-Visual Speech Recognition (AVSR) model enhances traditional speech recognition, particularly in noisy environments, by incorporating visual modalities like lip movements and facial expressions. While traditional AVSR models trained on large-scale datasets with numerous parameters can achieve remarkable accuracy, often surpassing human performance, they also come with high training costs and deployment challenges. To address these issues, we introduce an efficient AVSR model that reduces the number of parameters through the integration of a Dual Conformer Interaction Module (DCIM). In addition, we propose a pre-training method that further optimizes model performance by selectively updating parameters, leading to significant improvements in efficiency. Unlike conventional models that require the system to independently learn the hierarchical relationship between audio and visual modalities, our approach incorporates this distinction directly into the model architecture. This design enhances both efficiency and performance, resulting in a more practical and effective solution for AVSR tasks.

Paper number 57:
Title: Efficient Dual-Blind Deconvolution for Joint Radar-Communication Systems Using ADMM: Enhancing Channel Estimation and Signal Recovery in 5G mmWave Networks
Authors: Anis Hamadouche, Mathini Sellathurai
Abstract: This paper introduces a novel framework for jointly estimating unknown radar channels and transmit signals in millimeter-wave (mmWave) Joint Radar-Communication (JRC) systems, a problem often referred to as dual-blind deconvolution. The proposed method employs the Alternating Direction Method of Multipliers (ADMM) to iteratively refine the radar channel G (or H) and the transmitted signal X under convex constraints, incorporating both smooth and non-smooth penalty terms via proximal operators. By enforcing a bounded perturbation model for the radar channel and a strict transmit power budget, the algorithm aligns well with practical hardware limits. Extensive simulations demonstrate that the proposed approach reliably addresses the dual-blind deconvolution challenge, resulting in effective radar channel estimation and robust communication performance. Notably, the framework's iterative structure readily accommodates hardware considerations and different system configurations, making it well-suited for emerging mmWave JRC scenarios. Its adaptability and computational efficiency highlight the potential for wider adoption in next-generation wireless networks, where radar detection and communications increasingly share bandwidth and hardware resources.

Paper number 58:
Title: A Peaceman-Rachford Splitting Approach with Deep Equilibrium Network for Channel Estimation
Authors: Dingli Yuan, Shitong Wu, Haoran Tang, Lu Yang, Chenghui Peng
Abstract: Multiple-input multiple-output (MIMO) is pivotal for wireless systems, yet its high-dimensional, stochastic channel poses significant challenges for accurate estimation, highlighting the critical need for robust estimation techniques. In this paper, we introduce a novel channel estimation method for the MIMO system. The main idea is to construct a fixed-point equation for channel estimation, which can be implemented into the deep equilibrium (DEQ) model with a fixed network. Specifically, the Peaceman-Rachford (PR) splitting method is applied to the dual form of the regularized minimization problem to construct fixed-point equation with non-expansive property. Then, the fixed-point equation is implemented into the DEQ model with a fixed layer, leveraging its advantage of the low training complexity. Moreover, we provide a rigorous theoretical analysis, demonstrating the convergence and optimality of our approach. Additionally, simulations of hybrid far- and near-field channels demonstrate that our approach yields favorable results, indicating its ability to advance channel estimation in MIMO system.

Paper number 59:
Title: A capacity renting framework for shared energy storage considering peer-to-peer energy trading of prosumers with privacy protection
Authors: Yingcong Sun, Laijun Chen, Yue Chen, Mingrui Tang, Shengwei Mei
Abstract: Shared energy storage systems (ESS) present a promising solution to the temporal imbalance between energy generation from renewable distributed generators (DGs) and the power demands of prosumers. However, as DG penetration rates rise, spatial energy imbalances become increasingly significant, necessitating the integration of peer-to-peer (P2P) energy trading within the shared ESS framework. Two key challenges emerge in this context: the absence of effective mechanisms and the greater difficulty for privacy protection due to increased data communication. This research proposes a capacity renting framework for shared ESS considering P2P energy trading of prosumers. In the proposed framework, prosumers can participate in P2P energy trading and rent capacities from shared ESS. A generalized Nash game is formulated to model the trading process and the competitive interactions among prosumers, and the variational equilibrium of the game is proved to be equivalent to the optimal solution of a quadratic programming (QP) problem. To address the privacy protection concern, the problem is solved using the alternating direction method of multipliers (ADMM) with the Paillier cryptosystem. Finally, numerical simulations demonstrate the impact of P2P energy trading on the shared ESS framework and validate the effectiveness of the proposed privacy-preserving algorithm.

Paper number 60:
Title: Improved Estimation Accuracy in OFDM-based Joint Communication and Sensing through Kalman Tracking and Interpolation
Authors: Charlotte Muth, Leon Schmidt, Shrinivas Chimmalgi, Laurent Schmalen
Abstract: We investigate a monostatic orthogonal frequency-division multiplexing (OFDM)-based joint communication and sensing (JCAS) system for object tracking. Our setup consists of a transmitter and receiver equipped with an antenna array for fully digital beamforming. The native resolution of all radar-like sensing, including OFDM radar sensing, is limited by the observation time and bandwidth. In this work, we improve the parameter estimates through interpolation methods and tracking algorithms. We verify our method by comparing the root mean squared error (RMSE) of the estimated range, velocity and angle and by comparing the mean Euclidean distance between the estimated and true position. We demonstrate how both a Kalman filter for tracking, and interpolation methods using zero-padding and the chirp Z-transform (CZT) improve the estimation error. We discuss the computational complexity of the different methods. We propose the KalmanCZT approach that combines tracking via Kalman filtering and interpolation via the CZT, resulting in a solution with flexible resolution that significantly improves the range RMSE.

Paper number 61:
Title: Localization Phenomena in Large-Scale Networked Systems: Robustness and Fragility of Dynamics
Authors: Poorva Shukla, Bassam Bamieh
Abstract: We study phenomena where some eigenvectors of a graph Laplacian are largely confined in small subsets of the graph. These localization phenomena are similar to those generally termed Anderson Localization in the Physics literature, and are related to the complexity of the structure of large graphs in still unexplored ways. Using spectral perturbation theory and pseudo-spectrum analysis, we explain how the presence of localized eigenvectors gives rise to fragilities (low robustness margins) to unmodeled node or link dynamics. Our analysis is demonstrated by examples of networks with relatively low complexity, but with features that appear to induce eigenvector localization. The implications of this newly-discovered fragility phenomenon are briefly discussed.

Paper number 62:
Title: EOM Minimum Point Bias Voltage Estimation for Application in Quantum Computing
Authors: Frank Obernosterer, Raimund Meyer, Robert Koch, Gerd Kilian, Ewald Hedrich, Christian Kelm
Abstract: In quantum computing systems the quantum states of qubits can be modified among others by applying light pulses. In order to achieve low computing error rates these pulses have to be precisely shaped in magnitude and phase. In practical applications, both acousto-optic (AOM) and electro-optic modulators (EOM) are used for this purpose. The advantages of EOMs, in particular Mach-Zehnder modulators (MZMs), include e.g. higher bandwidth, compactness, good integrability and better noise performance. On the other hand, EOMs are challenging regarding their control voltage regulation as they experience a bias drift, i.e. voltage shifts in the modulator's operating point. Such a shift significantly impairs the quality of the modulation, which is why EOMs usually require a bias control loop to track the DC operating point. This work addresses the estimation of the instantaneous bias voltage using a small pilot tone, where the optical output power of the EOM is tracked by a photodetector feedback signal.

Paper number 63:
Title: Time Difference of Arrival Source Localization: Exact Linear Solutions for the General 3D Problem
Authors: Niraj K. Inamdar
Abstract: The time difference of arrival (TDOA) problem admits exact, purely algebraic solutions for the situation in which there are 4 and 5 sensors and a single source whose position is to be determined in 3 dimensions. The solutions are exact in the sense that there is no least squares operation (i.e., projection) involved in the solution. The solutions involve no linearization or iteration, and are algebraically transparent via vector algebra in Cartesian coordinates. The solution with 5 sensors requires no resolution of sign ambiguities; the solution with 4 sensors requires resolution of one sign ambiguity. Solutions are effected using only TDOA and not, e.g., frequency difference of arrival (FDOA) or angle of arrival (AOA). We first present the 5-sensor solution and then follow with the 4-sensor scenario. Numerical experiments are presented showing the performance of the calculations in the case of no noise, before closing with conclusions. Performance of the calculations is exact within numerical error, and in the small fraction of cases in which source localization does not occur, it is driven by misidentification in resolution of sign ambiguity without priors. We therefore believe the calculations have substantial practical utility for their speed and exactness.

Paper number 64:
Title: Embedding Similarity Guided License Plate Super Resolution
Authors: Abderrezzaq Sendjasni, Mohamed-Chaker Larabi
Abstract: Super-resolution (SR) techniques play a pivotal role in enhancing the quality of low-resolution images, particularly for applications such as security and surveillance, where accurate license plate recognition is crucial. This study proposes a novel framework that combines pixel-based loss with embedding similarity learning to address the unique challenges of license plate super-resolution (LPSR). The introduced pixel and embedding consistency loss (PECL) integrates a Siamese network and applies contrastive loss to force embedding similarities to improve perceptual and structural fidelity. By effectively balancing pixel-wise accuracy with embedding-level consistency, the framework achieves superior alignment of fine-grained features between high-resolution (HR) and super-resolved (SR) license plates. Extensive experiments on the CCPD dataset validate the efficacy of the proposed framework, demonstrating consistent improvements over state-of-the-art methods in terms of PSNR_RGB, PSNR_Y and optical character recognition (OCR) accuracy. These results highlight the potential of embedding similarity learning to advance both perceptual quality and task-specific performance in extreme super-resolution scenarios.

Paper number 65:
Title: Gaming on Coincident Peak Shaving: Equilibrium and Strategic Behavior
Authors: Liudong Chen, Bolun Xu
Abstract: Coincident peak demand charges are imposed by power system operators or electric utilities when the overall system demand, aggregated across multiple consumers, reaches its peak. These charges incentivize consumers to reduce their demand during peak periods, a practice known as coincident peak shaving. In this paper, we analyze the coincident peak shaving problem through the lens of game theory, developing a theoretical model to examine the impact of strategic consumer behavior on system efficiency. We demonstrate that the game structure exhibits varying characteristics - concave, quasiconcave/discontinuous, or non-concave/discontinuous - depending on the extent of consumers demand-shifting capabilities. For a two-agent, two-period setting, we derive closed-form Nash equilibrium solutions under each condition and generalize our findings to cases with multiple agents. We prove the stability of the equilibrium points and present an algorithm for computing equilibrium outcomes across all game scenarios. We also show that the peak-shaving effectiveness of the game model matches that of the centralized peak-shaving model but with increased levels of anarchy. In the cases of quasiconcave and non-concave game conditions, we analytically demonstrate in the two-agent setting that anarchy increases with consumers' flexibility and inequity, as measured by their marginal shifting costs, and we also analyze the influence of the number of agents on anarchy. Finally, we provide numerical simulations to validate our theoretical results.

Paper number 66:
Title: Manifold Filter-Combine Networks
Authors: David R. Johnson, Joyce A. Chew, Edward De Brouwer, Smita Krishnaswamy, Deanna Needell, Michael Perlmutter
Abstract: In order to better understand manifold neural networks (MNNs), we introduce Manifold Filter-Combine Networks (MFCNs). Our filter-combine framework parallels the popular aggregate-combine paradigm for graph neural networks (GNNs) and naturally suggests many interesting families of MNNs which can be interpreted as manifold analogues of various popular GNNs. We propose a method for implementing MFCNs on high-dimensional point clouds that relies on approximating an underlying manifold by a sparse graph. We then prove that our method is consistent in the sense that it converges to a continuum limit as the number of data points tends to infinity, and we numerically demonstrate its effectiveness on real-world and synthetic data sets.

Paper number 67:
Title: Prosody Analysis of Audiobooks
Authors: Charuta Pethe, Bach Pham, Felix D Childress, Yunting Yin, Steven Skiena
Abstract: Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.

Paper number 68:
Title: Broadband ptychographic imaging for biological samples
Authors: Huixiang Lin, Fucai Zhang
Abstract: Ptychography is an attractive advance of coherent diffraction imaging (CDI), which can provide high lateral resolution and wide field of view. The theoretical resolution of ptychography is dose-limited, therefore making ptychography workable with a broadband source will be highly beneficial. However, broad spectra of light source conflict with the high coherence assumption in CDI that the current reconstruction algorithm were built upon. In this paper, we demonstrated that incorporation of a blind deconvolution in the reconstruction algorithm can improve the image quality of ptychography with broadband source. This broadband reconstruction algorithm can obtain high-quality amplitude and phase images of complex-valued samples requiring no knowledge of the illumination spectrum. Optical experiments using biological samples demonstrate the effectiveness of our method. The significant improvement in low coherence tolerance by our approach can pave the way for implementing ultrafast imaging with femtosecond or attosecond lasers or high-flux ptychographic imaging with laboratory EUV or X-ray sources.

Paper number 69:
Title: Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning
Authors: Ruoqi Zhang, Ziwei Luo, Jens Sjölund, Thomas B. Schön, Per Mattsson
Abstract: This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at this https URL.

Paper number 70:
Title: Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search
Authors: Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone
Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. Furthermore, higher-fidelity evaluations of the optimization objectives often entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information about the optimal value or the optimal solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method transfers across tasks distributions over parameters of a Gaussian process surrogate model by implementing particle-based variational Bayesian updates. Theoretical insights based on the analysis of the expected regret substantiate the benefits of acquiring transferable knowledge across tasks. Furthermore, experimental results across synthetic and real-world examples reveal that the proposed acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.

Paper number 71:
Title: Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and Gated Monolingual Datastores
Authors: Jiaming Zhou, Shiwan Zhao, Hui Wang, Tian-Hao Zhang, Haoqin Sun, Xuechen Wang, Yong Qin
Abstract: The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR). However, its direct application to multilingual scenarios like code-switching, presents challenges. Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language. To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference. Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process. We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.

Paper number 72:
Title: Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition
Authors: Chien-Chun Wang, Li-Wei Chen, Cheng-Kang Chou, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang
Abstract: While pre-trained automatic speech recognition (ASR) systems demonstrate impressive performance on matched domains, their performance often degrades when confronted with channel mismatch stemming from unseen recording environments and conditions. To mitigate this issue, we propose a novel channel-aware data simulation method for robust ASR training. Our method harnesses the synergistic power of channel-extractive techniques and generative adversarial networks (GANs). We first train a channel encoder capable of extracting embeddings from arbitrary audio. On top of this, channel embeddings are extracted using a minimal amount of target-domain data and used to guide a GAN-based speech synthesizer. This synthesizer generates speech that faithfully preserves the phonetic content of the input while mimicking the channel characteristics of the target domain. We evaluate our method on the challenging Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving relative character error rate (CER) reductions of 20.02% and 9.64%, respectively, compared to the baselines. These results highlight the efficacy of our channel-aware data simulation method for bridging the gap between source- and target-domain acoustics.

Paper number 73:
Title: A Lightweight and Real-Time Binaural Speech Enhancement Model with Spatial Cues Preservation
Authors: Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun
Abstract: Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under fixed-speaker conditions, but with a much lower computational cost and a certain degree of SCP capability. The reproducible code and audio examples are available at this https URL.

Paper number 74:
Title: Efficient Video-Based ALPR System Using YOLO and Visual Rhythm
Authors: Victor Nascimento Ribeiro, Nina S. T. Hirata
Abstract: Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate information from image or a video capture. These systems have gained popularity due to the wide availability of low-cost surveillance cameras and advances in Deep Learning. Typically, video-based ALPR systems rely on multiple frames to detect the vehicle and recognize the license plates. Therefore, we propose a system capable of extracting exactly one frame per vehicle and recognizing its license plate characters from this singular image using an Optical Character Recognition (OCR) model. Early experiments show that this methodology is viable.

Paper number 75:
Title: Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models
Authors: Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi
Abstract: We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance this http URL addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and this http URL results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in this http URL evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource this http URL,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR this http URL contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence this http URL provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence this http URL work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate this http URL leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field.

Paper number 76:
Title: A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval
Authors: Shuo Tong, Han Liu, Runyuan Guo, Wenqing Wang, Xueqiong Tian, Lingyun Wei, Lin Zhang, Huayong Wu, Ding Liu, Youmin Zhang
Abstract: Data-driven soft sensors are crucial in predicting key performance indicators in industrial systems. However, current methods predominantly rely on the supervised learning paradigms of parameter updating, which inherently faces challenges such as high development costs, poor robustness, training instability, and lack of interpretability. Recently, large language models (LLMs) have demonstrated significant potential across various domains, notably through In-Context Learning (ICL), which enables high-performance task execution with minimal input-label demonstrations and no prior training. This paper aims to replace supervised learning with the emerging ICL paradigm for soft sensor modeling to address existing challenges and explore new avenues for advancement. To achieve this, we propose a novel framework called the Few-shot Uncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes the Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware Few-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial Knowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling zero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based context demonstrations of structured data to prompt LLMs to execute ICL for predicting and propose a context sample retrieval augmentation strategy to improve performance. Additionally, we explored LLMs' AIGC and probabilistic characteristics to propose self-explanation and uncertainty quantification methods for constructing a trustworthy soft sensor. Extensive experiments demonstrate that our method achieved state-of-the-art predictive performance, strong robustness, and flexibility, effectively mitigates training instability found in traditional methods. To the best of our knowledge, this is the first work to establish soft sensor utilizing LLMs.
    