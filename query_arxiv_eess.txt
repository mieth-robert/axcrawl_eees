
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network
Authors: Sainan xiao, Wangdong Yang, Buwen Cao, Jintao Wu
Abstract: Electrocardiogram (ECG) signals are frequently corrupted by noise, such as baseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which significantly degrade their diagnostic utility. To address this issue, we propose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a Double Recurrent Dense Network architecture. In contrast to traditional approaches, we introduce a double recurrent scheme to enhance information reuse from both ECG waveforms and the estimated clean image. For ECG waveform processing, our basic model employs LSTM layers cascaded with DenseNet blocks. The estimated clean ECG image, obtained by subtracting predicted noise components from the noisy input, is iteratively fed back into the model. This dual recurrent architecture enables comprehensive utilization of both temporal waveform features and spatial image details, leading to more effective noise suppression. Experimental results on the MIT-BIH dataset demonstrate that our method achieves superior performance compared to conventional image denoising methods in terms of PSNR and SSIM while also surpassing classical ECG denoising techniques in both SNR and RMSE.

Paper number 2:
Title: OccuEMBED: Occupancy Extraction Merged with Building Energy Disaggregation for Occupant-Responsive Operation at Scale
Authors: Yufei Zhang (1), Andrew Sonta (1) ((1) ETHOS Lab, EPFL-ENAC-IIC)
Abstract: Buildings account for a significant share of global energy consumption and emissions, making it critical to operate them efficiently. As electricity grids become more volatile with renewable penetration, buildings must provide flexibility to support grid stability. Building automation plays a key role in enhancing efficiency and flexibility via centralized operations, but it must prioritize occupant-centric strategies to balance energy and comfort targets. However, incorporating occupant information into large-scale, centralized building operations remains challenging due to data limitations. We investigate the potential of using whole-building smart meter data to infer both occupancy and system operations. Integrating these insights into data-driven building energy analysis allows more occupant-centric energy-saving and flexibility at scale. Specifically, we propose OccuEMBED, a unified framework for occupancy inference and system-level load analysis. It combines two key components: a probabilistic occupancy profile generator, and a controllable and interpretable load disaggregator supported by Kolmogorov-Arnold Networks (KAN). This design embeds knowledge of occupancy patterns and load-occupancy-weather relationships into deep learning models. We conducted comprehensive evaluations to demonstrate its effectiveness across synthetic and real-world datasets compared to various occupancy inference baselines. OccuEMBED always achieved average F1 scores above 0.8 in discrete occupancy inference and RMSE within 0.1-0.2 for continuous occupancy ratios. We further demonstrate how OccuEMBED integrates with building load monitoring platforms to display occupancy profiles, analyze system-level operations, and inform occupant-responsive strategies. Our model lays a robust foundation in scaling occupant-centric building management systems to meet the challenges of an evolving energy system.

Paper number 3:
Title: Improving Local Air Quality Predictions Using Transfer Learning on Satellite Data and Graph Neural Networks
Authors: Finn Gueterbock, Raul Santos-Rodriguez, Jeffrey N. Clark
Abstract: Air pollution is a significant global health risk, contributing to millions of premature deaths annually. Nitrogen dioxide (NO2), a harmful pollutant, disproportionately affects urban areas where monitoring networks are often sparse. We propose a novel method for predicting NO2 concentrations at unmonitored locations using transfer learning with satellite and meteorological data. Leveraging the GraphSAGE framework, our approach integrates autoregression and transfer learning to enhance predictive accuracy in data-scarce regions like Bristol. Pre-trained on data from London, UK, our model achieves a 8.6% reduction in Normalised Root Mean Squared Error (NRMSE) and a 32.6% reduction in Gradient RMSE compared to a baseline model. This work demonstrates the potential of virtual sensors for cost-effective air quality monitoring, contributing to actionable insights for climate and health interventions.

Paper number 4:
Title: An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids
Authors: Noor ul Misbah Khanum, Hayssam Dahrouj, Ramesh C. Bansal, Hissam Mouayad Tawfik
Abstract: Microgrids have emerged as a pivotal solution in the quest for a sustainable and energy-efficient future. While microgrids offer numerous advantages, they are also prone to issues related to reliably forecasting renewable energy demand and production, protecting against cyberattacks, controlling operational costs, optimizing power flow, and regulating the performance of energy management systems (EMS). Tackling these energy management challenges is essential to facilitate microgrid applications and seamlessly incorporate renewable energy resources. Artificial intelligence (AI) has recently demonstrated immense potential for optimizing energy management in microgrids, providing efficient and reliable solutions. This paper highlights the combined benefits of enabling AI-based methodologies in the energy management systems of microgrids by examining the applicability and efficiency of AI-based EMS in achieving specific technical and economic objectives. The paper also points out several future research directions that promise to spearhead AI-driven EMS, namely the development of self-healing microgrids, integration with blockchain technology, use of Internet of things (IoT), and addressing interpretability, data privacy, scalability, and the prospects to generative AI in the context of future AI-based EMS.

Paper number 5:
Title: Image Restoration via Multi-domain Learning
Authors: Xingyu Jiang, Ning Gao, Xiuhui Zhang, Hongkun Dou, Shaowen Fu, Xiaoqing Zhong, Hongjue Li, Yue Deng
Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from various degradation phenomena. Consequently, image restoration has emerged as a key solution and garnered substantial attention. Although recent Transformer architectures have demonstrated impressive success across various restoration tasks, their considerable model complexity poses significant challenges for both training and real-time deployment. Furthermore, instead of investigating the commonalities among different degradations, most existing restoration methods focus on modifying Transformer under limited restoration priors. In this work, we first review various degradation phenomena under multi-domain perspective, identifying common priors. Then, we introduce a novel restoration framework, which integrates multi-domain learning into Transformer. Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain structure that facilitates local-region-global multi-receptive field modeling to replace vanilla self-attention. Additionally, in Feed-Forward Network, we incorporate multi-scale learning to fuse multi-domain features at different resolutions. Comprehensive experimental results across ten restoration tasks, such as dehazing, desnowing, motion deblurring, defocus deblurring, rain streak/raindrop removal, cloud removal, shadow removal, underwater enhancement and low-light enhancement, demonstrate that our proposed model outperforms state-of-the-art methods and achieves a favorable trade-off among restoration performance, parameter size, computational cost and inference latency. The code is available at: this https URL.

Paper number 6:
Title: VIMPPI: Enhancing Model Predictive Path Integral Control with Variational Integration for Underactuated Systems
Authors: Igor Alentev, Lev Kozlov, Ivan Domrachev, Simeon Nedelchev
Abstract: This paper presents VIMPPI, a novel control approach for underactuated double pendulum systems developed for the AI Olympics competition. We enhance the Model Predictive Path Integral framework by incorporating variational integration techniques, enabling longer planning horizons without additional computational cost. Operating at 500-700 Hz with control interpolation and disturbance detection mechanisms, VIMPPI substantially outperforms both baseline methods and alternative MPPI implementations

Paper number 7:
Title: StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation
Authors: Yi Liu, Xinyi Liu, Panwang Xia, Qiong Wu, Yi Wan, Yongjun Zhang
Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details by leveraging information from stereo image pairs. However, existing stereo super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook cross-view geometric consistency and are limited to fixed-scale upsampling. The key issue is that previous upsampling methods use convolution to independently process deep features of different views, lacking cross-view and non-local information perception, making it difficult to select beneficial information from multi-view scenes adaptively. In this work, we propose Stereo Implicit Neural Representation (StereoINR), which innovatively models stereo image pairs as continuous implicit representations. This continuous representation breaks through the scale limitations, providing a unified solution for arbitrary-scale stereo super-resolution reconstruction of left-right views. Furthermore, by incorporating spatial warping and cross-attention mechanisms, StereoINR enables effective cross-view information fusion and achieves significant improvements in pixel-level geometric consistency. Extensive experiments across multiple datasets show that StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training-distribution scales.

Paper number 8:
Title: Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility
Authors: Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim
Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control. A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage. Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy. Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.

Paper number 9:
Title: Model-Based Closed-Loop Control Algorithm for Stochastic Partial Differential Equation Control
Authors: Peiyan Hu, Haodonog Feng, Yue Wang, Zhiming Ma
Abstract: Neural operators have demonstrated promise in modeling and controlling systems governed by Partial Differential Equations (PDEs). Beyond PDEs, Stochastic Partial Differential Equations (SPDEs) play a critical role in modeling systems influenced by randomness, with applications in finance, physics, and beyond. However, controlling SPDE-governed systems remains a significant challenge. On the one hand, the regularity of the system's state (which can be intuitively understood as smoothness) deteriorates, making modeling and generalization more challenging. On the other hand, this stochasticity also renders control more unstable and thus less accurate. To address this gap, we propose the Model-Based Closed-Loop Control Algorithm (MB-CC), the first model-based closed-loop control method for SPDEs. MB-CC introduces two key innovations to enhance control robustness and efficiency: a Regularity Feature (RF) block and a closed-loop strategy with an operator-encoded policy network. The RF block, inspired by the regularity structure theory of SPDEs, addresses noise-induced irregularities by transforming the network's input, including the system state and noise-perturbed external forces, into a refined feature space for improved forward prediction. Compared to previous works using regularity features, we introduce a new parameterization, data augmentation, and extend the RF block as a plug-and-play component. Additionally, to achieve closed-loop control, we introduce an operator-encoded policy network to map the current state to optimal control, which integrates physical priors and swiftly makes decisions based on states returned by the environment. We conduct a systematic evaluation of MB-CC on two notable SPDEs, showcasing its effectiveness and efficiency. The ablation studies show its ability to handle stochasticity more effectively.

Paper number 10:
Title: A critical note on the sideband peak count-index technique: failure for nonlinear damage characterization of impacted CFRP plates
Authors: Bernd Köhler, Frank Schubert
Abstract: It is widely accepted, that nonlinear elastodynamic methods are superior to linear methods in detecting early stages of material deterioration. A number of recently developed methods are reported to be particularly sensitive to nonlinearities and thus appropriate to indicate early damage. We applied systematically one of the methods, the sideband peak count index (SPC-I), to a series of increasingly damaged carbon fiber reinforced plastic (CFRP) plates. Our data leads to different conclusions. The SPC-I values are influenced by (usually undocumented) variations in the index calculation procedure, which is not acceptable for a robust method. Moreover, the behavior of the index when the ultrasound amplitude is varied contradicts material nonlinearity as a direct and significant contributor to the index value. To clarify the apparent contradiction of our results with the previously published statements, it is recommended that (a) our data are re-evaluated by independent researchers and (b) the experiments already published are repeated or (if sufficient data is available) also re-evaluated.

Paper number 11:
Title: Score-based Self-supervised MRI Denoising
Authors: Jiachen Tu, Yaokun Shi, Fan Lam
Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset.

Paper number 12:
Title: UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes
Authors: Mark C. Eid, Ana I.L. Namburete, João F. Henriques
Abstract: Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.

Paper number 13:
Title: A New k-Space Model for Non-Cartesian Fourier Imaging
Authors: Chin-Cheng Chan, Justin P. Haldar
Abstract: For the past several decades, it has been popular to reconstruct Fourier imaging data using model-based approaches that can easily incorporate physical constraints and advanced regularization/machine learning priors. The most common modeling approach is to represent the continuous image as a linear combination of shifted "voxel" basis functions. Although well-studied and widely-deployed, this voxel-based model is associated with longstanding limitations, including high computational costs, slow convergence, and a propensity for artifacts. In this work, we reexamine this model from a fresh perspective, identifying new issues that may have been previously overlooked (including undesirable approximation, periodicity, and nullspace characteristics). Our insights motivate us to propose a new model that is more resilient to the limitations (old and new) of the previous approach. Specifically, the new model is based on a Fourier-domain basis expansion rather than the standard image-domain voxel-based approach. Illustrative results, which are presented in the context of non-Cartesian MRI reconstruction, demonstrate that the new model enables improved image quality (reduced artifacts) and/or reduced computational complexity (faster computations and improved convergence).

Paper number 14:
Title: Unsupervised Blind Speech Separation with a Diffusion Prior
Authors: Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury
Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: this https URL.

Paper number 15:
Title: V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models
Authors: Guilherme Vieira Neto, Marcos Eduardo Valle
Abstract: EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at this https URL.

Paper number 16:
Title: Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology
Authors: Fuyao Chen, Yuexi Du, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey
Abstract: Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging.

Paper number 17:
Title: Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference
Authors: Haoyang Pei, Ding Xia, Xiang Xu, William Moore, Yao Wang, Hersh Chandarana, Li Feng
Abstract: Purpose: Deep learning has demonstrated strong potential for MRI reconstruction, but conventional supervised learning methods require high-quality reference images, which are often unavailable in practice. Self-supervised learning offers an alternative, yet its performance degrades at high acceleration rates. To overcome these limitations, we propose hybrid learning, a novel two-stage training framework that combines self-supervised and supervised learning for robust image reconstruction. Methods: Hybrid learning is implemented in two sequential stages. In the first stage, self-supervised learning is employed to generate improved images from noisy or undersampled reference data. These enhanced images then serve as pseudo-ground truths for the second stage, which uses supervised learning to refine reconstruction performance and support higher acceleration rates. We evaluated hybrid learning in two representative applications: (1) accelerated 0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of the brain without access to fully sampled ground truth. Results: For spiral-UTE lung MRI, hybrid learning consistently improved image quality over both self-supervised and conventional supervised methods across different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping, hybrid learning achieved superior T1 quantification accuracy across a wide dynamic range, outperforming self-supervised learning in all tested conditions. Conclusions: Hybrid learning provides a practical and effective solution for training deep MRI reconstruction networks when only low-quality or incomplete reference data are available. It enables improved image quality and accurate quantitative mapping across different applications and field strengths, representing a promising technique toward broader clinical deployment of deep learning-based MRI.

Paper number 18:
Title: Robust Management of Airport Security Queues Considering Passenger Non-compliance with Chance-Constrained Optimization
Authors: Shangqing Cao, Aparimit Kasliwal, Huangyi Zheng, Masoud Reihanifar, Francesc Robuste, Mark Hansen
Abstract: The long waiting time at airport security has become an emergent issue as demand for air travel continues to grow. Not only does queuing at security cause passengers to miss their flights, but also reduce the amount of time passengers spend at the airport post-security, potentially leading to less revenue for the airport operator. One of the key issues to address to reduce waiting time is the management of arrival priority. As passengers on later flights can arrive before passengers on earlier flights, the security system does not always process passengers in the order of the degree of urgency. In this paper, we propose a chance-constrained optimization model that decides in which time slot passengers should be recommended to arrive. We use chance constraints to obtain solutions that take the uncertainty in passenger non-compliance into account. The experimental results, based on a sample day of flight schedules at the Barcelona airport, show a reduction of 85% in the total waiting time. Compared to the deterministic case, in which passengers are assumed to fully comply with the recommendations, we see a 30% increase in the reduction of the total waiting time. This highlights the importance of considering variation in passenger compliance in the management of airport security queues.

Paper number 19:
Title: Connected C-Core Hybrid SRMs for EV Applications
Authors: Gholamreza Davarpanah, Sajjad Mohammadi
Abstract: This paper proposes a new class of permanent magnet-assisted three-phase switched reluctance motors (PM-SRMs) designed to achieve significantly higher torque density for electric vehicle (EV) propulsion systems. Eight distinct motor topologies are systematically investigated, including a non-PM baseline design, three innovative PM arrangement strategies, and two optimized rotor/stator teeth configurations (22-pole and 26-pole variants). The study presents analytical models including magnetic equivalent circuits (MECs), detailed operating principles, and generalized design formulations that account for both electromagnetic and structural considerations. A key contribution is the introduction of the point-of-conversion (PoC) concept, which optimizes PM placement by minimizing magnetic path reluctance. Comparative analysis demonstrates torque density improvements over conventional SRMs and existing PM-assisted designs while maintaining structural robustness. Experimental validation confirms that the proposed 24/22 configuration with inter-phase PMs delivers higher torque per PM volume compared to state-of-the-art designs. The findings provide insights for EV motor designers seeking to balance performance, cost, and reliability.

Paper number 20:
Title: A Feedback Control Framework for Incentivised Suburban Parking Utilisation and Urban Core Traffic Relief
Authors: Abdul Baseer Satti, James Saunderson, Wynita Griggs, S. M. Nawazish Ali, Nameer Al Khafaf, Saman Ahmadi, Mahdi Jalili, Jakub Marecek, Robert Shorten
Abstract: Urban traffic congestion, exacerbated by inefficient parking management and cruising for parking, significantly hampers mobility and sustainability in smart cities. Drivers often face delays searching for parking spaces, influenced by factors such as accessibility, cost, distance, and available services such as charging facilities in the case of electric vehicles. These inefficiencies contribute to increased urban congestion, fuel consumption, and environmental impact. Addressing these challenges, this paper proposes a feedback control incentivisation-based system that aims to better distribute vehicles between city and suburban parking facilities offering park-and-charge/-ride services. Individual driver behaviours are captured via discrete choice models incorporating factors of importance to parking location choice among drivers, such as distance to work, public transport connectivity, charging infrastructure availability, and amount of incentive offered; and are regulated through principles of ergodic control theory. The proposed framework is applied to an electric vehicle park-and-charge/-ride problem, and demonstrates how predictable long-term behaviour of the system can be guaranteed.

Paper number 21:
Title: ProTCT: Projection quantification and fidelity constraint integrated deep reconstruction for Tangential CT
Authors: Bingan Yuan, Bowei Liu, Zheng Fang
Abstract: Tangential computed tomography (TCT) is a useful tool for imaging the large-diameter samples, such as oil pipelines and rockets. However, TCT projections are truncated along the detector direction, resulting in degraded slices with radial artifacts. Meanwhile, existing methods fail to reconstruct decent images because of the ill-defined sampling condition in the projection domain and oversmoothing in the cross-section domain. In this paper, we propose a projection quantification and fidelity constraint integrated deep TCT reconstruction method (ProTCT) to improve the slice quality. Specifically, the sampling conditions for reconstruction are analysed, offering practical guidelines for TCT system design. Besides, a deep artifact-suppression network together with a fidelity-constraint module that operates across both projection and cross-section domains to remove artifacts and restore edge details. Demonstrated on simulated and real datasets, the ProTCT shows good performance in structure restoration and detail retention. This work contributes to exploring the sampling condition and improving the slice quality of TCT, further promoting the application of large view field CT imaging.

Paper number 22:
Title: Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition
Authors: Weiyi Zhang, Peranut Chotcomwongse, Yinwen Li, Pusheng Xu, Ruijie Yao, Lianhao Zhou, Yuxuan Zhou, Hui Feng, Qiping Zhou, Xinyue Wang, Shoujin Huang, Zihao Jin, Florence H.T. Chung, Shujun Wang, Yalin Zheng, Mingguang He, Danli Shi, Paisan Ruamviboonsuk
Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment in diabetic patients. Treatment responses to intravitreal therapies vary, highlighting the need for patient stratification to predict therapeutic benefits and enable personalized strategies. To our knowledge, this study is the first to explore pre-treatment stratification for predicting DME treatment responses. To advance this research, we organized the 2nd Asia-Pacific Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The competition focused on improving predictive accuracy for anti-VEGF therapy responses using ophthalmic OCT images. We provided a dataset containing tens of thousands of OCT images from 2,000 patients with labels across four sub-tasks. This paper details the competition's structure, dataset, leading methods, and evaluation metrics. The competition attracted strong scientific community participation, with 170 teams initially registering and 41 reaching the final round. The top-performing team achieved an AUC of 80.06%, highlighting the potential of AI in personalized DME treatment and clinical decision-making.

Paper number 23:
Title: DeepSync: A Learning Framework for Pervasive Localization using Code Synchronization on Compressed Cellular Spectrum
Authors: Aritrik Ghosh, Nakul Garg, Nirupam Roy
Abstract: Pervasive localization is essential for continuous tracking applications, yet existing solutions face challenges in balancing power consumption and accuracy. GPS, while precise, is impractical for continuous tracking of micro-assets due to high power requirements. Recent advances in non-linear compressed spectrum sensing offer low-power alternatives, but existing implementations achieve only coarse positioning through Received Signal Strength Indicator (RSSI) measurements. We present DeepSync, a deep learning framework that enables precise localization using compressed cellular spectrum. Our key technical insight lies in formulating sub-sample timing estimation as a template matching problem, solved through a novel architecture combining temporal CNN encoders for multi-frame processing with cross-attention mechanisms. The system processes non-linear inter-modulated spectrum through hierarchical feature extraction, achieving robust performance at SNR levels below -10dB -- a regime where conventional timing estimation fails. By integrating real cellular infrastructure data with physics-based ray-tracing simulations, DeepSync achieves 2.128-meter median accuracy while consuming significantly less power than conventional systems. Real-world evaluations demonstrate 10x improvement over existing compressed spectrum approaches, establishing a new paradigm for ultra-low-power localization.

Paper number 24:
Title: Formation Maneuver Control Based on the Augmented Laplacian Method
Authors: Xinzhe Zhou, Xuyang Wang, Xiaoming Duan, Yuzhu Bai, Jianping He
Abstract: This paper proposes a novel formation maneuver control method for both 2-D and 3-D space, which enables the formation to translate, scale, and rotate with arbitrary orientation. The core innovation is the novel design of weights in the proposed augmented Laplacian matrix. Instead of using scalars, we represent weights as matrices, which are designed based on a specified rotation axis and allow the formation to perform rotation in 3-D space. To further improve the flexibility and scalability of the formation, the rotational axis adjustment approach and dynamic agent reconfiguration method are developed, allowing formations to rotate around arbitrary axes in 3-D space and new agents to join the formation. Theoretical analysis is provided to show that the proposed approach preserves the original configuration of the formation. The proposed method maintains the advantages of the complex Laplacian-based method, including reduced neighbor requirements and no reliance on generic or convex nominal configurations, while achieving arbitrary orientation rotations via a more simplified implementation. Simulations in both 2-D and 3-D space validate the effectiveness of the proposed method.

Paper number 25:
Title: Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency
Authors: Xinyu Liang, Frits de Nijs, Buser Say, Hao Wang
Abstract: Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfort, presents a significant challenge for traditional HVAC systems. These systems often fail to dynamically adjust to real-time changes in electricity market rates or individual comfort preferences, leading to increased energy costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL) Artificial Intelligence framework that optimizes HVAC performance by incorporating real-time user feedback and responding to fluctuating electricity prices. Unlike conventional systems that require predefined information about occupancy or comfort levels, our approach learns and adapts based on ongoing user input. By integrating the occupancy prediction model with reinforcement learning, the system improves operational efficiency and reduces energy costs in line with electricity market dynamics, thereby contributing to demand response initiatives. Through simulations, we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort. This feedback-driven approach ensures personalized comfort control without the need for predefined settings, offering a scalable solution that balances individual preferences with economic and environmental goals.

Paper number 26:
Title: Automatic Basis Function Selection in Iterative Learning Control: A Sparsity-Promoting Approach Applied to an Industrial Printer
Authors: Tjeerd Ickenroth, Max van Haren, Johan Kon, Max van Meer, Jilles van hulst, Tom Oomen
Abstract: Iterative learning control (ILC) techniques are capable of improving the tracking performance of control systems that repeatedly perform similar tasks by utilizing data from past iterations. The aim of this paper is to design a systematic approach for learning parameterized feedforward signals with limited complexity. The developed method involves an iterative learning control in conjunction with a data-driven sparse subset selection procedure for basis function selection. The ILC algorithm that employs sparse optimization is able to automatically select relevant basis functions and is validated on an industrial flatbed printer.

Paper number 27:
Title: Integrating Building Thermal Flexibility Into Distribution System: A Privacy-Preserved Dispatch Approach
Authors: Shuai Lu, Zeyin Hou, Wei Gu, Yijun Xu
Abstract: The inherent thermal storage capacity of buildings brings considerable thermal flexibility to the heating/cooling loads, which are promising demand response resources for power systems. It is widely believed that integrating the thermal flexibility of buildings into the distribution system can improve the operating economy and reliability of the system. However, the private information of the buildings needs to be transferred to the distribution system operator (DSO) to achieve a coordinated optimization, bringing serious privacy concerns to users. Given this issue, we propose a novel privacy-preserved optimal dispatch approach for the distribution system incorporating buildings. Using it, the DSO can exploit the thermal flexibility of buildings without accessing their private information, such as model parameters and indoor temperature profiles. Specifically, we first develop an optimal dispatch model for the distribution system integrating buildings, which can be extended to other storage-like flexibility resources. Second, we reveal that the privacy-preserved integration of buildings is a joint privacy preservation problem for both parameters and state variables and then design a privacy-preserved algorithm based on transformation-based encryption, constraint relaxation, and constraint extension techniques. Besides, we implement a detailed privacy analysis for the proposed method, considering both semi-honest adversaries and external eavesdroppers. Case studies demonstrate the accuracy, privacy-preserved performance, and computational efficiency of the proposed method.

Paper number 28:
Title: Shortlisting Protection Configurations for HVDC Grids and Electrical Energy Hubs
Authors: Merijn Van Deyck, Geraint Chaffey, Dirk Van Hertem
Abstract: This paper proposes a methodology for shortlisting protection system configurations for large HVDC switching stations, which are expected in multiterminal HVDC grids and electrical energy hubs (or energy islands). This novel approach focuses on the configuration of protection equipment and the arrangement of lines and converters in various protection zones, instead of expert decisions on protection strategies based on numerous simulations. A graph-based approach that allows high-level evaluation of possible DC fault impacts is presented. This fault impact evaluation method can evaluate many possible protection configurations allowing the selection of less obvious choices, as experts cannot consider all possible configurations, especially when the switching station size increases. A filtering process is applied to reduce the number of possible configurations based on multiple protection performance metrics which are evaluated for different power flow scenarios. The results for these performance metrics can be compared for configurations with different numbers of HVDC circuit breakers to assess the benefit of increasing the amount of protection equipment in different network topologies. It is also shown that, through continued filtering using additional performance metrics or fault scenarios, the number of possible breaker, cable and converter configurations can be further reduced, leading to a protection design that is well suited for many operational scenarios. The results of the shortlisting process provide insights on the required number of HVDC circuit breakers to limit fault impacts to a given value. Moreover, observed trends in the results could, in future studies, contribute to new design principles and priorities, allowing system developers to more effectively design HVDC protection systems for different operational scenarios and possible investment levels.

Paper number 29:
Title: AI-assisted Automatic Jump Detection and Height Estimation in Volleyball Using a Waist-worn IMU
Authors: Weiyi Xu, Chunzhuo Wang, Meng Shang, Camilla De Bleecker, Maria Torres Vega, Jos Vanrenterghem, Bart Vanrumste
Abstract: The physical load of jumps plays a critical role in injury prevention for volleyball players. However, manual video analysis of jump activities is time-intensive and costly, requiring significant effort and expensive hardware setups. The advent of the inertial measurement unit (IMU) and machine learning algorithms offers a convenient and efficient alternative. Despite this, previous research has largely focused on either jump classification or physical load estimation, leaving a gap in integrated solutions. This study aims to present a pipeline to automatically detect jumps and predict heights using data from a waist-worn IMU. The pipeline leverages a Multi-Stage Temporal Convolutional Network (MS-TCN) to detect jump segments in time-series data and classify the specific jump category. Subsequently, jump heights are estimated using three downstream regression machine learning models based on the identified segments. Our method is verified on a dataset comprising 10 players and 337 jumps. Compared to the result of VERT in height estimation (R-squared=-1.53), a commercial device commonly used in jump landing tasks, our method not only accurately identifies jump activities and their specific types (F1-score=0.90) but also demonstrates superior performance in height prediction (R-squared=0.50). This integrated solution offers a promising tool for monitoring physical load and mitigating injury risk in volleyball players.

Paper number 30:
Title: Design and Application of Energy-saving Sub-Optimal Sliding Mode Control
Authors: Michael Ruderman
Abstract: The recently introduced energy-saving extension of the sub-optimal sliding mode control (SOSMC), which is known in the literature for the last two and half decades, incorporates a control-off mode that allows for saving energy during the finite-time convergence process. This novel energy-saving algorithm (denoted by ES-SOSMC) assumes the systems with relative degree two between the sliding variable and the switching control with a bounded magnitude, while the matched upper-bounded perturbations are not necessarily continuous. The design and practical application of the ES-SOSMC are the subject of this chapter. A method for parameterizing the ES-SOSMC through a constrained minimization of the energy cost function is recalled which guarantees the total energy consumption is lower than that of the conventional SOSMC. Also the residual steady-state oscillations (chattering), occurring when additional (actuator) dynamics are taken into account, are addressed. An application example for scanning and machining a rough surface, both of which require a stiff position control in contact with a moving surface, demonstrates practical suitability of the control. Here, ES-SOSMC is compared with SOSMC by showing an equivalent tracking and stabilization performance and evaluating the energy-saving operation with respect to a fuel consumption norm.

Paper number 31:
Title: Priority-Driven Safe Model Predictive Control Approach to Autonomous Driving Applications
Authors: Francesco Prignoli, Ying Shuai Quan, Mohammad Jeddi, Jonas Sjöberg, Paolo Falcone
Abstract: This paper demonstrates the applicability of the safe model predictive control (SMPC) framework to autonomous driving scenarios, focusing on the design of adaptive cruise control (ACC) and automated lane-change systems. Building on the SMPC approach with priority-driven constraint softening -- which ensures the satisfaction of \emph{hard} constraints under external disturbances by selectively softening a predefined subset of adjustable constraints -- we show how the algorithm dynamically relaxes lower-priority, comfort-related constraints in response to unexpected disturbances while preserving critical safety requirements such as collision avoidance and lane-keeping. A learning-based algorithm approximating the time consuming SMPC is introduced to enable real-time execution. Simulations in real-world driving scenarios subject to unpredicted disturbances confirm that this prioritized softening mechanism consistently upholds stringent safety constraints, underscoring the effectiveness of the proposed method.

Paper number 32:
Title: Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication
Authors: Xiyu Wang, Gilberto Berardinelli, Hei Victor Cheng, Petar Popovski, Ramoni Adeogun
Abstract: Mobile users are prone to experience beam failure due to beam drifting in millimeter wave (mmWave) communications. Sensing can help alleviate beam drifting with timely beam changes and low overhead since it does not need user feedback. This work studies the problem of optimizing sensing-aided communication by dynamically managing beams allocated to mobile users. A multi-beam scheme is introduced, which allocates multiple beams to the users that need an update on the angle of departure (AoD) estimates and a single beam to the users that have satisfied AoD estimation precision. A deep reinforcement learning (DRL) assisted method is developed to optimize the beam allocation policy, relying only upon the sensing echoes. For comparison, a heuristic AoD-based method using approximated Cramér-Rao lower bound (CRLB) for allocation is also presented. Both methods require neither user feedback nor prior state evolution information. Results show that the DRL-assisted method achieves a considerable gain in throughput than the conventional beam sweeping method and the AoD-based method, and it is robust to different user speeds.

Paper number 33:
Title: Beyond Diagonal RIS Design for Parameter Estimation With and Without Eavesdropping
Authors: Özlem Tuğfe Demir, Sinan Gezici
Abstract: In this letter, we investigate the transmission of a complex-valued parameter vector from a transmitter to an intended receiver, considering both the presence and absence of an eavesdropper. The direct links from the transmitter to both the intended receiver and the eavesdropper are assumed to be blocked, and communications occur solely through cascaded channels facilitated by a beyond-diagonal reconfigurable intelligent surface (BD-RIS). While previous research has considered this system under conventional (diagonal) RIS assistance, we extend the setup to incorporate BD-RIS and quantify the resulting improvement in estimation performance at the intended receiver. This performance is measured by the trace of the Fisher information matrix (FIM), or equivalently, the average Fisher information, while simultaneously limiting the estimation capability of the eavesdropper. We propose solutions and algorithms for optimizing the BD-RIS response matrix and demonstrate their effectiveness. Numerical results reveal that the BD-RIS provides a significant enhancement in estimation quality compared to conventional diagonal RIS architectures.

Paper number 34:
Title: On the Potential of Electrified Supply Chains to Provide Long Duration Demand Flexibility
Authors: Rina Davila Severiano, Constance Crozier, Mark O Malley
Abstract: Demand flexibility can offset some of the variability introduced on the supply-side by variable renewable generation. However, most efforts (e.g. control of residential vehicle charging) focus on short durations -- typically on the scale of minutes to hours. This paper investigates whether a fully electrified supply chain (transport and manufacturing) could provide demand flexibility over longer durations, exploiting the latency that typically exists between the processing of raw material to the delivery of finished product. Using a case study of the cement industry along the East Coast of the United States, we demonstrate that electrified supply chains could shift gigawatt-hours (GWh) of electricity demand for durations of more than a week, largely following wind power variability. Furthermore, we show that this occurs using low levels of carbon taxing (below $50/tn), at which battery storage is not economically viable. A sensitivity analysis shows potential to provide flexibility in all considered cost scenarios, although where the flexibility comes from can change (e.g. transport vs manufacturing). We show that today's cost of electrified heavy goods vehicles are the most significant parameter -- with substantially lower costs yielding a more demand-flexible supply chain.

Paper number 35:
Title: Quantum Noise Limited Temperature-Change Estimation for Phase-OTDR Employing Coherent Detection
Authors: Huwei Wang, Roman Ermakov, Francesco Da Ros, Darko Zibar
Abstract: The quantum limit is a fundamental lower bound on the uncertainty when estimating a parameter in a system dominated by the minimum amount of noise (quantum noise). For the first time, we derive and demonstrate a quantum limit for temperature-change estimation for coherent phase-OTDR sensing-systems.

Paper number 36:
Title: Zero Dynamics Attack Detection and Isolation in Cyber-Physical Systems with Event-triggered Communication
Authors: Ali Eslami, Khashayar Khorasani
Abstract: This paper investigates the problem of Zero Dynamics (ZD) cyber-attack detection and isolation in Cyber-Physical Systems (CPS). By utilizing the notion of auxiliary systems with event-based communications, we will develop a detection mechanism capable of detecting and isolating the ZD cyber-attack even when the attackers have full knowledge of the dynamics of the auxiliary system and can launch False Data Injection (FDI) attacks on all the communication channels. More specifically, we will utilize a self-triggering rule for the communication channels connecting the auxiliary system with the Command & Control (C&C) center, leveraging its properties to detect the ZD cyber-attack. Finally, the effectiveness and capabilities of our approach are verified and demonstrated through simulation case studies.

Paper number 37:
Title: Smooth optimization algorithms for global and locally low-rank regularizers
Authors: Rodrigo A. Lobos, Javier Salazar Cavazos, Raj Rao Nadakuditi, Jeffrey A. Fessler
Abstract: Many inverse problems and signal processing problems involve low-rank regularizers based on the nuclear norm. Commonly, proximal gradient methods (PGM) are adopted to solve this type of non-smooth problems as they can offer fast and guaranteed convergence. However, PGM methods cannot be simply applied in settings where low-rank models are imposed locally on overlapping patches; therefore, heuristic approaches have been proposed that lack convergence guarantees. In this work we propose to replace the nuclear norm with a smooth approximation in which a Huber-type function is applied to each singular value. By providing a theoretical framework based on singular value function theory, we show that important properties can be established for the proposed regularizer, such as: convexity, differentiability, and Lipschitz continuity of the gradient. Moreover, we provide a closed-form expression for the regularizer gradient, enabling the use of standard iterative gradient-based optimization algorithms (e.g., nonlinear conjugate gradient) that can easily address the case of overlapping patches and have well-known convergence guarantees. In addition, we provide a novel step-size selection strategy based on a quadratic majorizer of the line-search function that leverages the Huber characteristics of the proposed regularizer. Finally, we assess the proposed optimization framework by providing empirical results in dynamic magnetic resonance imaging (MRI) reconstruction in the context of locally low-rank models with overlapping patches.

Paper number 38:
Title: S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram
Authors: Xilin Gong, Yongkai Chen, Shushan Wu, Fang Wang, Ping Ma, Wenxuan Zhong
Abstract: Echocardiogram is the most commonly used imaging modality in cardiac assessment duo to its non-invasive nature, real-time capability, and cost-effectiveness. Despite its advantages, most clinical echocardiograms provide only two-dimensional views, limiting the ability to fully assess cardiac anatomy and function in three dimensions. While three-dimensional echocardiography exists, it often suffers from reduced resolution, limited availability, and higher acquisition costs. To overcome these challenges, we propose a deep learning framework S2MNet that reconstructs continuous and high-fidelity 3D heart models by integrating six slices of routinely acquired 2D echocardiogram views. Our method has three advantages. First, our method avoid the difficulties on training data acquasition by simulate six of 2D echocardiogram images from corresponding slices of a given 3D heart mesh. Second, we introduce a deformation field-based method, which avoid spatial discontinuities or structural artifacts in 3D echocardiogram reconstructions. We validate our method using clinically collected echocardiogram and demonstrate that our estimated left ventricular volume, a key clinical indicator of cardiac function, is strongly correlated with the doctor measured GLPS, a clinical measurement that should demonstrate a negative correlation with LVE in medical theory. This association confirms the reliability of our proposed 3D construction method.

Paper number 39:
Title: The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review
Authors: Jingguo Qu, Xinyang Han, Man-Lik Chui, Yao Pu, Simon Takadiyi Gunda, Ziman Chen, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying
Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.

Paper number 40:
Title: Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled Systems via Particle Filter Reinforcement Learning
Authors: Haokun Yu, Jingyuan Zhou, Kaidi Yang
Abstract: This paper addresses the problem of parameter privacy-preserving data sharing in coupled systems, where a data provider shares data with a data user but wants to protect its sensitive parameters. The shared data affects not only the data user's decision-making but also the data provider's operations through system interactions. To trade off control performance and privacy, we propose an interaction-aware privacy-preserving data sharing approach. Our approach generates distorted data by minimizing a combination of (i) mutual information, quantifying privacy leakage of sensitive parameters, and (ii) the impact of distorted data on the data provider's control performance, considering the interactions between stakeholders. The optimization problem is formulated into a Bellman equation and solved by a particle filter reinforcement learning (RL)-based approach. Compared to existing RL-based methods, our formulation significantly reduces history dependency and efficiently handles scenarios with continuous state space. Validated in a mixed-autonomy platoon scenario, our method effectively protects sensitive driving behavior parameters of human-driven vehicles (HDVs) against inference attacks while maintaining negligible impact on fuel efficiency.

Paper number 41:
Title: Turbo-ICL: In-Context Learning-Based Turbo Equalization
Authors: Zihang Song, Matteo Zecchin, Bipin Rajendran, Osvaldo Simeone
Abstract: This paper introduces a novel in-context learning (ICL) framework, inspired by large language models (LLMs), for soft-input soft-output channel equalization in coded multiple-input multiple-output (MIMO) systems. The proposed approach learns to infer posterior symbol distributions directly from a prompt of pilot signals and decoder feedback. A key innovation is the use of prompt augmentation to incorporate extrinsic information from the decoder output as additional context, enabling the ICL model to refine its symbol estimates iteratively across turbo decoding iterations. Two model variants, based on Transformer and state-space architectures, are developed and evaluated. Extensive simulations demonstrate that, when traditional linear assumptions break down, e.g., in the presence of low-resolution quantization, ICL equalizers consistently outperform conventional model-based baselines, even when the latter are provided with perfect channel state information. Results also highlight the advantage of Transformer-based models under limited training diversity, as well as the efficiency of state-space models in resource-constrained scenarios.

Paper number 42:
Title: Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment
Authors: Muhy Eddin Za'ter, Amir Sajad, Bri-Mathias Hodge
Abstract: This paper introduces a novel approach to the power system security assessment using Multi-Task Learning (MTL), and reformulating the problem as a multi-label classification task. The proposed MTL framework simultaneously assesses static, voltage, transient, and small-signal stability, improving both accuracy and interpretability with respect to the most state of the art machine learning methods. It consists of a shared encoder and multiple decoders, enabling knowledge transfer between stability tasks. Experiments on the IEEE 68-bus system demonstrate a measurable superior performance of the proposed method compared to the extant state-of-the-art approaches.

Paper number 43:
Title: Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation
Authors: Diego Adame, Jose A. Nunez, Fabian Vazquez, Nayeli Gurrola, Huimin Li, Haoteng Tang, Bin Fu, Pengfei Gu
Abstract: Convolutional neural network (CNN) and Transformer-based architectures are two dominant deep learning models for polyp segmentation. However, CNNs have limited capability for modeling long-range dependencies, while Transformers incur quadratic computational complexity. Recently, State Space Models such as Mamba have been recognized as a promising approach for polyp segmentation because they not only model long-range interactions effectively but also maintain linear computational complexity. However, Mamba-based architectures still struggle to capture topological features (e.g., connected components, loops, voids), leading to inaccurate boundary delineation and polyp segmentation. To address these limitations, we propose a new approach called Topo-VM-UNetV2, which encodes topological features into the Mamba-based state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for the training and test images, which are then used to compute topology attention maps. Specifically, we first compute persistence diagrams of the PMs, then we generate persistence score maps by assigning persistence values (i.e., the difference between death and birth times) of each topological feature to its birth location, finally we transform persistence scores into attention weights using the sigmoid function. Stage 2: These topology attention maps are integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to form a topology-guided semantics and detail infusion (Topo-SDI) module for enhancing the segmentation results. Extensive experiments on five public polyp segmentation datasets demonstrate the effectiveness of our proposed method. The code will be made publicly available.

Paper number 44:
Title: Preliminary Explorations with GPT-4o(mni) Native Image Generation
Authors: Pu Cao, Feng Zhou, Junyi Ji, Qingye Kong, Zhixiang Lv, Mingjian Zhang, Xuekun Zhao, Siqi Wu, Yinghui Lin, Qing Song, Lu Yang
Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimodal comprehension, its image-generation process demonstrates abilities surpassing those of traditional image-generation tasks. Thus, regarding the dimensions of model capabilities, we evaluate its performance across six task categories: traditional image generation tasks, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. These tasks not only assess the quality and conditional alignment of the model's outputs but also probe deeper into GPT-4o's understanding of real-world concepts. Our results reveal that GPT-4o performs impressively well in general-purpose synthesis tasks, showing strong capabilities in text-to-image generation, visual stylization, and low-level image processing. However, significant limitations remain in its ability to perform precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. Furthermore, when faced with knowledge-intensive or domain-specific scenarios, such as scientific illustrations or mathematical plots, the model often exhibits hallucinations, factual errors, or structural inconsistencies. These findings suggest that while GPT-4o marks a substantial advancement in unified multimodal generation, there is still a long way to go before it can be reliably applied to professional or safety-critical domains.

Paper number 45:
Title: Constraint Selection in Optimization-Based Controllers
Authors: Haejoon Lee, Panagiotis Rousseas, Dimitra Panagou
Abstract: Human-machine collaboration often involves constrained optimization problems for decision-making processes. However, when the machine is a dynamical system with a continuously evolving state, infeasibility due to multiple conflicting constraints can lead to dangerous outcomes. In this work, we propose a heuristic-based method that resolves infeasibility at every time step by selectively disregarding a subset of soft constraints based on the past values of the Lagrange multipliers. Compared to existing approaches, our method requires the solution of a smaller optimization problem to determine feasibility, resulting in significantly faster computation. Through a series of simulations, we demonstrate that our algorithm achieves performance comparable to state-of-the-art methods while offering improved computational efficiency.

Paper number 46:
Title: Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation
Authors: Yiming Qin, Zhu Xu, Yang Liu
Abstract: Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at this https URL .

Paper number 47:
Title: ADMM-Based Training for Spiking Neural Networks
Authors: Giovanni Perin, Cesare Bidini, Riccardo Mazzieri, Michele Rossi
Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to their high potential in time-series processing combined with minimal energy consumption. However, they still lack a dedicated and efficient training algorithm. The popular backpropagation with surrogate gradients, adapted from stochastic gradient descent (SGD)-derived algorithms, has several drawbacks when used as an optimizer for SNNs. Specifically, it suffers from low scalability and numerical imprecision. In this paper, we propose a novel SNN training method based on the alternating direction method of multipliers (ADMM). Our ADMM-based training aims to solve the problem of the SNN step function's non-differentiability. We formulate the problem, derive closed-form updates, and empirically show the optimizer's convergence properties, great potential, and possible new research directions to improve the method in a simulated proof-of-concept.

Paper number 48:
Title: OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours
Authors: Hanie Moghaddasi, Christina Chambers, Sarah N. Mattson, Jeffrey R. Wozniak, Claire D. Coles, Raja Mukherjee, Michael Suttie
Abstract: Lip segmentation plays a crucial role in various domains, such as lip synchronization, lipreading, and diagnostics. However, the effectiveness of supervised lip segmentation is constrained by the availability of lip contour in the training phase. A further challenge with lip segmentation is its reliance on image quality , lighting, and skin tone, leading to inaccuracies in the detected boundaries. To address these challenges, we propose a sequential lip segmentation method that integrates attention UNet and multidimensional input. We unravel the micro-patterns in facial images using local binary patterns to build multidimensional inputs. Subsequently, the multidimensional inputs are fed into sequential attention UNets, where the lip contour is reconstructed. We introduce a mask generation method that uses a few anatomical landmarks and estimates the complete lip contour to improve segmentation accuracy. This mask has been utilized in the training phase for lip segmentation. To evaluate the proposed method, we use facial images to segment the upper lips and subsequently assess lip-related facial anomalies in subjects with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method, we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in upper lip segmentation. To further evaluate the method, we implemented classifiers to identify those with FAS. Using a generative adversarial network (GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study populations. This method could be used to improve lip segmentation accuracy, especially around Cupid's bow, and shed light on distinct lip-related characteristics of FAS.

Paper number 49:
Title: Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet
Authors: Md Kamrujjaman Mobin, Md Saiful Islam, Sadik Al Barid, Md Masum
Abstract: Electrocardiogram (ECG) classification is crucial for automated cardiac disease diagnosis, yet existing methods often struggle to capture local morphological details and long-range temporal dependencies simultaneously. To address these challenges, we propose Cardioformer, a novel multi-granularity hybrid model that integrates cross-channel patching, hierarchical residual learning, and a two-stage self-attention mechanism. Cardioformer first encodes multi-scale token embeddings to capture fine-grained local features and global contextual information and then selectively fuses these representations through intra- and inter-granularity self-attention. Extensive evaluations on three benchmark ECG datasets under subject-independent settings demonstrate that model consistently outperforms four state-of-the-art baselines. Our Cardioformer model achieves the AUROC of 96.34$\pm$0.11, 89.99$\pm$0.12, and 95.59$\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming PatchTST, Reformer, Transformer, and Medformer models. It also demonstrates strong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41% on PTB-XL when trained on MIMIC-IV. These findings underscore the potential of Cardioformer to advance automated ECG analysis, paving the way for more accurate and robust cardiovascular disease diagnosis. We release the source code at this https URL.

Paper number 50:
Title: Barrier Function Overrides For Non-Convex Fixed Wing Flight Control and Self-Driving Cars
Authors: Eric Squires, Phillip Odom, Zsolt Kira
Abstract: Reinforcement Learning (RL) has enabled vast performance improvements for robotics systems. To achieve these results though, the agent often must randomly explore the environment, which for safety critical systems presents a significant challenge. Barrier functions can solve this challenge by enabling an override that approximates the RL control input as closely as possible without violating a safety constraint. Unfortunately, this override can be computationally intractable in cases where the dynamics are not convex in the control input or when time is discrete, as is often the case when training RL systems. We therefore consider these cases, developing novel barrier functions for two non-convex systems (fixed wing aircraft and self-driving cars performing lane merging with adaptive cruise control) in discrete time. Although solving for an online and optimal override is in general intractable when the dynamics are nonconvex in the control input, we investigate approximate solutions, finding that these approximations enable performance commensurate with baseline RL methods with zero safety violations. In particular, even without attempting to solve for the optimal override at all, performance is still competitive with baseline RL performance. We discuss the tradeoffs of the approximate override solutions including performance and computational tractability.

Paper number 51:
Title: Learning to Drive Anywhere with Model-Based Reannotation11
Authors: Noriaki Hirose, Lydia Ignatova, Kyle Stachowicz, Catherine Glossop, Sergey Levine, Dhruv Shah
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.

Paper number 52:
Title: Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks
Authors: Mohamed-Khalil Bouzidi, Christian Schlauch, Nicole Scheuerer, Yue Yao, Nadja Klein, Daniel Göhring, Jörg Reichardt
Abstract: Fueled by motion prediction competitions and benchmarks, recent years have seen the emergence of increasingly large learning based prediction models, many with millions of parameters, focused on improving open-loop prediction accuracy by mere centimeters. However, these benchmarks fail to assess whether such improvements translate to better performance when integrated into an autonomous driving stack. In this work, we systematically evaluate the interplay between state-of-the-art motion predictors and motion planners. Our results show that higher open-loop accuracy does not always correlate with better closed-loop driving behavior and that other factors, such as temporal consistency of predictions and planner compatibility, also play a critical role. Furthermore, we investigate downsized variants of these models, and, surprisingly, find that in some cases models with up to 86% fewer parameters yield comparable or even superior closed-loop driving performance. Our code is available at this https URL.

Paper number 53:
Title: The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction
Authors: Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian Wöhler
Abstract: Multimodal learning is an emerging research topic across multiple disciplines but has rarely been applied to planetary science. In this contribution, we identify that reflectance parameter estimation and image-based 3D reconstruction of lunar images can be formulated as a multimodal learning problem. We propose a single, unified transformer architecture trained to learn shared representations between multiple sources like grayscale images, digital elevation models, surface normals, and albedo maps. The architecture supports flexible translation from any input modality to any target modality. Predicting DEMs and albedo maps from grayscale images simultaneously solves the task of 3D reconstruction of planetary surfaces and disentangles photometric parameters and height information. Our results demonstrate that our foundation model learns physically plausible relations across these four modalities. Adding more input modalities in the future will enable tasks such as photometric normalization and co-registration.

Paper number 54:
Title: Toward a Sparse and Interpretable Audio Codec
Authors: John Vinyard
Abstract: Most widely-used modern audio codecs, such as Ogg Vorbis and MP3, as well as more recent "neural" codecs like Meta's Encodec or the Descript Audio Codec are based on block-coding; audio is divided into overlapping, fixed-size "frames" which are then compressed. While they often yield excellent reproductions and can be used for downstream tasks such as text-to-audio, they do not produce an intuitive, directly-interpretable representation. In this work, we introduce a proof-of-concept audio encoder that represents audio as a sparse set of events and their times-of-occurrence. Rudimentary physics-based assumptions are used to model attack and the physical resonance of both the instrument being played and the room in which a performance occurs, hopefully encouraging a sparse, parsimonious, and easy-to-interpret representation.

Paper number 55:
Title: HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder
Authors: Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim
Abstract: Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \textit{dual masking} strategy: during pre-training we randomly occlude 50\% of spatial patches and 50\% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity. The resulting model contains about $1.8\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.

Paper number 56:
Title: Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data
Authors: Amin Ghafourian, Andrew Lee, Dechen Gao, Tyler Beer, Kin Yen, Iman Soltani
Abstract: Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: this https URL.

Paper number 57:
Title: Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes
Authors: Youngjoon Lee, Jinu Gong, Joonhyuk Kang
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming-distance decoding. Our proposed KAN with ECOC method outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy under diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first integration of ECOC with KAN for enhancing multi-class medical image classification performance.

Paper number 58:
Title: Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition
Authors: Zhiyuan Chen, Keyi Li, Yifan Jia, Le Ye, Yufei Ma
Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at this https URL.

Paper number 59:
Title: Versatile Distributed Maneuvering with Generalized Formations using Guiding Vector Fields
Authors: Yang Lu, Sha Luo, Pengming Zhu, Weijia Yao, Hector Garcia de Marina, Xinglong Zhang, Xin Xu
Abstract: This paper presents a unified approach to realize versatile distributed maneuvering with generalized formations. Specifically, we decompose the robots' maneuvers into two independent components, i.e., interception and enclosing, which are parameterized by two independent virtual coordinates. Treating these two virtual coordinates as dimensions of an abstract manifold, we derive the corresponding singularity-free guiding vector field (GVF), which, along with a distributed coordination mechanism based on the consensus theory, guides robots to achieve various motions (i.e., versatile maneuvering), including (a) formation tracking, (b) target enclosing, and (c) circumnavigation. Additional motion parameters can generate more complex cooperative robot motions. Based on GVFs, we design a controller for a nonholonomic robot model. Besides the theoretical results, extensive simulations and experiments are performed to validate the effectiveness of the approach.

Paper number 60:
Title: Towards Facial Image Compression with Consistency Preserving Diffusion Prior
Authors: Yimin Zhou, Yichong Xia, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou
Abstract: With the widespread application of facial image data across various domains, the efficient storage and transmission of facial images has garnered significant attention. However, the existing learned face image compression methods often produce unsatisfactory reconstructed image quality at low bit rates. Simply adapting diffusion-based compression methods to facial compression tasks results in reconstructed images that perform poorly in downstream applications due to insufficient preservation of high-frequency information. To further explore the diffusion prior in facial image compression, we propose Facial Image Compression with a Stable Diffusion Prior (FaSDiff), a method that preserves consistency through frequency enhancement. FaSDiff employs a high-frequency-sensitive compressor in an end-to-end framework to capture fine image details and produce robust visual prompts. Additionally, we introduce a hybrid low-frequency enhancement module that disentangles low-frequency facial semantics and stably modulates the diffusion prior alongside visual prompts. The proposed modules allow FaSDiff to leverage diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments show that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code will be released after the paper is accepted.

Paper number 61:
Title: Mechanical Power Modeling and Energy Efficiency Maximization for Movable Antenna Systems
Authors: Xin Wei, Weidong Mei, Xuan Huang, Zhi Chen, Boyu Ning
Abstract: Movable antennas (MAs) have recently garnered significant attention in wireless communications due to their capability to reshape wireless channels via local antenna movement within a confined region. However, to achieve accurate antenna movement, MA drivers introduce non-negligible mechanical power consumption, rendering energy efficiency (EE) optimization more critical compared to conventional fixed-position antenna (FPA) systems. To address this problem, we develop in this paper a fundamental power consumption model for stepper motor-driven MA systems by resorting to basic electric motor theory. Based on this model, we formulate an EE maximization problem by jointly optimizing an MA's position, moving speed, and transmit power. However, this problem is difficult to solve optimally due to the intricate relationship between the mechanical power consumption and the design variables. To tackle this issue, we first uncover a hidden monotonicity of the EE performance with respect to the MA's moving speed. Then, we apply the Dinkelbach algorithm to obtain the optimal transmit power in a semi-closed form for any given MA position, followed by an enumeration to determine the optimal MA position. Numerical results demonstrate that despite the additional mechanical power consumption, the MA system can outperform the conventional FPA system in terms of EE.

Paper number 62:
Title: Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates
Authors: Rodrigo Diaz, Mark Sandler
Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von Kármán plate, are computationally demanding and lack differentiability, limiting inverse modelling and real-time applications. We introduce a fast, differentiable, GPU-accelerated modal framework built with the JAX library, providing efficient simulations and enabling gradient-based inverse modelling. Benchmarks show that our approach significantly outperforms CPU and GPU-based implementations, particularly for simulations with many modes. Inverse modelling experiments demonstrate that our approach can recover physical parameters, including tension, stiffness, and geometry, from both synthetic and experimental data. Although fitting physical parameters is more sensitive to initialisation compared to other methods, it provides greater interpretability and more compact parameterisation. The code is released as open source to support future research and applications in differentiable physical modelling and sound synthesis.

Paper number 63:
Title: Learning Power Control Protocol for In-Factory 6G Subnetworks
Authors: Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun
Abstract: In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal "Genie" approach by only 5%.

Paper number 64:
Title: Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI
Authors: Jianpeng Qi, Chao Liu, Chengxiang Xu, Rui Wang, Junyu Dong, Yanwei Yu
Abstract: Timely and efficient dissemination of service information is critical in compute-first networking systems, where user requests arrive dynamically and computing resources are constrained. In such systems, the access point (AP) plays a key role in forwarding user requests to a server based on its latest received service information. This paper considers a single-source, single-destination system and introduces an Age-and-Value-Aware (AVA) metric that jointly captures both the timeliness and the task relevance of service information. Unlike traditional freshness-based metrics, AVA explicitly incorporates variations in server-side service capacity and AP forwarding decisions, allowing more context-aware update evaluation. Building upon AVA, we propose a reinforcement learning-based update policy that learns to selectively transmit service information updates to the AP. It aims to maximize overall task success while minimizing unnecessary communications. Extensive simulations under diverse user request patterns and varying service capacities demonstrate that AVA reduces the update frequency by over 90% on average compared to baselines, with reductions reaching 98% in certain configurations. Crucially, this reduction is achieved without compromising the accuracy of task execution or the quality of decision making.

Paper number 65:
Title: Extending the Control Plane of Container Orchestrators for I/O Virtualization
Authors: Garegin Grigoryan, Minseok Kwon, M. Mustafa Rafique
Abstract: Single Root Input/Output Virtualization (SR-IOV) is a standard technology for forking a single PCI express device and providing it to applications while ensuring performance isolation. It enables container orchestrators to share a limited number of physical network interfaces without incurring significant virtualization overhead. The allocation of virtualized network devices to containers, however, needs to be more configurable based on the bandwidth needs of running applications. Moreover, container orchestrators' network control over the virtualized interfaces is limited by the abilities of SR-IOV. We explore the design considerations for a system with controlled SR-IOV virtualization and present ConRDMA, a novel architecture that enables fine control of RDMA virtualization for containers. Our evaluation shows that ConRDMA enables containers to use RDMA allocated bandwidth more efficiently and to select best-suited nodes to meet their varying communication requirements.

Paper number 66:
Title: Learning Music Audio Representations With Limited Data
Authors: Christos Plachouras, Emmanouil Benetos, Johan Pauwels
Abstract: Large deep-learning models for music, including those focused on learning general-purpose music audio representations, are often assumed to require substantial training data to achieve high performance. If true, this would pose challenges in scenarios where audio data or annotations are scarce, such as for underrepresented music traditions, non-popular genres, and personalized music creation and listening. Understanding how these models behave in limited-data scenarios could be crucial for developing techniques to tackle them. In this work, we investigate the behavior of several music audio representation models under limited-data learning regimes. We consider music models with various architectures, training paradigms, and input durations, and train them on data collections ranging from 5 to 8,000 minutes long. We evaluate the learned representations on various music information retrieval tasks and analyze their robustness to noise. We show that, under certain conditions, representations from limited-data and even random models perform comparably to ones from large-dataset models, though handcrafted features outperform all learned representations in some tasks.

Paper number 67:
Title: Centralized Decision-Making for Platooning By Using SPaT-Driven Reference Speeds
Authors: Melih Yazgan, Süleyman Tatar, J. Marius Zöllner
Abstract: This paper introduces a centralized approach for fuel-efficient urban platooning by leveraging real-time Vehicle- to-Everything (V2X) communication and Signal Phase and Timing (SPaT) data. A nonlinear Model Predictive Control (MPC) algorithm optimizes the trajectories of platoon leader vehicles, employing an asymmetric cost function to minimize fuel-intensive acceleration. Following vehicles utilize a gap- and velocity-based control strategy, complemented by dynamic platoon splitting logic communicated through Platoon Control Messages (PCM) and Platoon Awareness Messages (PAM). Simulation results obtained from the CARLA environment demonstrate substantial fuel savings of up to 41.2%, along with smoother traffic flows, fewer vehicle stops, and improved intersection throughput.

Paper number 68:
Title: Discretized Approximate Ancestral Sampling
Authors: Alfredo De la Fuente, Saurabh Singh, Jona Ballé
Abstract: The Fourier Basis Density Model (FBM) was recently introduced as a flexible probability model for band-limited distributions, i.e. ones which are smooth in the sense of having a characteristic function with limited support around the origin. Its density and cumulative distribution functions can be efficiently evaluated and trained with stochastic optimization methods, which makes the model suitable for deep learning applications. However, the model lacked support for sampling. Here, we introduce a method inspired by discretization--interpolation methods common in Digital Signal Processing, which directly take advantage of the band-limited property. We review mathematical properties of the FBM, and prove quality bounds of the sampled distribution in terms of the total variation (TV) and Wasserstein--1 divergences from the model. These bounds can be used to inform the choice of hyperparameters to reach any desired sample quality. We discuss these results in comparison to a variety of other sampling techniques, highlighting tradeoffs between computational complexity and sampling quality.

Paper number 69:
Title: ABAMGuid+: An Enhanced Aerocapture Guidance Framework using Augmented Bank Angle Modulation
Authors: Kyle A. Sonandres, Thomas R. Palazzo, Jonathan P. How
Abstract: Aerocapture consists of converting a hyperbolic approach trajectory into a captured target orbit utilizing the aerodynamic forces generated via a single pass through the atmosphere. Aerocapture guidance systems must be robust to significant environmental variations and modeling uncertainty, particularly regarding atmospheric properties and delivery conditions. Recent work has shown that enabling control over both bank angle and angle of attack, a strategy referred to as augmented bank angle modulation (ABAM), can improve robustness to entry state and atmospheric uncertainties. In this work, we derive optimal control solutions for an aerocapture vehicle using ABAM. We first formulate the problem using a linear aerodynamic model and derive closed-form optimal control profiles using Pontryagin's Minimum Principle. To increase modeling fidelity, we also consider a quadratic aerodynamic model and obtain the solution directly using the optimality conditions. Both formulations are solved numerically using Gauss pseudospectral methods (via GPOPS, a software tool for pseudospectral optimal control), to validate the analytic solutions. We then introduce a novel aerocapture guidance algorithm, ABAMGuid+, which indirectly minimizes propellant usage by mimicking the structure of the optimal control solution, enabling efficient guidance by avoiding the complexity of solving the full optimal control problem online. Extensive Monte Carlo simulations of a Uranus aerocapture mission demonstrate that ABAMGuid+ increases capture success rates and reduces post-capture propellant requirements relative to previous methods.

Paper number 70:
Title: Robust Multi-Agent Decision-Making in Finite-Population Games
Authors: Shinkyu Park, Lucas C. D. Bezerra
Abstract: We study the robustness of an agent decision-making model in finite-population games, with a particular focus on the Kullback-Leibler Divergence Regularized Learning (KLD-RL) model. Specifically, we examine how the model's parameters influence the effects of various sources of noise and modeling inaccuracies -- factors commonly encountered in engineering applications of population games -- on agents' decision-making. Our analysis provides insights into how these parameters can be effectively tuned to mitigate such effects. Theoretical results are supported by numerical examples and simulation studies that validate the analysis and illustrate practical strategies for parameter selection.

Paper number 71:
Title: Interoperable and scalable echosounder data processing with Echopype
Authors: Wu-Jung Lee, Landung Setiawan, Caesar Tuguinay, Emilio Mayorga, Valentina Staneva
Abstract: Echosounders are high-frequency sonar systems used to sense fish and zooplankton underwater. Their deployment on a variety of ocean observing platforms is generating vast amounts of data at an unprecedented speed from the oceans. Efficient and integrative analysis of these data, whether across different echosounder instruments or in combination with other oceanographic datasets, is crucial for understanding marine ecosystem response to the rapidly changing climate. Here we present Echopype, an open-source Python software library designed to address this need. By standardizing data as labeled, multi-dimensional arrays encoded in the widely embraced netCDF data model following a community convention, Echopype enhances the interoperability of echosounder data, making it easier to explore and use. By leveraging scientific Python libraries optimized for distributed computing, Echopype achieves computational scalability, enabling efficient processing in both local and cloud computing environments. Echopype's modularized package structure further provides a unified framework for expanding support for additional instrument raw data formats and incorporating new analysis functionalities. We plan to continue developing Echopype by supporting and collaborating with the echosounder user community, and envision that the growth of this package will catalyze the integration of echosounder data into broader regional and global ocean observation strategies.

Paper number 72:
Title: Modelling of a DC-DC Buck Converter Using Long-Short-Term-Memory (LSTM)
Authors: Muhy Eddin Za'ter
Abstract: Artificial neural networks make it possible to identify black-box models. Based on a recurrent nonlinear autoregressive exogenous neural network, this research provides a technique for simulating the static and dynamic behavior of a DC-DC power converter. This approach employs an algorithm for training a neural network using the inputs and outputs (currents and voltages) of a Buck converter. The technique is validated using simulated data of a realistic Simulink-programmed nonsynchronous Buck converter model and experimental findings. The correctness of the technique is determined by comparing the predicted outputs of the neural network to the actual outputs of the system, thereby confirming the suggested strategy. Simulation findings demonstrate the practicability and precision of the proposed black-box method.

Paper number 73:
Title: Mitigating Evasion Attacks in Federated Learning-Based Signal Classifiers
Authors: Su Wang, Rajeev Sahay, Adam Piaseczny, Christopher G. Brinton
Abstract: Recent interest in leveraging federated learning (FL) for radio signal classification (SC) tasks has shown promise but FL-based SC remains susceptible to model poisoning adversarial attacks. These adversarial attacks mislead the ML model training process, damaging ML models across the network and leading to lower SC performance. In this work, we seek to mitigate model poisoning adversarial attacks on FL-based SC by proposing the Underlying Server Defense of Federated Learning (USD-FL). Unlike existing server-driven defenses, USD-FL does not rely on perfect network information, i.e., knowing the quantity of adversaries, the adversarial attack architecture, or the start time of the adversarial attacks. Our proposed USD-FL methodology consists of deriving logits for devices' ML models on a reserve dataset, comparing pair-wise logits via 1-Wasserstein distance and then determining a time-varying threshold for adversarial detection. As a result, USD-FL effectively mitigates model poisoning attacks introduced in the FL network. Specifically, when baseline server-driven defenses do have perfect network information, USD-FL outperforms them by (i) improving final ML classification accuracies by at least 6%, (ii) reducing false positive adversary detection rates by at least 10%, and (iii) decreasing the total number of misclassified signals by over 8%. Moreover, when baseline defenses do not have perfect network information, we show that USD-FL achieves accuracies of approximately 74.1% and 62.5% in i.i.d. and non-i.i.d. settings, outperforming existing server-driven baselines, which achieve 52.1% and 39.2% in i.i.d. and non-i.i.d. settings, respectively.

Paper number 74:
Title: Generalizable Sleep Staging via Multi-Level Domain Alignment
Authors: Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan
Abstract: Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different domains, and a Sequence-level Feature Alignment to minimize the discrepancy of sequential features among different domains. SleepDG is validated on five public datasets, achieving the state-of-the-art performance.

Paper number 75:
Title: Tuning a Cascaded Online Feedback Optimization Controller for Provision of Distributed Flexibility
Authors: Irina Zettl, Florian Klein-Helmkamp, Florian Schmidtke, Lukas Ortmann, Andreas Ulbig
Abstract: Coordinating a high number of flexibility providing units (e.g. to provide ancillary services for the transmission system) across various grid layers requires new control concepts. A flexibility request at a point of common coupling can be met by utilizing a cascaded control structure based on online feedback optimization. In this paper the influence of the parameterization of the individual controllers on the performance of the hierarchical flexibility provision is studied on a three-level test system. The results show a high interdependency between the choice of control parameters of one controller and the behavior of other controllers as well as a significant impact on the accuracy and speed of flexibility provision. With a careful tuning, a cascaded structure based on online feedback optimization can achieve efficient vertical coordination of flexibility providing units.

Paper number 76:
Title: Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer
Authors: Yusen Wu, Phuong Nguyen, Rose Yesha, Yelena Yesha
Abstract: Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect the prediction results of machine learning models. However, current methods have limitations in detecting drift, for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a pre-trained Vision Transformer model to extract relevant features, using mammography as a case study, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 99.1%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.

Paper number 77:
Title: GreenLight-Gym: Reinforcement learning benchmark environment for control of greenhouse production systems
Authors: Bart van Laatum, Eldert J. van Henten, Sjoerd Boersma
Abstract: This study presents GreenLight-Gym, a new, fast, open-source benchmark environment for developing reinforcement learning (RL) methods in greenhouse crop production control. Built on the state-of-the-art GreenLight model, it features a differentiable C++ implementation leveraging the CasADi framework for efficient numerical integration. GreenLight-Gym improves simulation speed by a factor of 17 over the original GreenLight implementation. A modular Python environment wrapper enables flexible configuration of control tasks and RL-based controllers. This flexibility is demonstrated by learning controllers under parametric uncertainty using two well-known RL algorithms. GreenLight-Gym provides a standardized benchmark for advancing RL methodologies and evaluating greenhouse control solutions under diverse conditions. The greenhouse control community is encouraged to use and extend this benchmark to accelerate innovation in greenhouse crop production.

Paper number 78:
Title: Variational Source-Channel Coding for Semantic Communication
Authors: Yulong Feng, Jing Xu, Liujun Hu, Guanghui Yu, Xiangyang Duan
Abstract: Semantic communication technology emerges as a pivotal bridge connecting AI with classical communication. The current semantic communication systems are generally modeled as an Auto-Encoder (AE). AE lacks a deep integration of AI principles with communication strategies due to its inability to effectively capture channel dynamics. This gap makes it difficult to justify the need for joint source-channel coding (JSCC) and to explain why performance improves. This paper begins by exploring lossless and lossy communication, highlighting that the inclusion of data distortion distinguishes semantic communication from classical communication. It breaks the conditions for the separation theorem to hold and explains why the amount of data transferred by semantic communication is less. Therefore, employing JSCC becomes imperative for achieving optimal semantic communication. Moreover, a Variational Source-Channel Coding (VSCC) method is proposed for constructing semantic communication systems based on data distortion theory, integrating variational inference and channel characteristics. Using a deep learning network, we develop a semantic communication system employing the VSCC method and demonstrate its capability for semantic transmission. We also establish semantic communication systems of equivalent complexity employing the AE method and the VAE method. Experimental results reveal that the VSCC model offers superior interpretability compared to AE model, as it clearly captures the semantic features of the transmitted data, represented as the variance of latent variables in our experiments. In addition, VSCC model exhibits superior semantic transmission capabilities compared to VAE model. At the same level of data distortion evaluated by PSNR, VSCC model exhibits stronger human interpretability, which can be partially assessed by SSIM.

Paper number 79:
Title: Linear Receive Beamforming for CAPA Systems
Authors: Chongjun Ouyang, Zhaolin Wang, Xingqi Zhang, Yuanwei Liu
Abstract: The performance of linear receive beamforming in continuous-aperture array (CAPA)-based uplink communications is analyzed. Three continuous beamforming techniques are proposed under the criteria of maximum-ratio combining (MRC), zero-forcing (ZF), and minimum mean-squared error (MMSE). \romannumeral1) For \emph{MRC beamforming}, a closed-form expression for the beamformer is derived to maximize per-user signal power. The achieved uplink rate and mean-squared error (MSE) in detecting received data symbols are analyzed. \romannumeral2) For \emph{ZF beamforming}, a closed-form beamformer is derived based on channel correlation to eliminate interference. As a further advance, its optimality in maximizing effective channel gain while ensuring zero inter-user interference is proven. \romannumeral3) \emph{MMSE beamforming} is established as the optimal linear receive approach for CAPAs in terms of maximizing per-user rate and minimizing MSE. Closed-form expressions are derived for the MMSE beamformer and the achievable sum-rate and sum-MSE. It is mathematically proven that all proposed beamformers lie within the signal subspace spanned by users' spatial responses. Numerical results demonstrate that CAPAs outperform conventional spatially-discrete arrays (SPDAs) by achieving higher sum-rates and lower sum-MSEs under the proposed linear beamforming techniques.

Paper number 80:
Title: Doubly-Dispersive MIMO Channels with Stacked Intelligent Metasurfaces: Modeling, Parametrization, and Receiver Design
Authors: Kuranage Roche Rayan Ranasinghe, Iván Alexander Morales Sandoval, Hyeon Seok Rou, Giuseppe Thadeu Freitas de Abreu, George C. Alexandropoulos
Abstract: Introduced with the advent of statistical wireless channel models for high mobility communications and having a profound role in communication-centric (CC) integrated sensing and communications (ISAC), the doubly-dispersive (DD) channel structure has long been heralded as a useful tool enabling the capture of the most important fading effects undergone by an arbitrary time-domain transmit signal propagating through some medium. However, the incorporation of this model into multiple-input multiple-output (MIMO) system setups, relying on the recent paradigm-shifting transceiver architecture based on stacked intelligent metasurfaces (SIM), in an environment with reconfigurable intelligent surfaces (RISs) remains an open problem due to the many intricate details that have to be accounted for. In this paper, we fill this gap by introducing a novel DD MIMO channel model that incorporates an arbitrary number of RISs in the ambient, as well as SIMs equipping both the transmitter and receiver. We then discuss how the proposed metasurfaces-parametrized DD (MPDD) channel model can be seamlessly applied to waveforms that are known to perform well in DD environments, namely, orthogonal frequency division multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine frequency division multiplexing (AFDM), with each having their own inherent advantages and disadvantages. An illustrative application of the programmable functionality of the proposed model is finally presented to showcase its potential for boosting the performance of the aforementioned waveforms. Our numerical results indicate that the design of waveforms suitable to mitigating the effects of DD channels is significantly impacted by the emerging SIM technology.

Paper number 81:
Title: Higher-Order Meta Distribution Reliability Analysis of Wireless Networks
Authors: Mehdi Monemi, Mehdi Rasti, S. Ali Mousavi, Matti Latva-aho, Martin Haenggi
Abstract: Communication reliability, as defined by 3GPP, refers to the probability of providing a desired quality of service (QoS). This metric is typically quantified for wireless networks by averaging the QoS success indicator over spatial and temporal random variables. Recently, the meta distribution (MD) has emerged as a two-level performance analysis tool for wireless networks, offering a detailed examination of the outer level (i.e., system-level) reliability versus the inner level (i.e., link-level) reliability thresholds. Most existing studies focus on first-order spatiotemporal MD reliability analyses, and the benefits of leveraging MD reliability for applications beyond this structure remain unexplored, a gap addressed in this paper. We propose a framework for the analysis of higher-order MD reliability of wireless networks considering different levels of temporal dynamicity of random elements in the network where the MD at each layer is leveraged to be used in calculating the MD of the higher layer. We then provide two applications for this framework and provide a detailed analytical and numerical study of the higher-order MD reliability for both examples. The results demonstrate the value of the hierarchical representation of MD reliability across three domains and the impact of the inner-layers target reliabilities on the overall MD reliability measure.

Paper number 82:
Title: End-to-end localized deep learning for Cryo-ET
Authors: Vinith Kishore, Valentin Debarnot, Ricardo D. Righetto, AmirEhsan Khorashadizadeh, Benjamin D. Engel, Ivan Dokmanić
Abstract: Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular environments. Accurate reconstruction of high-resolution volumes is complicated by the very low signal-to-noise ratio and a restricted range of sample tilts, creating a missing wedge of Fourier information. Recent self-supervised deep learning approaches, which post-process initial reconstructions done by filtered backprojection (FBP), have significantly improved reconstruction quality, but they are computationally expensive, demand large memory, and require retraining for each new dataset. End-to-end supervised learning is an appealing alternative but is impeded by the lack of ground truth and the large memory demands of high-resolution volumetric data. Training on synthetic data often leads to overfitting and poor generalization to real data, and, to date, no general end-to-end deep learning reconstructors exist for cryo-ET. In this work, we introduce CryoLithe, a local, memory-efficient reconstruction network that directly estimates the volume from an aligned tilt-series, overcoming the suboptimal FBP. We demonstrate that leveraging transform-domain locality makes our network robust to distribution shifts, enabling effective supervised training and giving excellent results on real data -- without retraining or fine-tuning.

Paper number 83:
Title: Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction
Authors: Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga Çukur
Abstract: Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL aggregates locally trained model weights into a global model, inherently constraining all sites to use a homogeneous model architecture. This rigidity forces sites to compromise on architectures tailored to their compute resources and application-specific needs, making conventional FL unsuitable for model-heterogeneous settings where each site may prefer a distinct architecture. To overcome this limitation, we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that learns the distribution of multi-site MR images. For high-fidelity synthesis, we propose a novel site-prompted GAT prior that controllably synthesizes realistic MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its own reconstruction model -- using an architecture of its choice -- on a hybrid dataset augmenting its local MRI dataset with GAT-generated synthetic MR images emulating datasets from other sites. This hybrid training strategy enables site-specific reconstruction models to generalize more effectively across diverse data distributions while preserving data privacy. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT enables flexible, model-heterogeneous collaborations and achieves superior within-site and cross-site reconstruction performance compared to state-of-the-art FL baselines.

Paper number 84:
Title: Uniform Resampling vs. Image Blur: Aliasing Approximation via Isotropic Gaussian Filtering
Authors: Suayb S. Arslan, Lukas Vogelsang, Michal Fux, Pawan Sinha
Abstract: One of the key approximations to range simulation is downscaling the image, dictated by the natural trigonometric relationships that arise due to long-distance viewing. It is well-known that standard downsampling applied to an image without prior low-pass filtering leads to a type of signal distortion called \textit{aliasing}. In this study, we aim at modeling the distortion due to aliasing and show that a downsampled/upsampled image after an interpolation process can be very well approximated through the application of isotropic Gaussian low-pass filtering to the original image. In other words, the distortion due to aliasing can approximately be generated by low-pass filtering the image with a carefully determined cut-off frequency. We have found that the standard deviation of the isotropic Gaussian kernel $\sigma$ and the reduction factor $m$ (also called downsampling ratio) satisfy an approximate $m \approx 2 \sigma$ relationship. We provide both theoretical and practical arguments using two relatively small face datasets (Chicago DB, LRFID) as well as TinyImageNet to corroborate this empirically observed relationship.

Paper number 85:
Title: On the $H$-property for Step-graphons: Residual Case
Authors: Wanting Gao, Xudong Chen
Abstract: We investigate the $H$-property for step-graphons. Specifically, we sample graphs $G_n$ on $n$ nodes from a step-graphon and evaluate the probability that $G_n$ has a Hamiltonian decomposition in the asymptotic regime as $n\to\infty$. It has been shown that for almost all step-graphons, this probability converges to either zero or one. We focus in this paper on the residual case where the zero-one law does not apply. We show that the limit of the probability still exists and provide an explicit expression of it. We present a complete proof of the result and validate it through numerical studies.

Paper number 86:
Title: Parameter Invariance Analysis of Moment Equations Using Dulmage-Mendelsohn Decomposition
Authors: Akito Igarashi, Yutaka Hori
Abstract: Living organisms maintain stable functioning amid environmental fluctuations through homeostasis, a property that preserves a system's behavior despite changes in environmental conditions. To elucidate homeostasis in stochastic biochemical reactions, theoretical tools for assessing population-level invariance under parameter perturbations are crucial. In this paper, we propose a systematic method for identifying the stationary moments that remain invariant under parameter perturbations by leveraging the structural properties of the stationary moment equations. A key step in this development is addressing the underdetermined nature of moment equations, which has traditionally made it difficult to characterize how stationary moments depend on system parameters. To overcome this, we utilize the Dulmage-Mendelsohn (DM) decomposition of the coefficient matrix to extract welldetermined subequations and reveal their hierarchical structure. Leveraging this structure, we identify stationary moments whose partial derivatives with respect to parameters are structurally zero, facilitating the exploration of fundamental constraints that govern homeostatic behavior in stochastic biochemical systems.

Paper number 87:
Title: SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting
Authors: Tianning Feng, Juntong Ni, Ezequiel Gleichgerrcht, Wei Jin
Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure risk forecasting using interictal epileptiform activity (IEA) surrogate biomarkers and long episode (LE) biomarkers from responsive neurostimulation (RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages structured, clinically relevant features and integrates CNN-based patch embedding, multi-head self-attention, and squeeze-and-excitation blocks to model both short-term dynamics and long-term seizure cycles. Tested across five patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC of 76.29 percent. Compared to statistical, machine learning, and deep learning baselines, it demonstrates enhanced generalizability and seizure risk forecasting performance under class imbalance. This work supports future clinical integration of interpretable and robust seizure forecasting tools for personalized epilepsy management.

Paper number 88:
Title: MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction
Authors: Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim
Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging. Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.

Paper number 89:
Title: FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech
Authors: Linhan Ma, Dake Guo, He Wang, Jin Xu, Lei Xie
Abstract: Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.

Paper number 90:
Title: A vector quantized masked autoencoder for audiovisual speech emotion recognition
Authors: Samir Sadok, Simon Leglaive, Renaud Séguier
Abstract: An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.

Paper number 91:
Title: SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning
Authors: Pu Ren, N. Benjamin Erichson, Junyi Guo, Shashank Subramanian, Omer San, Zarija Lukic, Michael W. Mahoney
Abstract: Super-resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets, including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will help to advance SR methods for science.

Paper number 92:
Title: Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise
Authors: Tianfu Qi, Jun Wang, Xiaoping Li
Abstract: Communication systems suffer from mixed noise consisting of both non-Gaussian impulsive noise (IN) and white Gaussian noise (WGN) in many practical applications. However, there is little literature about the channel capacity under mixed noise. In this paper, we first investigate statistical properties of the mixed noise model and demonstrate the existence and uniqueness of the capacity-achieving input distribution under the $p$-th moment constraint. Then, we derive lower and upper capacity bounds with closed expressions. It is shown that the lower bounds can degenerate to the well-known Shannon formula under special scenarios. More importantly, we obtain the convergence of the lower and upper bound and therefore, the asymptotic and analytical capacity expression is obtained. In addition, the capacity for specific modulations and the corresponding lower bounds are discussed. Numerical results reveal that the capacity decreases as the impulsiveness of the mixed noise becomes dominant and the proposed capacity bounds are very tight.

Paper number 93:
Title: Generalized Arlery-Tan-Rabaste-Levenshtein Lower Bounds on Ambiguity Function and Their Asymptotic Achievability
Authors: Lingsheng Meng, Yong Liang Guan, Yao Ge, Zilong Liu, Pingzhi Fan
Abstract: This paper presents generalized Arlery-Tan-Rabaste-Levenshtein lower bounds on the maximum aperiodic ambiguity function (AF) magnitude of unimodular sequences under certain delay-Doppler low ambiguity zones (LAZ). Our core idea is to explore the upper and lower bounds on the Frobenius norm of the weighted auto- and cross-AF matrices by introducing two weight vectors associated with the delay and Doppler shifts, respectively. As a second major contribution, we demonstrate that our derived lower bounds are asymptotically achievable with selected Chu sequence sets by analyzing their maximum auto- and cross- AF magnitudes within certain LAZ.

Paper number 94:
Title: A First-Order Gradient Approach for the Connectivity Optimization of Markov Chains
Authors: Christian P.C. Franssen, Alessandro Zocca, Bernd F. Heidergott
Abstract: Graphs are commonly used to model various complex systems, including social networks, power grids, transportation networks, and biological systems. In many applications, the connectivity of these networks can be expressed through the Mean First Passage Times (MFPTs) of a Markov chain modeling a random walker on the graph. In this paper, we generalize the network metrics based on Markov chains' MFPTs and extend them to networks affected by uncertainty, in which edges may fail and hence not be present according to a pre-determined stochastic model. To find optimally connected Markov chains, we present a parameterization-free method for optimizing the MFPTs of the Markov chain. More specifically, we present an efficient Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm in the context of Markov chain optimization. The proposed algorithm is suitable for both fixed and random networks. Using various numerical experiments, we demonstrate scalability compared to established benchmarks. Importantly, our algorithm finds an optimal solution without requiring prior knowledge of edge failure probabilities, allowing for an online optimization approach.

Paper number 95:
Title: Exact Imposition of Safety Boundary Conditions in Neural Reachable Tubes
Authors: Aditya Singh, Zeyuan Feng, Somil Bansal
Abstract: Hamilton-Jacobi (HJ) reachability analysis is a widely adopted verification tool to provide safety and performance guarantees for autonomous systems. However, it involves solving a partial differential equation (PDE) to compute a safety value function, whose computational and memory complexity scales exponentially with the state dimension, making its direct application to large-scale systems intractable. To overcome these challenges, DeepReach, a recently proposed learning-based approach, approximates high-dimensional reachable tubes using neural networks (NNs). While shown to be effective, the accuracy of the learned solution decreases with system complexity. One of the reasons for this degradation is a soft imposition of safety constraints during the learning process, which corresponds to the boundary conditions of the PDE, resulting in inaccurate value functions. In this work, we propose ExactBC, a variant of DeepReach that imposes safety constraints exactly during the learning process by restructuring the overall value function as a weighted sum of the boundary condition and the NN output. Moreover, the proposed variant no longer needs a boundary loss term during the training process, thus eliminating the need to balance different loss terms. We demonstrate the efficacy of the proposed approach in significantly improving the accuracy of the learned value function for four challenging reachability tasks: a rimless wheel system with state resets, collision avoidance in a cluttered environment, autonomous rocket landing, and multi-aircraft collision avoidance.

Paper number 96:
Title: Is speckle noise more challenging to mitigate than additive noise?
Authors: Reihaneh Malekian, Hao Xing, Arian Maleki
Abstract: We study the problem of estimating a function in the presence of both speckle and additive noises, commonly referred to as the de-speckling problem. Although additive noise has been thoroughly explored in nonparametric estimation, speckle noise, prevalent in applications such as synthetic aperture radar, ultrasound imaging, and digital holography, has not received as much attention. Consequently, there is a lack of theoretical investigations into the fundamental limits of mitigating the speckle this http URL paper is the first step in filling this gap. Our focus is on investigating the minimax estimation error for estimating a $\beta$-Hölder continuous function and determining the rate of the minimax risk. Specifically, if $n$ represents the number of data points, $f$ denotes the underlying function to be estimated, $\hat{\nu}_n$ is an estimate of $f$, and $\sigma_n$ is the standard deviation of the additive Gaussian noise, then $\inf_{\hat{\nu}_n} \sup_f \mathbb{E}_f\| \hat{\nu}_n - f \|^2_2$ decays at the rate $(\max(1,\sigma_n^4)/n)^{\frac{2\beta}{2\beta+1}}$. Note that the rate achieved under purely additive noise is $({\sigma_n^2/n})^{\frac{2\beta}{2\beta+1}}$. We will provide a detailed comparison of this rate with the one obtained in the presence of both noise types across different regimes of their relative magnitudes, and discuss the insights that emerge from these comparisons.

Paper number 97:
Title: Opacity Enforcement by Edit Functions Under Incomparable Observations
Authors: Wei Duan, Ruotian Liu, Maria Pia Fanti, Christoforos N. Hadjicostis, Zhiwu Li
Abstract: As an information-flow privacy property, opacity characterizes whether a malicious external observer (referred to as an intruder) is able to infer the secret behavior of a system. This paper addresses the problem of opacity enforcement using edit functions in discrete event systems modeled by partially observed deterministic finite automata. A defender uses the edit function as an interface at the output of a system to manipulate actual observations through insertion, substitution, and deletion operations so that the intruder will be prevented from inferring the secret behavior of the system. Unlike existing work which usually assumes that the observation capabilities of the intruder and the defender are identical, we consider a more general setting where they may observe incomparable subsets of events generated by the this http URL characterize whether the defender has the ability to enforce opacity of the system under this setting, the notion of \emph{$ic$-enforceability} is introduced. Then, the opacity enforcement problem is transformed to a two-player game, with imperfect information between the system and the defender, which can be used to determine a feasible decision-making strategy for the defender. Within the game scheme, an edit mechanism is constructed to enumerate all feasible edit actions following system behavior. We further show that an $ic$-enforcing edit function (if one exists) can be synthesized from the edit mechanism to enforce opacity.

Paper number 98:
Title: GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments
Authors: Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara
Abstract: Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT tackles the sparse rewards limitations of online RL algorithms and delivers high-quality solutions in real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior offline and online RL approaches.

Paper number 99:
Title: Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration
Authors: Shigeki Karita, Yuma Koizumi, Heiga Zen, Haruko Ishikawa, Robin Scheibler, Michiel Bacchiani
Abstract: Training data cleaning is a new application for generative model-based speech restoration (SR). This paper introduces Miipher-2, an SR model designed for million-hour scale data, for training data cleaning for large-scale generative models like large language models. Key challenges addressed include generalization to unseen languages, operation without explicit conditioning (e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a frozen, pre-trained Universal Speech Model (USM), supporting over 300 languages, as a robust, conditioning-free feature extractor. To optimize efficiency and minimize memory, Miipher-2 incorporates parallel adapters for predicting clean USM features from noisy inputs and employs the WaveFit neural vocoder for waveform synthesis. These components were trained on 3,000 hours of multi-lingual, studio-quality recordings with augmented degradations, while USM parameters remained fixed. Experimental results demonstrate Miipher-2's superior or comparable performance to conventional SR models in word-error-rate, speaker similarity, and both objective and subjective sound quality scores across all tested languages. Miipher-2 operates efficiently on consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling the processing of a million-hour speech dataset in approximately three days using only 100 such accelerators.
    