
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Automated diagnosis of lung diseases using vision transformer: a comparative study on chest x-ray classification
Authors: Muhammad Ahmad, Sardar Usman, Ildar Batyrshin, Muhammad Muzammil, K. Sajid, M. Hasnain, Muhammad Jalal, Grigori Sidorov
Abstract: Background: Lung disease is a significant health issue, particularly in children and elderly individuals. It often results from lung infections and is one of the leading causes of mortality in children. Globally, lung-related diseases claim many lives each year, making early and accurate diagnoses crucial. Radiographs are valuable tools for the diagnosis of such conditions. The most prevalent lung diseases, including pneumonia, asthma, allergies, chronic obstructive pulmonary disease (COPD), bronchitis, emphysema, and lung cancer, represent significant public health challenges. Early prediction of these conditions is critical, as it allows for the identification of risk factors and implementation of preventive measures to reduce the likelihood of disease onset Methods: In this study, we utilized a dataset comprising 3,475 chest X-ray images sourced from from Mendeley Data provided by Talukder, M. A. (2023) [14], categorized into three classes: normal, lung opacity, and pneumonia. We applied five pre-trained deep learning models, including CNN, ResNet50, DenseNet, CheXNet, and U-Net, as well as two transfer learning algorithms such as Vision Transformer (ViT) and Shifted Window (Swin) to classify these images. This approach aims to address diagnostic issues in lung abnormalities by reducing reliance on human intervention through automated classification systems. Our analysis was conducted in both binary and multiclass settings. Results: In the binary classification, we focused on distinguishing between normal and viral pneumonia cases, whereas in the multi-class classification, all three classes (normal, lung opacity, and viral pneumonia) were included. Our proposed methodology (ViT) achieved remarkable performance, with accuracy rates of 99% for binary classification and 95.25% for multiclass classification.

Paper number 2:
Title: FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG Emotion Recognition
Authors: Haiqi Liu, C. L. Philip Chen, Tong Zhang
Abstract: Cross-subject EEG emotion recognition is challenged by significant inter-subject variability and intricately entangled intra-subject variability. Existing works have primarily addressed these challenges through domain adaptation or generalization strategies. However, they typically require extensive target subject data or demonstrate limited generalization performance to unseen subjects. Recent few-shot learning paradigms attempt to address these limitations but often encounter catastrophic overfitting during subject-specific adaptation with limited samples. This article introduces the few-shot adapter with a cross-view fusion method called FACE for cross-subject EEG emotion recognition, which leverages dynamic multi-view fusion and effective subject-specific adaptation. Specifically, FACE incorporates a cross-view fusion module that dynamically integrates global brain connectivity with localized patterns via subject-specific fusion weights to provide complementary emotional information. Moreover, the few-shot adapter module is proposed to enable rapid adaptation for unseen subjects while reducing overfitting by enhancing adapter structures with meta-learning. Experimental results on three public EEG emotion recognition benchmarks demonstrate FACE's superior generalization performance over state-of-the-art methods. FACE provides a practical solution for cross-subject scenarios with limited labeled data.

Paper number 3:
Title: Foundation Model for Whole-Heart Segmentation: Leveraging Student-Teacher Learning in Multi-Modal Medical Imaging
Authors: Abdul Qayyum, Moona Mazher, Devran Ugurlu, Jose Alonso Solis Lemus, Cristobal Rodero, Steven A Niederer
Abstract: Whole-heart segmentation from CT and MRI scans is crucial for cardiovascular disease analysis, yet existing methods struggle with modality-specific biases and the need for extensive labeled datasets. To address these challenges, we propose a foundation model for whole-heart segmentation using a self-supervised learning (SSL) framework based on a student-teacher architecture. Our model is pretrained on a large, unlabeled dataset of CT and MRI scans, leveraging the xLSTM backbone to capture long-range spatial dependencies and complex anatomical structures in 3D medical images. By incorporating multi-modal pretraining, our approach ensures strong generalization across both CT and MRI modalities, mitigating modality-specific variations and improving segmentation accuracy in diverse clinical settings. The use of large-scale unlabeled data significantly reduces the dependency on manual annotations, enabling robust performance even with limited labeled data. We further introduce an xLSTM-UNet-based architecture for downstream whole-heart segmentation tasks, demonstrating its effectiveness on few-label CT and MRI datasets. Our results validate the robustness and adaptability of the proposed model, highlighting its potential for advancing automated whole-heart segmentation in medical imaging.

Paper number 4:
Title: Learning Beamforming Codebooks for Active Sensing with Reconfigurable Intelligent Surface
Authors: Zhongze Zhang, Wei Yu
Abstract: This paper explores the design of beamforming codebooks for the base station (BS) and for the reconfigurable intelligent surfaces (RISs) in an active sensing scheme for uplink localization, in which the mobile user transmits a sequence of pilots to the BS through reflection at the RISs, and the BS and the RISs are adaptively configured by carefully choosing BS beamforming codeword and RIS codewords from their respective codebooks in a sequential manner to progressively focus onto the user. Most existing codebook designs for RIS are not tailored for active sensing, by which we mean the choice of the next codeword should depend on the measurements made so far, and the sequence of codewords should dynamically focus reflection toward the user. Moreover, most existing codeword selection methods rely on exhaustive search in beam training to identify the codeword with the highest signal-to-noise ratio (SNR), thus incurring substantial pilot overhead as the size of the codebook scales. This paper proposes learning-based approaches for codebook construction and for codeword selection for active sensing. The proposed learning approach aims to locate a target in the service area by recursively selecting a sequence of BS beamforming codewords and RIS codewords from the respective codebooks as more measurements become available without exhaustive beam training. The codebook design and the codeword selection fuse key ideas from the vector quantized-variational autoencoder (VQ-VAE) and the long short-term memory (LSTM) network to learn respectively the discrete function space of the codebook and the temporal dependencies between measurements.

Paper number 5:
Title: 3D Structural Phenotype of the Optic Nerve Head at the Intersection of Glaucoma and Myopia - A Key to Improving Glaucoma Diagnosis in Myopic Populations
Authors: Swati Sharma, Fabian A. Braeu, Thanadet Chuangsuwanich, Tin A. Tun, Quan V Hoang, Rachel Chong, Shamira Perera, Ching-Lin Ho, Rahat Husain, Martin L. Buist, Tin Aung, MichaÃ«l J.A. Girard
Abstract: Purpose: To characterize the 3D structural phenotypes of the optic nerve head (ONH) in patients with glaucoma, high myopia, and concurrent high myopia and glaucoma, and to evaluate their variations across these conditions. Participants: A total of 685 optical coherence tomography (OCT) scans from 754 subjects of Singapore-Chinese ethnicity, including 256 healthy (H), 94 highly myopic (HM), 227 glaucomatous (G), and 108 highly myopic with glaucoma (HMG) cases. Methods: We segmented the retinal and connective tissues from OCT volumes and their boundary edges were converted into 3D point clouds. To classify the 3D point clouds into four ONH conditions, i.e., H, HM, G, and HMG, a specialized ensemble network was developed, consisting of an encoder to transform high-dimensional input data into a compressed latent vector, a decoder to reconstruct point clouds from the latent vector, and a classifier to categorize the point clouds into the four ONH conditions. Results: The classification network achieved high accuracy, distinguishing H, HM, G, and HMG classes with a micro-average AUC of 0.92 $\pm$ 0.03 on an independent test set. The decoder effectively reconstructed point clouds, achieving a Chamfer loss of 0.013 $\pm$ 0.002. Dimensionality reduction clustered ONHs into four distinct groups, revealing structural variations such as changes in retinal and connective tissue thickness, tilting and stretching of the disc and scleral canal opening, and alterations in optic cup morphology, including shallow or deep excavation, across the four conditions. Conclusions: This study demonstrated that ONHs exhibit distinct structural signatures across H, HM, G, and HMG conditions. The findings further indicate that ONH morphology provides sufficient information for classification into distinct clusters, with principal components capturing unique structural patterns within each group.

Paper number 6:
Title: Temporally-Consistent Bilinearly Recurrent Autoencoders for Control Systems
Authors: Ananda Chakrabarti, Indranil Nayak, Debdipta Goswami
Abstract: This paper introduces the temporally-consistent bilinearly recurrent autoencoder (tcBLRAN), a Koopman operator based neural network architecture for modeling a control-affine nonlinear control system. The proposed method extends traditional Koopman autoencoders (KAE) by incorporating bilinear recurrent dynamics that are consistent across predictions, enabling accurate long-term forecasting for control-affine systems. This overcomes the roadblock that KAEs face when encountered with limited and noisy training datasets, resulting in a lack of generalizability due to inconsistency in training data. Through a blend of deep learning and dynamical systems theory, tcBLRAN demonstrates superior performance in capturing complex behaviors and control systems dynamics, providing a superior data-driven modeling technique for control systems and outperforming the state-of-the-art Koopman bilinear form (KBF) learned by autoencoder networks.

Paper number 7:
Title: QSID-MPC: Model Predictive Control with System Identification from Quantized Data
Authors: Shahab Ataei, Dipankar Maity, Debdipta Goswami
Abstract: Least-square system identification is widely used for data-driven model-predictive control (MPC) of unknown or partially known systems. This letter investigates how the system identification and subsequent MPC is affected when the state and input data is quantized. Specifically, we examine the fundamental connection between model error and quantization resolution and how that affects the stability and boundedness of the MPC tracking error. Furthermore, we demonstrate that, with a sufficiently rich dataset, the model error is bounded by a function of quantization resolution and the MPC tracking error is also ultimately bounded similarly. The theory is validated through numerical experiments conducted on two different linear dynamical systems.

Paper number 8:
Title: Joint Sparse Graph for Enhanced MIMO-AFDM Receiver Design
Authors: Qu Luo, Jing Zhu, Zilong Liu, Yanqun Tang, Pei Xiao, Gaojie Chen, Jia Shi
Abstract: Affine frequency division multiplexing (AFDM) is a promising chirp-assisted multicarrier waveform for future high-mobility communications. This paper is devoted to enhanced receiver design for multiple input and multiple output AFDM (MIMO-AFDM) systems. Firstly, we introduce a unified variational inference (VI) approach to approximate the target posterior distribution, under which the belief propagation (BP) and expectation propagation (EP)-based algorithms are derived. As both VI-based detection and low-density parity-check (LDPC) decoding can be expressed by bipartite graphs in MIMO-AFDM systems, we construct a joint sparse graph (JSG) by merging the graphs of these two for low-complexity receiver design. Then, based on this graph model, we present the detailed message propagation of the proposed JSG. Additionally, we propose an enhanced JSG (E-JSG) receiver based on the linear constellation encoding model. The proposed E-JSG eliminates the need for interleavers, de-interleavers, and log-likelihood ratio transformations, thus leading to concurrent detection and decoding over the integrated sparse graph. To further reduce detection complexity, we introduce a sparse channel method by approaximating multiple graph edges with insignificant channel coefficients into a single edge on the VI graph. Simulation results show the superiority of the proposed receivers in terms of computational complexity, detection and decoding latency, and error rate performance compared to the conventional ones.

Paper number 9:
Title: PSO-UNet: Particle Swarm-Optimized U-Net Framework for Precise Multimodal Brain Tumor Segmentation
Authors: Shoffan Saifullah, RafaÅ DreÅ¼ewski
Abstract: Medical image segmentation, particularly for brain tumor analysis, demands precise and computationally efficient models due to the complexity of multimodal MRI datasets and diverse tumor morphologies. This study introduces PSO-UNet, which integrates Particle Swarm Optimization (PSO) with the U-Net architecture for dynamic hyperparameter optimization. Unlike traditional manual tuning or alternative optimization approaches, PSO effectively navigates complex hyperparameter search spaces, explicitly optimizing the number of filters, kernel size, and learning rate. PSO-UNet substantially enhances segmentation performance, achieving Dice Similarity Coefficients (DSC) of 0.9578 and 0.9523 and Intersection over Union (IoU) scores of 0.9194 and 0.9097 on the BraTS 2021 and Figshare datasets, respectively. Moreover, the method reduces computational complexity significantly, utilizing only 7.8 million parameters and executing in approximately 906 seconds, markedly faster than comparable U-Net-based frameworks. These outcomes underscore PSO-UNet's robust generalization capabilities across diverse MRI modalities and tumor classifications, emphasizing its clinical potential and clear advantages over conventional hyperparameter tuning methods. Future research will explore hybrid optimization strategies and validate the framework against other bio-inspired algorithms to enhance its robustness and scalability.

Paper number 10:
Title: Pitch Contour Exploration Across Audio Domains: A Vision-Based Transfer Learning Approach
Authors: Jakob AbeÃer, Simon SchwÃ¤r, Meinard MÃ¼ller
Abstract: This study examines pitch contours as a unifying semantic construct prevalent across various audio domains including music, speech, bioacoustics, and everyday sounds. Analyzing pitch contours offers insights into the universal role of pitch in the perceptual processing of audio signals and contributes to a deeper understanding of auditory mechanisms in both humans and animals. Conventional pitch-tracking methods, while optimized for music and speech, face challenges in handling much broader frequency ranges and more rapid pitch variations found in other audio domains. This study introduces a vision-based approach to pitch contour analysis that eliminates the need for explicit pitch-tracking. The approach uses a convolutional neural network, pre-trained for object detection in natural images and fine-tuned with a dataset of synthetically generated pitch contours, to extract key contour parameters from the time-frequency representation of short audio segments. A diverse set of eight downstream tasks from four audio domains were selected to provide a challenging evaluation scenario for cross-domain pitch contour analysis. The results show that the proposed method consistently surpasses traditional techniques based on pitch-tracking on a wide range of tasks. This suggests that the vision-based approach establishes a foundation for comparative studies of pitch contour characteristics across diverse audio domains.

Paper number 11:
Title: Insights into the explainability of Lasso-based DeePC for nonlinear systems
Authors: Gianluca Giacomelli, Simone Formentin, Victor G. Lopez, Matthias A. MÃ¼ller, Valentina Breschi
Abstract: Data-enabled Predictive Control (DeePC) has recently gained the spotlight as an easy-to-use control technique that allows for constraint handling while relying on raw data only. Initially proposed for linear time-invariant systems, several DeePC extensions are now available to cope with nonlinear systems. Nonetheless, these solutions mainly focus on ensuring the controller's effectiveness, overlooking the explainability of the final result. As a step toward explaining the outcome of DeePC for the control of nonlinear systems, in this paper, we focus on analyzing the earliest and simplest DeePC approach proposed to cope with nonlinearities in the controlled system, using a Lasso regularization. Our theoretical analysis highlights that the decisions undertaken by DeePC with Lasso regularization are unexplainable, as control actions are determined by data incoherent with the system's local behavior. This result is true even when the available input/output samples are grouped according to the different operating conditions explored during data collection. This result is confirmed by our numerical study, which highlights the benefits of data grouping in terms of performance while showing that explainability remains a challenge in control design via DeePC.

Paper number 12:
Title: Bridging the Sim-to-real Gap: A Control Framework for Imitation Learning of Model Predictive Control
Authors: Seungtaek Kim, Jonghyup Lee, Kyoungseok Han, Seibum B. Choi
Abstract: To address the computational challenges of Model Predictive Control (MPC), recent research has studied on using Deep Neural Networks (DNNs) trained through imitation learning to approximate the MPC. However, this introduces a common issue in learning-based control: the simulation-to-reality (sim-to-real) gap. Therefore, Domain Randomization (DR) has been widely used to mitigate this gap by introducing perturbations in the source domain. However, this led to low data collection efficiency and an overly conservative control strategy. This study proposes a new control framework that deals with this issue from a control perspective inspired by Robust Tube MPC. The framework ensures the DNN operates in the same environment as the source domain, handling the sim-to-real gap with great data collection efficiency. Moreover, a parameter governor is introduced to address the DNN's inability to adapt to model parameter variations, enabling the system to satisfy MPC constraints more robustly under changing conditions. The proposed framework was validated through a cart-pole system case study compared by DR baselines, demonstrating that a single MPC-demonstrated trajectory in the source domain was sufficient for controlling the cart-pole in the target domain. Furthermore, the system effectively handled model parameter variations, allowing for a less conservative control.

Paper number 13:
Title: $L^2$FMamba: Lightweight Light Field Image Super-Resolution with State Space Model
Authors: Zeqiang Wei, Kai Jin, Zeyi Hou, Kuan Song, Xiuzhuang Zhou
Abstract: Transformers bring significantly improved performance to the light field image super-resolution task due to their long-range dependency modeling capability. However, the inherently high computational complexity of their core self-attention mechanism has increasingly hindered their advancement in this task. To address this issue, we first introduce the LF-VSSM block, a novel module inspired by progressive feature extraction, to efficiently capture critical long-range spatial-angular dependencies in light field images. LF-VSSM successively extracts spatial features within sub-aperture images, spatial-angular features between sub-aperture images, and spatial-angular features between light field image pixels. On this basis, we propose a lightweight network, $L^2$FMamba (Lightweight Light Field Mamba), which integrates the LF-VSSM block to leverage light field features for super-resolution tasks while overcoming the computational challenges of Transformer-based approaches. Extensive experiments on multiple light field datasets demonstrate that our method reduces the number of parameters and complexity while achieving superior super-resolution performance with faster inference speed.

Paper number 14:
Title: A Framework for Predicting Runtime Savings from Discrete-Event Simulation Model Simplification Operations
Authors: Mohd Shoaib, Navonil Mustafee, Varun Ramamohan
Abstract: Abstraction or substitution and aggregation are the most widely used simulation model simplification operations. Abstraction involves replacing subsystems within a discrete-event simulation (DES) with one or more quantities - typically random variables - representing the lengths of stay in the subsystems(s) in question to create a `simplified' system comprising only of subsystems of interest to the analysis at hand. Aggregation involves replacing more than one subsystem of the original `parent' simulation with a single subsystem. However, the model simplification process itself can be expensive, in terms of the computational runtime and effort required to collect the data required to estimate the distributions of the length of stay variables, the distribution-fitting process, and testing and validation of the simplified model. Moreover, the savings in simulation runtime that the simplification process yields is \textit{a priori} unknown to the modeller. In this context, a method that predicts the runtime savings (RS) from DES model simplification operations before their execution - at the conceptualisation stage of the simplified model development process - may help judge whether its development is indeed worth undertaking. In this paper, we present a queueing-theoretic framework for the prediction of RS from model simplification operations. Our framework is applicable for DES models comprising $M/M/, M/G/ \text{ and } G/G/$ subsystems. The performance of the RS prediction framework is demonstrated using multiple computational experiments. Our proposed framework contributes to the literature around DES model complexity and more broadly to DES runtime prediction.

Paper number 15:
Title: Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT
Authors: Xiaoqing Zhang, Hanfeng Shi, Xiangyu Li, Haili Ye, Tao Xu, Na Li, Yan Hu, Fan Lv, Jiangfan Chen, Jiang Liu
Abstract: Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally. The eye's retina is an extension of the brain and has great potential in PD screening. Recent studies have suggested that texture features extracted from retinal layers can be adopted as biomarkers for PD diagnosis under optical coherence tomography (OCT) images. Frequency domain learning techniques can enhance the feature representations of deep neural networks (DNNs) by decomposing frequency components involving rich texture features. Additionally, previous works have not exploited texture features for automated PD screening in OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet Filter (AWF) that serves as the Practical Texture Feature Amplifier to fully leverage the merits of texture features to boost the PD screening performance of DNNs with the aid of frequency domain learning. Specifically, AWF first enhances texture feature representation diversities via channel mixer, then emphasizes informative texture feature representations with the well-designed adaptive wavelet filtering token mixer. By combining the AWFs with the DNN stem, AWFNet is constructed for automated PD screening. Additionally, we introduce a novel Balanced Confidence (BC) Loss by mining the potential of sample-wise predicted probabilities of all classes and class frequency prior, to further boost the PD screening performance and trustworthiness of AWFNet. The extensive experiments manifest the superiority of our AWFNet and BC over state-of-the-art methods in terms of PD screening performance and trustworthiness.

Paper number 16:
Title: Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection
Authors: Yongting Hu, Yuxin Lin, Chengliang Liu, Xiaoling Luo, Xiaoyan Dou, Qihao Xu, Yong Xu
Abstract: Multi-view diabetic retinopathy (DR) detection has recently emerged as a promising method to address the issue of incomplete lesions faced by single-view DR. However, it is still challenging due to the variable sizes and scattered locations of lesions. Furthermore, existing multi-view DR methods typically merge multiple views without considering the correlations and redundancies of lesion information across them. Therefore, we propose a novel method to overcome the challenges of difficult lesion information learning and inadequate multi-view fusion. Specifically, we introduce a two-branch network to obtain both local lesion features and their global dependencies. The high-frequency component of the wavelet transform is used to exploit lesion edge information, which is then enhanced by global semantic to facilitate difficult lesion learning. Additionally, we present a cross-view fusion module to improve multi-view fusion and reduce redundancy. Experimental results on large public datasets demonstrate the effectiveness of our method. The code is open sourced on this https URL.

Paper number 17:
Title: Optimal Parameter Adaptation for Safety-Critical Control via Safe Barrier Bayesian Optimization
Authors: Shengbo Wang, Ke Li, Zheng Yan, Zhenyuan Guo, Song Zhu, Guanghui Wen, Shiping Wen
Abstract: Safety is of paramount importance in control systems to avoid costly risks and catastrophic damages. The control barrier function (CBF) method, a promising solution for safety-critical control, poses a new challenge of enhancing control performance due to its direct modification of original control design and the introduction of uncalibrated parameters. In this work, we shed light on the crucial role of configurable parameters in the CBF method for performance enhancement with a systematical categorization. Based on that, we propose a novel framework combining the CBF method with Bayesian optimization (BO) to optimize the safe control performance. Considering feasibility/safety-critical constraints, we develop a safe version of BO using the barrier-based interior method to efficiently search for promising feasible configurable parameters. Furthermore, we provide theoretical criteria of our framework regarding safety and optimality. An essential advantage of our framework lies in that it can work in model-agnostic environments, leaving sufficient flexibility in designing objective and constraint functions. Finally, simulation experiments on swing-up control and high-fidelity adaptive cruise control are conducted to demonstrate the effectiveness of our framework.

Paper number 18:
Title: Parameter Design for Secure Affine Frequency Division Multiplexing Waveform
Authors: Zhang Di, Wang Zeyin, Tang Yanqun, Wu Dongdong, Yuan Muzi
Abstract: The secure affine frequency division multiplexing (AFDM) waveform design is a main concern in high-mobility networks. In this article, we employ the four key parameters in AFDM to design secure waveforms, and afterward we analyze the role of the four parameters to reveal the design guideline. We find that c1 is bounded by the Doppler shifts and preset guard. The parameter c2 exhibits a minimum periodicity of 1, with the effective range [0, 1] rather than any real number. The adjustable parameters c1 and c2 introduce additional degrees of freedom to the AFDM waveform, thereby enhancing the anti-eavesdropping performance. In addition, excessive Lmax that determines the preset guard interval leads to a security-risk interval and poses eavesdropping risks. Therefore, the optimal Lmax equals maximum delay. Numerical simulations verify the accuracy of our analysis.

Paper number 19:
Title: RIS-Assisted Passive Localization (RAPL): An Efficient Zero-Overhead Framework Using Conditional Sample Mean
Authors: Jiawei Yao, Yijie Mao, Mingzhe Chen, Ye Hu
Abstract: Reconfigurable Intelligent Surface (RIS) has been recognized as a promising solution for enhancing localization accuracy. Traditional RIS-based localization methods typically rely on prior channel knowledge, beam scanning, and pilot-based assistance. These approaches often result in substantial energy and computational overhead, and require real-time coordination between the base station (BS) and the RIS. To address these challenges, in this work, we move beyond conventional methods and introduce a novel data-driven, multiple RISs-assisted passive localization approach (RAPL). The proposed method includes two stages, the angle-of-directions (AoDs) between the RISs and the user is estimated by using the conditional sample mean in the first stage, and then the user's position is determined based on the estimated multiple AoD pairs in the second stage. This approach only utilizes the existing communication signals between the user and the BS, relying solely on the measurement of received signal power at each BS antenna for a set of randomly generated phase shifts across all RISs. Moreover, by obviating the need for real-time RIS phase shift optimization or user-to-BS pilot transmissions, the method introduces no additional communication overhead, making it highly suitable for deployment in real-world networks. The proposed scheme is then extended to multi-RIS scenarios considering both parallel and cascaded RIS topologies. Numerical results show that the proposed RAPL improves localization accuracy while significantly reducing energy and signaling overhead compared to conventional methods.

Paper number 20:
Title: ASP-VMUNet: Atrous Shifted Parallel Vision Mamba U-Net for Skin Lesion Segmentation
Authors: Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Qi Zhao, Changyu Zeng, Wenpei Bai, Guangliang Cheng
Abstract: Skin lesion segmentation is a critical challenge in computer vision, and it is essential to separate pathological features from healthy skin for diagnostics accurately. Traditional Convolutional Neural Networks (CNNs) are limited by narrow receptive fields, and Transformers face significant computational burdens. This paper presents a novel skin lesion segmentation framework, the Atrous Shifted Parallel Vision Mamba UNet (ASP-VMUNet), which integrates the efficient and scalable Mamba architecture to overcome limitations in traditional CNNs and computationally demanding Transformers. The framework introduces an atrous scan technique that minimizes background interference and expands the receptive field, enhancing Mamba's scanning capabilities. Additionally, the inclusion of a Parallel Vision Mamba (PVM) layer and a shift round operation optimizes feature segmentation and fosters rich inter-segment information exchange. A supplementary CNN branch with a Selective-Kernel (SK) Block further refines the segmentation by blending local and global contextual information. Tested on four benchmark datasets (ISIC16/17/18 and PH2), ASP-VMUNet demonstrates superior performance in skin lesion segmentation, validated by comprehensive ablation studies. This approach not only advances medical image segmentation but also highlights the benefits of hybrid architectures in medical imaging technology. Our code is available at this https URL.

Paper number 21:
Title: Iterative Learning Predictive Control for Constrained Uncertain Systems
Authors: Riccardo Zuliani, Efe C. Balta, Alisa Rupenyan, John Lygeros
Abstract: Iterative learning control (ILC) improves the performance of a repetitive system by learning from previous trials. ILC can be combined with Model Predictive Control (MPC) to mitigate non-repetitive disturbances, thus improving overall system performance. However, existing approaches either assume perfect model knowledge or fail to actively learn system uncertainties, leading to conservativeness. To address these limitations we propose a binary mixed-integer ILC scheme, combined with a convex MPC scheme, that ensures robust constraint satisfaction, non-increasing nominal cost, and convergence to optimal performance. Our scheme is designed for uncertain nonlinear systems subject to both bounded additive stochastic noise and additive uncertain components. We showcase the benefits of our scheme in simulation.

Paper number 22:
Title: A multiobjective approach to robust predictive control barrier functions for discrete-time systems
Authors: Alexandre Didier, Melanie N. Zeilinger
Abstract: We present an optimisation-based approach to ensure robust asymptotic stability stability of a desired set in the state space of nonlinear dynamical systems, while optimising a general control objective. The approach relies on the decrease of a robust predictive control barrier function (PCBF), which is defined as the optimal value function of a slack minimisation problem with respect to the target set. We show continuity of the proposed robust PCBF, allowing the introduction of a decrease constraint in the control objective minimisation. The PCBF decrease is given with respect to a warmstart value based on a feasible solution at the prior time step. Thereby, the control objective can be optimised while ensuring robust asymptotic stability of the target set. We demonstrate the effectiveness of the proposed formulation on a linear space rendezvous and nonlinear lane changing problem.

Paper number 23:
Title: TFIC: End-to-End Text-Focused Image Compression for Coding for Machines
Authors: Stefano Della Fiore, Alessandro Gnutti, Marco Dalai, Pierangelo Migliorati, Riccardo Leonardi
Abstract: Traditional image compression methods aim to faithfully reconstruct images for human perception. In contrast, Coding for Machines focuses on compressing images to preserve information relevant to a specific machine task. In this paper, we present an image compression system designed to retain text-specific features for subsequent Optical Character Recognition (OCR). Our encoding process requires half the time needed by the OCR module, making it especially suitable for devices with limited computational capacity. In scenarios where on-device OCR is computationally prohibitive, images are compressed and later processed to recover the text content. Experimental results demonstrate that our method achieves significant improvements in text extraction accuracy at low bitrates, even improving over the accuracy of OCR performed on uncompressed images, thus acting as a local pre-processing step.

Paper number 24:
Title: Single-Step Latent Consistency Model for Remote Sensing Image Super-Resolution
Authors: Xiaohui Sun, Jiangwei Mo, Hanlin Wu, Jie Ma
Abstract: Recent advancements in diffusion models (DMs) have greatly advanced remote sensing image super-resolution (RSISR). However, their iterative sampling processes often result in slow inference speeds, limiting their application in real-time tasks. To address this challenge, we propose the latent consistency model for super-resolution (LCMSR), a novel single-step diffusion approach designed to enhance both efficiency and visual quality in RSISR tasks. Our proposal is structured into two distinct stages. In the first stage, we pretrain a residual autoencoder to encode the differential information between high-resolution (HR) and low-resolution (LR) images, transitioning the diffusion process into a latent space to reduce computational costs. The second stage focuses on consistency diffusion learning, which aims to learn the distribution of residual encodings in the latent space, conditioned on LR images. The consistency constraint enforces that predictions at any two timesteps along the reverse diffusion trajectory remain consistent, enabling direct mapping from noise to data. As a result, the proposed LCMSR reduces the iterative steps of traditional diffusion models from 50-1000 or more to just a single step, significantly improving efficiency. Experimental results demonstrate that LCMSR effectively balances efficiency and performance, achieving inference times comparable to non-diffusion models while maintaining high-quality output.

Paper number 25:
Title: On the Completeness and Ordering of Path-Complete Barrier Functions
Authors: Mahathi Anand, RaphaÃ«l Jungers, Majid Zamani, Frank AllgÃ¶wer
Abstract: This paper is concerned with path-complete barrier functions which offer a graph-based methodology for verifying safety properties in switched systems. The path-complete framework leverages algebraic (barrier functions) as well as combinatorial (graph) components to characterize a set of safety conditions for switched systems, thus offering high flexibility (two degrees of freedom) in searching for suitable safety certificates. In this paper, we do not propose any new safety criteria. Instead, we further investigate the role that the combinatorial component plays in the safety verification problem. First, we prove that path-completeness, which is a property on a graph that describes the switching sequences, is necessary to obtain a set of valid safety conditions. As a result, the path-complete framework is able to provide a complete characterization of safety conditions for switched systems. Furthermore, we provide a systematic methodology for comparing two path-complete graphs and the conservatism associated with the resulting safety conditions. Specifically, we prove that under some conditions, such as when there exists a simulation relation between two path-complete graphs, it is possible to conclude that one graph is always able to provide less conservative safety conditions than another, independent of the algebraic properties of the switched system and the template of the barrier function under consideration. Such a result paves the way for a systematic use of the path-complete frame- work with barrier functions, as one can then consistently choose the appropriate graph that provides less conservative safety conditions.

Paper number 26:
Title: Prompt-Guided Dual-Path UNet with Mamba for Medical Image Segmentation
Authors: Shaolei Zhang, Jinyan Liu, Tianyi Qian, Xuesong Li
Abstract: Convolutional neural networks (CNNs) and transformers are widely employed in constructing UNet architectures for medical image segmentation tasks. However, CNNs struggle to model long-range dependencies, while transformers suffer from quadratic computational complexity. Recently, Mamba, a type of State Space Models, has gained attention for its exceptional ability to model long-range interactions while maintaining linear computational complexity. Despite the emergence of several Mamba-based methods, they still present the following limitations: first, their network designs generally lack perceptual capabilities for the original input data; second, they primarily focus on capturing global information, while often neglecting local details. To address these challenges, we propose a prompt-guided CNN-Mamba dual-path UNet, termed PGM-UNet, for medical image segmentation. Specifically, we introduce a prompt-guided residual Mamba module that adaptively extracts dynamic visual prompts from the original input data, effectively guiding Mamba in capturing global information. Additionally, we design a local-global information fusion network, comprising a local information extraction module, a prompt-guided residual Mamba module, and a multi-focus attention fusion module, which effectively integrates local and global information. Furthermore, inspired by Kolmogorov-Arnold Networks (KANs), we develop a multi-scale information extraction module to capture richer contextual information without altering the resolution. We conduct extensive experiments on the ISIC-2017, ISIC-2018, DIAS, and DRIVE. The results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in multiple medical image segmentation tasks.

Paper number 27:
Title: Iterative Decoder of Channel-polarized Multilevel Coding for Data Center Networks
Authors: Takeshi Kakizaki, Masanori Nakamura, Fukutaro Hamaoka, Shuto Yamamoto
Abstract: Data center networks (DCNs) require a low-cost, low-power optical transceiver to handle increased traffic from generative artificial intelligence, video streaming services, and more. Improving the required signal-to-noise ratio (RSNR) by digital signal processing such as forward error correction (FEC) mitigates the requirements for electrical and optical components. The optical transceivers in DCNs exploit a low-complexity soft-decision (SD) FEC, consisting of short block-length linear error-correcting codes and a low-complexity SD decoder (SDD), such as a Chase decoder and ordered statistical decoding. The low complexity SDD efficiently approaches a maximum likelihood decoding (MLD). However, the decoding performance of MLD is limited by its finite block length. In this paper, we describe the detail of our proposed channel-polarized multilevel coding with iterative decoding (CP-MLC-ID), which improves the decoding performance. The 19.5$\%$-OH CP-MLC-ID 128-bit extended Bose-Chaudhuri-Hocquenghem (eBCH) and KP4 codes outperform the concatenated eBCH and KP4 codes with a net coding gain of 0.25 and 0.40 dB for the same and double the number of SDDs, respectively. We also investigate the dependency of the decoding performance on the size of a bit interleaver. The performance degradation of CP-MLC-ID using an 8-bit interleaver is about 0.1 dB compared to using the large-bit interleaver. Our results indicate that even a weak connection by exclusive-OR between codewords improves the decoding performance, compared to simple concatenated codes in the DCNs.

Paper number 28:
Title: GIViC: Generative Implicit Video Compression
Authors: Ge Gao, Siyue Teng, Tianhao Peng, Fan Zhang, David Bull
Abstract: While video compression based on implicit neural representations (INRs) has recently demonstrated great potential, existing INR-based video codecs still cannot achieve state-of-the-art (SOTA) performance compared to their conventional or autoencoder-based counterparts given the same coding configuration. In this context, we propose a Generative Implicit Video Compression framework, GIViC, aiming at advancing the performance limits of this type of coding methods. GIViC is inspired by the characteristics that INRs share with large language and diffusion models in exploiting long-term dependencies. Through the newly designed implicit diffusion process, GIViC performs diffusive sampling across coarse-to-fine spatiotemporal decompositions, gradually progressing from coarser-grained full-sequence diffusion to finer-grained per-token diffusion. A novel Hierarchical Gated Linear Attention-based transformer (HGLA), is also integrated into the framework, which dual-factorizes global dependency modeling along scale and sequential axes. The proposed GIViC model has been benchmarked against SOTA conventional and neural codecs using a Random Access (RA) configuration (YUV 4:2:0, GOPSize=32), and yields BD-rate savings of 15.94%, 22.46% and 8.52% over VVC VTM, DCVC-FM and NVRC, respectively. As far as we are aware, GIViC is the first INR-based video codec that outperforms VTM based on the RA coding configuration. The source code will be made available.

Paper number 29:
Title: Single Shot AI-assisted quantification of KI-67 proliferation index in breast cancer
Authors: Deepti Madurai Muthu, Priyanka S, Lalitha Rani N, P. G. Kubendran Amos
Abstract: Reliable quantification of Ki-67, a key proliferation marker in breast cancer, is essential for molecular subtyping and informed treatment planning. Conventional approaches, including visual estimation and manual counting, suffer from interobserver variability and limited reproducibility. This study introduces an AI-assisted method using the YOLOv8 object detection framework for automated Ki-67 scoring. High-resolution digital images (40x magnification) of immunohistochemically stained tumor sections were captured from Ki-67 hotspot regions and manually annotated by a domain expert to distinguish Ki-67-positive and negative tumor cells. The dataset was augmented and divided into training (80%), validation (10%), and testing (10%) subsets. Among the YOLOv8 variants tested, the Medium model achieved the highest performance, with a mean Average Precision at 50% Intersection over Union (mAP50) exceeding 85% for Ki-67-positive cells. The proposed approach offers an efficient, scalable, and objective alternative to conventional scoring methods, supporting greater consistency in Ki-67 evaluation. Future directions include developing user-friendly clinical interfaces and expanding to multi-institutional datasets to enhance generalizability and facilitate broader adoption in diagnostic practice.

Paper number 30:
Title: Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components
Authors: Yuanyuan Zhang, Sijie Xiong, Rui Yang, EngGee Lim, Yutao Yue
Abstract: Radar-based wellness monitoring is becoming an effective measurement to provide accurate vital signs in a contactless manner, but data scarcity retards the related research on deep-learning-based methods. Data augmentation is commonly used to enrich the dataset by modifying the existing data, but most augmentation techniques can only couple with classification tasks. To enable the augmentation for regression tasks, this research proposes a spectrogram augmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g., heartbeat detection, electrocardiogram reconstruction) with both classification and regression tasks involved. The proposed method is designed to increase the diversity of input samples while the augmented spectrogram is still faithful to the original ground truth vital sign. In addition, Horcrux proposes to inject zero values in specific areas to enhance the awareness of the deep learning model on subtle cardiac features, improving the performance for the limited dataset. Experimental result shows that Horcrux achieves an overall improvement of 16.20% in cardiac monitoring and has the potential to be extended to other spectrogram-based tasks. The code will be released upon publication.

Paper number 31:
Title: InterSliceBoost: Identifying Tissue Layers in Three-dimensional Ultrasound Images for Chronic Lower Back Pain (cLBP) Assessment
Authors: Zixue Zeng, Matthew Cartier, Xiaoyan Zhao, Pengyu Chen, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison C. Bean, Ryan P. Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Kang Kim, Ajay D. Wasan, Jiantao Pu
Abstract: Available studies on chronic lower back pain (cLBP) typically focus on one or a few specific tissues rather than conducting a comprehensive layer-by-layer analysis. Since three-dimensional (3-D) images often contain hundreds of slices, manual annotation of these anatomical structures is both time-consuming and error-prone. We aim to develop and validate a novel approach called InterSliceBoost to enable the training of a segmentation model on a partially annotated dataset without compromising segmentation performance. The architecture of InterSliceBoost includes two components: an inter-slice generator and a segmentation model. The generator utilizes residual block-based encoders to extract features from adjacent image-mask pairs (IMPs). Differential features are calculated and input into a decoder to generate inter-slice IMPs. The segmentation model is trained on partially annotated datasets (e.g., skipping 1, 2, 3, or 7 images) and the generated inter-slice IMPs. To validate the performance of InterSliceBoost, we utilized a dataset of 76 B-mode ultrasound scans acquired on 29 subjects enrolled in an ongoing cLBP study. InterSliceBoost, trained on only 33% of the image slices, achieved a mean Dice coefficient of 80.84% across all six layers on the independent test set, with Dice coefficients of 73.48%, 61.11%, 81.87%, 95.74%, 83.52% and 88.74% for segmenting dermis, superficial fat, superficial fascial membrane, deep fat, deep fascial membrane, and muscle. This performance is significantly higher than the conventional model trained on fully annotated images (p<0.05). InterSliceBoost can effectively segment the six tissue layers depicted on 3-D B-model ultrasound images in settings with partial annotations.

Paper number 32:
Title: GRN+: A Simplified Generative Reinforcement Network for Tissue Layer Analysis in 3D Ultrasound Images for Chronic Low-back Pain
Authors: Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Xin Meng, Jiantao Pu
Abstract: 3D ultrasound delivers high-resolution, real-time images of soft tissues, which is essential for pain research. However, manually distinguishing various tissues for quantitative analysis is labor-intensive. To streamline this process, we developed and validated GRN+, a novel multi-model framework that automates layer segmentation with minimal annotated data. GRN+ combines a ResNet-based generator and a U-Net segmentation model. Through a method called Segmentation-guided Enhancement (SGE), the generator produces new images and matching masks under the guidance of the segmentation model, with its weights adjusted according to the segmentation loss gradient. To prevent gradient explosion and secure stable training, a two-stage backpropagation strategy was implemented: the first stage propagates the segmentation loss through both the generator and segmentation model, while the second stage concentrates on optimizing the segmentation model alone, thereby refining mask prediction using the generated images. Tested on 69 fully annotated 3D ultrasound scans from 29 subjects with six manually labeled tissue layers, GRN+ outperformed all other semi-supervised methods in terms of the Dice coefficient using only 5% labeled data, despite not using unlabeled data for unsupervised training. Additionally, when applied to fully annotated datasets, GRN+ with SGE achieved a 2.16% higher Dice coefficient while incurring lower computational costs compared to other models. Overall, GRN+ provides accurate tissue segmentation while reducing both computational expenses and the dependency on extensive annotations, making it an effective tool for 3D ultrasound analysis in cLBP patients.

Paper number 33:
Title: Optimal Safe Sequencing and Motion Control for Mixed Traffic Roundabouts
Authors: Yingqing Chen, Christos G. Cassandras
Abstract: This paper develops an Optimal Safe Sequencing (OSS) control framework for Connected and Automated Vehicles (CAVs) navigating a single-lane roundabout in mixed traffic, where both CAVs and Human-Driven Vehicles (HDVs) coexist. The framework jointly optimizes vehicle sequencing and motion control to minimize travel time, energy consumption, and discomfort while ensuring speed-dependent safety guarantees and adhering to velocity and acceleration constraints. This is achieved by integrating (a) a Safe Sequencing (SS) policy that ensures merging safety without requiring any knowledge of HDV behavior, and (b) a Model Predictive Control with Control Lyapunov Barrier Functions (MPC-CLBF) framework, which optimizes CAV motion control while mitigating infeasibility and myopic control issues common in the use of Control Barrier Functions (CBFs) to provide safety guarantees. Simulation results across various traffic demands, CAV penetration rates, and control parameters demonstrate the framework's effectiveness and stability.

Paper number 34:
Title: A Systematic Review of EEG-based Machine Intelligence Algorithms for Depression Diagnosis, and Monitoring
Authors: Amir Nassibi, Christos Papavassiliou, Ildar Rakhmatulin, Danilo Mandic, S. Farokh Atashzar
Abstract: Depression disorder is a serious health condition that has affected the lives of millions of people around the world. Diagnosis of depression is a challenging practice that relies heavily on subjective studies and, in most cases, suffers from late findings. Electroencephalography (EEG) biomarkers have been suggested and investigated in recent years as a potential transformative objective practice. In this article, for the first time, a detailed systematic review of EEG-based depression diagnosis approaches is conducted using advanced machine learning techniques and statistical analyses. For this, 938 potentially relevant articles (since 1985) were initially detected and filtered into 139 relevant articles based on the review scheme 'preferred reporting items for systematic reviews and meta-analyses (PRISMA).' This article compares and discusses the selected articles and categorizes them according to the type of machine learning techniques and statistical analyses. Algorithms, preprocessing techniques, extracted features, and data acquisition systems are discussed and summarized. This review paper explains the existing challenges of the current algorithms and sheds light on the future direction of the field. This systematic review outlines the issues and challenges in machine intelligence for the diagnosis of EEG depression that can be addressed in future studies and possibly in future wearable technologies.

Paper number 35:
Title: Nordic perspective on System Integrity Protection Schemes in relation to capacity allocation
Authors: Gabriel Malmer, Arvid Rolander, Emil Hillberg, Olof Samuelsson, Susanne Ackeby, Lars NordstrÃ¶m
Abstract: The urgent need to address climate change prompts societies worldwide to adopt carbon neutral energy and electrification. To facilitate this, a range of technologies and policies will be needed. Alternatives to traditional power grid reinforcement, such as grid-enhancing technologies and system automation, are particularly attractive due to their potentially low cost and fast deployment time. One alternative is System Integrity Protection Schemes (SIPS) - automatic and curative remedial actions (RAs) which can boost grid transfer capacities without compromising with reliability since they can act faster than manual control. The use of SIPS however is scattered, with limited coordination between countries, and the full potential of using SIPS for capacity enhancement is not yet realized. The aim of this paper is to provide a case study and comparison of SIPS in the Nordic countries, particularly in relation to capacity allocation. It also seeks to harmonize terminology relating to ancillary services, RAs, and SIPS. Finally, it examines and compares the inclusion of RAs and SIPS in different Capacity Calculation Methodologies (CCMs). In both main EU CCMs - Net Transfer Capacity (NTC) and Flow-Based (FB) - RAs play a pronounced role. The paper is based on a survey and interviews with Nordic stakeholders, along with a literature review and analysis of public data. The results indicate a large variation in SIPS use across the Nordics. Regarding terminology, we suggest that SIPS is a subcategory of RAs which overlaps with ancillary services. Concerning CCMs, NTC is unable to fully represent capacity constraints in meshed AC systems, which in turn hinders systematic capacity enhancement using RAs. FB on the other hand explicitly includes RAs in the capacity domain. A lower bound for the economic value of RAs can be calculated, amounting to 11.5 million EUR in the Nordics in Nov and Dec 2024.

Paper number 36:
Title: Unpaired Translation of Chest X-ray Images for Lung Opacity Diagnosis via Adaptive Activation Masks and Cross-Domain Alignment
Authors: Junzhi Ning, Dominic Marshall, Yijian Gao, Xiaodan Xing Yang Nan, Yingying Fang, Sheng Zhang, Matthieu Komorowski, Guang Yang
Abstract: Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and monitoring cardiopulmonary diseases. However, lung opac- ities in CXRs frequently obscure anatomical structures, impeding clear identification of lung borders and complicating the localization of pathology. This challenge significantly hampers segmentation accuracy and precise lesion identification, which are crucial for diagnosis. To tackle these issues, our study proposes an unpaired CXR translation framework that converts CXRs with lung opacities into counterparts without lung opacities while preserving semantic features. Central to our approach is the use of adaptive activation masks to selectively modify opacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs without opacity issues align with feature maps and prediction labels from a pre-trained CXR lesion classifier, facilitating the interpretability of the translation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT datasets, demonstrating superior translation quality through lower Frechet Inception Distance (FID) and Kernel Inception Distance (KID) scores compared to existing meth- ods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation on RSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs and JSRT CXRs show our method enhances segmentation accuracy of lung borders and improves lesion classification, further underscoring its potential in clinical settings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs. 77.03%; MIMIC ARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%; JSRT: mIoU: 91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach advances CXR imaging analysis, especially in investigating segmentation impacts through image translation techniques.

Paper number 37:
Title: A Spectrum-based Filter Design for Periodic Control of Systems with Time Delay
Authors: Can Kutlu YÃ¼ksel, TomÃ¡Å¡ VyhlÃ­dal, Jaroslav BuÅ¡ek, Martin HromÄÃ­k, Silviu-Iulian Niculescu
Abstract: A fully analytical controller design is proposed to tackle a periodic control problem for stable linear systems with an input delay. Applying the internal model control scheme, the controller design reduces to designing a filter, which is done through the placement of poles and zeros. The zeros are placed to compensate for the harmonics and to achieve the desired degree of properness for the filter. For placing the poles, a quasi-optimal procedure is proposed utilizing the standard LQR method. Given the high-dimensionality of the filter due to targeting a large number of harmonics, the design, as well as controller implementation, is performed over a state-space representation. A thorough experimental case study is included to demonstrate both the practical feasibility and effectiveness of the proposed control design. The experimental validation is performed on a physical system, the goal of which is to reject periodic vibrations acting on a mass-spring-damper setup where the sensor and the actuator are non-collocated.

Paper number 38:
Title: Collaborative Satisfaction of Long-Term Spatial Constraints in Multi-Agent Systems: A Distributed Optimization Approach (extended version)
Authors: Farhad Mehdifar, Mani H. Dhullipalla, Charalampos P. Bechlioulis, Dimos V. Dimarogonas
Abstract: This paper addresses the problem of collaboratively satisfying long-term spatial constraints in multi-agent systems. Each agent is subject to spatial constraints, expressed as inequalities, which may depend on the positions of other agents with whom they may or may not have direct communication. These constraints need to be satisfied asymptotically or after an unknown finite time. The agents' objective is to collectively achieve a formation that fulfills all constraints. The problem is initially framed as a centralized unconstrained optimization, where the solution yields the optimal configuration by maximizing an objective function that reflects the degree of constraint satisfaction. This function encourages collaboration, ensuring agents help each other meet their constraints while fulfilling their own. When the constraints are infeasible, agents converge to a least-violating solution. A distributed consensus-based optimization scheme is then introduced, which approximates the centralized solution, leading to the development of distributed controllers for single-integrator agents. Finally, simulations validate the effectiveness of the proposed approach.

Paper number 39:
Title: Prototyping and Test of the "Canis" HTS Planar Coil Array for Stellarator Field Shaping
Authors: D. Nash, D.A. Gates, W.S. Walsh, M. Slepchenkov, D. Guan, A.D. Cate, B. Chen, M. Dickerson, W. Harris, U. Khera, M. Korman, S. Srinivasan, C.P.S. Swanson, A. van Riel, R.H. Wu, A.S. Basurto, B. Berzin, E. Brown, C. Chen, T. Ikuss, W.B. Kalb, C. Khurana, B.D. Koehne, T.G. Kruger, S. Noronha, J. Olatunji, R. Powser, K. Tamhankar, K. Tang, A. Tarifa, M. Savastianov, J. Wasserman, C. Yang
Abstract: Thea Energy, Inc. is currently developing the "Eos" planar coil stellarator, the Company's first integrated fusion system capable of forming optimized stellarator magnetic fields without complex and costly modular coils. To demonstrate the field shaping capability required to enable Eos, Thea Energy designed, constructed, and tested the "Canis" 3x3 array of high-temperature superconductor (HTS) planar shaping coils after successfully demonstrating a single shaping coil prototype. Through the Canis 3x3 magnet array program, Thea Energy manufactured nine HTS shaping coils and developed the cryogenic test and measurement infrastructure necessary to validate the array's performance. Thea Energy operated the array at 20 K, generating several stellarator-relevant magnetic field shapes and demonstrating closed loop field control of the superconducting magnets to within 1% of predicted field, a margin of error acceptable for operation of an integrated stellarator. The Canis magnet array test campaign provides a proof of concept for HTS planar shaping coils as a viable approach to confining stellarator plasmas.

Paper number 40:
Title: Improving Food Image Recognition with Noisy Vision Transformer
Authors: Tonmoy Ghosh, Edward Sazonov
Abstract: Food image recognition is a challenging task in computer vision due to the high variability and complexity of food images. In this study, we investigate the potential of Noisy Vision Transformers (NoisyViT) for improving food classification performance. By introducing noise into the learning process, NoisyViT reduces task complexity and adjusts the entropy of the system, leading to enhanced model accuracy. We fine-tune NoisyViT on three benchmark datasets: Food2K (2,000 categories, ~1M images), Food-101 (101 categories, ~100K images), and CNFOOD-241 (241 categories, ~190K images). The performance of NoisyViT is evaluated against state-of-the-art food recognition models. Our results demonstrate that NoisyViT achieves Top-1 accuracies of 95%, 99.5%, and 96.6% on Food2K, Food-101, and CNFOOD-241, respectively, significantly outperforming existing approaches. This study underscores the potential of NoisyViT for dietary assessment, nutritional monitoring, and healthcare applications, paving the way for future advancements in vision-based food computing. Code for reproducing NoisyViT for food recognition is available at NoisyViT_Food.

Paper number 41:
Title: Enhancing V2X Communications with UAV-mounted Reconfigurable Intelligent Surfaces
Authors: Salim Janji, PaweÅ Sroka, Adrian Kliks
Abstract: This paper addresses the crucial need for reliable wireless communication in vehicular networks, particularly vital for the safety and efficacy of (semi-)autonomous driving amid increasing traffic. We explore the use of Reconfigurable Intelligent Surfaces (RISes) mounted on Drone Relay Stations (DRS) to enhance communication reliability. Our study formulates an optimization problem to pinpoint the optimal location and orientation of the DRS, thereby creating an additional propagation path for vehicle-to-everything (V2X) communications. We introduce a heuristic approach that combines trajectory optimization for DRS positioning and a Q-learning scheme for RIS orientation. Our results not only confirm the convergence of the Q-learning algorithm but also demonstrate significant communication improvements achieved by integrating a DRS into V2X networks.

Paper number 42:
Title: Rank-Based Modeling for Universal Packets Compression in Multi-Modal Communications
Authors: Xuanhao Luo, Zhiyuan Peng, Zhouyu Li, Ruozhou Yu, Yuchen Liu
Abstract: The rapid increase in networked systems and data transmission requires advanced data compression solutions to optimize bandwidth utilization and enhance network performance. This study introduces a novel byte-level predictive model using Transformer architecture, capable of handling the redundancy and diversity of data types in network traffic as byte sequences. Unlike traditional methods that require separate compressors for different data types, this unified approach sets new benchmarks and simplifies predictive modeling across various data modalities such as video, audio, images, and text, by processing them at the byte level. This is achieved by predicting subsequent byte probability distributions, encoding them into a sparse rank sequence using lossless entropy coding, and significantly reducing both data size and entropy. Experimental results show that our model achieves compression ratios below 50%, while offering models of various sizes tailored for different communication devices. Additionally, we successfully deploy these models on a range of edge devices and servers, demonstrating their practical applicability and effectiveness in real-world network scenarios. This approach significantly enhances data throughput and reduces bandwidth demands, making it particularly valuable in resource-constrained environments like the Internet of Things sensor networks.

Paper number 43:
Title: Angular-Based Hybrid Beamforming for Wideband THz Massive MIMO Systems: Mitigating Beam Split by Leveraging Angular Spread
Authors: Ibrahim Yildirim, Tho Le-Ngoc
Abstract: Beam split is a critical challenge in wideband THz massive MIMO systems, arising from frequency-dependent beam misalignment that degrades communication performance, particularly in scenarios with narrow beamwidths and large arrays. This work proposes an angular-based hybrid beamforming framework that leverages angular spread to mitigate the beam split effect. Instead of relying on precise angular spread modeling, we utilize coarse angular information to guide the design of subcarrier-specific beams, effectively reducing misalignment across subcarriers. By broadening the effective beamwidth through angular spread, the proposed method enhances user coverage and alleviates beam split without requiring complex time-delay units or hardware-intensive solutions. Simulation results demonstrate that the proposed approach achieves significant improvements in spectral efficiency and beamforming accuracy while maintaining low computational and hardware complexity. This work provides a practical and efficient solution for addressing beam split in next-generation wideband THz communication systems.

Paper number 44:
Title: Dom, cars don't fly! -- Or do they? In-Air Vehicle Maneuver for High-Speed Off-Road Navigation
Authors: Anuj Pokhrel, Aniket Datar, Xuesu Xiao
Abstract: When pushing the speed limit for aggressive off-road navigation on uneven terrain, it is inevitable that vehicles may become airborne from time to time. During time-sensitive tasks, being able to fly over challenging terrain can also save time, instead of cautiously circumventing or slowly negotiating through. However, most off-road autonomy systems operate under the assumption that the vehicles are always on the ground and therefore limit operational speed. In this paper, we present a novel approach for in-air vehicle maneuver during high-speed off-road navigation. Based on a hybrid forward kinodynamic model using both physics principles and machine learning, our fixed-horizon, sampling-based motion planner ensures accurate vehicle landing poses and their derivatives within a short airborne time window using vehicle throttle and steering commands. We test our approach in extensive in-air experiments both indoors and outdoors, compare it against an error-driven control method, and demonstrate that precise and timely in-air vehicle maneuver is possible through existing ground vehicle controls.

Paper number 45:
Title: A multi-axis Nanopositioner based on Near-Field Acoustic Levitation and Electromagnetic Actuation
Authors: K. S. Vikrant, Prosanto Biswas, S. O. Reza Moheimani
Abstract: Positioners based on near-field acoustic levitation (NFAL) offer high positioning resolution and bandwidth, primarily along the Z axis, because of the large acoustic Z-stiffness and squeeze film damping. However, their XY-positioning resolution and bandwidth are several orders of magnitude lower because of the limited acoustic stiffness and damping in these directions. In this paper, we increase the XY-stiffness and damping by using a steady-current-based planar electromagnetic trap and eddy current damping technique, respectively. Specifically, NFAL is used to levitate a magnetic platform, which is then electromagnetically trapped in the XY-plane. Eddy currents generated by a thin copper plate beneath the levitating platform increase in-plane damping by a factor of 52, thereby reducing vibrations inherent in the NFAL technique. Additionally, the planar coil used for electromagnetic trapping provides multi-axis positioning capabilities. We demonstrate 3-axis linear motion with a root mean square (rms) positioning resolution of better than 20 nm along all axes. The in-plane motion range and bandwidth achieved are 1.42 mm and 16 Hz, respectively, while a motion range of 40 micrometers with a positioning bandwidth of 171 Hz is achieved along the Z-axis.

Paper number 46:
Title: Mining-Gym: A Configurable RL Benchmarking Environment for Truck Dispatch Scheduling
Authors: Chayan Banerjee, Kien Nguyen, Clinton Fookes
Abstract: Mining process optimization particularly truck dispatch scheduling is a critical factor in enhancing the efficiency of open pit mining operations However the dynamic and stochastic nature of mining environments characterized by uncertainties such as equipment failures truck maintenance and variable haul cycle times poses significant challenges for traditional optimization methods While Reinforcement Learning RL has shown promise in adaptive decision making for mining logistics its practical deployment requires rigorous evaluation in realistic and customizable simulation environments The lack of standardized benchmarking environments limits fair algorithm comparisons reproducibility and the real world applicability of RL based approaches in open pit mining settings To address this challenge we introduce Mining Gym a configurable open source benchmarking environment designed for training testing and comparing RL algorithms in mining process optimization Built on Discrete Event Simulation DES and seamlessly integrated with the OpenAI Gym interface Mining Gym provides a structured testbed that enables the direct application of advanced RL algorithms from Stable Baselines The framework models key mining specific uncertainties such as equipment failures queue congestion and the stochasticity of mining processes ensuring a realistic and adaptive learning environment Additionally Mining Gym features a graphical user interface GUI for intuitive mine site configuration a comprehensive data logging system a built in KPI dashboard and real time visual representation of the mine site These capabilities facilitate standardized reproducible evaluations across multiple RL strategies and baseline heuristics

Paper number 47:
Title: Optimal Modified Feedback Strategies in LQ Games under Control Imperfections
Authors: Mahdis Rabbani, Navid Mojahed, Shima Nazari
Abstract: Game-theoretic approaches and Nash equilibrium have been widely applied across various engineering domains. However, practical challenges such as disturbances, delays, and actuator limitations can hinder the precise execution of Nash equilibrium strategies. This work explores the impact of such implementation imperfections on game trajectories and players' costs within the context of a two-player linear quadratic (LQ) nonzero-sum game. Specifically, we analyze how small deviations by one player affect the state and cost function of the other player. To address these deviations, we propose an adjusted control policy that not only mitigates adverse effects optimally but can also exploit the deviations to enhance performance. Rigorous mathematical analysis and proofs are presented, demonstrating through a representative example that the proposed policy modification achieves up to $61\%$ improvement compared to the unadjusted feedback policy and up to $0.59\%$ compared to the feedback Nash strategy.

Paper number 48:
Title: Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning
Authors: Gautham Udayakumar Bekal, Ahmed Ghareeb, Ashish Pujari
Abstract: Buildings with Heating, Ventilation, and Air Conditioning (HVAC) systems play a crucial role in ensuring indoor comfort and efficiency. While traditionally governed by physics-based models, the emergence of big data has enabled data-driven methods like Deep Reinforcement Learning (DRL). However, Reinforcement Learning (RL)-based techniques often suffer from sample inefficiency and limited generalization, especially across varying HVAC systems. We introduce a model-based reinforcement learning framework that uses a Hypernetwork to continuously learn environment dynamics across tasks with different action spaces. This enables efficient synthetic rollout generation and improved sample usage. Our approach demonstrates strong backward transfer in a continual learning setting after training on a second task, minimal fine-tuning on the first task allows rapid convergence within just 5 episodes and thus outperforming Model Free Reinforcement Learning (MFRL) and effectively mitigating catastrophic forgetting. These findings have significant implications for reducing energy consumption and operational costs in building management, thus supporting global sustainability goals. Keywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks, Transfer and Continual Learning, Catastrophic Forgetting

Paper number 49:
Title: Tumor monitoring and detection of lymph node metastasis using quantitative ultrasound and immune cytokine profiling in dogs undergoing radiation therapy: a pilot study
Authors: Mick Gardner, Audrey Billhymer, Rebecca Kamerer, Joanna Schmit, Trevor Park, Julie Nguyen-Edquilang, Rita Miller, Kim A Selting, Michael Oelze
Abstract: Quantitative ultrasound (QUS) characterizes the composition of cells to distinguish diseased from healthy tissue. QUS can reflect the complexity of the tumor and detect early lymph node (LN) metastasis ex vivo. The objective in this study was to gather preliminary QUS and cytokine data from dogs undergoing radiation therapy and correlate QUS data with both LN metastasis and tumor response. Spontaneous solid tumors were evaluated with QUS before and up to one year after receiving RT. Additionally, regional LNs were evaluated with QUS in vivo, then excised and examined with histopathology to detect metastasis. Paired t-tests were used to compare QUS data of metastatic and non-metastatic LNs within patients. Furthermore, paired t-tests compared pre- versus post-RT QUS data. Serum was collected at each time point for cytokine profiles. Most statistical tests were underpowered to produce significant p values, but interesting trends were observed. The lowest p values for LN tests were found with the envelope statistics K (p = 0.142) and mu (p = 0.181), which correspond to cell structure and number of scatterers. For tumor response, the lowest p values were found with K (p = 0.115) and mu (p = 0.127) when comparing baseline QUS data with QUS data 1 week after RT. Monocyte chemoattractant protein 1 (MCP-1) was significantly higher in dogs with cancer when compared to healthy controls (p = 1.12e-4). A weak correlation was found between effective scatterer diameter (ESD) and Transforming growth factor beta 1 (TGFB-1). While statistical tests on the preliminary QUS data alone were underpowered to detect significant differences among groups, our methods create a basis for future studies.

Paper number 50:
Title: Adaptive Multi-Order Graph Regularized NMF with Dual Sparsity for Hyperspectral Unmixing
Authors: Hui Chen, Liangyu Liu, Xianchao Xiu, Wanquan Liu
Abstract: Hyperspectral unmixing (HU) is a critical yet challenging task in remote sensing. However, existing nonnegative matrix factorization (NMF) methods with graph learning mostly focus on first-order or second-order nearest neighbor relationships and usually require manual parameter tuning, which fails to characterize intrinsic data structures. To address the above issues, we propose a novel adaptive multi-order graph regularized NMF method (MOGNMF) with three key features. First, multi-order graph regularization is introduced into the NMF framework to exploit global and local information comprehensively. Second, these parameters associated with the multi-order graph are learned adaptively through a data-driven approach. Third, dual sparsity is embedded to obtain better robustness, i.e., $\ell_{1/2}$-norm on the abundance matrix and $\ell_{2,1}$-norm on the noise matrix. To solve the proposed model, we develop an alternating minimization algorithm whose subproblems have explicit solutions, thus ensuring effectiveness. Experiments on simulated and real hyperspectral data indicate that the proposed method delivers better unmixing results.

Paper number 51:
Title: Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment
Authors: Guanglu Dong, Xiangyu Liao, Mingyang Li, Guihuan Guo, Chao Ren
Abstract: Generative Adversarial Networks (GANs) have been widely applied to image super-resolution (SR) to enhance the perceptual quality. However, most existing GAN-based SR methods typically perform coarse-grained discrimination directly on images and ignore the semantic information of images, making it challenging for the super resolution networks (SRN) to learn fine-grained and semantic-related texture details. To alleviate this issue, we propose a semantic feature discrimination method, SFD, for perceptual SR. Specifically, we first design a feature discriminator (Feat-D), to discriminate the pixel-wise middle semantic features from CLIP, aligning the feature distributions of SR images with that of high-quality images. Additionally, we propose a text-guided discrimination method (TG-D) by introducing learnable prompt pairs (LPP) in an adversarial manner to perform discrimination on the more abstract output feature of CLIP, further enhancing the discriminative ability of our method. With both Feat-D and TG-D, our SFD can effectively distinguish between the semantic feature distributions of low-quality and high-quality images, encouraging SRN to generate more realistic and semantic-relevant textures. Furthermore, based on the trained Feat-D and LPP, we propose a novel opinion-unaware no-reference image quality assessment (OU NR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any additional targeted training. Extensive experiments on classical SISR, real-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our proposed methods.

Paper number 52:
Title: Partitioned Task Offloading for Low-Latency and Reliable Task Completion in 5G MEC
Authors: Parisa Fard Moshiri, Murat Simsek, Burak Kantarci
Abstract: The demand for MEC has increased with the rise of data-intensive applications and 5G networks, while conventional cloud models struggle to satisfy low-latency requirements. While task offloading is crucial for minimizing latency on resource-constrained User Equipment (UE), fully offloading of all tasks to MEC servers may result in overload and possible task drops. Overlooking the effect of number of dropped tasks can significantly undermine system efficiency, as each dropped task results in unfulfilled service demands and reduced reliability, directly impacting user experience and overall network performance. In this paper, we employ task partitioning, enabling partitions of task to be processed locally while assigning the rest to MEC, thus balancing the load and ensuring no task drops. This methodology enhances efficiency via Mixed Integer Linear Programming (MILP) and Cuckoo Search, resulting in effective task assignment and minimum latency. Moreover, we ensure each user's RB allocation stays within the maximum limit while keeping latency low. Experimental results indicate that this strategy surpasses both full offloading and full local processing, providing significant improvements in latency and task completion rates across diverse number of users. In our scenario, MILP task partitioning results in 24% reduction in latency compared to MILP task offloading for the maximum number of users, whereas Cuckoo search task partitioning yields 18% latency reduction in comparison with Cuckoo search task offloading.

Paper number 53:
Title: Exploring Textual Semantics Diversity for Image Transmission in Semantic Communication Systems using Visual Language Model
Authors: Peishan Huang, Dong Li
Abstract: In recent years, the rapid development of machine learning has brought reforms and challenges to traditional communication systems. Semantic communication has appeared as an effective strategy to effectively extract relevant semantic signals semantic segmentation labels and image features for image transmission. However, the insufficient number of extracted semantic features of images will potentially result in a low reconstruction accuracy, which hinders the practical applications and still remains challenging for solving. In order to fill this gap, this letter proposes a multi-text transmission semantic communication (Multi-SC) system, which uses the visual language model (VLM) to assist in the transmission of image semantic signals. Unlike previous image transmission semantic communication systems, the proposed system divides the image into multiple blocks and extracts multiple text information from the image using a modified large language and visual assistant (LLaVA), and combines semantic segmentation tags with semantic text for image recovery. Simulation results show that the proposed text semantics diversity scheme can significantly improve the reconstruction accuracy compared with related works.

Paper number 54:
Title: Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model
Authors: Changyong He, Jin Zeng, Jiawei Zhang, Jiajie Guo
Abstract: Time-of-Flight (ToF) sensors efficiently capture scene depth, but the nonlinear depth construction procedure often results in extremely large noise variance or even invalid areas. Recent methods based on deep neural networks (DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when presented with severe noise corruption due to limited prior knowledge of ToF data distribution. In this paper, we propose DepthCAD, a novel ToF denoising approach that ensures global structural smoothness by leveraging the rich prior knowledge in Stable Diffusion and maintains local metric accuracy by steering the diffusion process with confidence guidance. To adopt the pretrained image diffusion model to ToF depth denoising, we apply the diffusion on raw ToF correlation measurements with dynamic range normalization before converting to depth maps. Experimental results validate the state-of-the-art performance of the proposed scheme, and the evaluation on real data further verifies its robustness against real-world ToF noise.

Paper number 55:
Title: Noisier2Inverse: Self-Supervised Learning for Image Reconstruction with Correlated Noise
Authors: Nadja Gruber, Johannes Schwab, Markus Haltmeier, Ander Biguri, Clemens Dlaska, Gyeongha Hwang
Abstract: We propose Noisier2Inverse, a correction-free self-supervised deep learning approach for general inverse prob- lems. The proposed method learns a reconstruction function without the need for ground truth samples and is ap- plicable in cases where measurement noise is statistically correlated. This includes computed tomography, where detector imperfections or photon scattering create correlated noise patterns, as well as microscopy and seismic imaging, where physical interactions during measurement introduce dependencies in the noise structure. Similar to Noisier2Noise, a key step in our approach is the generation of noisier data from which the reconstruction net- work learns. However, unlike Noisier2Noise, the proposed loss function operates in measurement space and is trained to recover an extrapolated image instead of the original noisy one. This eliminates the need for an extrap- olation step during inference, which would otherwise suffer from ill-posedness. We numerically demonstrate that our method clearly outperforms previous self-supervised approaches that account for correlated noise.

Paper number 56:
Title: Noise Resilient Over-The-Air Federated Learning In Heterogeneous Wireless Networks
Authors: Zubair Shaban, Nazreen Shah, Ranjitha Prasad
Abstract: In 6G wireless networks, Artificial Intelligence (AI)-driven applications demand the adoption of Federated Learning (FL) to enable efficient and privacy-preserving model training across distributed devices. Over-The-Air Federated Learning (OTA-FL) exploits the superposition property of multiple access channels, allowing edge users in 6G networks to efficiently share spectral resources and perform low-latency global model aggregation. However, these advantages come with challenges, as traditional OTA-FL techniques suffer due to the joint effects of Additive White Gaussian Noise (AWGN) at the server, fading, and both data and system heterogeneity at the participating edge devices. In this work, we propose the novel Noise Resilient Over-the-Air Federated Learning (NoROTA-FL) framework to jointly tackle these challenges in federated wireless networks. In NoROTA-FL, the local optimization problems find controlled inexact solutions, which manifests as an additional proximal constraint at the clients. This approach provides robustness against straggler-induced partial work, heterogeneity, noise, and fading. From a theoretical perspective, we leverage the zeroth- and first-order inexactness and establish convergence guarantees for non-convex optimization problems in the presence of heterogeneous data and varying system capabilities. Experimentally, we validate NoROTA-FL on real-world datasets, including FEMNIST, CIFAR10, and CIFAR100, demonstrating its robustness in noisy and heterogeneous environments. Compared to state-of-the-art baselines such as COTAF and FedProx, NoROTA-FL achieves significantly more stable convergence and higher accuracy, particularly in the presence of stragglers.

Paper number 57:
Title: Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization
Authors: Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, Jie Hao
Abstract: With the widespread application of automatic speech recognition (ASR) systems, their vulnerability to adversarial attacks has been extensively studied. However, most existing adversarial examples are generated on specific individual models, resulting in a lack of transferability. In real-world scenarios, attackers often cannot access detailed information about the target model, making query-based attacks unfeasible. To address this challenge, we propose a technique called Acoustic Representation Optimization that aligns adversarial perturbations with low-level acoustic characteristics derived from speech representation models. Rather than relying on model-specific, higher-layer abstractions, our approach leverages fundamental acoustic representations that remain consistent across diverse ASR architectures. By enforcing an acoustic representation loss to guide perturbations toward these robust, lower-level representations, we enhance the cross-model transferability of adversarial examples without degrading audio quality. Our method is plug-and-play and can be integrated with any existing attack methods. We evaluate our approach on three modern ASR models, and the experimental results demonstrate that our method significantly improves the transferability of adversarial examples generated by previous methods while preserving the audio quality.

Paper number 58:
Title: Perception-Enhanced Multitask Multimodal Semantic Communication for UAV-Assisted Integrated Sensing and Communication System
Authors: Ziji Guo, Haonan Tong, Zhilong Zhang, Danpu Liu
Abstract: Recent advances in integrated sensing and communication (ISAC) unmanned aerial vehicles (UAVs) have enabled their widespread deployment in critical applications such as emergency management. This paper investigates the challenge of efficient multitask multimodal data communication in UAV-assisted ISAC systems, in the considered system model, hyperspectral (HSI) and LiDAR data are collected by UAV-mounted sensors for both target classification and data reconstruction at the terrestrial BS. The limited channel capacity and complex environmental conditions pose significant challenges to effective air-to-ground communication. To tackle this issue, we propose a perception-enhanced multitask multimodal semantic communication (PE-MMSC) system that strategically leverages the onboard computational and sensing capabilities of UAVs. In particular, we first propose a robust multimodal feature fusion method that adaptively combines HSI and LiDAR semantics while considering channel noise and task requirements. Then the method introduces a perception-enhanced (PE) module incorporating attention mechanisms to perform coarse classification on UAV side, thereby optimizing the attention-based multimodal fusion and transmission. Experimental results demonstrate that the proposed PE-MMSC system achieves 5\%--10\% higher target classification accuracy compared to conventional systems without PE module, while maintaining comparable data reconstruction quality with acceptable computational overheads.

Paper number 59:
Title: QINCODEC: Neural Audio Compression with Implicit Neural Codebooks
Authors: Zineb Lahrichi (S2A), GaÃ«tan Hadjeres, Gael Richard (S2A), Geoffroy Peeters (S2A)
Abstract: Neural audio codecs, neural networks which compress a waveform into discrete tokens, play a crucial role in the recent development of audio generative models. State-of-the-art codecs rely on the end-to-end training of an autoencoder and a quantization bottleneck. However, this approach restricts the choice of the quantization methods as it requires to define how gradients propagate through the quantizer and how to update the quantization parameters online. In this work, we revisit the common practice of joint training and propose to quantize the latent representations of a pre-trained autoencoder offline, followed by an optional finetuning of the decoder to mitigate degradation from quantization. This strategy allows to consider any off-the-shelf quantizer, especially state-of-the-art trainable quantizers with implicit neural codebooks such as QINCO2. We demonstrate that with the latter, our proposed codec termed QINCODEC, is competitive with baseline codecs while being notably simpler to train. Finally, our approach provides a general framework that amortizes the cost of autoencoder pretraining, and enables more flexible codec design.

Paper number 60:
Title: Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation
Authors: Max W. Y. Lam, Yijin Xing, Weiya You, Jingcheng Wu, Zongyu Yin, Fuqiang Jiang, Hangyu Liu, Feng Liu, Xingda Li, Wei-Tsung Lu, Hanyu Chen, Tong Feng, Tianwei Zhao, Chien-Hung Liu, Xuchen Song, Yang Li, Yahui Zhou
Abstract: Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of "musical thoughts", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models. Our samples are available at this https URL.

Paper number 61:
Title: High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting
Authors: Qian Wang, Zhihao Zhan, Jialei He, Zhituo Tu, Xiang Zhu, Jie Yuan
Abstract: Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring. Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors. This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.

Paper number 62:
Title: Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with Keypoints-Guided Diffusion Models
Authors: Ruixi You, Hecheng Jia, Feng Xu
Abstract: Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and high-resolution imaging capabilities but its unique imaging mechanism makes interpretation heavily reliant on expert knowledge, limiting interpretability, especially in complex target tasks. Translating SAR images into optical images is a promising solution to enhance interpretation and support downstream tasks. Most existing research focuses on scene-level translation, with limited work on object-level translation due to the scarcity of paired data and the challenge of accurately preserving contour and texture details. To address these issues, this study proposes a keypoint-guided diffusion model (KeypointDiff) for SAR-to-optical image translation of unpaired aircraft targets. This framework introduces supervision on target class and azimuth angle via keypoints, along with a training strategy for unpaired data. Based on the classifier-free guidance diffusion architecture, a class-angle guidance module (CAGM) is designed to integrate class and angle information into the diffusion generation process. Furthermore, adversarial loss and consistency loss are employed to improve image fidelity and detail quality, tailored for aircraft targets. During sampling, aided by a pre-trained keypoint detector, the model eliminates the requirement for manually labeled class and azimuth information, enabling automated SAR-to-optical translation. Experimental results demonstrate that the proposed method outperforms existing approaches across multiple metrics, providing an efficient and effective solution for object-level SAR-to-optical translation and downstream tasks. Moreover, the method exhibits strong zero-shot generalization to untrained aircraft types with the assistance of the keypoint detector.

Paper number 63:
Title: An Overview of Low-Rank Structures in the Training and Adaptation of Large Models
Authors: Laura Balzano, Tianjiao Ding, Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, Peng Wang, Zhangyang Wang, Can Yaras
Abstract: The rise of deep learning has revolutionized data processing and prediction in signal processing and machine learning, yet the substantial computational demands of training and deploying modern large-scale deep models present significant challenges, including high computational costs and energy consumption. Recent research has uncovered a widespread phenomenon in deep networks: the emergence of low-rank structures in weight matrices and learned representations during training. These implicit low-dimensional patterns provide valuable insights for improving the efficiency of training and fine-tuning large-scale models. Practical techniques inspired by this phenomenon, such as low-rank adaptation (LoRA) and training, enable significant reductions in computational cost while preserving model performance. In this paper, we present a comprehensive review of recent advances in exploiting low-rank structures for deep learning and shed light on their mathematical foundations. Mathematically, we present two complementary perspectives on understanding the low-rankness in deep networks: (i) the emergence of low-rank structures throughout the whole optimization dynamics of gradient and (ii) the implicit regularization effects that induce such low-rank structures at convergence. From a practical standpoint, studying the low-rank learning dynamics of gradient descent offers a mathematical foundation for understanding the effectiveness of LoRA in fine-tuning large-scale models and inspires parameter-efficient low-rank training strategies. Furthermore, the implicit low-rank regularization effect helps explain the success of various masked training approaches in deep neural networks, ranging from dropout to masked self-supervised learning.

Paper number 64:
Title: Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement
Authors: Ming Lei, Christophe Baehr
Abstract: This paper establishes a unified framework integrating geometric flows with deep learning through three fundamental innovations. First, we propose a thermodynamically coupled Ricci flow that dynamically adapts parameter space geometry to loss landscape topology, formally proved to preserve isometric knowledge embedding (Theorem~\ref{thm:isometric}). Second, we derive explicit phase transition thresholds and critical learning rates (Theorem~\ref{thm:critical}) through curvature blowup analysis, enabling automated singularity resolution via geometric surgery (Lemma~\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic duality (Theorem~\ref{thm:ads}) between neural networks and conformal field theories, providing entanglement entropy bounds for regularization design. Experiments demonstrate 2.1$\times$ convergence acceleration and 63\% topological simplification while maintaining $\mathcal{O}(N\log N)$ complexity, outperforming Riemannian baselines by 15.2\% in few-shot accuracy. Theoretically, we prove exponential stability (Theorem~\ref{thm:converge}) through a new Lyapunov function combining Perelman entropy with Wasserstein gradient flows, fundamentally advancing geometric deep learning.

Paper number 65:
Title: RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning
Authors: Abdulmoneam Ali, Ahmed Arafa
Abstract: We address the problem of cluster identity estimation in a personalized federated learning (PFL) setting in which users aim to learn different personal models. The backbone of effective learning in such a setting is to cluster users into groups whose objectives are similar. A typical approach in the literature is to achieve this by training users' data on different proposed personal models and assign them to groups based on which model achieves the lowest value of the users' loss functions. This process is to be done iteratively until group identities converge. A key challenge in such a setting arises when users have noisy labeled data, which may produce misleading values of their loss functions, and hence lead to ineffective clustering. To overcome this challenge, we propose a label-agnostic data similarity-based clustering algorithm, coined RCC-PFL, with three main advantages: the cluster identity estimation procedure is independent from the training labels; it is a one-shot clustering algorithm performed prior to the training; and it requires fewer communication rounds and less computation compared to iterative-based clustering methods. We validate our proposed algorithm using various models and datasets and show that it outperforms multiple baselines in terms of average accuracy and variance reduction.

Paper number 66:
Title: SG-GAN: Fine Stereoscopic-Aware Generation for 3D Brain Point Cloud Up-sampling from a Single Image
Authors: Bowen Hu, Weiheng Yao, Sibo Qiao, Hieu Pham, Shuqiang Wang, Michael Kwok-Po Ng
Abstract: In minimally-invasive brain surgeries with indirect and narrow operating environments, 3D brain reconstruction is crucial. However, as requirements of accuracy for some new minimally-invasive surgeries (such as brain-computer interface surgery) are higher and higher, the outputs of conventional 3D reconstruction, such as point cloud (PC), are facing the challenges that sample points are too sparse and the precision is insufficient. On the other hand, there is a scarcity of high-density point cloud datasets, which makes it challenging to train models for direct reconstruction of high-density brain point clouds. In this work, a novel model named stereoscopic-aware graph generative adversarial network (SG-GAN) with two stages is proposed to generate fine high-density PC conditioned on a single image. The Stage-I GAN sketches the primitive shape and basic structure of the organ based on the given image, yielding Stage-I point clouds. The Stage-II GAN takes the results from Stage-I and generates high-density point clouds with detailed features. The Stage-II GAN is capable of correcting defects and restoring the detailed features of the region of interest (ROI) through the up-sampling process. Furthermore, a parameter-free-attention-based free-transforming module is developed to learn the efficient features of input, while upholding a promising performance. Comparing with the existing methods, the SG-GAN model shows superior performance in terms of visual quality, objective measurements, and performance in classification, as demonstrated by comprehensive results measured by several evaluation metrics including PC-to-PC error and Chamfer distance.

Paper number 67:
Title: Purely Speckled Intensity Images Need for SAR Despeckling with SDS-SAR
Authors: Liang Chen, Yifei Yin, Hao Shi, Jingfei He, Wei Li
Abstract: Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods impose high demands on speckle modeling or specific data, limiting their practicality in real-world applications. To address these challenges, we propose a Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for all SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs on par with supervised approaches on synthetic data and outperforms them on actual data.

Paper number 68:
Title: Dynamic Electromagnetic Navigation
Authors: Jasan Zughaibi, Bradley J. Nelson, Michael Muehlebach
Abstract: Magnetic navigation offers wireless control over magnetic objects, which has important medical applications, such as targeted drug delivery and minimally invasive surgery. Magnetic navigation systems are categorized into systems using permanent magnets and systems based on electromagnets. Electromagnetic Navigation Systems (eMNSs) are believed to have a superior actuation bandwidth, facilitating trajectory tracking and disturbance rejection. This greatly expands the range of potential medical applications and includes even dynamic environments as encountered in cardiovascular interventions. To showcase the dynamic capabilities of eMNSs, we successfully stabilize a (non-magnetic) inverted pendulum on the tip of a magnetically driven arm. Our approach employs a model-based framework that leverages Lagrangian mechanics to capture the interaction between the mechanical dynamics and the magnetic field. Using system identification, we estimate unknown parameters, the actuation bandwidth, and characterize the system's nonlinearity. To explore the limits of electromagnetic navigation and evaluate its scalability, we characterize the electrical system dynamics and perform reference measurements on a clinical-scale eMNS, affirming that the proposed dynamic control methodologies effectively translate to larger coil configurations. A state-feedback controller stabilizes the inherently unstable pendulum, and an iterative learning control scheme enables accurate tracking of non-equilibrium trajectories. Furthermore, to understand structural limitations of our control strategy, we analyze the influence of magnetic field gradients on the motion of the system. To our knowledge, this is the first demonstration to stabilize a 3D inverted pendulum through electromagnetic navigation.

Paper number 69:
Title: A Concept for Semi-Automatic Configuration of Sufficiently Valid Simulation Setups for Automated Driving Systems
Authors: Niklas Braun, Markus Steimle, Martin TÃ¶rngren, Markus Maurer
Abstract: As simulation is increasingly used in scenario-based approaches to test Automated Driving Systems, the credibility of simulation results is a major concern. Arguably, credibility depends on the validity of the simulation setup and simulation models. When selecting appropriate simulation models, a trade-off must be made between validity, often connected to the model's fidelity, and cost of computation. However, due to the large number of test cases, expert-based methods to create sufficiently valid simulation setups seem infeasible. We propose using design contracts in order to semi-automatically compose simulation setups for given test cases from simulation models and to derive requirements for the simulation models, supporting separation of concerns between simulation model developers and users. Simulation model contracts represent their validity domains by capturing a validity guarantee and the associated operating conditions in an assumption. We then require the composition of the simulation model contracts to refine a test case contract. The latter contract captures the operating conditions of the test case in its assumption and validity requirements in its guarantee. Based on this idea, we present a framework that supports the compositional configuration of simulation setups based on the contracts and a method to derive runtime monitors for these simulation setups.

Paper number 70:
Title: Free-Space Optical Channel Turbulence Prediction: A Machine Learning Approach
Authors: Md Zobaer Islam, Ethan Abele, Fahim Ferdous Hossain, Arsalan Ahmad, Sabit Ekin, John F. O'Hara
Abstract: Channel turbulence is a formidable obstacle for free-space optical (FSO) communication. Anticipation of turbulence levels is highly important for mitigating disruptions but has not been demonstrated without dedicated, auxiliary hardware. We show that machine learning (ML) can be applied to raw FSO data streams to rapidly predict channel turbulence levels with no additional sensing hardware. FSO was conducted through a controlled channel in the lab under six distinct turbulence levels, and the efficacy of using ML to classify turbulence levels was examined. ML-based turbulence level classification was found to be >98% accurate with multiple ML training parameters. Classification effectiveness was found to depend on the timescale of changes between turbulence levels but converges when turbulence stabilizes over about a one minute timescale.

Paper number 71:
Title: Unsupervised Blind Joint Dereverberation and Room Acoustics Estimation with Diffusion Models
Authors: Jean-Marie Lemercier, Eloi Moliner, Simon Welker, Vesa VÃ¤limÃ¤ki, Timo Gerkmann
Abstract: This paper presents an unsupervised method for single-channel blind dereverberation and room impulse response (RIR) estimation, called BUDDy. The algorithm is rooted in Bayesian posterior sampling: it combines a likelihood model enforcing fidelity to the reverberant measurement, and an anechoic speech prior implemented by an unconditional diffusion model. We design a parametric filter representing the RIR, with exponential decay for each frequency subband. Room acoustics estimation and speech dereverberation are jointly carried out, as the filter parameters are iteratively estimated and the speech utterance refined along the reverse diffusion trajectory. In a blind scenario where the RIR is unknown, BUDDy successfully performs speech dereverberation in various acoustic scenarios, significantly outperforming other blind unsupervised baselines. Unlike supervised methods, which often struggle to generalize, BUDDy seamlessly adapts to different acoustic conditions. This paper extends our previous work by offering new experimental results and insights into the algorithm's versatility. We demonstrate the robustness of our proposed method to new acoustic and speaker conditions, as well as its adaptability to high-resolution singing voice dereverberation, using both instrumental metrics and subjective listening evaluation. We study BUDDy's performance for RIR estimation and observe it surpasses a state-of-the-art supervised DNN-based estimator on mismatched acoustic conditions. Finally, we investigate the sensitivity of informed dereverberation methods to RIR estimation errors, thereby motivating the joint acoustic estimation and dereverberation design. Audio examples and code can be found online.

Paper number 72:
Title: Infinite-Horizon Optimal Wireless Control Over Shared State-Dependent Fading Channels for IIoT Systems
Authors: Shuling Wang, Peizhe Li, Shanying Zhu, Cailian Chen, Xinping Guan
Abstract: Heterogeneous systems consisting of a multiloop wireless control system (WCS) and a mobile agent system (MAS) are ubiquitous in Industrial Internet of Things systems. Within these systems, the positions of mobile agents may lead to shadow fading on the wireless channel that the WCS is controlled over and can significantly compromise its performance, requiring joint coordination between the WCS and MAS. Such coordination introduces different time steps and hybrid state spaces consisting of logical components and continuous components. This paper focuses on the infinite-horizon optimal control of MAS to ensure the performance of WCS while minimizing an average cost for the heterogeneous system subject to safety constraints. A state-dependent fading channel is modeled to capture interference among transmission links, as well as the effects of mobile agents' movements on successful wireless transmission. In order to address the heterogeneous system dynamics, the optimal control problem is formulated as the optimal constrained set stabilization of the MAS by establishing a necessary and sufficient condition for the Lyapunov-like performance of WCS with the expected decay rates. Using the semi-tensor product of matrices, a constrained optimal state transition graph is constructed to encode the constrained system dynamics as well as objective function, which further reduces the problem into a minimum-mean cycle problem for the graph. By studying the properties of the graph, the feasibility is proven, and an effective algorithm is proposed for the construction of optimal input sequences. An illustrative example is provided to demonstrate effectiveness of the proposed method.

Paper number 73:
Title: Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound Localisation Microscopy with Large Tissue Displacements
Authors: Jipeng Yan, Qingyuan Tan, Shusei Kawara, Jingwen Zhu, Bingxue Wang, Matthieu Toulemonde, Honghai Liu, Ying Tan, Meng-Xing Tang
Abstract: Super-Resolution Ultrasound (SRUS) imaging through localising and tracking microbubbles, also known as Ultrasound Localisation Microscopy (ULM), has demonstrated significant potential for reconstructing microvasculature and flows with sub-diffraction resolution in clinical diagnostics. However, imaging organs with large tissue movements, such as those caused by respiration, presents substantial challenges. Existing methods often require breath holding to maintain accumulation accuracy, which limits data acquisition time and ULM image saturation. To improve image quality in the presence of large tissue movements, this study introduces an approach integrating high-frame-rate ultrasound with online precise robotic probe control. Tested on a microvasculature phantom with translation motions up to 20 mm, twice the aperture size of the matrix array used, our method achieved real-time tracking of the moving phantom and imaging volume rate at 85 Hz, keeping majority of the target volume in the imaging field of view. ULM images of the moving cross channels in the phantom were successfully reconstructed in post-processing, demonstrating the feasibility of super-resolution imaging under large tissue motions. This represents a significant step towards ULM imaging of organs with large motion.

Paper number 74:
Title: FIPER: Generalizable Factorized Features for Robust Low-Level Vision Models
Authors: Yang-Che Sun, Cheng Yu Yeo, Ernie Chu, Jun-Cheng Chen, Yu-Lun Liu
Abstract: In this work, we propose using a unified representation, termed Factorized Features, for low-level vision tasks, where we test on Single Image Super-Resolution (SISR) and Image Compression. Motivated by the shared principles between these tasks, they require recovering and preserving fine image details, whether by enhancing resolution for SISR or reconstructing compressed data for Image Compression. Unlike previous methods that mainly focus on network architecture, our proposed approach utilizes a basis-coefficient decomposition as well as an explicit formulation of frequencies to capture structural components and multi-scale visual features in images, which addresses the core challenges of both tasks. We replace the representation of prior models from simple feature maps with Factorized Features to validate the potential for broad generalizability. In addition, we further optimize the pipelines by leveraging the mergeable-basis property of our Factorized Features, which consolidates shared structures on multi-frame compression and super-resolution. Extensive experiments show that our unified representation delivers state-of-the-art performance, achieving an average relative improvement of 204.4% in PSNR over the baseline in Super-Resolution (SR) and 9.35% BD-rate reduction in Image Compression compared to the previous SOTA.

Paper number 75:
Title: A Unified Fault Ride Through Technique for Virtual Oscillator based Grid Forming Controllers
Authors: Ritwik Ghosh
Abstract: Grid-forming technology has a crucial role in achieving the future all renewable power grid. Among different types of grid-forming controllers, Virtual Oscillator (VO) based Controllers (VOCs) are the most advanced. VOCs outperform the conventional droop-based grid-forming controllers in terms of dynamic performance and synchronization stability by adapting time-domain-based implementation. However, because of the time-domain-based implementation, the same Fault Ride-through (FRT) techniques for droop-based controllers are incompatible with VOCs. Existing literature has successfully incorporated current limiting techniques in VOCs to protect the converters during severe transient conditions. Nevertheless, some very important aspects of FRT requirements are not attended to by the existing literature on VOCs, such as maintaining synchronization with the network during a fault, minimizing power oscillation during a fault, and at the fault clearance. First, this article introduces a unique analytical approach for quantifying the underlying dynamics of VOCs during faults. Next, using the mentioned analysis and in-depth reasoning, the systematic development of a unique FRT control architecture for VOCs is presented. The proposed FRT technique has unified both current and voltage synchronization in the same architecture to work successfully under three-phase and unbalanced faults. The performance of the proposed controller is thoroughly investigated and compared with existing VOCs.

Paper number 76:
Title: Personalized Continual EEG Decoding: Retaining and Transferring Knowledge
Authors: Dan Li, Hye-Bin Shin, Kang Yin, Seong-Whan Lee
Abstract: The significant inter-subject variability in electroen-cephalogram (EEG) signals often results in substantial changes to neural network weights as data distributions shift. This variability frequently causes catastrophic forgetting in continual EEG decoding tasks, where previously acquired knowledge is overwritten as new subjects are introduced. While retraining the entire dataset for each new subject can mitigate forgetting, this approach imposes significant computational costs, rendering it impractical for real-world applications. Therefore, an ideal brain-computer interface (BCI) model should incrementally learn new information without requiring complete retraining, thereby reducing computational overhead. Existing EEG decoding meth-ods typically rely on large, centralized source-domain datasets for pre-training to improve model generalization. However, in practical scenarios, data availability is often constrained by privacy concerns. Furthermore, these methods are susceptible to catastrophic forgetting in continual EEG decoding tasks, significantly limiting their utility in long-term learning scenarios. To address these issues, we propose the Personalized Continual EEG Decoding (PCED) framework for continual EEG decoding. The framework uses Euclidean Alignment for fast domain adap-tation, reducing inter-subject variability. To retain knowledge and prevent forgetting, it includes an exemplar replay mechanism that preserves key information from past tasks. A reservoir sampling-based memory management strategy optimizes exemplar storage to handle memory constraints in long-term learning. Experiments on the OpenBMI dataset with 54 subjects show that PCED balances knowledge retention and classification performance, providing an efficient solution for real-world BCI applications.

Paper number 77:
Title: Fast Switching in Mixed-Integer Model Predictive Control
Authors: Artemi Makarow, Christian Kirches
Abstract: We derive stability results for finite control set and mixed-integer model predictive control and propose a unified theoretical framework. The presentation rests upon the inherent robustness properties of common model predictive control with stabilizing terminal conditions and techniques for solving mixed-integer optimal control problems by continuous optimization. Partial outer convexification and binary relaxation transform mixed-integer problems into common optimal control problems. We derive nominal asymptotic stability for the resulting relaxed system formulation and implement sum-up rounding to restore efficiently integer feasibility. If fast control switching is technically possible and inexpensive, we can approximate the relaxed system behavior in the state space arbitrarily close. We integrate input perturbed model predictive control with practical asymptotic stability. Numerical experiments support our theoretical findings and illustrate practical relevance of fast and systematic control switching.

Paper number 78:
Title: Joint Phase Time Array: Opportunities, Challenges and System Design Considerations
Authors: Young-Han Nam, Ahmad AlAmmouri, Jianhua Mo, Jianzhong Chalrie Zhang
Abstract: This paper presents a novel approach to designing millimeter-wave (mmWave) cellular communication systems, based on joint phase time array (JPTA) radio frequency (RF) frontend architecture. JPTA architecture comprises time-delay components appended to conventional phase shifters, which offer extra degrees of freedom to be exploited for designing frequency-selective analog beams. Hence, a mmWave device equipped with JPTA can receive and transmit signals in multiple directions in a single time slot per RF chain, one direction per frequency subband, which alleviates the traditional constraint of one analog beam per transceiver chain per time slot. The utilization of subband-specific analog beams offers a new opportunity in designing mmWave systems, allowing for enhanced cell capacity and reduced pilot overhead. To understand the practical feasibility of JPTA, a few challenges and system design considerations are discussed in relation to the performance and complexity of the JPTA systems. For example, frequency-selective beam gain losses are present for the subband analog beams, e.g., up to 1 dB losses for 2 subband cases, even with the state-of-the-art JPTA delay and phase optimization methods. Despite these side effects, system-level analysis reveals that the JPTA system is capable of improving cell capacity: 5-percentile cell throughput by up to 65 per cent. To the best of the author's knowledge, this paper is the first paper explaining the system-level benefits and system-design challenges of JPTA, with an analysis of the performance tradeoff based on an intuitive metric of beam gain losses.

Paper number 79:
Title: Aberration Correcting Vision Transformers for High-Fidelity Metalens Imaging
Authors: Byeonghyeon Lee, Youbin Kim, Yongjae Jo, Hyunsu Kim, Hyemi Park, Yangkyu Kim, Debabrata Mandal, Praneeth Chakravarthula, Inki Kim, Eunbyung Park
Abstract: Metalens is an emerging optical system with an irreplaceable merit in that it can be manufactured in ultra-thin and compact sizes, which shows great promise in various applications. Despite its advantage in miniaturization, its practicality is constrained by spatially varying aberrations and distortions, which significantly degrade the image quality. Several previous arts have attempted to address different types of aberrations, yet most of them are mainly designed for the traditional bulky lens and ineffective to remedy harsh aberrations of the metalens. While there have existed aberration correction methods specifically for metalens, they still fall short of restoration quality. In this work, we propose a novel aberration correction framework for metalens-captured images, harnessing Vision Transformers (ViT) that have the potential to restore metalens images with non-uniform aberrations. Specifically, we devise a Multiple Adaptive Filters Guidance (MAFG), where multiple Wiener filters enrich the degraded input images with various noise-detail balances and a cross-attention module reweights the features considering the different degrees of aberrations. In addition, we introduce a Spatial and Transposed self-Attention Fusion (STAF) module, which aggregates features from spatial self-attention and transposed self-attention modules to further ameliorate aberration correction. We conduct extensive experiments, including correcting aberrated images and videos, and clean 3D reconstruction. The proposed method outperforms the previous arts by a significant margin. We further fabricate a metalens and verify the practicality of our method by restoring the images captured with the manufactured metalens. Code and pre-trained models are available at this https URL.

Paper number 80:
Title: TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model
Authors: Meilong Xu, Saumya Gupta, Xiaoling Hu, Chen Li, Shahira Abousamra, Dimitris Samaras, Prateek Prasanna, Chao Chen
Abstract: Accurately modeling multi-class cell topology is crucial in digital pathology, as it provides critical insights into tissue structure and pathology. The synthetic generation of cell topology enables realistic simulations of complex tissue environments, enhances downstream tasks by augmenting training data, aligns more closely with pathologists' domain knowledge, and offers new opportunities for controlling and generalizing the tumor microenvironment. In this paper, we propose a novel approach that integrates topological constraints into a diffusion model to improve the generation of realistic, contextually accurate cell topologies. Our method refines the simulation of cell distributions and interactions, increasing the precision and interpretability of results in downstream tasks such as cell detection and classification. To assess the topological fidelity of generated layouts, we introduce a new metric, Topological Frechet Distance (TopoFD), which overcomes the limitations of traditional metrics like FID in evaluating topological structure. Experimental results demonstrate the effectiveness of our approach in generating multi-class cell layouts that capture intricate topological relationships. Code is available at this https URL.

Paper number 81:
Title: Pfungst and Clever Hans: Identifying the unintended cues in a widely used Alzheimer's disease MRI dataset using explainable deep learning
Authors: Christian Tinauer, Maximilian Sackl, Rudolf Stollberger, Stefan Ropele, Christian Langkammer
Abstract: Backgrounds. Deep neural networks have demonstrated high accuracy in classifying Alzheimer's disease (AD). This study aims to enlighten the underlying black-box nature and reveal individual contributions of T1-weighted (T1w) gray-white matter texture, volumetric information and preprocessing on classification performance. Methods. We utilized T1w MRI data from the Alzheimer's Disease Neuroimaging Initiative to distinguish matched AD patients (990 MRIs) from healthy controls (990 MRIs). Preprocessing included skull stripping and binarization at varying thresholds to systematically eliminate texture information. A deep neural network was trained on these configurations, and the model performance was compared using McNemar tests with discrete Bonferroni-Holm correction. Layer-wise Relevance Propagation (LRP) and structural similarity metrics between heatmaps were applied to analyze learned features. Results. Classification performance metrics (accuracy, sensitivity, and specificity) were comparable across all configurations, indicating a negligible influence of T1w gray- and white signal texture. Models trained on binarized images demonstrated similar feature performance and relevance distributions, with volumetric features such as atrophy and skull-stripping features emerging as primary contributors. Conclusions. We revealed a previously undiscovered Clever Hans effect in a widely used AD MRI dataset. Deep neural networks classification predominantly rely on volumetric features, while eliminating gray-white matter T1w texture did not decrease the performance. This study clearly demonstrates an overestimation of the importance of gray-white matter contrasts, at least for widely used structural T1w images, and highlights potential misinterpretation of performance metrics.

Paper number 82:
Title: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis
Authors: Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao
Abstract: While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{S-DiT}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to S-DiT to reduce the difficulty of alignment learning without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that S-DiT achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at this https URL.

Paper number 83:
Title: Adaptive Mixture of Experts Learning for Robust Audio Spoofing Detection
Authors: Qixian Chen, Yuxiong Xu, Sara Mandelli, Sheng Li, Bin Li
Abstract: In audio spoofing detection, most studies rely on clean datasets, making models susceptible to real-world post-processing attacks, such as channel compression and noise. To overcome this challenge, we propose the Adaptive Mixture of Experts Learning (AMEL) framework, which enhances resilience by leveraging attack-specific knowledge and adapting dynamically to varied attack conditions. Specifically, AMEL utilizes Attack-Specific Experts (ASE) fine-tuned with Low-Rank Adaptation (LoRA), enabling each expert to target specific post-processing patterns while requiring only 1.12\% of the parameters needed for full fine-tuning. Furthermore, we introduce Dynamic Expert Aggregation (DEA), which adaptively selects and integrates expert knowledge to enhance the robustness of spoofing detection. Experimental results demonstrate that AMEL significantly enhances robustness by improving noise resilience and exhibiting greater adaptability to previously unseen post-processing methods compared to models relying on full fine-tuning. Additionally, our framework outperforms both single expert and simple average ensemble under various mixed attacks, demonstrating its superior robustness and adaptability in managing complex, real-world conditions.

Paper number 84:
Title: Internet of Things-Based Smart Precision Farming in Soilless Agriculture:Opportunities and Challenges for Global Food Security
Authors: Monica Dutta, Deepali Gupta, Sumegh Tharewal, Deepam Goyal, Jasminder Kaur Sandhu, Manjit Kaur, Ahmad Ali Alzubi, Jazem Mutared Alanazi
Abstract: The rapid growth of the global population and the continuous decline in cultivable land pose significant threats to food security. This challenge worsens as climate change further reduces the availability of farmland. Soilless agriculture, such as hydroponics, aeroponics, and aquaponics, offers a sustainable solution by enabling efficient crop cultivation in controlled environments. The integration of the Internet of Things (IoT) with smart precision farming improves resource efficiency, automates environmental control, and ensures stable and high-yield crop production. IoT-enabled smart farming systems utilize real-time monitoring, data-driven decision-making, and automation to optimize water and nutrient usage while minimizing human intervention. This paper explores the opportunities and challenges of IoT-based soilless farming, highlighting its role in sustainable agriculture, urban farming, and global food security. These advanced farming methods ensure greater productivity, resource conservation, and year-round cultivation. However, they also face challenges such as high initial investment, technological dependency, and energy consumption. Through a comprehensive study, bibliometric analysis, and comparative analysis, this research highlights current trends and research gaps. It also outlines future directions for researchers, policymakers, and industry stakeholders to drive innovation and scalability in IoT-driven soilless agriculture. By emphasizing the benefits of vertical farming and Controlled Environment Agriculture (CEA)-enabled soilless techniques, this paper supports informed decision-making to address food security challenges and promote sustainable agricultural innovations.

Paper number 85:
Title: Joint Superimposed Pilot-aided Channel Estimation and Data Detection for FTN Signaling over Doubly-Selective Channels
Authors: Simin Keykhosravi, Ebrahim Bedeer
Abstract: Faster-than-Nyquist (FTN) signaling and superimposed pilot (SP) techniques are effective solutions for significantly enhancing the spectral efficiency (SE) in next-generation wireless communication systems. This paper proposes an innovative SP-aided channel estimation method for FTN signaling enhancing the SE over doubly-selective (i.e., time- and frequency-selective) channels. To avoid complex channel tracking, we utilize a basis expansion model (BEM) to characterize doubly-selective channel variations. We propose a frame structure that superimposes a known periodic pilot sequence onto the information sequence, avoiding SE loss by eliminating the additional overhead of multiplexed pilot (MP). Additionally, we find the optimal FTN signaling SP sequence that minimizes the mean square error (MSE) of the channel estimation. Expanding on our proposed SP-aided channel estimation method, we propose two detection methods: (1) an SP-aided separate channel estimation and data detection (SCEDD) method performing a single channel estimation followed by iterative data detection via a turbo equalizer, serving as a baseline for evaluating the SP-aided channel estimation method, and (2) an SP-aided joint channel estimation and data detection (JCEDD) method, which extends the SCEDD by updating the channel estimate in each turbo equalization iteration, becoming our primary focus for its superior performance. At equivalent SE and a higher fading rate on the order of $10^{-3}$, our simulations show that SP-aided SCEDD method outperforms MP-aided techniques in both MSE and BER, while the SP-aided JCEDD method delivers remarkable performance, where reference approaches fail to track rapid channel variations.

Paper number 86:
Title: Multi-Disease-Aware Training Strategy for Cardiac MR Image Segmentation
Authors: Hong Zheng, Yucheng Chen, Nan Mu, Xiaoning Li
Abstract: Accurate segmentation of the ventricles from cardiac magnetic resonance images (CMRIs) is crucial for enhancing the diagnosis and analysis of heart conditions. Deep learning-based segmentation methods have recently garnered significant attention due to their impressive performance. However, these segmentation methods are typically good at partitioning regularly shaped organs, such as the left ventricle (LV) and the myocardium (MYO), whereas they perform poorly on irregularly shaped organs, such as the right ventricle (RV). In this study, we argue that this limitation of segmentation models stems from their insufficient generalization ability to address the distribution shift of segmentation targets across slices, cardiac phases, and disease conditions. To overcome this issue, we present a Multi-Disease-Aware Training Strategy (MTS) and restructure the introduced CMRI datasets into multi-disease datasets. Additionally, we propose a specialized data processing technique for preprocessing input images to support the MTS. To validate the effectiveness of our method, we performed control group experiments and cross-validation tests. The experimental results show that (1) network models trained using our proposed strategy achieved superior segmentation performance, particularly in RV segmentation, and (2) these networks exhibited robust performance even when applied to data from unknown diseases.

Paper number 87:
Title: Maximum Likelihood Estimation Based Complex-Valued Robust Chinese Remainder Theorem and Its Fast Algorithm
Authors: Xiaoping Li, Shiyang Sun, Qunying Liao, Xiang-Gen Xia
Abstract: Recently, a multi-channel self-reset analog-to-digital converter (ADC) system with complex-valued moduli has been proposed. This system enables the recovery of high dynamic range complex-valued bandlimited signals at low sampling rates via the Chinese remainder theorem (CRT). In this paper, we investigate complex-valued CRT (C-CRT) with erroneous remainders, where the errors follow wrapped complex Gaussian distributions. Based on the existing real-valued CRT utilizing maximum likelihood estimation (MLE), we propose a fast MLE-based C-CRT (MLE C-CRT). The proposed algorithm requires only $2L$ searches to obtain the optimal estimate of the common remainder, where $L$ is the number of moduli. Once the common remainder is estimated, the complex number can be determined using the C-CRT. Furthermore, we obtain a necessary and sufficient condition for the fast MLE C-CRT to achieve robust estimation. Finally, we apply the proposed algorithm to ADCs. The results demonstrate that the proposed algorithm outperforms the existing methods.

Paper number 88:
Title: Learning to segment anatomy and lesions from disparately labeled sources in brain MRI
Authors: Meva Himmetoglu, Ilja Ciernik, Ender Konukoglu (for the Alzheimer's Disease Neuroimaging Initiative)
Abstract: Segmenting healthy tissue structures alongside lesions in brain Magnetic Resonance Images (MRI) remains a challenge for today's algorithms due to lesion-caused disruption of the anatomy and lack of jointly labeled training datasets, where both healthy tissues and lesions are labeled on the same images. In this paper, we propose a method that is robust to lesion-caused disruptions and can be trained from disparately labeled training sets, i.e., without requiring jointly labeled samples, to automatically segment both. In contrast to prior work, we decouple healthy tissue and lesion segmentation in two paths to leverage multi-sequence acquisitions and merge information with an attention mechanism. During inference, an image-specific adaptation reduces adverse influences of lesion regions on healthy tissue predictions. During training, the adaptation is taken into account through meta-learning and co-training is used to learn from disparately labeled training images. Our model shows an improved performance on several anatomical structures and lesions on a publicly available brain glioblastoma dataset compared to the state-of-the-art segmentation methods.

Paper number 89:
Title: Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design
Authors: Konstantinos Ntougias, Symeon Chatzinotas, Ioannis Krikidis
Abstract: The emerging reflecting intelligent surface (RIS) technology promises to enhance the capacity of wireless communication systems via passive reflect beamforming. However, the product path loss limits its performance gains. Fully-connected (FC) active RIS, which integrates reflect-type power amplifiers into the RIS elements, has been recently introduced in response to this issue. Also, sub-connected (SC) active RIS and hybrid FC-active/passive RIS variants, which employ a limited number of reflect-type power amplifiers, have been proposed to provide energy savings. Nevertheless, their flexibility in balancing diverse capacity requirements and power consumption constraints is limited. In this direction, this study introduces novel hybrid RIS structures, wherein at least one reflecting sub-surface (RS) adopts the SC-active RIS design. The asymptotic signal-to-noise-ratio of the FC-active/passive and the proposed hybrid RIS variants is analyzed in a single-user single-input single-output setup. Furthermore, the transmit and RIS beamforming weights are jointly optimized in each scenario to maximize the energy efficiency of a hybrid RIS-aided multi-user multiple-input single-output downlink system subject to the power consumption constraints of the base station and the active RSs. Numerical simulation and analytic results highlight the performance gains of the proposed RIS designs over benchmarks, unveil non-trivial trade-offs, and provide valuable insights.

Paper number 90:
Title: Morphological Symmetries in Robotics
Authors: Daniel OrdoÃ±ez-Apraez, Giulio Turrisi, Vladimir Kostic, Mario Martin, Antonio Agudo, Francesc Moreno-Noguer, Massimiliano Pontil, Claudio Semini, Carlos Mastalli
Abstract: We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models through data augmentation, or by applying equivariant/invariant constraints on the model's architecture. In the context of analytical methods, we employ abstract harmonic analysis to decompose the robot's dynamics into a superposition of lower-dimensional, independent dynamics. We substantiate our claims with both synthetic and real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we introduce the repository MorphoSymm to facilitate the practical use of the theory and applications outlined in this work.

Paper number 91:
Title: SpeechVerse: A Large-scale Generalizable Audio Language Model
Authors: Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, Zhaocheng Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, Xilai Li, Karel Mundnich, Monica Sunkara, Sravan Bodapati, Sundararajan Srinivasan, Kyu J Han, Katrin Kirchhoff
Abstract: Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.

Paper number 92:
Title: A Comprehensive Survey with Critical Analysis for Deepfake Speech Detection
Authors: Lam Pham, Phat Lam, Dat Tran, Hieu Tang, Tin Nguyen, Alexander Schindler, Florian Skopik, Alexander Polonsky, Canh Vu
Abstract: Thanks to advancements in deep learning, speech generation systems now power a variety of real-world applications, such as text-to-speech for individuals with speech disorders, voice chatbots in call centers, cross-linguistic speech translation, etc. While these systems can autonomously generate human-like speech and replicate specific voices, they also pose risks when misused for malicious purposes. This motivates the research community to develop models for detecting synthesized speech (e.g., fake speech) generated by deep-learning-based models, referred to as the Deepfake Speech Detection task. As the Deepfake Speech Detection task has emerged in recent years, there are not many survey papers proposed for this task. Additionally, existing surveys for the Deepfake Speech Detection task tend to summarize techniques used to construct a Deepfake Speech Detection system rather than providing a thorough analysis. This gap motivated us to conduct a comprehensive survey, providing a critical analysis of the challenges and developments in Deepfake Speech Detection. Our survey is innovatively structured, offering an in-depth analysis of current challenge competitions, public datasets, and the deep-learning techniques that provide enhanced solutions to address existing challenges in the field. From our analysis, we propose hypotheses on leveraging and combining specific deep learning techniques to improve the effectiveness of Deepfake Speech Detection systems. Beyond conducting a survey, we perform extensive experiments to validate these hypotheses and propose a highly competitive model for the task of Deepfake Speech Detection. Given the analysis and the experimental results, we finally indicate potential and promising research directions for the Deepfake Speech Detection task.

Paper number 93:
Title: A Rapid Trajectory Optimization and Control Framework for Resource-Constrained Applications
Authors: Deep Parikh, Thomas L. Ahrens, Manoranjan Majji
Abstract: This paper presents a computationally efficient model predictive control formulation that uses an integral Chebyshev collocation method to enable rapid operations of autonomous agents. By posing the finite-horizon optimal control problem and recursive re-evaluation of the optimal trajectories, minimization of the L2 norms of the state and control errors are transcribed into a quadratic program. Control and state variable constraints are parameterized using Chebyshev polynomials and are accommodated in the optimal trajectory generation programs to incorporate the actuator limits and keep-out constraints. Differentiable collision detection of polytopes is leveraged for optimal collision avoidance. Results obtained from the collocation methods are benchmarked against the existing approaches on an edge computer to outline the performance improvements. Finally, collaborative control scenarios involving multi-agent space systems are considered to demonstrate the technical merits of the proposed work.

Paper number 94:
Title: A Generalist Audio Foundation Model for Comprehensive Body Sound Auscultation
Authors: Pingjie Wang, Liudan Zhao, Zihan Zhao, Miao He, Xin Sun, Ya Zhang, Kun Sun, Yanfeng Wang, Yu Wang
Abstract: Accurate and efficient auscultation-based diagnostics are vital for early disease detection, especially in resource-limited settings where specialized clinical expertise is scarce. Traditional auscultation, which heavily depends on clinician experience, suffers from significant inter-observer variability, while existing AI models often falter due to the limitations of non-representative training data. In this study, we introduce AuscultaBase, a novel AI-driven diagnostic framework that harnesses self-supervised and contrastive learning techniques alongside large-scale, multi-source data integration to advance body sound analysis. By generating robust feature representations, AuscultaBase markedly enhances performance in abnormality detection, disease classification, and activity recognition tasks. Comprehensive evaluations on our newly established benchmark, AuscultaBench, demonstrate that AuscultaBase consistently outperforms state-of-the-art methods across key performance metrics, underscoring its potential as a scalable and cost-effective tool for clinical screening and early disease intervention. The code and model checkpoint has been released in this https URL.

Paper number 95:
Title: RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression
Authors: Uri Gadot, Assaf Shocher, Shie Mannor, Gal Chechik, Assaf Hallak
Abstract: Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression.

Paper number 96:
Title: Explaining Control Policies through Predicate Decision Diagrams
Authors: Debraj Chakraborty, Clemens Dubslaff, Sudeep Kanav, Jan Kretinsky, Christoph Weinhuber
Abstract: Safety-critical controllers of complex systems are hard to construct manually. Automated approaches such as controller synthesis or learning provide a tempting alternative but usually lack explainability. To this end, learning decision trees (DTs) have been prevalently used towards an interpretable model of the generated controllers. However, DTs do not exploit shared decision-making, a key concept exploited in binary decision diagrams (BDDs) to reduce their size and thus improve explainability. In this work, we introduce predicate decision diagrams (PDDs) that extend BDDs with predicates and thus unite the advantages of DTs and BDDs for controller representation. We establish a synthesis pipeline for efficient construction of PDDs from DTs representing controllers, exploiting reduction techniques for BDDs also for PDDs.

Paper number 97:
Title: A Quantum Neural Network Transfer-Learning Model for Forecasting Problems with Continuous and Discrete Variables
Authors: Ismael Abdulrahman
Abstract: This study introduces simple yet effective continuous- and discrete-variable quantum neural network (QNN) models as a transfer-learning approach for forecasting tasks. The CV-QNN features a single quantum layer with two qubits to establish entanglement and utilizes a minimal set of quantum gates, including displacement, rotation, beam splitter, squeezing, and a non-Gaussian cubic-phase gate, with a maximum of eight trainable parameters. A key advantage of this model is its ability to be trained on a single dataset, after which the learned parameters can be transferred to other forecasting problems with little to no fine-tuning. Initially trained on the Kurdistan load demand dataset, the model's frozen parameters are successfully applied to various forecasting tasks, including energy consumption, traffic flow, weather conditions, and cryptocurrency price prediction, demonstrating strong performance. Furthermore, the study introduces a discrete-variable quantum model with an equivalent 2- and 4-wire configuration and presents a performance assessment, showing good but relatively lower effectiveness compared to the continuous-variable model.

Paper number 98:
Title: CTorch: PyTorch-Compatible GPU-Accelerated Auto-Differentiable Projector Toolbox for Computed Tomography
Authors: Xiao Jiang, Grace J. Grace, J. Webster Stayman
Abstract: This work introduces CTorch, a PyTorch-compatible, GPU-accelerated, and auto-differentiable projector toolbox designed to handle various CT geometries with configurable projector algorithms. CTorch provides flexible scanner geometry definition, supporting 2D fan-beam, 3D circular cone-beam, and 3D non-circular cone-beam geometries. Each geometry allows view-specific definitions to accommodate variations during scanning. Both flat- and curved-detector models may be specified to accommodate various clinical devices. CTorch implements four projector algorithms: voxel-driven, ray-driven, distance-driven (DD), and separable footprint (SF), allowing users to balance accuracy and computational efficiency based on their needs. All the projectors are primarily built using CUDA C for GPU acceleration, then compiled as Python-callable functions, and wrapped as PyTorch network module. This design allows direct use of PyTorch tensors, enabling seamless integration into PyTorch's auto-differentiation framework. These features make CTorch an flexible and efficient tool for CT imaging research, with potential applications in accurate CT simulations, efficient iterative reconstruction, and advanced deep-learning-based CT reconstruction.

Paper number 99:
Title: Imaging Intravoxel Vessel Size Distribution in the Brain Using Susceptibility Contrast Enhanced MRI
Authors: Natenael B. Semmineh, Indranil Guha, Deborah Healey, Anagha Chandrasekharan, Jerrold L. Boxerman, C. Chad Quarles
Abstract: Vascular remodelling is inherent to the pathogenesis of many diseases including cancer, neurodegeneration, fibrosis, hypertension, and diabetes. In this paper, a new susceptibility-contrast based MRI approach is established to non-invasively image intravoxel vessel size distribution (VSD), enabling a more comprehensive and quantitative assessment of vascular remodelling. The approach utilizes high-resolution light-sheet fluorescence microscopy images of rodent brain vasculature, simulating gradient echo sampling of free induction decay and spin echo (GESFIDE) MRI signals for the three-dimensional vascular networks, and training a deep learning model to predict cerebral blood volume (CBV) and VSD from GESFIDE signals. The results from ex vivo experiments demonstrated strong correlation (r = 0.96) between the true and predicted CBV. High similarity between true and predicted VSDs was observed (mean Bhattacharya Coefficient = 0.92). With further in vivo validation, intravoxel VSD imaging could become a transformative preclinical and clinical tool for interrogating disease and treatment induced vascular remodelling.
    