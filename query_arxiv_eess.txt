
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals
Authors: Seong-A Park, Jong-Eui Chae, Sungdong Kim, Hyung-Chul Lee, Hyun-Lim Yang
Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient prognosis, necessitating the integrated analysis of multiple physiological signals. While recent research has analyzed single signals such as electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a proposal for an approach that encompasses the complex signal analysis required in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul National University hospital PHYsiological signal Masked representation learning) model extracts physiological features reflecting the electrical, pressure, and fluid characteristics of the cardiac cycle in the process of restoring three masked physiological signals based on self-supervised learning (SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing multiple physical characteristics, the model can extract more enriched features only using non-invasive signals. We evaluated the model's performance in clinical downstream tasks such as hypotension, stroke volume, systolic blood pressure, diastolic blood pressure, and age prediction. Our results showed that the SNUPHY-M significantly outperformed supervised or SSL models, especially in prediction tasks using non-invasive signals. To the best of our knowledge, SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis involving ECG, PPG, and ABP signals. This approach effectively supports clinical decision-making and enables precise diagnostics, contributing significantly to the early diagnosis and management of hemodynamics without invasiveness.

Paper number 2:
Title: USEANet: Ultrasound-Specific Edge-Aware Multi-Branch Network for Lightweight Medical Image Segmentation
Authors: Jingyi Gao, Di Wu, Baha lhnaini
Abstract: Ultrasound image segmentation faces unique challenges including speckle noise, low contrast, and ambiguous boundaries, while clinical deployment demands computationally efficient models. We propose USEANet, an ultrasound-specific edge-aware multi-branch network that achieves optimal performance-efficiency balance through four key innovations: (1) ultrasound-specific multi-branch processing with specialized modules for noise reduction, edge enhancement, and contrast improvement; (2) edge-aware attention mechanisms that focus on boundary information with minimal computational overhead; (3) hierarchical feature aggregation with adaptive weight learning; and (4) ultrasound-aware decoder enhancement for optimal segmentation refinement. Built on an ultra-lightweight PVT-B0 backbone, USEANet significantly outperforms existing methods across five ultrasound datasets while using only 3.64M parameters and 0.79G FLOPs. Experimental results demonstrate superior segmentation accuracy with 67.01 IoU on BUSI dataset, representing substantial improvements over traditional approaches while maintaining exceptional computational efficiency suitable for real-time clinical applications. Code is available at this https URL.

Paper number 3:
Title: WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks
Authors: Felipe Álvarez Barrientos, Tomás Banduc, Isabeau Sirven, Francisco Sahli Costabal
Abstract: The contractile motion of the heart is strongly determined by the distribution of the fibers that constitute cardiac tissue. Strain analysis informed with the orientation of fibers allows to describe several pathologies that are typically associated with impaired mechanics of the myocardium, such as cardiovascular disease. Several methods have been developed to estimate strain-derived metrics from traditional imaging techniques. However, the physical models underlying these methods do not include fiber mechanics, restricting their capacity to accurately explain cardiac function. In this work, we introduce WarpPINN-fibers, a physics-informed neural network framework to accurately obtain cardiac motion and strains enhanced by fiber information. We train our neural network to satisfy a hyper-elastic model and promote fiber contraction with the goal to predict the deformation field of the heart from cine magnetic resonance images. For this purpose, we build a loss function composed of three terms: a data-similarity loss between the reference and the warped template images, a regularizer enforcing near-incompressibility of cardiac tissue and a fiber-stretch penalization that controls strain in the direction of synthetically produced fibers. We show that our neural network improves the former WarpPINN model and effectively controls fiber stretch in a synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers outperforms alternative methodologies in landmark-tracking and strain curve prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We expect that our method will enable a more precise quantification of cardiac strains through accurate deformation fields that are consistent with fiber physiology, without requiring imaging techniques more sophisticated than MRI.

Paper number 4:
Title: Generalized User-Oriented Image Semantic Coding Empowered by Large Vision-Language Model
Authors: Sin-Yu Huang, Vincent W.S. Wong
Abstract: Semantic communication has shown outstanding performance in preserving the overall source information in wireless transmission. For semantically rich content such as images, human users are often interested in specific regions depending on their intent. Moreover, recent semantic coding models are mostly trained on specific datasets. However, real-world applications may involve images out of the distribution of training dataset, which makes generalization a crucial but largely unexplored problem. To incorporate user's intent into semantic coding, in this paper, we propose a generalized user-oriented image semantic coding (UO-ISC) framework, where the user provides a text query indicating its intent. The transmitter extracts features from the source image which are relevant to the user's query. The receiver reconstructs an image based on those features. To enhance the generalization ability, we integrate contrastive language image pre-training (CLIP) model, which is a pretrained large vision-language model (VLM), into our proposed UO-ISC framework. To evaluate the relevance between the reconstructed image and the user's query, we introduce the user-intent relevance loss, which is computed by using a pretrained large VLM, large language-and-vision assistant (LLaVA) model. When performing zero-shot inference on unseen objects, simulation results show that the proposed UO-ISC framework outperforms the state-of-the-art query-aware image semantic coding in terms of the answer match rate.

Paper number 5:
Title: Bridging Centralized and Distributed Frameworks in Unknown Input Observer Design
Authors: Ruixuan Zhao, Guitao Yang, Peng Li, Boli Chen
Abstract: State estimation for linear time-invariant systems with unknown inputs is a fundamental problem in various research domains. In this article, we establish conditions for the design of unknown input observers (UIOs) from a geometric approach perspective. Specifically, we derive a necessary and sufficient geometric condition for the existence of a centralized UIO. Compared to existing results, our condition offers a more general design framework, allowing designers the flexibility to estimate partial information of the system state. Furthermore, we extend the centralized UIO design to distributed settings. In contrast to existing distributed UIO approaches, which require each local node to satisfy the rank condition regarding the unknown input and output matrices, our method accommodates cases where a subset of nodes does not meet this requirement. This relaxation significantly broadens the range of practical applications. Simulation results are provided to demonstrate the effectiveness of the proposed design.

Paper number 6:
Title: Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities
Authors: Jarvis Haupt, Qin Lu, Yanning Shen, Jia Chen, Yue Dong, Dan McCreary, Mehmet Akçakaya, Georgios B. Giannakis
Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years -- including large language models, automated coding assistants, and advanced image and speech generation technologies -- are the result of monumental human achievements. These breakthroughs reflect mastery across multiple technical disciplines and the resolution of significant technological challenges. However, some of the most profound challenges may still lie ahead. These challenges are not purely technical but pertain to the fair and responsible use of AI in ways that genuinely improve the global human condition. This article explores one promising application aligned with that vision: the use of AI tools to facilitate and enhance education, with a specific focus on signal processing (SP). It presents two interrelated perspectives: identifying and addressing technical limitations, and applying AI tools in practice to improve educational experiences. Primers are provided on several core technical issues that arise when using AI in educational settings, including how to ensure fairness and inclusivity, handle hallucinated outputs, and achieve efficient use of resources. These and other considerations -- such as transparency, explainability, and trustworthiness -- are illustrated through the development of an immersive, structured, and reliable "smart textbook." The article serves as a resource for researchers and educators seeking to advance AI's role in engineering education.

Paper number 7:
Title: Multi-Agent Inverse Reinforcement Learning for Identifying Pareto-Efficient Coordination -- A Distributionally Robust Approach
Authors: Luke Snow, Vikram Krishnamurthy
Abstract: Multi-agent inverse reinforcement learning (IRL) aims to identify Pareto-efficient behavior in a multi-agent system, and reconstruct utility functions of the individual agents. Motivated by the problem of detecting UAV coordination, how can we construct a statistical detector for Pareto-efficient behavior given noisy measurements of the decisions of a multi-agent system? This paper approaches this IRL problem by deriving necessary and sufficient conditions for a dataset of multi-agent system dynamics to be consistent with Pareto-efficient coordination, and providing algorithms for recovering utility functions which are consistent with the system dynamics. We derive an optimal statistical detector for determining Pareto-efficient coordination from noisy system measurements, which minimizes Type-I statistical detection error. Then, we provide a utility estimation algorithm which minimizes the worst-case estimation error over a statistical ambiguity set centered at empirical observations; this min-max solution achieves distributionally robust IRL, which is crucial in adversarial strategic interactions. We illustrate these results in a detailed example for detecting Pareto-efficient coordination among multiple UAVs given noisy measurement recorded at a radar. We then reconstruct the utility functions of the UAVs in a distributionally robust sense.

Paper number 8:
Title: Efficient High-Order Participation Factor Computation via Batch-Structured Tensor Contraction
Authors: Mahsa Sajjadi, Kaiyang Huang, Kai Sun
Abstract: Participation factors (PFs) quantify the interaction between system modes and state variables, and they play a crucial role in various applications such as modal analysis, model reduction, and control design. With increasing system complexity, especially due to power electronic devices and renewable integration, the need for scalable and high-order nonlinear PF (NPF) computation has become more critical. This paper presents an efficient tensor-based method for calculating NPFs up to an arbitrary order. Traditional computation of PFs directly from normal form theory is computationally expensive -- even for second-order PFs -- and becomes infeasible for higher orders due to memory constraints. To address this, a tensor contraction-based approach is introduced that enables the calculation of high-order PFs using a batching strategy. The batch sizes are dynamically determined based on the available computational resources, allowing scalable and memory-efficient computation.

Paper number 9:
Title: Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography
Authors: Harshit Agrawal, Ari Hietanen, Simo Särkkä
Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam computed tomography (CBCT) scans. Although deep learning-based methods show promise in estimating scatter from CBCT measurements, their deployment in mobile CBCT systems or edge devices is still limited due to the large memory footprint of the networks. This study addresses the issue by applying networks at varying resolutions and suggesting an optimal one, based on speed and accuracy. Methods: First, the reconstruction error in down-up sampling of CBCT scatter signal was examined at six resolutions by comparing four interpolation methods. Next, a recent state-of-the-art method was trained across five image resolutions and evaluated for the reductions in floating-point operations (FLOPs), inference times, and GPU memory requirements. Results: Reducing the input size and network parameters achieved a 78-fold reduction in FLOPs compared to the baseline method, while maintaining comarable performance in terms of mean-absolute-percentage-error (MAPE) and mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to 4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times 10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and 12, respectively. Further experiments comparing scatter-corrected reconstructions on a large, simulated dataset and real CBCT scans from water and Sedentex CT phantoms clearly demonstrated the robustness of our method. Conclusion: This study highlights the underappreciated role of downsampling in deep learning-based scatter estimation. The substantial reduction in FLOPs and GPU memory requirements achieved by our method enables scatter correction in resource-constrained environments, such as mobile CBCT and edge devices.

Paper number 10:
Title: 6G Resilience -- White Paper
Authors: Hirley Alves, Nurul H. Mahmood, Onel L. A. López, Sumudu Samarakoon, Seppo Yrjölä, Matti Latva-Aho, Markku Juntti, Ari Pouttu, Armin Dekorsy, Arthur Sousa de Sena, Aydin Sezgin, Bho Matthiesen, Chafika Benzaid, Chathuranga Weeraddana, David Hutchison, Dileepa Marasinghe, Doganalp Ergenc, Eduard Jorswieck, Erkki Harjula, Falko Dressler, Harri Saarnisaari, Italo Atzeni, Jaap Van De Beek, Jacek Rak, Konstantin Mikhaylov, Lauri Loven, Madhusanka Liyanage, Marcos Katz, Marja Matinmikko-Blue, Mehdi Rasti, Mika Ylianttila Nhan Nguyen, Pawani Porambage, Petar Popovski, Petri Ahokangas, Premanandana Rajatheva, Robert-Jeron Reifert, Tharaka Hewa, Tommy Svensson
Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged, complex disruptions. Mobile networks' shift from efficiency-first to sustainability-aware has motivated this white paper to assert that resilience is a primary design goal, alongside sustainability and efficiency, encompassing technology, architecture, and economics. We promote resilience by analysing dependencies between mobile networks and other critical systems, such as energy, transport, and emergency services, and illustrate how cascading failures spread through infrastructures. We formalise resilience using the 3R framework: reliability, robustness, resilience. Subsequently, we translate this into measurable capabilities: graceful degradation, situational awareness, rapid reconfiguration, and learning-driven improvement and recovery. Architecturally, we promote edge-native and locality-aware designs, open interfaces, and programmability to enable islanded operations, fallback modes, and multi-layer diversity (radio, compute, energy, timing). Key enablers include AI-native control loops with verifiable behaviour, zero-trust security rooted in hardware and supply-chain integrity, and networking techniques that prioritise critical traffic, time-sensitive flows, and inter-domain coordination. Resilience also has a techno-economic aspect: open platforms and high-quality complementors generate ecosystem externalities that enhance resilience while opening new markets. We identify nine business-model groups and several patterns aligned with the 3R objectives, and we outline governance and standardisation. This white paper serves as an initial step and catalyst for 6G resilience. It aims to inspire researchers, professionals, government officials, and the public, providing them with the essential components to understand and shape the development of 6G resilience.

Paper number 11:
Title: Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data
Authors: Xueyi Wang, C. J. C. (Claudine)Lamoth, Elisabeth Wilhelm
Abstract: A sleep forecast allows individuals and healthcare providers to anticipate and proactively address factors influencing restful rest, ultimately improving mental and physical well-being. This work presents an adaptive spatial and temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model combines convolutional layers to capture spatial feature interactions between multiple features and recurrent neural network layers to handle longer-term temporal health-related data. A domain classifier is further integrated to generalize across different subjects. We conducted several experiments using five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes (1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline models, achieving its lowest RMSE (0.282) with a seven-day input window and a one-day predicting window. Moreover, the method maintained strong performance even when forecasting multiple days into the future, demonstrating its versatility for real-world applications. Visual comparisons reveal that the model accurately tracks both the overall sleep score level and daily fluctuations. These findings prove that the proposed framework provides a robust and adaptable solution for personalized sleep forecasting using sparse data from commercial wearable devices and domain adaptation techniques.

Paper number 12:
Title: Design of Reliable and Resilient Electric Power Systems for Wide-Body All-Electric Aircraft
Authors: Mona Ghassemi
Abstract: To achieve net-zero emissions by 2050, all-electric transportation is a promising option. In the U.S., the transportation sector contributes the largest share (29 percent) of greenhouse gas emissions. While electric vehicles are approaching maturity, aviation is only beginning to develop electrified aircraft for commercial flights. More than 75 percent of aviation emissions come from large aircraft, and this impact will worsen with 4-5 percent annual air travel growth. Aircraft electrification has led to two types: more electric aircraft (MEA) and all-electric aircraft (AEA). A MEA replaces subsystems such as hydraulics with electric alternatives, whereas an AEA uses electrically driven subsystems and provides thrust fully from electrochemical energy units (EEUs). For wide-body AEA, thrust demand is about 25 MW plus 1 MW for non-thrust loads, creating major challenges for electric power system (EPS) design. Achieving maximum power density requires minimizing mass and volume. Increasing voltage into the kilovolt range using medium-voltage direct current (MVDC) is a feasible option to enhance power transfer. Consequently, designing an MVDC EPS for wide-body AEA is critical. Because EPS failures could jeopardize passenger safety, reliability and resilience are essential. This chapter presents a load-flow model for DC systems to determine power flows in both normal and single-contingency conditions, followed by analysis of optimal MVDC EPS architectures. A complete EPS for wide-body AEA is introduced, with EEUs and non-propulsion loads located, distances estimated, and flow studies performed. Multiple architectures are evaluated for reliability, power density, power loss, and cost to identify optimal solutions.

Paper number 13:
Title: Decentralized Local Voltage Control for Active Distribution Networks
Authors: Diana Vieira Fernandes, Soummya Kar, Carlos Santos Silva
Abstract: Distribution networks face challenges from the increasing deployment of Distributed Energy Resources (DERs) and the emergence of bidirectional power flows. We propose a decentralized Volt/VAr control method based on a saddle-point reformulation and consensus+innovation (C+I) updates. Each agent at a controllable bus computes and enforces its own set-points using only neighbor communication. Our method embeds passive buses directly, preserves network physics through a linearized Jacobian model, and avoids any supervisory nodes. Simulation results on a modified CIGRE low-voltage network show voltage stability improvement within operational limits, indicating the viability of a fully decentralized (edge-based) Volt/VAr control solution.

Paper number 14:
Title: Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)
Authors: Michael Caulfield, Randy Palamar, Darren Dahunsi, Mohammad Rahim Sobhani, Negar Majidi, Roger Zemp
Abstract: Recent developments in Row Column Arrays (RCAs) have presented promising options for volumetric imaging without the need for the excessive channel counts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top Orthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that imaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning (FORCES) allow for full transmit and receive focusing everywhere in the image plane. However, due to its fixed elevational focus and large transmit aperture, FORCES experiences poor elevational focusing away from the focal point. In this study we present a modification to the FORCES imaging scheme by applying Retrospective Transmit Beamforming (RTB) in the elevational direction to allow for elevational transmit focusing everywhere in the imaging plane. We evaluate FORCES and uFORCES methods, with and without RTB applied, when imaging both a cyst and wire phantom. With experiment we show improved elevational focusing capabilities away from the focal point when RTB is applied to both FORCES and uFORCES. At the focal point, performance with RTB remains comparable or improved relative to standard FORCES. This is quantified by the measurement of Full Width Half Max when imaging the wire phantom, and by the generalized Contrast to Noise Ratio when imaging the tubular cyst phantom. We also demonstrate the volumetric imaging capabilities of FORCES RTB with the wire phantom.

Paper number 15:
Title: Optimal Control of an SIR Model with Noncompliance as a Social Contagion
Authors: Chloe Ngo, Christian Parkinson, Weinan Wang
Abstract: We propose and study a compartmental model for epidemiology with human behavioral effects. Specifically, our model incorporates governmental prevention measures aimed at lowering the disease infection rate, but we split the population into those who comply with the measures and those who do not comply and therefore do not receive the reduction in infectivity. We then allow the attitude of noncompliance to spread as a social contagion parallel to the disease. We derive the reproductive ratio for our model and provide stability analysis for the disease-free equilibria. We then propose a control scenario wherein a policy-maker with access to control variables representing disease prevention mandates, treatment efforts, and educational campaigns aimed at encouraging compliance minimizes a cost functional incorporating several cost concerns. We characterize optimal controls via the Pontryagin optimality principle and present simulations which demonstrate the behavior of the control maps in several different parameter regimes.

Paper number 16:
Title: Signed Graph Learning with Hidden Nodes
Authors: Rong Ye, Xue-Qin Jiang, Hui Feng, Jian Wang, Runhe Qiu
Abstract: Signed graphs, which are characterized by both positive and negative edge weights, have recently attracted significant attention in the field of graph signal processing (GSP). Existing works on signed graph learning typically assume that all graph nodes are available. However, in some specific applications, only a subset of nodes can be observed while the remaining nodes stay hidden. To address this challenge, we propose a novel method for identifying signed graph that accounts for hidden nodes, termed \textit{signed graph learning with hidden nodes under column-sparsity regularization} (SGL-HNCS). Our method is based on the assumption that graph signals are smooth over signed graphs, i.e., signal values of two nodes connected by positive (negative) edges are similar (dissimilar). Rooted in this prior assumption, the topology inference of a signed graph is formulated as a constrained optimization problem with column-sparsity regularization, where the goal is to reconstruct the signed graph Laplacian matrix without disregarding the influence of hidden nodes. We solve the constrained optimization problem using a tailored block coordinate descent (BCD) approach. Experimental results using synthetic data and real-world data demonstrate the efficiency of the proposed SGL-HNCS method.

Paper number 17:
Title: Sequential Spectral Clustering of Data Sequences
Authors: G Dhinesh Chandran, Kota Srinivas Reddy, Srikrishna Bhashyam
Abstract: We study the problem of nonparametric clustering of data sequences, where each data sequence comprises i.i.d. samples generated from an unknown distribution. The true clusters are the clusters obtained using the Spectral clustering algorithm (SPEC) on the pairwise distance between the true distributions corresponding to the data sequences. Since the true distributions are unknown, the objective is to estimate the clusters by observing the minimum number of samples from the data sequences for a given error probability. To solve this problem, we propose the Sequential Spectral clustering algorithm (SEQ-SPEC), and show that it stops in finite time almost surely and is exponentially consistent. We also propose a computationally more efficient algorithm called the Incremental Approximate Sequential Spectral clustering algorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed algorithms perform better than the fixed sample size SPEC, the Sequential $K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage clustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally efficient, performs close to SEQ-SPEC on both synthetic and real-world datasets. To the best of our knowledge, this is the first work on spectral clustering of data sequences under a sequential framework.

Paper number 18:
Title: KAN-Therm: A Lightweight Battery Thermal Model Using Kolmogorov-Arnold Network
Authors: Soumyoraj Mallick, Sanchita Ghosh, Tanushree Roy
Abstract: Battery management systems (BMSs) rely on real-time estimation of battery temperature distribution in battery cells to ensure safe and optimal operation of Lithium-ion batteries (LIBs). However, physical BMS often suffers from memory and computational resource limitations required by highfidelity models. Temperature prediction using physics-based models becomes challenging due to their higher computational time. In contrast, machine learning based approaches offer faster predictions but demand larger memory overhead. In this work, we develop a lightweight and efficient Kolmogorov-Arnold networks (KAN) based thermal model, KAN-Therm, to predict the core temperature of a cylindrical battery. We have compared the memory overhead and computation costs of our method with Multi-layer perceptron (MLP), recurrent neural network (RNN), and long shortterm memory (LSTM) network. Our results show that the proposed KAN-Therm model exhibit the best prediction accuracy with the least memory overhead and computation time.

Paper number 19:
Title: JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information
Authors: Ziqi Yan, Zhichao Zhang
Abstract: Wiener filtering in the joint time-vertex fractional Fourier transform (JFRFT) domain has shown high effectiveness in denoising time-varying graph signals. Traditional filtering models use grid search to determine the transform-order pair and compute filter coefficients, while learnable ones employ gradient-descent strategies to optimize them; both require complete prior information of graph signals. To overcome this shortcoming, this letter proposes a data-model co-driven denoising approach, termed neural-network-aided joint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the JFRFT-domain Wiener filter model into a neural network and updates the transform-order pair and filter coefficients through a data-driven approach. This design enables effective denoising using only partial prior information. Experiments demonstrate that JFRFFNet achieves significant improvements in output signal-to-noise ratio compared with some state-of-the-art methods.

Paper number 20:
Title: Automotive sound field reproduction using deep optimization with spatial domain constraint
Authors: Yufan Qian, Tianshu Qu, Xihong Wu
Abstract: Sound field reproduction with undistorted sound quality and precise spatial localization is desirable for automotive audio systems. However, the complexity of automotive cabin acoustic environment often necessitates a trade-off between sound quality and spatial accuracy. To overcome this limitation, we propose Spatial Power Map Net (SPMnet), a learning-based sound field reproduction method that improves both sound quality and spatial localization in complex environments. We introduce a spatial power map (SPM) constraint, which characterizes the angular energy distribution of the reproduced field using beamforming. This constraint guides energy toward the intended direction to enhance spatial localization, and is integrated into a multi-channel equalization framework to also improve sound quality under reverberant conditions. To address the resulting non-convexity, deep optimization that use neural networks to solve optimization problems is employed for filter design. Both in situ objective and subjective evaluations confirm that our method enhances sound quality and improves spatial localization within the automotive cabin. Furthermore, we analyze the influence of different audio materials and the arrival angles of the virtual sound source in the reproduced sound field, investigating the potential underlying factors affecting these results.

Paper number 21:
Title: MAPSS: Manifold-based Assessment of Perceptual Source Separation
Authors: Amir Ivry, Samuele Cornell, Shinji Watanabe
Abstract: Objective assessment of source-separation systems still mismatches subjective human perception, especially when leakage and self-distortion interact. We introduce the Perceptual Separation (PS) and Perceptual Match (PM), the first pair of measures that functionally isolate these two factors. Our intrusive method begins with generating a bank of fundamental distortions for each reference waveform signal in the mixture. Distortions, references, and their respective system outputs from all sources are then independently encoded by a pre-trained self-supervised learning model. These representations are aggregated and projected onto a manifold via diffusion maps, which aligns Euclidean distances on the manifold with dissimilarities of the encoded waveforms. On this manifold, the PM measures the Mahalanobis distance from each output to its attributed cluster that consists of its reference and distortions embeddings, capturing self-distortion. The PS accounts for the Mahalanobis distance of the output to the attributed and to the closest non-attributed clusters, quantifying leakage. Both measures are differentiable and granular, operating at a resolution as low as 50 frames per second. We further derive, for both measures, deterministic error radius and non-asymptotic, high-probability confidence intervals (CIs). Experiments on English, Spanish, and music mixtures show that the PS and PM nearly always achieve the highest linear correlation coefficients with human mean-opinion scores than 14 competitors, reaching as high as 86.36% for speech and 87.21% for music. We observe, at worst, an error radius of 1.39% and a probabilistic 95% CI of 12.21% for these coefficients, which improves reliable and informed evaluation. Using mutual information, the measures complement each other most as their values decrease, suggesting they are jointly more informative as system performance degrades.

Paper number 22:
Title: On Sampling of Multiple Correlated Stochastic Signals
Authors: Lin Jin, Hang Sheng, Hui Feng, Bo Hu
Abstract: Multiple stochastic signals possess inherent statistical correlations, yet conventional sampling methods that process each channel independently result in data redundancy. To leverage this correlation for efficient sampling, we model correlated channels as a linear combination of a smaller set of uncorrelated, wide-sense stationary latent sources. We establish a theoretical lower bound on the total sampling density for zero mean-square error reconstruction, proving it equals the ratio of the joint spectral bandwidth of latent sources to the number of correlated signal channels. We then develop a constructive multi-band sampling scheme that attains this bound. The proposed method operates via spectral partitioning of the latent sources, followed by spatio-temporal sampling and interpolation. Experiments on synthetic and real datasets confirm that our scheme achieves near-lossless reconstruction precisely at the theoretical sampling density, validating its efficiency.

Paper number 23:
Title: Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery
Authors: Yinzheng Zhao, Zhihao Zhao, Rundong Jiang, Louisa Sackewitz, Quanmin Liang, Mathias Maier, Daniel Zapp, Peter Charbel Issa, Mohammad Ali Nasseri
Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their integration within a multimodal deep learning (DL) framework for predicting postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH) patients. Methods: We utilized a publicly available longitudinal OCT dataset at five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage specific segmentation model delineated related structures, and an automated pipeline extracted quantitative, composite, qualitative, and dynamic features. Binary logistic regression models, constructed with and without dynamic parameters, assessed their incremental predictive value for best-corrected visual acuity (BCVA). A multimodal DL model combining clinical variables, OCT-derived features, and raw OCT images was developed and benchmarked against regression models. Results: The segmentation model achieved high accuracy across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses identified base diameter, ellipsoid zone integrity, and macular hole area as significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates consistently improved logistic regression AUC, especially at the 3-month follow-up. The multimodal DL model outperformed logistic regression, yielding higher AUCs and overall accuracy at each stage. The difference is as high as 0.12, demonstrating the complementary value of raw image volume and dynamic parameters. Conclusions: Integrating dynamic parameters into the multimodal DL model significantly enhances the accuracy of predictions. This fully automated process therefore represents a promising clinical decision support tool for personalized postoperative management in macular hole surgery.

Paper number 24:
Title: Virtual staining for 3D X-ray histology of bone implants
Authors: Sarah C. Irvine, Christian Lucas, Diana Krüger, Bianca Guedert, Julian Moosmann, Berit Zeller-Plumhoff
Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative to conventional 2D histology, enabling volumetric imaging of biological tissues without the need for physical sectioning or chemical staining. However, the inherent greyscale image contrast of X-ray tomography limits its biochemical specificity compared to traditional histological stains. Within digital pathology, deep learning-based virtual staining has demonstrated utility in simulating stained appearances from label-free optical images. In this study, we extend virtual staining to the X-ray domain by applying cross-modality image translation to generate artificially stained slices from synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image pairs of micro-CT and toluidine blue-stained histology from bone-implant samples, we trained a modified CycleGAN network tailored for limited paired data. Whole slide histology images were downsampled to match the voxel size of the CT data, with on-the-fly data augmentation for patch-based training. The model incorporates pixelwise supervision and greyscale consistency terms, producing histologically realistic colour outputs while preserving high-resolution structural detail. Our method outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the model can be applied to full CT volumes to generate virtually stained 3D datasets, enhancing interpretability without additional sample preparation. While features such as new bone formation were able to be reproduced, some variability in the depiction of implant degradation layers highlights the need for further training data and refinement. This work introduces virtual staining to 3D X-ray imaging and offers a scalable route for chemically informative, label-free tissue characterisation in biomedical research.

Paper number 25:
Title: A novel method and dataset for depth-guided image deblurring from smartphone Lidar
Authors: Antonio Montanaro, Diego Valsesia
Abstract: Modern smartphones are equipped with Lidar sensors providing depth-sensing capabilities. Recent works have shown that this complementary sensor allows to improve various tasks in image processing, including deblurring. However, there is a current lack of datasets with realistic blurred images and paired mobile Lidar depth maps to further study the topic. At the same time, there is also a lack of blind zero-shot methods that can deblur a real image using the depth guidance without requiring extensive training sets of paired data. In this paper, we propose an image deblurring method based on denoising diffusion models that can leverage the Lidar depth guidance and does not require training data with paired Lidar depth maps. We also present the first dataset with real blurred images with corresponding Lidar depth maps and sharp ground truth images, acquired with an Apple iPhone 15 Pro, for the purpose of studying Lidar-guided deblurring. Experimental results on this novel dataset show that Lidar guidance is effective and the proposed method outperforms state-of-the-art deblurring methods in terms of perceptual quality.

Paper number 26:
Title: Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG
Authors: Davoud Hajhassani (GIPSA-VIBS), Quentin Barthélemy, Jérémie Mattout (CRNL, CRNL-COPHY), Marco Congedo (GIPSA-VIBS)
Abstract: Electroencephalography (EEG) signal cleaning has long been a critical challenge in the research community. The presence of artifacts can significantly degrade EEG data quality, complicating analysis and potentially leading to erroneous interpretations. While various artifact rejection methods have been proposed, the gold standard remains manual visual inspection by human experts-a process that is time-consuming, subjective, and impractical for large-scale EEG studies. Existing techniques are often hindered by a strong reliance on manual hyperparameter tuning, sensitivity to outliers, and high computational costs. In this paper, we introduce the improved Riemannian Potato Field (iRPF), a fast and fully automated method for EEG artifact rejection that addresses key limitations of current approaches. We evaluate iRPF against several state-of-the-art artifact rejection methods, using two publicly available EEG databases, labeled for various artifact types, comprising 226 EEG recordings. Our results demonstrate that iRPF outperforms all competitors across multiple metrics, with gains of up to 22% in recall, 102% in specificity, 54% in precision, and 24% in F1-score, compared to Isolation Forest, Autoreject, Riemannian Potato, and Riemannian Potato Field, respectively. Statistical analysis confirmed the significance of these improvements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most comparisons. Additionally, on a typical EEG recording iRPF performs artifact cleaning in under 8 milliseconds per epoch using a standard laptop, highlighting its efficiency for large-scale EEG data processing and real-time applications. iRPF offers a robust and data-driven artifact rejection solution for high-quality EEG pre-processing in brain-computer interfaces and clinical neuroimaging applications.

Paper number 27:
Title: Voltage Synchronization and Proportional Current Sharing of Grid-Forming Inverters
Authors: Qianxi Tang, Li Peng
Abstract: Most previously proposed controllers are analyzed in the small-signal/quasi-steady regime rather than large-signal or transient stability for grid-forming inverters (GFMI). Additionally, methods that presume system-wide data--global measurements and complete grid-model knowledge--are challenging to realize in practice and unsuitable for large-scale operation. Moreover, proportional current sharing is rarely embedded into them. The whole system is a high-order, nonlinear differential system, making analysis intractable without principled simplifications. Hence, contraction stability analysis in GFMI is proposed to guarantee the large-signal stability. Furthermore, a contraction-based controller is proposed to synchronize GFMI. Additionally, this paper proposes integrating an auxiliary virtual-impedance layer into the contraction-based controller to achieve proportional current sharing, while the GFMI retains global stability and voltage synchronization. A dispatchable virtual oscillator control (dVOC), also known as the Andronov--Hopf oscillator (AHO) is used to validate the proposed contraction stability analysis and contraction-based controller with virtual-impedance. It is proved that the complex multi-converter system can achieve output-feedback contraction under large-signal operation. Therefore, without requiring system-wide data, the proposed method offers voltage synchronization, decentralized stability conditions for the transient stability of AHO and proportional current sharing, beyond prior small-signal, quasi-steady analysis.

Paper number 28:
Title: On the Relation of Characteristic Modes of Different Conducting Structures
Authors: Leonardo Mörlein, Dirk Manteuffel
Abstract: A formalism is derived to analyze the scattering of a conducting structure based on the characteristic modes of another structure whose surface is a superset of the first structure. This enables the analysis and comparison of different structures using a common basis of characteristic modes. Additionally, it is shown that the scattering matrices and perturbation matrices are no longer diagonal in these cases. Based on this, a modal transformation matrix is defined to describe the mapping between the characteristic fields and the weighting coefficients of the two structures. This matrix enables the conversion of the perturbation matrices in different bases. Finally, two examples are provided along with a discussion of some aspects of the theory. The first example aims to validate and illustrate the formalism. The second example shows how the formalism can be applied in the design process of an antenna element that is gradually modified, starting from a base structure.

Paper number 29:
Title: Over-the-Air Adversarial Attack Detection: from Datasets to Defenses
Authors: Li Wang, Xiaoyan Lei, Haorui He, Lei Wang, Jie Shi, Zhizheng Wu
Abstract: Automatic Speaker Verification (ASV) systems can be used for voice-enabled applications for identity verification. However, recent studies have exposed these systems' vulnerabilities to both over-the-line (OTL) and over-the-air (OTA) adversarial attacks. Although various detection methods have been proposed to counter these threats, they have not been thoroughly tested due to the lack of a comprehensive data set. To address this gap, we developed the AdvSV 2.0 dataset, which contains 628k samples with a total duration of 800 hours. This dataset incorporates classical adversarial attack algorithms, ASV systems, and encompasses both OTL and OTA scenarios. Furthermore, we introduce a novel adversarial attack method based on a Neural Replay Simulator (NRS), which enhances the potency of adversarial OTA attacks, thereby presenting a greater threat to ASV systems. To defend against these attacks, we propose CODA-OCC, a contrastive learning approach within the one-class classification framework. Experimental results show that CODA-OCC achieves an EER of 11.2% and an AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-art detection methods.

Paper number 30:
Title: Towards Efficient and Secure Cloud Control Systems: Advances, Challenges, and Future Directions
Authors: Yasir Ali, Tayyab Manzoor, Huan Yang, Asif Ali, Yuanqing Xia
Abstract: Networked Control Systems (NCSs) have been instrumental in realizing fully connected and responsive intelligent environments within the context of real-time virtual control and management. However, traditional NCSs face considerable challenges in handling the vast amounts of data generated by large-scale control applications, particularly in terms of data acquisition, storage, and computational processing. To address these challenges, the emergence of cloud computing and advancements in control theory have empowered the new paradigm known as Cloud Control Systems (CCSs). Recently, CCSs have received substantial attention from industries for their potential properties, such as large-scale data management, complex computations, and data-centric optimized decisions. This study presents an extensive review of recent progress in CCSs spanning over multiple studies published between 2012 and 2025. Specifically, the focus is on providing a taxonomy of the current findings in CCS research, encompassing various perspectives, such as its efficient implementations in industrial automation, security and privacy considerations, and cloud-based control techniques. Each category is examined in depth through selected state-of-the-art analyses of different approaches and contrasting methodologies. Furthermore, we discuss future directions aimed at designing more efficient and practical CCSs. The insights gained from this study can help researchers, practitioners, and decision-makers in their domain for effective CCS design and deployment.

Paper number 31:
Title: Listening for "You": Enhancing Speech Image Retrieval via Target Speaker Extraction
Authors: Wenhao Yang, Jianguo Wei, Wenhuan Lu, Xinyue Song, Xianghu Yue
Abstract: Image retrieval using spoken language cues has emerged as a promising direction in multimodal perception, yet leveraging speech in multi-speaker scenarios remains challenging. We propose a novel Target Speaker Speech-Image Retrieval task and a framework that learns the relationship between images and multi-speaker speech signals in the presence of a target speaker. Our method integrates pre-trained self-supervised audio encoders with vision models via target speaker-aware contrastive learning, conditioned on a Target Speaker Extraction and Retrieval module. This enables the system to extract spoken commands from the target speaker and align them with corresponding images. Experiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly outperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3 speaker scenarios, respectively - substantial improvements over single speaker baselines and state-of-the-art models. Our approach demonstrates potential for real-world deployment in assistive robotics and multimodal interaction systems.

Paper number 32:
Title: Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems
Authors: Huayan Guo, Jichen Zhang, Junhui Rao, Ross Murch, Vincent K. N. Lau
Abstract: Pixel-based fluid antennas provide enhanced multiplexing gains and quicker radiation pattern switching than traditional designs. However, this innovation introduces challenges for channel estimation and analog precoding due to the state-non-separable channel response problem. This paper explores a multiuser MIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements from a real-world prototype. We present a sparse channel recovery framework for uplink channel sounding, employing an approximate separable channel response model with DNN-based antenna radiation functions. We then propose two low-complexity channel estimation algorithms that leverage orthogonal matching pursuit and variational Bayesian inference to accurately recover channel responses across various scattering cluster angles. These estimations enable the prediction of composite channels for all fluid antenna states, leading to an analog precoding scheme that optimally selects switching states for different antennas. Our simulation results indicate that the proposed approach significantly outperforms several baseline methods, especially in high signal-to-noise ratio environments with numerous users.

Paper number 33:
Title: A Comparative Analysis of Robust and Reliable Designs Using the Compromised Design Support Problem: A Case Study in Hot Rod Rolling Processes
Authors: Maryam Ghasemzadeh, H M Dilshad Alam Digonta, Anand Balu Nellippallil, Anton van Beek
Abstract: Design under uncertainty is a challenging problem, as a systems performance can be highly sensitive to variations in input parameters and model uncertainty. A conventional approach to addressing such problems is robust optimization, which seeks to enhance design performance by reducing sensitivity to uncertainty. Alternatively, reliability-based design focuses on optimizing performance while ensuring that failure constraints are satisfied with a specified probability. While both methods are well established, their integration into multi-objective and multi-stakeholder decision-making frameworks remains a challenging problem. In this study, we extend the Compromise Decision Support Problem (cDSP) framework to incorporate reliability-based design considerations and evaluate its performance in comparison to the conventional robust-based cDSP formulation. The developed framework has been validated on a multidisciplinary hot rod rolling process including parametric and model uncertainties. The results compare the predicted performance under robust and reliable scenarios, validating the efficiency of the approach in managing uncertainties for complex, multidisciplinary systems. Specifically, we found that the two methods exhibit markedly different performance when the predicted performance follows a non-normal distribution, a situation that arises in non-linear systems with parametric uncertainty. Based on this insight, we offer guidance to designers on the conditions under which each method is most appropriate.

Paper number 34:
Title: Taming Spontaneous Stop-and-Go Traffic Waves: A Computational Mechanism Design Perspective
Authors: Di Shen, Qi Dai, Suzhou Huang, Dimitar Filev
Abstract: It is well known that stop-and-go waves can be generated spontaneously in traffic even without bottlenecks. Can such undesirable traffic patterns, induced by intrinsic human driving behaviors, be tamed effectively and inexpensively? Taking advantage of emerging connectivity and autonomy technologies, we envision a simple yet realistic traffic control system to achieve this goal. To prove the concept, we design such a system to suppress these waves while maximizing traffic throughput in the Tadaki setting: a circular road with varying number of vehicles. We first introduce our driver behavior model and demonstrate how our calibrated human driving agents can closely reproduce the observed human driving patterns in the original Tadaki experiment. We then propose a simple control system mediated via connected automated vehicles (CAV) whose ideal speed parameter is treated as a system-level control variable adapted to the local vehicle density of the traffic. The objective of the control system is set up as a tradeoff: maximizing throughput while minimizing traffic oscillation. Following computational mechanism design, we search for the optimal control policy as a function of vehicle density and the tradeoff attitude parameter. This can be done by letting all vehicles play a simulated game of CAV-modulated traffic under such a control system. Our simulation results show that the improvements in traffic efficiency and smoothness are substantial. Finally, we envision how such a traffic control system can be realized in an environment with smart vehicles connected to a smart infrastructure or via a scheme of variable speed advisory.

Paper number 35:
Title: Taming Spontaneous Stop-and-Go Traffic Waves: A Bifurcation Perspective of A Dynamical Map
Authors: Suzhou Huang, Jian Hu
Abstract: We consider a discrete-time dynamical system in a car-following context. The system was recently introduced to parsimoniously model human driving behavior based on utility maximization. The parameters of the model were calibrated using vehicle trajectory data from the Sugiyama experiment. It was shown that such a system can accurately reproduce the observed collective phenomena of a more elaborate experiment by Tadaki et al. Once the heterogeneity and noise are switched off, the model defines a map of the corresponding discrete-time dynamical system. We first perform a bifurcation analysis of the map by studying the stability of its limit solutions: a free-flow fixed point and a stop-and-go quasi-periodic orbit. When the vehicle density is varied, our model displays a bifurcation diagram qualitatively similar to those found in a class of optimal velocity models based on an ordinary differential equation approach, including regimes where one or both of the limit solutions are stable. In a 2D bifurcation diagram we further demonstrate that imposing a vehicle density-dependent speed advisory can dissipate the stop-and-go quasi-periodic orbit. This in turn lays the mathematical foundation for a simple, yet effective proposal [1] to tame stop-and-go waves, improving traffic flow and smoothness simultaneously via variable speed advisory.

Paper number 36:
Title: Short-term cognitive fatigue of spatial selective attention after face-to-face conversations in virtual noisy environments
Authors: Ľuboš Hládek, Piotr Majdak, Robert Baumgartner
Abstract: Spatial selective attention is an important asset for communication in cocktail party situations but may be compromised by short-term cognitive fatigue. Here we tested whether an effortful conversation in a highly ecological setting depletes task performance in an auditory spatial selective attention task. Young participants with normal hearing performed the task before and after (1) having a real dyadic face-to-face conversation on a free topic in a virtual reverberant room with simulated interfering conversations and background babble noise at 72 dB SPL for 30 minutes, (2) passively listening to the interfering conversations and babble noise, or (3) having the conversation in quiet. Self-reported perceived effort and fatigue increased after conversations in noise and passive listening relative to the reports after conversations in quiet. In contrast to our expectations, response times in the attention task decreased, rather than increased, after conversation in noise and accuracy did not change systematically in any of the conditions on the group level. Unexpectedly, we observed strong training effects between the individual sessions in our within-subject design even after one hour of training on a different day.

Paper number 37:
Title: Acoustic to Articulatory Speech Inversion for Children with Velopharyngeal Insufficiency
Authors: Saba Tabatabaee, Suzanne Boyce, Liran Oren, Mark Tiede, Carol Espy-Wilson
Abstract: Traditional clinical approaches for assessing nasality, such as nasopharyngoscopy and nasometry, involve unpleasant experiences and are problematic for children. Speech Inversion (SI), a noninvasive technique, offers a promising alternative for estimating articulatory movement without the need for physical instrumentation. In this study, an SI system trained on nasalance data from healthy adults is augmented with source information from electroglottography and acoustically derived F0, periodic and aperiodic energy estimates as proxies for glottal control. This model achieves 16.92% relative improvement in Pearson Product-Moment Correlation (PPMC) compared to a previous SI system for nasalance estimation. To adapt the SI system for nasalance estimation in children with Velopharyngeal Insufficiency (VPI), the model initially trained on adult speech was fine-tuned using children with VPI data, yielding an 7.90% relative improvement in PPMC compared to its performance before fine-tuning.

Paper number 38:
Title: In-Loop Filtering Using Learned Look-Up Tables for Video Coding
Authors: Zhuoyuan Li, Jiacheng Li, Yao Li, Jialin Li, Li Li, Dong Liu, Feng Wu
Abstract: In-loop filtering (ILF) is a key technology in video coding standards to reduce artifacts and enhance visual quality. Recently, neural network-based ILF schemes have achieved remarkable coding gains, emerging as a powerful candidate for next-generation video coding standards. However, the use of deep neural networks (DNN) brings significant computational and time complexity or high demands for dedicated hardware, making it challenging for general use. To address this limitation, we study a practical ILF solution by adopting look-up tables (LUTs). After training a DNN with a restricted reference range for ILF, all possible inputs are traversed, and the output values of the DNN are cached into LUTs. During the coding process, the filtering process is performed by simply retrieving the filtered pixel through locating the input pixels and interpolating between the cached values, instead of relying on heavy inference computations. In this paper, we propose a universal LUT-based ILF framework, termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of filtering LUTs and propose a series of customized indexing mechanisms to enable better filtering reference perception with limited storage consumption. Second, we propose the cross-component indexing mechanism to enable the filtering of different color components jointly. Third, in order to make our solution practical for coding uses, we propose the LUT compaction scheme to enable the LUT pruning, achieving a lower storage cost of the entire solution. The proposed framework is implemented in the VVC reference software. Experimental results show that the proposed framework achieves on average 0.82%/2.97%/1.63% and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI and RA configurations, respectively. Compared to DNN-based solutions, our proposed solution has much lower time complexity and storage cost.

Paper number 39:
Title: Region-Specific Audio Tagging for Spatial Sound
Authors: Jinzheng Zhao, Yong Xu, Haohe Liu, Davide Berghi, Xinyuan Qian, Qiuqiang Kong, Junqi Zhao, Mark D. Plumbley, Wenwu Wang
Abstract: Audio tagging aims to label sound events appearing in an audio recording. In this paper, we propose region-specific audio tagging, a new task which labels sound events in a given region for spatial audio recorded by a microphone array. The region can be specified as an angular space or a distance from the microphone. We first study the performance of different combinations of spectral, spatial, and position features. Then we extend state-of-the-art audio tagging systems such as pre-trained audio neural networks (PANNs) and audio spectrogram transformer (AST) to the proposed region-specific audio tagging task. Experimental results on both the simulated and the real datasets show the feasibility of the proposed task and the effectiveness of the proposed method. Further experiments show that incorporating the directional features is beneficial for omnidirectional tagging.

Paper number 40:
Title: Learning-Based Data-Assisted Port-Hamiltonian Control for Free-Floating Space Manipulators
Authors: Mostafa Eslami, Maryam Babazadeh
Abstract: A generic data-assisted control architecture within the port-Hamiltonian framework is proposed, introducing a physically meaningful observable that links conservative dynamics to all actuation, dissipation, and disturbance channels. A robust, model-based controller combined with a high-gain decentralized integrator establishes large robustness margins and strict time-scale separation, ensuring that subsequent learning cannot destabilize the primary dynamics. Learning, selected for its generalizability, is then applied to capture complex, unmodeled effects, despite inherent delay and transient error during adaptation. Formal Lyapunov analysis with explicit stability bounds guarantees convergence under bounded learning errors. The structured design confines learning to the simplest part of the dynamics, enhancing data efficiency while preserving physical interpretability. The approach is generic, with a free-floating space manipulator orientation control task, including integrated null-space collision avoidance, serving as a case study to demonstrate robust tracking performance and applicability to broader robotic domains.

Paper number 41:
Title: A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks
Authors: Sajjad Hussain
Abstract: Accurate pathloss prediction is essential for the design and optimization of UAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches have shown strong potential, their generalization across diverse environments, robustness to noisy inputs, and sensitivity to UAV altitude remain underexplored. To address these challenges, we propose a UNet-based deep learning architecture that combines multi-scale feature extraction, convolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP) bottleneck for efficient context aggregation. The model predicts pathloss maps from log-distance, line-of-sight (LOS) mask, and building mask inputs. In addition, we develop a fully vectorized LOS mask computation algorithm that significantly accelerates pre-processing and enables large-scale dataset generation. Extensive evaluations on both in-house ray-tracing data and the RadioMapSeer benchmark demonstrate that the proposed model outperforms several state-of-the-art baselines in accuracy and efficiency. All source code is publicly released to support reproducibility and future research.

Paper number 42:
Title: A neural drift-plus-penalty algorithm for network power allocation and routing
Authors: Ahmed Rashwan, Keith Briggs, Chris Budd
Abstract: The drift-plus-penalty method is a Lyapunov optimisation technique commonly applied to network routing problems. It reduces the original stochastic planning task to a sequence of greedy optimizations, enabling the design of distributed routing algorithms which stabilize data queues while simultaneously optimizing a specified penalty function. While drift-plus-penalty methods have desirable asymptotic properties, they tend to incur higher network delay than alternative control methods, especially under light network load. In this work, we propose a learned variant of the drift-plus-penalty method that can preserve its theoretical guarantees, while being flexible enough to learn routing strategies directly from a model of the problem. Our approach introduces a novel mechanism for learning routing decisions and employs an optimal transport-based method for link scheduling. Applied to the joint task of transmit-power allocation and data routing, the method achieves consistent improvements over common baselines under a broad set of scenarios.

Paper number 43:
Title: Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates
Authors: Sreejeet Maity, Aritra Mitra
Abstract: We consider the problem of learning the optimal policy in a discounted, infinite-horizon reinforcement learning (RL) setting where the reward signal is subject to adversarial corruption. Such corruption, which may arise from extreme noise, sensor faults, or malicious attacks, can severely degrade the performance of classical algorithms such as Q-learning. To address this challenge, we propose a new provably robust variant of the Q-learning algorithm that operates effectively even when a fraction of the observed rewards are arbitrarily perturbed by an adversary. Under the asynchronous sampling model with time-correlated data, we establish that despite adversarial corruption, the finite-time convergence rate of our algorithm matches that of existing results for the non-adversarial case, up to an additive term proportional to the fraction of corrupted samples. Moreover, we derive an information-theoretic lower bound revealing that the additive corruption term in our upper bounds is unavoidable. Next, we propose a variant of our algorithm that requires no prior knowledge of the statistics of the true reward distributions. The analysis of this setting is particularly challenging and is enabled by carefully exploiting a refined Azuma-Hoeffding inequality for almost-martingales, a technical tool that might be of independent interest. Collectively, our contributions provide the first finite-time robustness guarantees for asynchronous Q-learning, bridging a significant gap in robust RL.

Paper number 44:
Title: Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance
Authors: Ya-Ting Yang, Quanyan Zhu
Abstract: Cyber warfare has become a central element of modern conflict, especially within multi-domain operations. As both a distinct and critical domain, cyber warfare requires integrating defensive and offensive technologies into coherent strategies. While prior research has emphasized isolated tactics or fragmented technologies, a holistic understanding is essential for effective resource deployment and risk mitigation. Game theory offers a unifying framework for this purpose. It not only models attacker-defender interactions but also provides quantitative tools for equilibrium analysis, risk assessment, and strategic reasoning. Integrated with modern AI techniques, game-theoretic models enable the design and optimization of strategies across multiple levels of cyber warfare, from policy and strategy to operations, tactics, and technical implementations. These models capture the paradoxical logic of conflict, where more resources do not always translate into greater advantage, and where nonlinear dynamics govern outcomes. To illustrate the approach, this chapter examines RedCyber, a synthetic cyber conflict, demonstrating how game-theoretic methods capture the interdependencies of cyber operations. The chapter concludes with directions for future research on resilience, cros-echelon planning, and the evolving role of AI in cyber warfare.

Paper number 45:
Title: Regularization in Data-driven Predictive Control: A Convex Relaxation Perspective
Authors: Xu Shang, Yang Zheng
Abstract: This paper explores the role of regularization in data-driven predictive control (DDPC) through the lens of convex relaxation. Using a bi-level optimization framework, we model system identification as an inner problem and predictive control as an outer problem. Within this framework, we show that several regularized DDPC formulations, including l1-norm penalties, projection-based regularizers, and a newly introduced causality-based regularizer, can be viewed as convex relaxations of their respective bi-level problems. This perspective clarifies the conceptual links between direct and indirect data-driven control and highlights how regularization implicitly enforces system identification. We further propose an optimality-based variant, O-DDPC, which approximately solves the inner problem with all identification constraints via an iterative algorithm. Numerical experiments demonstrate that O-DDPC outperforms existing regularized DDPC by reducing both bias and variance errors. These results indicate that further benefits may be obtained by applying system identification techniques to pre-process the trajectory library in nonlinear settings. Overall, our analysis contributes to a unified convex relaxation view of regularization in DDPC and sheds light on its strong empirical performance beyond linear time-invariant systems.

Paper number 46:
Title: A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management
Authors: Julian Oelhaf, Georg Kordowich, Mehran Pashaei, Christian Bergler, Andreas Maier, Johann Jäger, Siming Bayer
Abstract: The integration of renewable and distributed energy resources reshapes modern power systems, challenging conventional protection schemes. This scoping review synthesizes recent literature on machine learning (ML) applications in power system protection and disturbance management, following the PRISMA for Scoping Reviews framework. Based on over 100 publications, three key objectives are addressed: (i) assessing the scope of ML research in protection tasks; (ii) evaluating ML performance across diverse operational scenarios; and (iii) identifying methods suitable for evolving grid conditions. ML models often demonstrate high accuracy on simulated datasets; however, their performance under real-world conditions remains insufficiently validated. The existing literature is fragmented, with inconsistencies in methodological rigor, dataset quality, and evaluation metrics. This lack of standardization hampers the comparability of results and limits the generalizability of findings. To address these challenges, this review introduces a ML-oriented taxonomy for protection tasks, resolves key terminological inconsistencies, and advocates for standardized reporting practices. It further provides guidelines for comprehensive dataset documentation, methodological transparency, and consistent evaluation protocols, aiming to improve reproducibility and enhance the practical relevance of research outcomes. Critical gaps remain, including the scarcity of real-world validation, insufficient robustness testing, and limited consideration of deployment feasibility. Future research should prioritize public benchmark datasets, realistic validation methods, and advanced ML architectures. These steps are essential to move ML-based protection from theoretical promise to practical deployment in increasingly dynamic and decentralized power systems.

Paper number 47:
Title: Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication
Authors: Omar Erak, Omar Alhussein, Hatem Abou-Zeid, Mehdi Bennis
Abstract: Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deployment in resource-constrained 6G networks. In this paper, we present a training-free framework for adaptive token merging in pretrained vision transformers to jointly reduce inference time and transmission resource usage. We formulate the selection of per-layer merging proportions as a multi-objective optimization problem to balance accuracy and computational cost. We employ Gaussian process-based Bayesian optimization to construct a Pareto frontier of optimal configurations, enabling flexible runtime adaptation to dynamic application requirements and channel conditions. Extensive experiments demonstrate that our method consistently outperforms other baselines and achieves significant reductions in floating-point operations while maintaining competitive accuracy across a wide range of signal-to-noise ratio (SNR) conditions. Additional results highlight the effectiveness of adaptive policies that adjust merging aggressiveness in response to channel quality, providing a practical mechanism to trade off latency and semantic fidelity on demand. These findings establish a scalable and efficient approach for deploying transformer-based semantic communication in future edge intelligence systems.

Paper number 48:
Title: Implementation of a 8-bit Wallace Tree Multiplier
Authors: Ayan Biswas, Jimmy Jin
Abstract: Wallace tree multipliers are a parallel digital multiplier architecture designed to minimize the worst-case time complexity of the circuit depth relative to the input size [1]. In particular, it seeks to perform long multiplication in the binary sense, reducing as many partial products per stage as possible through full and half adders circuits, achieving O(log(n)) where n = bit length of input. This paper provides an overview of the design, progress and methodology in the final project of ECE 55900, consisting of the schematic and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in Cadence Virtuoso, as well as any design attempts prior to the final product. This also includes our endeavors in designing the final MAC (Multiply Accumulate) unit with undefined targets, which we chose to implement as a 16 bit combinational multiply-add.

Paper number 49:
Title: Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments
Authors: Farhad Nawaz, Faizan M. Tariq, Sangjae Bae, David Isele, Avinash Singh, Nadia Figueroa, Nikolai Matni, Jovin D'sa
Abstract: Accurately reasoning about future parking spot availability and integrated planning is critical for enabling safe and efficient autonomous valet parking in dynamic, uncertain environments. Unlike existing methods that rely solely on instantaneous observations or static assumptions, we present an approach that predicts future parking spot occupancy by explicitly distinguishing between initially vacant and occupied spots, and by leveraging the predicted motion of dynamic agents. We introduce a probabilistic spot occupancy estimator that incorporates partial and noisy observations within a limited Field-of-View (FoV) model and accounts for the evolving uncertainty of unobserved regions. Coupled with this, we design a strategy planner that adaptively balances goal-directed parking maneuvers with exploratory navigation based on information gain, and intelligently incorporates wait-and-go behaviors at promising spots. Through randomized simulations emulating large parking lots, we demonstrate that our framework significantly improves parking efficiency, safety margins, and trajectory smoothness compared to existing approaches.

Paper number 50:
Title: The role of communication delays in the optimal control of spatially invariant systems
Authors: Luca Ballotta, Juncal Arbelaiz, Vijay Gupta, Luca Schenato, Mihailo R. Jovanović
Abstract: We study optimal proportional feedback controllers for spatially invariant systems when the controller has access to delayed state measurements received from different spatial locations. We analyze how delays affect the spatial locality of the optimal feedback gain leveraging the problem decoupling in the spatial frequency domain. For the cases of expensive control and small delay, we provide exact expressions of the optimal controllers in the limit for infinite control weight and vanishing delay, respectively. In the expensive control regime, the optimal feedback control law decomposes into a delay-aware filtering of the delayed state and the optimal controller in the delay-free setting. Under small delays, the optimal controller is a perturbation of the delay-free one which depends linearly on the delay. We illustrate our analytical findings with a reaction-diffusion process over the real line and a multi-agent system coupled through circulant matrices, showing that delays reduce the effectiveness of optimal feedback control and may require each subsystem within a distributed implementation to communicate with farther-away locations.

Paper number 51:
Title: Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments
Authors: Mohammed M. H. Qazzaz, Abdelaziz Salama, Maryam Hafeez, Syed A. R. Zaidi
Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic capability to exploit key performance monitoring (KPM) within Radio Intelligence Controller (RIC) to derive network optimisation through xApps. These xApps can leverage KPM knowledge to dynamically switch on/off the associated RUs where such a function is supported over the E2 interface. Several existing studies employ artificial intelligence (AI)/Machine Learning (ML) based approaches to realise such dynamic sleeping for increased energy efficiency (EE). Nevertheless, most of these approaches rely upon offloading user equipment (UE) to carve out a sleeping opportunity. Such an approach inherently creates load imbalance across the network. Such load imbalance may impact the throughput performance of offloaded UEs as they might be allocated a lower number of physical resource blocks (PRBs). Maintaining the same PRB allocation while addressing the EE at the network level is a challenging task. To that end, in this article, we present a comprehensive ML-based framework for joint optimisation of load balancing and EE for ORAN deployments. We formulate the problem as a multi-class classification system that predictively evaluates potential RU configurations before optimising the EE, mapping network conditions to three load balance categories (Well Balanced, Moderately Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate, Aggressive) accommodates different operational priorities between energy savings and performance assurance. Experimental evaluation using 4.26 million real network measurements from simulations demonstrates that our Random Forest model achieves 98.3% F1-macro performance, representing 195% improvement over traditional baseline strategies.

Paper number 52:
Title: Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles
Authors: Ian Nell, Shane Gilroy
Abstract: Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.

Paper number 53:
Title: BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging
Authors: Peng Zhou, Jiaming Qi, Hongmin Wu, Chen Wang, Yizhou Chen, Zeqing Zhang
Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging considering deformable bags' complicated and unpredictable nature. This paper presents an automated bagging system from the proposed adaptive Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The system dynamically adjusts its actions based on real-time visual feedback, removing the need for pre-existing knowledge of bag properties. Our framework incorporates Gaussian Mixture Models (GMM) for estimating SOI states, optimization techniques for SOI generation, motion planning via Constrained Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination using Model Predictive Control (MPC). Extensive experiments validate the capability of our system to perform precise and robust bagging across various objects, showcasing its adaptability. This work offers a new solution for robotic deformable object manipulation (DOM), particularly in automated bagging tasks. Video of this work is available at this https URL.

Paper number 54:
Title: Frozen differential scattering in reconfigurable complex media
Authors: Philipp del Hougne
Abstract: The sensitivity of transmission to the input wavefront is a hallmark feature of complex media and the basis for wavefront shaping techniques. Yet, intriguing special cases exist in which the output wavefront is "frozen" (agnostic to the input wavefront). This happens when special structure in the complex medium collapses the rank of its transmission matrix to unity. Here, we unveil that an analogous phenomenon exists more universally for differential scattering (including reflection) in reconfigurable complex media. Specifically, for a localized perturbation, the differential scattering matrix of any complex medium has rank one. One consequence is that the differential output signal is perfectly coherent irrespective of the input wavefront's coherence. Moreover, the thermal noise emitted into the frozen differential output mode has a particular structure that can be exploited for thermal noise management. We experimentally evidence frozen differential scattering in a rich-scattering wireless link parametrized by a programmable meta-atom. Then, we demonstrate "customized freezing" by optimizing the configuration of additional programmable meta-atoms that parametrize the wireless link, as envisioned for 6G networks. We impose particular shapes of the frozen differential output mode, and maximize its signal-to-thermal-noise ratio. Potential applications include filtering and stabilization of differential wavefronts, as well as imaging, sensing, and communication in complex media.

Paper number 55:
Title: Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner
Authors: Quentin Uhl, Tommaso Pavan, Julianna Gerold, Kwok-Shing Chan, Yohan Jun, Shohei Fujita, Aneri Bhatt, Yixin Ma, Qiaochu Wang, Hong-Hsi Lee, Susie Y. Huang, Berkin Bilgic, Ileana Jelescu
Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.

Paper number 56:
Title: ObjectReact: Learning Object-Relative Control for Visual Navigation
Authors: Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid
Abstract: Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an "image-relative" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning "object-relative" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a "relative" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed "ObjectReact", conditioned directly on a high-level "WayObject Costmap" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: this https URL

Paper number 57:
Title: RSMA-Enhanced Data Collection in RIS-Assisted Intelligent Consumer Transportation Systems
Authors: Chunjie Wang, Xuhui Zhang, Wenchao Liu, Jinke Ren, Shuqiang Wang, Yanyan Shen, Kejiang Ye, Kim Fung Tsang
Abstract: This paper investigates the data collection enhancement problem in a reconfigurable intelligent surface (RIS)-empowered intelligent consumer transportation system (ICTS). We propose a novel framework where a data center (DC) provides energy to pre-configured roadside unit (RSU) pairs during the downlink stage. While in the uplink stage, these RSU pairs utilize a hybrid rate-splitting multiple access (RSMA) and time-division multiple access (TDMA) protocol to transmit the processed data to the DC, while simultaneously performing local data processing using the harvested energy. Our objective is to maximize the minimal processed data volume of the RSU pairs by jointly optimizing the RIS downlink and uplink phase shifts, the transmit power of the DC and RSUs, the RSU computation resource allocation, and the time slot allocation. To address the formulated non-convex problem, we develop an efficient iterative algorithm integrating alternating optimization and sequential rank-one constraint relaxation methods. Extensive simulations demonstrate that the proposed algorithm significantly outperforms baseline schemes under diverse scenarios, validating its effectiveness in enhancing the data processing performance for intelligent transportation applications.

Paper number 58:
Title: Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations
Authors: Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini
Abstract: We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at this https URL.

Paper number 59:
Title: Towards Reliable Medical Image Segmentation by Modeling Evidential Calibrated Uncertainty
Authors: Ke Zou, Yidi Chen, Ling Huang, Xuedong Yuan, Xiaojing Shen, Meng Wang, Rick Siow Mong Goh, Yong Liu, Huazhu Fu
Abstract: Medical image segmentation is critical for disease diagnosis and treatment assessment. However, concerns regarding the reliability of segmentation regions persist among clinicians, mainly attributed to the absence of confidence assessment, robustness, and calibration to accuracy. To address this, we introduce DEviS, an easily implementable foundational model that seamlessly integrates into various medical image segmentation networks. DEviS not only enhances the calibration and robustness of baseline segmentation accuracy but also provides high-efficiency uncertainty estimation for reliable predictions. By leveraging subjective logic theory, we explicitly model probability and uncertainty for medical image segmentation. Here, the Dirichlet distribution parameterizes the distribution of probabilities for different classes of the segmentation results. To generate calibrated predictions and uncertainty, we develop a trainable calibrated uncertainty penalty. Furthermore, DEviS incorporates an uncertainty-aware filtering module, which designs the metric of uncertainty-calibrated error to filter out-of-distribution data. We conducted validation studies on publicly available datasets, including ISIC2018, KiTS2021, LiTS2017, and BraTS2019, to assess the accuracy and robustness of different backbone segmentation models enhanced by DEviS, as well as the efficiency and reliability of uncertainty estimation.

Paper number 60:
Title: Tannenbaum's gain-margin optimization meets Polyak's heavy-ball algorithm
Authors: Wuwei Wu, Jie Chen, Mihailo R. Jovanović, Tryphon T. Georgiou
Abstract: This paper highlights an apparent, yet relatively unknown link, between algorithm design in optimization theory and control synthesis in robust control. Specifically, quadratic optimization can be recast as a regulation problem within the frame of $H_\infty$ control. From this vantage point, the optimality of Polyak's fastest heavy-ball algorithm can be ascertained as a solution to a gain margin optimization problem. The approach is independent of Polyak's original and brilliant argument, and relies on foundational work by Tannenbaum who introduced and solved gain margin optimization via Nevanlinna-Pick interpolation theory. The link between first-order optimization methods and robust control sheds new light into the limits of algorithmic performance of such methods, and suggests a framework where similar computational tasks can be systematically studied and algorithms optimized. In particular, it raises the question as to whether periodically scheduled algorithms can achieve faster rates for quadratic optimization, in a manner analogous to periodic control that extends gain margin beyond that of time-invariant control. This turns out not to be the case, due to the analytic obstruction of a transmission zero that is inherent in causal schemes. Interestingly, this obstruction can be removed with implicit algorithms, cast as feedback regulation problems with causal, but not strictly causal dynamics, thereby devoid of the transmission zero at infinity and able to achieve superior convergence rates.

Paper number 61:
Title: Tunable Thresholds and Frequency Encoding in a Spiking NOD Controller
Authors: Ian Xul Belaustegui, Alessio Franci, Naomi Ehrich Leonard
Abstract: Spiking Nonlinear Opinion Dynamics (S-NOD) is an excitable decision-making model inspired by the spiking dynamics of neurons. S-NOD enables the design of agile decision-making that can rapidly switch between decision options in response to a changing environment. In S-NOD, decisions are represented by discrete opinion spikes that evolve in continuous time. Here, we extend previous analysis of S-NOD and explore its potential as a nonlinear controller with a tunable balance between robustness and responsiveness to input. We identify and provide necessary conditions for the bifurcation that determines the onset of periodic opinion spiking. We leverage this analysis to characterize the tunability of the input-output threshold for opinion spiking as a function of the model basal sensitivity and the tunable dependence of opinion spiking frequency on input magnitude above the threshold. We conclude with a discussion of S-NOD as a new neuromorphic control block and its extension to distributed spiking controllers.

Paper number 62:
Title: Data-Driven Reachability with Scenario Optimization and the Holdout Method
Authors: Elizabeth Dietrich, Rosalyn Devonport, Stephen Tu, Murat Arcak
Abstract: Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics.

Paper number 63:
Title: High Performance Signal Design for Optical OFDM Systems using Variational Autoencoder
Authors: Nam N. Luong, Chuyen T. Nguyen, Thanh V. Pham
Abstract: This letter proposes a design of low peak-to-average power ratio (PAPR), low symbol error rate (SER), and high data rate signal for optical orthogonal frequency division multiplexing (OFDM) systems. The proposed design leverages a variational autoencoder (VAE) incorporating gradual loss learning to jointly optimize the geometry and probability of the constellation's symbols. This not only enhances mutual information (MI) but also effectively reduces the PAPR while maintaining a low SER for reliable transmission. We evaluate the performance of the proposed VAE-based design by comparing the MI, SER, and PAPR against existing techniques. Simulation results demonstrate that the proposed method achieves a considerably lower PAPR while maintaining superior SER and MI performance for a wide range of SNRs.

Paper number 64:
Title: ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation
Authors: Feng Yuan, Yifan Gao, Wenbin Wu, Keqing Wu, Xiaotong Guo, Jie Jiang, Xin Gao
Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at this https URL

Paper number 65:
Title: Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net
Authors: Dongyi He, Shiyang Li, Bin Jiang, He Yan
Abstract: High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain Convolutional Neural Networks (CNNs) that fail to capture cross-channel time-frequency cues or on heavy transformer/Generative Adversarial Network (GAN) decoders that strain memory and stability. To address these limitations, we propose Spec2VolCAMU-Net, a lightweight architecture featuring a Multi-directional Time-Frequency Convolutional Attention Encoder for rich feature extraction and a Vision-Mamba U-Net decoder that uses linear-time state-space blocks for efficient long-range spatial modelling. We frame the goal of this work as establishing a new state of the art in the spatial fidelity of single-volume reconstruction, a foundational prerequisite for the ultimate aim of generating temporally coherent fMRI time series. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording Structural Similarity Index (SSIM) of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive Signal-to-Noise Ratio (PSNR) scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at this https URL.

Paper number 66:
Title: EM-Based Channel Estimation for mMIMO LEO SATCOM Under Imperfect Doppler Compensation
Authors: Abdollah Masoud Darya, Saeed Abdallah
Abstract: Massive multiple-input multiple-output low-Earth-orbit communication channels are highly time-varying due to severe Doppler shifts and propagation delays. While satellite-mobility-induced Doppler shifts can be compensated using known ephemeris data, those caused by user mobility require accurate user positioning information; the absence of such information contributes to amplified channel aging in conventional channel estimators. To address this challenge, we propose a data-aided channel estimator based on the expectation-maximization (EM) algorithm, combined with a discrete Legendre polynomial basis expansion method (DLP-BEM), to estimate the channel under imperfect Doppler compensation. The EM algorithm iteratively exploits hidden data symbols for improved channel estimation, while DLP-BEM regularizes the process by projecting the channel estimate onto a lower-dimensional subspace that mitigates estimation errors. Simulation results demonstrate the superiority of the proposed framework over existing methods in terms of normalized mean square error and symbol error rate.

Paper number 67:
Title: Unobservable Systems: No Problem for Noise Identification
Authors: Oliver Kost, Jindrich Dunik, Ivo Puncochar, Ondrej Straka
Abstract: This paper deals with the noise identification of a linear time-varying stochastic dynamic system described by the state-space model. In particular, the stress is laid on the design of the correlation measurement difference method for estimation of the state and measurement noise covariance matrices for both observable and \textit{unobservable} systems with possibly unknown input sequence. The method provides unbiased and consistent estimates and is implemented in a publicly available MATLAB toolbox and numerically evaluated.

Paper number 68:
Title: C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism
Authors: Mayank V. Golhar, Lucas Sebastian Galeano Fretes, Loren Ayers, Venkata S. Akshintala, Taylor L. Bobrow, Nicholas J. Durr
Abstract: Spatial computer vision techniques have the potential to improve the diagnostic performance of colonoscopy. However, the lack of 3D colonoscopy datasets for training and validation hinders their development. This paper introduces C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video Dataset, featuring enhanced realism designed to facilitate the quantitative evaluation of 3D colon reconstruction algorithms. 192 video sequences totaling 169,371 frames were captured by imaging 60 unique, high-fidelity silicone colon phantom segments. Ground truth depth, surface normals, optical flow, occlusion, diffuse maps, six-degree-of-freedom pose, coverage map, and 3D models are provided for 169 colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a gastroenterologist are provided with ground truth poses. Lastly, the dataset includes 15 videos with colon deformations for qualitative assessment. C3VDv2 emulates diverse and challenging scenarios for 3D reconstruction algorithms, including fecal debris, mucous pools, blood, debris obscuring the colonoscope lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2 will allow for more robust and representative development and evaluation of 3D reconstruction algorithms. Project Page - this https URL

Paper number 69:
Title: SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model
Authors: Chun Xie, Yuichi Yoshii, Itaru Kitahara
Abstract: X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.

Paper number 70:
Title: Binaural Target Speaker Extraction using HRTFs
Authors: Yoav Ellinson, Sharon Gannot
Abstract: In this work, we aim to imitate the human ability to selectively attend to a single speaker, even in the presence of multiple simultaneous talkers. To achieve this, we propose a novel approach for binaural target speaker extraction that leverages the listener's Head-Related Transfer Function (HRTF) to isolate the desired speaker. Notably, our method does not rely on speaker embeddings, making it speaker-independent and enabling strong generalization across multiple speech datasets and languages. We employ a fully complex-valued neural network that operates directly on the complex-valued Short-Time Fourier transform (STFT) of the mixed audio signals, and compare it to a Real-Imaginary (RI)-based neural network, demonstrating the advantages of the former. We first evaluate the method in an anechoic, noise-free scenario, achieving excellent extraction performance while preserving the binaural cues of the target signal. We then extend the evaluation to reverberant conditions. Our method proves robust, maintaining speech clarity and source directionality while simultaneously reducing reverberation. A comparative analysis with existing binaural Target Speaker Extraction (TSE) methods demonstrates that our approach attains performance on par with competing techniques in terms of noise reduction and perceptual quality, while offering a clear advantage in preserving binaural this http URL-page: this https URL

Paper number 71:
Title: On the Equivalence of Koopman Eigenfunctions and Commuting Symmetries
Authors: Xinyuan Jiang, Yan Li
Abstract: The Koopman operator framework offers a way to represent a nonlinear system as a linear one. The key to this simplification lies in the identification of eigenfunctions. While various data-driven algorithms have been developed for this problem, a theoretical characterization of Koopman eigenfunctions from geometric properties of the flow is still missing. This paper provides such a characterization by establishing an equivalence between a set of Koopman eigenfunctions and a set of commuting symmetries -- both assumed to span the tangent spaces at every point on a simply connected open set. Based on this equivalence, we build an explicit and convergent formula for the principal Koopman eigenfunctions defined on the region of attraction of a locally asymptotically stable equilibrium point, thereby offering a constructive formula to compute Koopman eigenfunctions.

Paper number 72:
Title: Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Organized Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)
Authors: Anindo Saha, Joeran S. Bosma, Jasper J. Twilt, Alexander B.C.D. Ng, Aqua Asif, Kirti Magudia, Peder Larson, Qinglin Xie, Xiaodong Zhang, Chi Pham Minh, Samuel N. Gitau, Ivo G. Schoots, Martijn F. Boomsma, Renato Cuocolo, Nikolaos Papanikolaou, Daniele Regge, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Baris Turkbey, Nancy A. Obuchowski, Jurgen J. Fütterer, Anwar R. Padhani, Hashim U. Ahmed, Tobias Nordström, Martin Eklund, Veeru Kasivisvanathan, Maarten de Rooij, Henkjan Huisman (on behalf of the PI-CAI, ProCAncer-I, COMFORT, STHLM3-MRI and PRIME consortia)
Abstract: In this intercontinental, confirmatory study, we include a retrospective cohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries) to train and externally validate the PI-CAI-2B model, i.e., an efficient, next-generation iteration of the state-of-the-art AI system that was developed for detecting Gleason grade group $\geq$2 prostate cancer on MRI during the PI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities in 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12 independent centers based in Europe, North America, Asia and Africa, are used for training and internal testing. Additionally, 2010 cases (2010 patients; 20 external cities in 12 countries) from population-based screening (STHLM3-MRI, IP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in Europe, North and South Americas, Asia and Australia, are used for external testing. Primary endpoint is the proportion of AI-based assessments in agreement with the standard of care diagnoses (i.e., clinical assessments made by expert uropathologists on histopathology, if available, or at least two expert urogenital radiologists in consensus; with access to patient history and peer consultation) in the detection of Gleason grade group $\geq$2 prostate cancer within the external testing cohorts. Our statistical analysis plan is prespecified with a hypothesis of diagnostic interchangeability to the standard of care at the PI-RADS $\geq$3 (primary diagnosis) or $\geq$4 (screening) cut-off, considering an absolute margin of 0.05 and reader estimates derived from the PI-CAI observer study (62 radiologists reading 400 cases). Secondary measures comprise the area under the receiver operating characteristic curve (AUROC) of the AI system stratified by imaging quality, patient age and patient ethnicity to identify underlying biases (if any).

Paper number 73:
Title: Preprocessing Algorithm Leveraging Geometric Modeling for Scale Correction in Hyperspectral Images for Improved Unmixing Performance
Authors: Praveen Sumanasekara, Athulya Ratnayake, Buddhi Wijenayake, Keshawa Ratnayake, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath
Abstract: Spectral variability significantly impacts the accuracy and convergence of hyperspectral unmixing algorithms. Many methods address complex spectral variability; yet large-scale distortions to the scale of the observed pixel signatures due to topography, illumination, and shadowing remain a major challenge. These variations often degrade unmixing performance and complicate model fitting. Because of this, correcting these variations can offer significant advantages in real-world GIS applications. In this paper, we propose a novel preprocessing algorithm that corrects scale-induced spectral variability prior to unmixing. By estimating and correcting these distortions to the scale of the pixel signatures, the algorithm produces pixel signatures with minimal distortions in scale. Since these distortions in scale (which hinder the performance of many unmixing methods) are greatly minimized in the output provided by the proposed method, the abundance estimation of the unmixing algorithms is significantly improved. We present a rigorous mathematical framework to describe and correct for scale variability and provide extensive experimental validation of the proposed algorithm. Furthermore, the algorithm's impact is evaluated across a wide range of state-of-the-art unmixing methods on two synthetic and two real hyperspectral datasets. The proposed preprocessing step consistently improves the performance of these algorithms, achieving error reductions of around 50%, even for algorithms specifically designed to handle spectral variability. This demonstrates that scale correction acts as a complementary step, facilitating more accurate unmixing with existing methods. The algorithm's generality, consistent impact, and significant influence highlight its potential as a key component in practical hyperspectral unmixing pipelines. The implementation code will be made publicly available upon publication.

Paper number 74:
Title: 3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator
Authors: Thomas Fehrenbach, Luis Omar Ortiz Abrego, Cornelius Hellge, Thomas Schierl, Jörg Ott
Abstract: Vehicle-to-everything (V2X) communication is a key technology for enabling intelligent transportation systems (ITS) that can improve road safety, traffic efficiency, and environmental sustainability. Among the various V2X applications, platooning is one of the most promising ones, as it allows a group of vehicles to travel closely together at high speeds, reducing fuel consumption and emissions. However, it poses significant challenges for wireless communication, such as high reliability and low latency. In this paper, we evaluate the benefits of group scheduling, also referred to as Mode 2d, which is based on a distributed and scheduled resource allocation scheme that allows the group of cars to select resources from a configured pool without network assistance. We evaluated the scheme through simulations, and the results show that this approach can meet the reliability, low latency, and data rate requirements for platooning.

Paper number 75:
Title: Developing a Framework to Simulate Quantitative Ultrasound Flow and Tissue Motion for Ultrafast Doppler Ultrasound
Authors: Qiang Fu, Changhui Li
Abstract: Ultrafast power Doppler imaging (uPDI) has achieved substantial progress and emerged as a key modality for both research and clinical applications. However, existing simulation tools are insufficient for generating three-dimensional (3D), quantitatively accurate flow fields with tissue motion that closely approximate in vivo conditions. In this study, we present an open-source framework, termed \emph{3D-Fully Quantitative Flow} (3D-FQFlow), designed to provide quantitative modeling of 3D vascular hemodynamics with physiologically realistic tissue motion for this http URL framework integrates a L-system-based vascular generator with hemodynamics modeling, a tissue motion simulator supporting user-defined or clinical-data-driven condition, an optimized ultrasound simulator, a GPU-accelerated image reconstruction module, and a quantitative analyzer (MSE/PSNR/SSIM). We demonstrate the workflow and performance of 3D-FQFlow using both synthetic vascular structures and clinical datasets. This framework provides ground-truth simulation models to support the development, validation, and benchmarking of uPDI techniques. The complete source code is freely available online at this https URL.

Paper number 76:
Title: Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis
Authors: Ifrat Ikhtear Uddin, Longwei Wang, KC Santosh
Abstract: Medical image analysis often faces significant challenges due to limited expert-annotated data, hindering both model generalization and clinical adoption. We propose an expert-guided explainable few-shot learning framework that integrates radiologist-provided regions of interest (ROIs) into model training to simultaneously enhance classification performance and interpretability. Leveraging Grad-CAM for spatial attention supervision, we introduce an explanation loss based on Dice similarity to align model attention with diagnostically relevant regions during training. This explanation loss is jointly optimized with a standard prototypical network objective, encouraging the model to focus on clinically meaningful features even under limited data conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from 77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to non-guided models. Grad-CAM visualizations further confirm that expert-guided training consistently aligns attention with diagnostic regions, improving both predictive reliability and clinical trustworthiness. Our findings demonstrate the effectiveness of incorporating expert-guided attention supervision to bridge the gap between performance and interpretability in few-shot medical image diagnosis.

Paper number 77:
Title: Joint Optimization of Energy Consumption and Completion Time in Federated Learning
Authors: Xinyu Zhou, Jun Zhao, Huimei Han, Claude Guet
Abstract: Federated Learning (FL) is an intriguing distributed machine learning approach due to its privacy-preserving characteristics. To balance the trade-off between energy and execution latency, and thus accommodate different demands and application scenarios, we formulate an optimization problem to minimize a weighted sum of total energy consumption and completion time through two weight parameters. The optimization variables include bandwidth, transmission power and CPU frequency of each device in the FL system, where all devices are linked to a base station and train a global model collaboratively. Through decomposing the non-convex optimization problem into two subproblems, we devise a resource allocation algorithm to determine the bandwidth allocation, transmission power, and CPU frequency for each participating device. We further present the convergence analysis and computational complexity of the proposed algorithm. Numerical results show that our proposed algorithm not only has better performance at different weight parameters (i.e., different demands) but also outperforms the state of the art.

Paper number 78:
Title: Single-Stage Optimization of Open-loop Stable Limit Cycles with Smooth, Symbolic Derivatives
Authors: Muhammad Saud Ul Hassan, Christian Hubicki
Abstract: Open-loop stable limit cycles are foundational to legged robotics, providing inherent self-stabilization that minimizes the need for computationally intensive feedback-based gait correction. While previous methods have primarily targeted specific robotic models, this paper introduces a general framework for rapidly generating limit cycles across various dynamical systems, with the flexibility to impose arbitrarily tight stability bounds. We formulate the problem as a single-stage constrained optimization problem and use Direct Collocation to transcribe it into a nonlinear program with closed-form expressions for constraints, objectives, and their gradients. Our method supports multiple stability formulations. In particular, we tested two popular formulations for limit cycle stability in robotics: (1) based on the spectral radius of a discrete return map, and (2) based on the spectral radius of the monodromy matrix, and tested five different constraint-satisfaction formulations of the eigenvalue problem to bound the spectral radius. We compare the performance and solution quality of the various formulations on a robotic swing-leg model, highlighting the Schur decomposition of the monodromy matrix as a method with broader applicability due to weaker assumptions and stronger numerical convergence properties. As a case study, we apply our method on a hopping robot model, generating open-loop stable gaits in under 2 seconds on an Intel Core i7-6700K, while simultaneously minimizing energy consumption even under tight stability constraints.

Paper number 79:
Title: Single-pixel edge enhancement of object via convolutional filtering with localized vortex phase
Authors: Jigme Zangpo, Hirokazu Kobayashi
Abstract: Microscopy is an essential tool in imaging research, and the edge-enhanced microscope by using the vortex filter is of particular interest as an optical information processing that highlights amplitude and phase edges of object in all directions. The application of this technique is not limited to the visible range, but edge enhancement of object in invisible wavelength is also crucial for near-infrared fluorescence and electronic circuit inspection through silicon semiconductors. One disadvantage of near-infrared imaging is that digital cameras such as CCD and CMOS become much more expensive than cameras for the visible spectrum. As an cost-effective method to implement invisible edge enhancement, the Fourier single-pixel imaging has already been proposed without using a camera, but using a single-pixel detector. However, this method requires 3 or 4 times more single-pixel measurements due to the three-phase or four-phase shift to detect optical complex amplitude in Fourier domain. In response, we propose a method for single-pixel edge enhancement of object via convolutional filtering with a localized vortex phase, eliminating the extra single-pixel measurements required by the phase-shifting method. Our simulation results show that the correlation coefficient between the ideal edges of an object and the edge enhanced by our proposed method is 0.95, indicating that our method is effective way to detect the edges. This novel and effective approach for enhancing and detecting the edges of object can be valuable in various invisible imaging applications.

Paper number 80:
Title: Channel Polarization under Channel Noise with Memory
Authors: Tianfu Qi, Jun Wang
Abstract: The channel polarization behavior of polar codes under noise with memory is investigated. By introducing a genie-aided channel model, we first show that the polarized subchannels still converge to extremal channels under the standard polar coding framework. More importantly, we explicitly quantify the gap between the mutual information achieved by ignoring memory effects and the actual capacity attained after sufficient polarization. It is proven that the channel capacity remains achievable even without prior knowledge of the channel noise. Furthermore, we demonstrate that the polarization rate is slower than that in the binary-input memoryless channel (BMC) case, provided that the channel transition function satisfies certain conditions. In particular, the Bhattacharyya parameter is asymptotically upper-bounded and lower-bounded by a polynomial function and an exponential function with respect to the block length, respectively.

Paper number 81:
Title: Communication Compression for Distributed Learning without Control Variates
Authors: Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani
Abstract: Distributed learning algorithms, such as the ones employed in Federated Learning (FL), require communication compression to reduce the cost of client uploads. The compression methods used in practice are often biased, making error feedback necessary both to achieve convergence under aggressive compression and to provide theoretical convergence guarantees. However, error feedback requires client-specific control variates, creating two key challenges: it violates privacy-preserving principles and demands stateful clients. In this paper, we propose Compressed Aggregate Feedback (CAFe), a novel distributed learning framework that allows highly compressible client updates by exploiting past aggregated updates, and does not require control variates. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. Experimental results confirm that CAFe outperforms existing distributed learning compression schemes.

Paper number 82:
Title: MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond
Authors: Muhammad Huzaifah, Geyu Lin, Tianchi Liu, Hardik B. Sailor, Kye Min Tan, Tarun K. Vangani, Qiongqiong Wang, Jeremy H. M. Wong, Jinyang Wu, Nancy F. Chen, Ai Ti Aw
Abstract: This technical report describes the MERaLiON-SpeechEncoder, a foundation model designed to support a wide range of downstream speech applications. Developed as part of Singapore's National Multimodal Large Language Model Programme, the MERaLiON-SpeechEncoder is tailored to address the speech processing needs in Singapore and the surrounding Southeast Asian region. The model currently supports mainly English, including the variety spoken in Singapore. We are actively expanding our datasets to gradually cover other languages in subsequent releases. The MERaLiON-SpeechEncoder was pre-trained from scratch on 200,000 hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling. We describe our training procedure and hyperparameter tuning experiments in detail below. Our evaluation demonstrates improvements to spontaneous and Singapore speech benchmarks for speech recognition, while remaining competitive to other state-of-the-art speech encoders across ten other speech tasks. We commit to releasing our model, supporting broader research endeavours, both in Singapore and beyond.

Paper number 83:
Title: A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions
Authors: Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen
Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered by the scarcity of labeled recordings, which restricts prompt diversity and undermines scoring reliability. To address this challenge, we propose a novel training paradigm that leverages a large language models (LLM) to generate diverse responses of a given proficiency level, converts responses into synthesized speech via speaker-aware text-to-speech synthesis, and employs a dynamic importance loss to adaptively reweight training instances based on feature distribution differences between synthesized and real speech. Subsequently, a multimodal large language model integrates aligned textual features with speech signals to predict proficiency scores directly. Experiments conducted on the LTTC dataset show that our approach outperforms methods relying on real data or conventional augmentation, effectively mitigating low-resource constraints and enabling ASA on opinion expressions with cross-modal information.

Paper number 84:
Title: The NTNU System at the S&I Challenge 2025 SLA Open Track
Authors: Hong-Yun Lin, Tien-Hong Lo, Yu-Hsuan Fang, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen
Abstract: A recent line of research on spoken language assessment (SLA) employs neural models such as BERT and wav2vec 2.0 (W2V) to evaluate speaking proficiency across linguistic and acoustic modalities. Although both models effectively capture features relevant to oral competence, each exhibits modality-specific limitations. BERT-based methods rely on ASR transcripts, which often fail to capture prosodic and phonetic cues for SLA. In contrast, W2V-based methods excel at modeling acoustic features but lack semantic interpretability. To overcome these limitations, we propose a system that integrates W2V with Phi-4 multimodal large language model (MLLM) through a score fusion strategy. The proposed system achieves a root mean square error (RMSE) of 0.375 on the official test set of the Speak & Improve Challenge 2025, securing second place in the competition. For comparison, the RMSEs of the top-ranked, third-ranked, and official baseline systems are 0.364, 0.384, and 0.444, respectively.

Paper number 85:
Title: An Improved U-Net Model for Offline handwriting signature denoising
Authors: Wanghui Xiao
Abstract: Handwriting signatures, as an important means of identity recognition, are widely used in multiple fields such as financial transactions, commercial contracts and personal affairs due to their legal effect and uniqueness. In forensic science appraisals, the analysis of offline handwriting signatures requires the appraiser to provide a certain number of signature samples, which are usually derived from various historical contracts or archival materials. However, the provided handwriting samples are often mixed with a large amount of interfering information, which brings severe challenges to handwriting identification work. This study proposes a signature handwriting denoising model based on the improved U-net structure, aiming to enhance the robustness of the signature recognition system. By introducing discrete wavelet transform and PCA transform, the model's ability to suppress noise has been enhanced. The experimental results show that this modelis significantly superior to the traditional methods in denoising effect, can effectively improve the clarity and readability of the signed images, and provide more reliable technical support for signature analysis and recognition.

Paper number 86:
Title: Pretrained Conformers for Audio Fingerprinting and Retrieval
Authors: Kemal Altwlkany, Elmedin Selmanovic, Sead Delalic
Abstract: Conformers have shown great results in speech processing due to their ability to capture both local and global interactions. In this work, we utilize a self-supervised contrastive learning framework to train conformer-based encoders that are capable of generating unique embeddings for small segments of audio, generalizing well to previously unseen data. We achieve state-of-the-art results for audio retrieval tasks while using only 3 seconds of audio to generate embeddings. Our models are almost completely immune to temporal misalignments and achieve state-of-the-art results in cases of other audio distortions such as noise, reverb or extreme temporal stretching. Code and models are made publicly available and the results are easy to reproduce as we train and test using popular and freely available datasets of different sizes.

Paper number 87:
Title: Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels
Authors: Hossein Ahmadi, Banafsheh Saffari, Sajjad Emdadi Mahdimahalleh, Mohammad Esmaeil Safari, Aria Ahmadi
Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio, spectrum monitoring, and secure wireless communication. However, existing solutions often rely on large labeled datasets or multi-stage training pipelines, which limit scalability and generalization in practice. We propose a unified Vision Transformer (ViT) framework that integrates supervised, self-supervised, and reconstruction objectives. The model combines a ViT encoder, a lightweight convolutional decoder, and a linear classifier; the reconstruction branch maps augmented signals back to their originals, anchoring the encoder to fine-grained I/Q structure. This strategy promotes robust, discriminative feature learning during pretraining, while partial label supervision in fine-tuning enables effective classification with limited labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and ViT baselines in low-label regimes, approaches ResNet-level accuracy with only 15-20% labeled data, and maintains strong performance across varying SNR levels. Overall, the framework provides a simple, generalizable, and label-efficient solution for AMR.

Paper number 88:
Title: A Fundamental Convergence Rate Bound for Gradient Based Online Optimization Algorithms with Exact Tracking
Authors: Alex Xinting Wu, Ian R. Petersen, Iman Shames
Abstract: In this paper, we consider algorithms with integral action for solving online optimization problems characterized by quadratic cost functions with a time-varying optimal point described by an $(n-1)$th order polynomial. Using a version of the internal model principle, the optimization algorithms under consideration are required to incorporate a discrete time $n$-th order integrator in order to achieve exact tracking. By using results on an optimal gain margin problem, we obtain a fundamental convergence rate bound for the class of linear gradient based algorithms exactly tracking a time-varying optimal point. This convergence rate bound is given by $ \left(\frac{\sqrt{\kappa} - 1 }{\sqrt{\kappa} + 1}\right)^{\frac{1}{n}}$, where $\kappa$ is the condition number for the set of cost functions under consideration. Using our approach, we also construct algorithms which achieve the optimal convergence rate as well as zero steady-state error when tracking a time-varying optimal point.

Paper number 89:
Title: AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs
Authors: Sidharth Surapaneni, Hoang Nguyen, Jash Mehta, Aman Tiwari, Oluwanifemi Bamgbose, Akshay Kalkunte, Sai Rajeswar, Sathwik Tejaswi Madhusudhan
Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce AU-Harness, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. AU-Harness provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.

Paper number 90:
Title: Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition
Authors: Yujian Ma, Jinqiu Sang, Ruizhe Li
Abstract: Large pre-trained speech models such as Whisper offer strong generalization but pose significant challenges for resource-efficient adaptation. Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its underlying mechanisms in speech tasks remain poorly understood. In this work, we conduct the first systematic mechanistic interpretability study of LoRA within the Whisper encoder for speech emotion recognition (SER). Using a suite of analytical tools, including layer contribution probing, logit-lens inspection, and representational similarity via singular value decomposition (SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a delayed specialization process that preserves general features in early layers before consolidating task-specific information, and a forward alignment, backward differentiation dynamic between LoRA's matrices. Our findings clarify how LoRA reshapes encoder hierarchies, providing both empirical insights and a deeper mechanistic understanding for designing efficient and interpretable adaptation strategies in large speech models. Our code is available at this https URL.
    