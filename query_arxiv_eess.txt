
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Design and Implementation of Parametrized Look-Up Tables for Post-Correction of Oversampling Low-Resolution ADCs
Authors: Morriel Kasher, Michael Tinston, Predrag Spasojevic
Abstract: We propose a framework for the design, optimization, and implementation of Look-Up Tables (LUTs) used to recover noisy, oversampled, quantized signals given a parametric input model. The LUTs emulate the spectral effects of pre-quantization dithering through an all-digital solution applied after quantization. This methodology decomposes the intractable LUT design problem into four distinct stages, each of which is addressed analytically using a model-driven approach without reliance on training. Three dithering methods are studied to improve spectral purity metrics. Two novel indexing schemes are proposed to limit the LUT memory overhead shown to compress the LUT size by over four orders of magnitude with marginal performance loss. The LUT design is tested with an oversampled noisy sinusoidal input quantized to 3 bits and shown to improve its Spurious-Free Dynamic Range (SFDR) by over 19 dBc with only 324 bytes of memory while maintaining the same 3-bit fixed-point precision at the digital output. This correction can be implemented using two-level combinational logic ensuring ultra-low latency and, hence, suitable for low-resolution wideband devices.

Paper number 2:
Title: Exploiting Movable Antennas in NOMA Networks: Joint Beamforming, Power Allocation and Antenna Position Optimization
Authors: Yufeng Zhou, Wen Chen, Qingqing Wu, Xusheng Zhu, Zhendong Li, Kunlun Wang, Qiong Wu
Abstract: This paper investigates the movable antenna (MA)- assisted downlink non-orthogonal multiple access (NOMA) network to maximize system throughput. In the considered scenario, both the base station (BS) and users are equipped with MA, and a predetermined successive interference cancellation (SIC) decoding order is adopted. Based on the field-response channel model, we formulate a complex, non-convex problem to jointly optimize the BS beamforming, power allocation, and MA positions at both the transmitter and receivers. To address this, we propose an efficient algorithm based on an alternating optimization (AO) framework, which decomposes the original problem into three distinct subproblems. By employing sequential parametric convex approximation (SPCA) and successive convex approximation (SCA) techniques, the non-convex constraints within each subproblem are transformed into tractable. This methodology ensures the algorithm converges to a stable, locally optimal solution. Numerical results validate that the proposed system, which fully exploits the degrees of freedom from antenna mobility at both ends, significantly outperforms benchmarks in terms of throughput.

Paper number 3:
Title: Max-Min Rate Optimization for Multigroup Multicast MISO Systems Via Novel Transmissive RIS Transceiver
Authors: Yuan Guo, Wen Chen, Qingqing Wu, Yanze Zhu, Yang Liu, Zhendong Li, Ying Wang
Abstract: This paper investigates a novel transmissive reconfigurable intelligent surface (RIS) transceiver architectureenabled multigroup multicast downlink communication system. Under this setup, an optimization problem is formulated to maximize the minimum rate of users across all groups, subject to the maximum available power of each RIS unit. Due to the nondifferentiable nature of the objective function, the max-min rate problem is challenging to solve. To tackle this difficult problem, we develop an iterative solution by leveraging the successive convex approximation (SCA) and the penalty function method. However, the above approach has high computational complexity and may lead to compromised performance. To overcome these drawbacks, we design an efficient second-order cone programming (SOCP)-based method using the weighted minimum mean squared error (WMMSE) framework to reduce computational complexity. Furthermore, to further reduce the computational complexity, we also propose a low-complexity and solver-free algorithm that analytically updates all variables by combining the smooth approximation theory and the majorization-minimization (MM) method. Numerical results are provided to verify the convergence and effectiveness of our proposed three algorithms. It is also demonstrated that the SOCP-based method outperforms the penalty-based algorithm in terms of both the achieved min rate and the computational complexity. In contrast, the lowcomplexity design achieves significantly lower complexity with only slightly degraded performance.

Paper number 4:
Title: An Explainable Equity-Aware P2P Energy Trading Framework for Socio-Economically Diverse Microgrid
Authors: Abhijan Theja, Mayukha Pal
Abstract: Fair and dynamic energy allocation in community microgrids remains a critical challenge, particularly when serving socio-economically diverse participants. Static optimization and cost-sharing methods often fail to adapt to evolving inequities, leading to participant dissatisfaction and unsustainable cooperation. This paper proposes a novel framework that integrates multi-objective mixed-integer linear programming (MILP), cooperative game theory, and a dynamic equity-adjustment mechanism driven by reinforcement learning (RL). At its core, the framework utilizes a bi-level optimization model grounded in Equity-regarding Welfare Maximization (EqWM) principles, which incorporate Rawlsian fairness to prioritize the welfare of the least advantaged participants. We introduce a Proximal Policy Optimization (PPO) agent that dynamically adjusts socio-economic weights in the optimization objective based on observed inequities in cost and renewable energy access. This RL-powered feedback loop enables the system to learn and adapt, continuously striving for a more equitable state. To ensure transparency, Explainable AI (XAI) is used to interpret the benefit allocations derived from a weighted Shapley value. Validated across six realistic scenarios, the framework demonstrates peak demand reductions of up to 72.6%, and significant cooperative gains. The adaptive RL mechanism further reduces the Gini coefficient over time, showcasing a pathway to truly sustainable and fair energy communities.

Paper number 5:
Title: Max-Min Fairness-Oriented Beamforming Design in HAPS-Enabled ISAC for 6G Networks
Authors: Parisa Kanani, Mohammad Javad Omidi, Mahmoud Modarres-Hashemi, Halim Yanikomeroglu
Abstract: This paper presents a high-altitude platform station (HAPS)-enabled integrated sensing and communication (ISAC) system designed for sixth-generation (6G) networks. Positioned in the stratosphere, HAPS serves as a super-macro base station, leveraging advanced beamforming techniques to enable communication and sensing simultaneously. This research addresses the need for equitable service distribution in 6G networks by focusing on fairness within the HAPS-ISAC system. It tackles a non-convex optimization problem that balances sensing beampattern gain and signal-to-interference-plus-noise ratio (SINR) requirements among communication users (CUs) using a max-min fairness approach while adhering to power constraints. The proposed HAPS-ISAC framework ensures efficient resource allocation, reliable coverage, and improved sensing accuracy. Simulation results validate the potential of HAPS-ISAC as a pivotal enabler for 6G networks and integrated communication-sensing systems.

Paper number 6:
Title: Flexible Intelligent Metasurfaces in High-Mobility MIMO Integrated Sensing and Communications
Authors: Kuranage Roche Rayan Ranasinghe, Jiancheng An, Iván Alexander Morales Sandoval, Hyeon Seok Rou, Giuseppe Thadeu Freitas de Abreu, Chau Yuen, Mérouane Debbah
Abstract: We propose a novel doubly-dispersive (DD) multiple-input multiple-output (MIMO) channel model incorporating flexible intelligent metasurfaces (FIMs), which is suitable for integrated sensing and communications (ISAC) in high-mobility scenarios. We then discuss how the proposed FIM-parameterized DD (FPDD) channel model can be applied in a logical manner to ISAC waveforms that are known to perform well in DD environments, namely, orthogonal frequency division multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine frequency division multiplexing (AFDM). Leveraging the proposed model, we formulate an achievable rate maximization problem with a strong sensing constraint for all the aforementioned waveforms, which we then solve via a gradient ascent algorithm with closed-form gradients presented as a bonus. Our numerical results indicate that the achievable rate is significantly impacted by the emerging FIM technology with careful parametrization essential in obtaining strong ISAC performance across all waveforms suitable to mitigating the effects of DD channels.

Paper number 7:
Title: Estimating Sensitivity Maps for X-Nuclei Magnetic Resonance Spectroscopic Imaging
Authors: Nicholas Dwork, Jeremy W. Gordon, Shuyu Tang, Peder E. Z. Larson
Abstract: The purpose of this research is to estimate sensitivity maps when imaging X-nuclei that may not have a significant presence throughout the field of view. We propose to estimate the coil's sensitivities by solving a least-squares problem where each row corresponds to an individual estimate of the sensitivity for a given voxel. Multiple estimates come from the multiple bins of the spectrum with spectroscopy, multiple times with dynamic imaging, or multiple frequencies when utilizing spectral excitation. The method presented in this manuscript, called the L2 optimal method, is compared to the commonly used RefPeak method which uses the spectral bin with the highest energy to estimate the sensitivity maps. The L2 optimal method yields more accurate sensitivity maps when imaging a numerical phantom and is shown to yield a higher signal-to-noise ratio when imaging the brain, pancreas, and heart with hyperpolarized pyruvate as the contrast agent with hyperpolarized MRI. The L2 optimal method is able to better estimate the sensitivity by extracting more information from the measurements.

Paper number 8:
Title: Approximating CCCV charging using SOC-dependent tapered charging power constraints in long-term microgrid planning
Authors: Hassan Zahid Butt, Xingpeng Li
Abstract: Traditional long-term microgrid planning models assume constant power charging for battery energy storage systems (BESS), overlooking efficiency losses that occur toward the end of charge due to rising internal resistance. While this issue can be mitigated at the cell level using constant current-constant voltage (CCCV) charging, it is impractical at the pack level in large-scale systems. However, battery management systems and inverter controls can emulate this effect by tapering charging power at high state-of-charge (SOC) levels, trading off charging speed for improved efficiency and reduced thermal stress. Ignoring this behavior in planning models can lead to undersized batteries and potential reliability issues. This paper proposes a tractable and scalable approach to approximate CCCV behavior using SOC-dependent tapered charging power (TCP) constraints. A MATLAB-based proof of concept demonstrates the energy delivery and efficiency benefits of tapering. The method is integrated into a long-term planning framework and evaluated under a synthetic load and solar profile. Results show tapering significantly affects BESS sizing, cost, and reliability under dynamic operating conditions that demand fast charging. These findings highlight tapering as a critical modeling factor for accurately capturing BESS performance in long-term microgrid planning.

Paper number 9:
Title: Enhancing Robustness of Control Barrier Function: A Reciprocal Resistance-based Approach
Authors: Xinming Wang, Zongyi Guo, Jianguo Guo, Jun Yang, Yunda Yan
Abstract: In this note, a new reciprocal resistance-based control barrier function (RRCBF) is developed to enhance the robustness of control barrier functions for disturbed affine nonlinear systems, without requiring explicit knowledge of disturbance bounds. By integrating a reciprocal resistance-like term into the conventional zeroing barrier function framework, we formally establish the concept of the reciprocal resistance-based barrier function (RRBF), rigorously proving the forward invariance of its associated safe set and its robustness against bounded disturbances. The RRBF inherently generates a buffer zone near the boundary of the safe set, effectively dominating the influence of uncertainties and external disturbances. This foundational concept is extended to formulate RRCBFs, including their high-order variants. To alleviate conservatism in the presence of complex, time-varying disturbances, we further introduce a disturbance observer-based RRCBF (DO-RRCBF), which exploits disturbance estimates to enhance safety guarantees and recover nominal control performance. The effectiveness of the proposed framework is validated through two simulation studies: a second-order linear system illustrating forward invariance in the phase plane, and an adaptive cruise control scenario demonstrating robustness in systems with high relative degree.

Paper number 10:
Title: A Fingerprint Database Generation Method for RIS-Assisted Indoor Positioning
Authors: Xin Cheng, Yu He, Menglu Li, Ruoguang Li, Feng Shu, Guangjie Han
Abstract: Reconfigurable intelligent surface (RIS) has emerged as a promising technology to enhance indoor wireless communication and sensing performance. However, the construction of reliable received signal strength (RSS)-based fingerprint databases for RIS-assisted indoor positioning remains an open challenge due to the lack of realistic and spatially consistent channel modeling methods. In this paper, we propose a novel method with open-source codes for generating RIS-assisted RSS fingerprint databases. Our method captures the complex RIS-assisted multipath behaviors by extended cluster-based channel modeling and the physical and electromagnetic properties of RIS and transmitter (Tx). And the spatial consistency is incorporated when simulating the fingerprint data collection across neighboring positions. Furthermore, the proposed method offers exceptional flexibility in configuring RIS and Tx parameters. Extensive simulations are conducted to evaluate the fingerprint database generated by the proposed method. Moreover, the positioning performance on the database using K-nearest neighbors (KNN) and deep neural network (DNN) is analyzed, providing valuable insights for the system design.

Paper number 11:
Title: GMM-Based Time-Varying Coverage Control
Authors: Behzad Zamani, James Kennedy, Airlie Chapman, Peter Dower, Chris Manzie, Simon Crase
Abstract: In coverage control problems that involve time-varying density functions, the coverage control law depends on spatial integrals of the time evolution of the density function. The latter is often neglected, replaced with an upper bound or calculated as a numerical approximation of the spatial integrals involved. In this paper, we consider a special case of time-varying density functions modeled as Gaussian Mixture Models (GMMs) that evolve with time via a set of time-varying sources (with known corresponding velocities). By imposing this structure, we obtain an efficient time-varying coverage controller that fully incorporates the time evolution of the density function. We show that the induced trajectories under our control law minimise the overall coverage cost. We elicit the structure of the proposed controller and compare it with a classical time-varying coverage controller, against which we benchmark the coverage performance in simulation. Furthermore, we highlight that the computationally efficient and distributed nature of the proposed control law makes it ideal for multi-vehicle robotic applications involving time-varying coverage control problems. We employ our method in plume monitoring using a swarm of drones. In an experimental field trial we show that drones guided by the proposed controller are able to track a simulated time-varying chemical plume in a distributed manner.

Paper number 12:
Title: Assessing the Reliability and Validity of a Balance Mat for Measuring Postural Stability: A Combined Robot-Human Approach
Authors: Abishek Shrestha, Damith Herath, Angie Fearon, Maryam Ghahramani
Abstract: Postural sway assessment is important for detecting balance problems and identifying people at risk of falls. Force plates (FP) are considered the gold standard postural sway assessment method in laboratory conditions, but their lack of portability and requirement of high-level expertise limit their widespread usage. This study evaluates the reliability and validity of a novel Balance Mat (BM) device, a low-cost portable alternative that uses optical fibre technology. The research includes two studies: a robot study and a human study. In the robot study, a UR10 robotic arm was used to obtain controlled sway patterns to assess the reliability and sensitivity of the BM. In the human study, 51 healthy young participants performed balance tasks on the BM in combination with an FP to evaluate the BM's validity. Sway metrics such as sway mean, sway absolute mean, sway root mean square (RMS), sway path, sway range, and sway velocity were calculated from both BM and FP and compared. Reliability was evaluated using the intra-class correlation coefficient (ICC), where values greater than 0.9 were considered excellent and values between 0.75 and 0.9 were considered good. Results from the robot study demonstrated good to excellent ICC values in both single and double-leg stances. The human study showed moderate to strong correlations for sway path and range. Using Bland-Altman plots for agreement analysis revealed proportional bias between the BM and the FP where the BM overestimated sway metrics compared to the FP. Calibration was used to improve the agreement between the devices. The device demonstrated consistent sway measurement across varied stance conditions, establishing both reliability and validity following appropriate calibration.

Paper number 13:
Title: Max-Min Beamforming for Large-Scale Cell-Free Massive MIMO: A Randomized ADMM Algorithm
Authors: Bin Wang, Jun Fang, Yue Xiao, Martin Haardt
Abstract: We consider the problem of max-min beamforming (MMB) for cell-free massive multi-input multi-output (MIMO) systems, where the objective is to maximize the minimum achievable rate among all users. Existing MMB methods are mainly based on deterministic optimization methods, which are computationally inefficient when the problem size grows large. To address this issue, we, in this paper, propose a randomized alternating direction method of multiplier (ADMM) algorithm for large-scale MMB problems. We first propose a novel formulation that transforms the highly challenging feasibility-checking problem into a linearly constrained optimization problem. An efficient randomized ADMM is then developed for solving the linearly constrained problem. Unlike standard ADMM, randomized ADMM only needs to solve a small number of subproblems at each iteration to ensure convergence, thus achieving a substantial complexity reduction. Our theoretical analysis reveals that the proposed algorithm exhibits an O(1/\bar{t}) convergence rate (\bar{t} represents the number of iterations), which is on the same order as its deterministic counterpart. Numerical results show that the proposed algorithm offers a significant complexity advantage over existing methods in solving the MMB problem.

Paper number 14:
Title: Research on Sectionalizing Switches Placement Problem of Distribution System Automation Based on Multi-Objective Optimization Analysis
Authors: Selma Cheshmeh Khavar, Arya Abdollahi
Abstract: Achieving high distribution-reliability levels and concurrently minimizing operating costs can be considered as the main issues in distribution system optimization. Determination of the optimal number and location of automation devices in the distribution system network is an essential issue from the reliability and economical points of view. To address these issues, this paper develops a multi-objective model, wherein the primary objective, optimal automation devices placement is implemented aiming at minimizing the operating costs, while in the second objective the reliability indices improvement is taken into account. So, modified non dominated sorting genetic algorithm, is developed and presented to solve this multi-objective mixed-integer non-linear programming problem. The feasibility of the proposed algorithm examined by application to two distribution feeders of the Tabriz distribution network containing the third feeder of the Azar substation with a distributed generation unit and first and third feeders of ElGoli substation which form a double feed feeder.

Paper number 15:
Title: FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems
Authors: Yizhou Peng, Yi-Wen Chao, Dianwen Ng, Yukun Ma, Chongjia Ni, Bin Ma, Eng Siong Chng
Abstract: Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine interactions by allowing real-time user interruptions and backchanneling, compared to traditional SDS that rely on turn-taking. However, existing benchmarks lack metrics for FD scenes, e.g., evaluating model performance during user interruptions. In this paper, we present a comprehensive FD benchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It assesses FDSDS's ability to handle user interruptions, manage delays, and maintain robustness in challenging scenarios with diverse novel metrics. We applied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and VITA-1.5) using over 40 hours of generated speech, with 293 simulated conversations and 1,200 interruptions. The results show that all models continue to face challenges, such as failing to respond to user interruptions, under frequent disruptions and noisy conditions. Demonstrations, data, and code will be released.

Paper number 16:
Title: Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks
Authors: Bowen Li, Junting Chen
Abstract: Dynamic low altitude networks offer significant potential for efficient and reliable data transport via unmanned aerial vehicles (UAVs) relays which usually operate with predetermined trajectories. However, it is challenging to optimize the data routing and resource allocation due to the time-varying topology and the need to control interference with terrestrial systems. Traditional schemes rely on time-expanded graphs with uniform and fine time subdivisions, making them impractical for interference-aware applications. This paper develops a dynamic space-time graph model with a cross-layer optimization framework that converts a joint routing and predictive resource allocation problem into a joint bottleneck path planning and resource allocation problem. We develop explicit deterministic bounds to handle the channel uncertainty and prove a monotonicity property in the problem structure that enables us to efficiently reach the globally optimal solution to the predictive resource allocation subproblem. Then, this approach is extended to multi-commodity transmission tasks through time-frequency allocation, and a bisection search algorithm is developed to find the optimum solution by leveraging the monotonicity of the feasible set family. Simulations verify that the single-commodity algorithm approaches global optimality with more than 30 dB performance gain over the classical graph-based methods for delay-sensitive and large data transportation. At the same time, the multi-commodity method achieves 100X improvements in dense service scenarios and enables an additional 20 dB performance gain by data segmenting.

Paper number 17:
Title: Assessment of Personality Dimensions Across Situations Using Conversational Speech
Authors: Alice Zhang, Skanda Muralidhar, Daniel Gatica-Perez, Mathew Magimai-Doss
Abstract: Prior research indicates that users prefer assistive technologies whose personalities align with their own. This has sparked interest in automatic personality perception (APP), which aims to predict an individual's perceived personality traits. Previous studies in APP have treated personalities as static traits, independent of context. However, perceived personalities can vary by context and situation as shown in psychological research. In this study, we investigate the relationship between conversational speech and perceived personality for participants engaged in two work situations (a neutral interview and a stressful client interaction). Our key findings are: 1) perceived personalities differ significantly across interactions, 2) loudness, sound level, and spectral flux features are indicative of perceived extraversion, agreeableness, conscientiousness, and openness in neutral interactions, while neuroticism correlates with these features in stressful contexts, 3) handcrafted acoustic features and non-verbal features outperform speaker embeddings in inference of perceived personality, and 4) stressful interactions are more predictive of neuroticism, aligning with existing psychological research.

Paper number 18:
Title: RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution
Authors: Weisong Zhao, Jingkai Zhou, Xiangyu Zhu, Weihua Chen, Xiao-Yu Zhang, Zhen Lei, Fan Wang
Abstract: Video Super-Resolution (VSR) has achieved significant progress through diffusion models, effectively addressing the over-smoothing issues inherent in GAN-based methods. Despite recent advances, three critical challenges persist in VSR community: 1) Inconsistent modeling of temporal dynamics in foundational models; 2) limited high-frequency detail recovery under complex real-world degradations; and 3) insufficient evaluation of detail enhancement and 4K super-resolution, as current methods primarily rely on 720P datasets with inadequate details. To address these challenges, we propose RealisVSR, a high-frequency detail-enhanced video diffusion model with three core innovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated with the Wan2.1 video diffusion to model the smooth and complex motions and suppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss) combining wavelet decomposition and HOG feature constraints for texture restoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing 1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal guidance of Wan2.1, our method requires only 5-25% of the training data volume compared to existing approaches. Extensive experiments on VSR benchmarks (REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our superiority, particularly in ultra-high-resolution scenarios.

Paper number 19:
Title: Machine Learning based Radio Environment Map Estimation for Indoor Visible Light Communication
Authors: Helena Serpi, Christina (Tanya)Politi
Abstract: An innovative method for radio map estimation in optical wireless communications is proposed that is based on Machine Learning rather than simulation techniques. Multi-Layer Perceptron (MLP) representation of indoor Visible Light Communication (VLC) systems is suggested, and signal propagation is estimated. The simulation and performance predictions are accurate, fast and require a reduced set of training sample size with respect to other counterparts, making this solution very suitable for real time estimation of an indoor VLC system. It is shown that by tweaking MLP parameters, such as sample size, number of epochs and batch size, one can balance the desired level of inference accuracy with training time and optimize the model's performance to meet real-time requirements.

Paper number 20:
Title: RegScore: Scoring Systems for Regression Tasks
Authors: Michal K. Grzeszczyk, Tomasz Szczepański, Pawel Renc, Siyeop Yoon, Jerome Charton, Tomasz Trzciński, Arkadiusz Sitek
Abstract: Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, sparse, and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and k-sparse ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) transformer to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at this https URL.

Paper number 21:
Title: Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge
Authors: Kang Wang, Chen Qin, Zhang Shi, Haoran Wang, Xiwen Zhang, Chen Chen, Cheng Ouyang, Chengliang Dai, Yuanhan Mo, Chenchen Dai, Xutong Kuang, Ruizhe Li, Xin Chen, Xiuzheng Yue, Song Tian, Alejandro Mora-Rubio, Kumaradevan Punithakumar, Shizhan Gong, Qi Dou, Sina Amirrajab, Yasmina Al Khalil, Cian M. Scannell, Lexiaozi Fan, Huili Yang, Xiaowu Sun, Rob van der Geest, Tewodros Weldebirhan Arega, Fabrice Meriaudeau, Caner Özer, Amin Ranem, John Kalkhof, İlkay Öksüz, Anirban Mukhopadhyay, Abdul Qayyum, Moona Mazher, Steven A Niederer, Carles Garcia-Cabrera, Eric Arazo, Michal K. Grzeszczyk, Szymon Płotka, Wanqin Ma, Xiaomeng Li, Rongjun Ge, Yongqing Kou, Xinrong Chen, He Wang, Chengyan Wang, Wenjia Bai, Shuo Wang
Abstract: Deep learning models have achieved state-of-the-art performance in automated Cardiac Magnetic Resonance (CMR) analysis. However, the efficacy of these models is highly dependent on the availability of high-quality, artifact-free images. In clinical practice, CMR acquisitions are frequently degraded by respiratory motion, yet the robustness of deep learning models against such artifacts remains an underexplored problem. To promote research in this domain, we organized the MICCAI CMRxMotion challenge. We curated and publicly released a dataset of 320 CMR cine series from 40 healthy volunteers who performed specific breathing protocols to induce a controlled spectrum of motion artifacts. The challenge comprised two tasks: 1) automated image quality assessment to classify images based on motion severity, and 2) robust myocardial segmentation in the presence of motion artifacts. A total of 22 algorithms were submitted and evaluated on the two designated tasks. This paper presents a comprehensive overview of the challenge design and dataset, reports the evaluation results for the top-performing methods, and further investigates the impact of motion artifacts on five clinically relevant biomarkers. All resources and code are publicly available at: this https URL

Paper number 22:
Title: High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins
Authors: Lorenzo Cazzella, Francesco Linsalata, Damiano Badini, Matteo Matteucci, Maurizio Magarini, Umberto Spagnolini
Abstract: The design of accurate Digital Twins (DTs) of electromagnetic environments strictly depends on the fidelity of the underlying environmental modeling. Evaluating the differences among diverse levels of modeling accuracy is key to determine the relevance of the model features towards both efficient and accurate DT simulations. In this paper, we propose two metrics, the Hausdorff ray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently compare the temporal, angular and power features between two ray tracing simulations performed on 3D scenarios featured by environmental changes. To evaluate the introduced metrics, we considered a high-fidelity digital twin model of an area of Milan, Italy and we enriched it with two different types of environmental changes: (i) the inclusion of parked vehicles meshes, and (ii) the segmentation of the buildings facade faces to separate the windows mesh components from the rest of the building. We performed grid-based and vehicular ray tracing simulations at 28 GHz carrier frequency on the obtained scenarios integrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular traffic simulator. Both the HRT and CRT metrics highlighted the areas of the scenarios where the simulated radio propagation features differ owing to the introduced mesh integrations, while the vehicular ray tracing simulations allowed to uncover the distance patterns arising along realistic vehicular trajectories.

Paper number 23:
Title: Bespoke multiresolution analysis of graph signals
Authors: Giacomo Elefante, Gianluca Giacchi, Michael Multerer, Jacopo Quizi
Abstract: We present a novel framework for discrete multiresolution analysis of graph signals. The main analytical tool is the samplet transform, originally defined in the Euclidean framework as a discrete wavelet-like construction, tailored to the analysis of scattered data. The first contribution of this work is defining samplets on graphs. To this end, we subdivide the graph into a fixed number of patches, embed each patch into a Euclidean space, where we construct samplets, and eventually pull the construction back to the graph. This ensures orthogonality, locality, and the vanishing moments property with respect to properly defined polynomial spaces on graphs. Compared to classical Haar wavelets, this framework broadens the class of graph signals that can efficiently be compressed and analyzed. Along this line, we provide a definition of a class of signals that can be compressed using our construction. We support our findings with different examples of signals defined on graphs whose vertices lie on smooth manifolds. For efficient numerical implementation, we combine heavy edge clustering, to partition the graph into meaningful patches, with landmark \texttt{Isomap}, which provides low-dimensional embeddings for each patch. Our results demonstrate the method's robustness, scalability, and ability to yield sparse representations with controllable approximation error, significantly outperforming traditional Haar wavelet approaches in terms of compression efficiency and multiresolution fidelity.

Paper number 24:
Title: Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning
Authors: Abdul Hannan, Zahid Mahmood, Rizwan Qureshi, Hazrat Ali
Abstract: Automatic classification of Diabetic Retinopathy (DR) can assist ophthalmologists in devising personalized treatment plans, making it a critical component of clinical practice. However, imbalanced data distribution in the dataset becomes a bottleneck in the generalization of deep learning models trained for DR classification. In this work, we combine global attention block (GAB) and category attention block (CAB) into the deep learning model, thus effectively overcoming the imbalanced data distribution problem in DR classification. Our proposed approach is based on an attention mechanism-based deep learning model that employs three pre-trained networks, namely, MobileNetV3-small, Efficientnet-b0, and DenseNet-169 as the backbone architecture. We evaluate the proposed method on two publicly available datasets of retinal fundoscopy images for DR. Experimental results show that on the APTOS dataset, the DenseNet-169 yielded 83.20% mean accuracy, followed by the MobileNetV3-small and EfficientNet-b0, which yielded 82% and 80% accuracies, respectively. On the EYEPACS dataset, the EfficientNet-b0 yielded a mean accuracy of 80%, while the DenseNet-169 and MobileNetV3-small yielded 75.43% and 76.68% accuracies, respectively. In addition, we also compute the F1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of 95.5%, and a kappa score of 88.2% for the experiments. Moreover, in our work, the MobileNetV3-small has 1.6 million parameters on the APTOS dataset and 0.90 million parameters on the EYEPACS dataset, which is comparatively less than other methods. The proposed approach achieves competitive performance that is at par with recently reported works on DR classification.

Paper number 25:
Title: Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?
Authors: Simon Malan, Benjamin van Niekerk, Herman Kamper
Abstract: We investigate the problem of segmenting unlabeled speech into word-like units and clustering these to create a lexicon. Prior work can be categorized into two frameworks. Bottom-up methods first determine boundaries and then cluster the fixed segmented words into a lexicon. In contrast, top-down methods incorporate information from the clustered words to inform boundary selection. However, it is unclear whether top-down information is necessary to improve segmentation. To explore this, we look at two similar approaches that differ in whether top-down clustering informs boundary selection. Our simple bottom-up strategy predicts word boundaries using the dissimilarity between adjacent self-supervised features, then clusters the resulting segments to construct a lexicon. Our top-down system is an updated version of the ES-KMeans dynamic programming method that iteratively uses K-means to update its boundaries. On the five-language ZeroSpeech benchmarks, both approaches achieve comparable state-of-the-art results, with the bottom-up system being nearly five times faster. Through detailed analyses, we show that the top-down influence of ES-KMeans can be beneficial (depending on factors like the candidate boundaries), but in many cases the simple bottom-up method performs just as well. For both methods, we show that the clustering step is a limiting factor. Therefore, we recommend that future work focus on improved clustering techniques and learning more discriminative word-like representations. Project code repository: this https URL.

Paper number 26:
Title: Comparison of Knowledge Distillation Methods for Low-complexity Multi-microphone Speech Enhancement using the FT-JNF Architecture
Authors: Robert Metzger, Mattes Ohlenbusch, Christian Rollwage, Simon Doclo
Abstract: Multi-microphone speech enhancement using deep neural networks (DNNs) has significantly progressed in recent years. However, many proposed DNN-based speech enhancement algorithms cannot be implemented on devices with limited hardware resources. Only lowering the complexity of such systems by reducing the number of parameters often results in worse performance. Knowledge Distillation (KD) is a promising approach for reducing DNN model size while preserving performance. In this paper, we consider the recently proposed Frequency-Time Joint Non-linear Filter (FT-JNF) architecture and investigate several KD methods to train smaller (student) models from a large pre-trained (teacher) model. Five KD methods are evaluated using direct output matching, the self-similarity of intermediate layers, and fused multi-layer losses. Experimental results on a simulated dataset using a compact array with five microphones show that three KD methods substantially improve the performance of student models compared to training without KD. A student model with only 25% of the teacher model's parameters achieves comparable PESQ scores at 0 dB SNR. Furthermore, a reduction of up to 96% in model size can be achieved with only a minimal decrease in PESQ scores.

Paper number 27:
Title: Truncated Gaussian Noise Estimation in State-Space Models
Authors: Rodrigo A. González, Angel L. Cedeño, Koen Tiels, Tom Oomen
Abstract: Within Bayesian state estimation, considerable effort has been devoted to incorporating constraints into state estimation for process optimization, state monitoring, fault detection and control. Nonetheless, in the domain of state-space system identification, the prevalent practice entails constructing models under Gaussian noise assumptions, which can lead to inaccuracies when the noise follows bounded distributions. With the aim of generalizing the Gaussian noise assumption to potentially truncated densities, this paper introduces a method for estimating the noise parameters in a state-space model subject to truncated Gaussian noise. Our proposed data-driven approach is rooted in maximum likelihood principles combined with the Expectation-Maximization algorithm. The efficacy of the proposed approach is supported by a simulation example.

Paper number 28:
Title: Cell-based VSC Analysis Methodology: From Graph Laplacian to Converter Degrees of Freedom
Authors: Daniele Falchi, Eduardo Prieto-Araujo, Oriol Gomis-Bellmunt
Abstract: Power-electronics-based converters are being considerably employed through the power system to interconnect multiple heterogeneous electrical layers. Furthermore, the intrinsic versatility to play with the converter network topology is widely exploited to accommodate a certain number of terminals and ports according with the specific application. On this regard, several converter arrangements can be encountered in power applications. Moreover, to properly establish both the operation and the control, the so-called degrees of freedom (DOFs) need to be assessed per each converter topology. On this matter, similarly to the well-known Clarke transformation, which clearly reveals the DOFs for the star-based topology system, further similar transformations can be achieved to depict the independent set of variables characterizing a certain converter structure. Referring to the cell-based class of Voltage Source Converter (VSC) topologies, including Modular Multilevel Converter (MMC); this article proposes a general methodology to determine the change of variable matrix transformation for several converter arrangements which are related to complete bi-partite and multi-partite graphs. The methodology lies in the graph Laplacian spectral analysis, which remarks the structural normal modes at the converter points of connections. Furthermore, for a complete characterization, the instantaneous power patterns formulations, based on the DOFs, are also introduced.

Paper number 29:
Title: SAM2-Aug: Prior knowledge-based Augmentation for Target Volume Auto-Segmentation in Adaptive Radiation Therapy Using Segment Anything Model 2
Authors: Guoping Xu, Yan Dai, Hengrui Zhao, Ying Zhang, Jie Deng, Weiguo Lu, You Zhang
Abstract: Purpose: Accurate tumor segmentation is vital for adaptive radiation therapy (ART) but remains time-consuming and user-dependent. Segment Anything Model 2 (SAM2) shows promise for prompt-based segmentation but struggles with tumor accuracy. We propose prior knowledge-based augmentation strategies to enhance SAM2 for ART. Methods: Two strategies were introduced to improve SAM2: (1) using prior MR images and annotations as contextual inputs, and (2) improving prompt robustness via random bounding box expansion and mask erosion/dilation. The resulting model, SAM2-Aug, was fine-tuned and tested on the One-Seq-Liver dataset (115 MRIs from 31 liver cancer patients), and evaluated without retraining on Mix-Seq-Abdomen (88 MRIs, 28 patients) and Mix-Seq-Brain (86 MRIs, 37 patients). Results: SAM2-Aug outperformed convolutional, transformer-based, and prompt-driven models across all datasets, achieving Dice scores of 0.86(liver), 0.89(abdomen), and 0.90(brain). It demonstrated strong generalization across tumor types and imaging sequences, with improved performance in boundary-sensitive metrics. Conclusions: Incorporating prior images and enhancing prompt diversity significantly boosts segmentation accuracy and generalizability. SAM2-Aug offers a robust, efficient solution for tumor segmentation in ART. Code and models will be released at this https URL.

Paper number 30:
Title: Real-time rail vehicle localisation using spatially resolved magnetic field measurements
Authors: Niklas Dieckow, Katharina Ostaszewski, Philip Heinisch, Henriette Struckmann, Hendrik Ranocha
Abstract: This work presents two complementary real-time rail vehicle localization methods based on magnetic field measurements and a pre-recorded magnetic map. The first uses a particle filter reweighted via magnetic similarity, employing a heavy-tailed non-Gaussian kernel for enhanced stability. The second is a stateless sequence alignment technique that transforms real-time magnetic signals into the spatial domain and matches them to the map using a similarity measure. Experiments with operational train data show that the particle filter achieves track-selective, sub-5-meter accuracy over 21.6 km, though its performance degrades at low speeds and during cold starts. Accuracy tests were constrained by the GNSS-based reference system. In contrast, the alignment-based method excels in cold-start scenarios, localizing within 30 m in 92 % of tests (100 % using top-3 matches). A hybrid approach combines both methods$\unicode{x2014}$alignment-based initialization followed by particle filter tracking. Runtime analysis confirms real-time capability on consumer-grade hardware. The system delivers accurate, robust localization suitable for safety-critical rail applications.

Paper number 31:
Title: Binaural Target Speaker Extraction using HRTFs and a Complex-Valued Neural Network
Authors: Yoav Ellinson, Sharon Gannot
Abstract: In this work, we aim to imitate the human ability to selectively attend to a single speaker, even in the presence of multiple simultaneous talkers. We propose a novel approach for binaural target speaker extraction that leverages the listener's Head-Related Transfer Function (HRTF) to isolate the desired speaker. Notably, our method does not rely on speaker embeddings, making it speaker-independent and enabling strong generalization across multiple speech datasets in different languages. We employ a fully complex-valued neural network that operates directly on the complex-valued Short-Time Fourier Transform (STFT) of the mixed audio signals. This deviates from conventional approaches that use spectrograms or treat the real and imaginary components of the STFT as separate real-valued inputs. We first evaluate the method in an anechoic, noise-free scenario, where it demonstrates excellent extraction performance while effectively preserving the binaural cues of the target signal. We then test a modified variant under mild reverberation conditions. This version remains robust in reverberant environments, maintaining speech clarity, preserving source directionality, and simultaneously reducing reverberation.

Paper number 32:
Title: A multi-dynamic low-rank deep image prior (ML-DIP) for real-time 3D cardiovascular MRI
Authors: Chong Chen, Marc Vornehm, Preethi Chandrasekaran, Muhammad A. Sultan, Syed M. Arshad, Yingmin Liu, Yuchi Han, Rizwan Ahmad
Abstract: Purpose: To develop a reconstruction framework for 3D real-time cine cardiovascular magnetic resonance (CMR) from highly undersampled data without requiring fully sampled training data. Methods: We developed a multi-dynamic low-rank deep image prior (ML-DIP) framework that models spatial image content and temporal deformation fields using separate neural networks. These networks are optimized per scan to reconstruct the dynamic image series directly from undersampled k-space data. ML-DIP was evaluated on (i) a 3D cine digital phantom with simulated premature ventricular contractions (PVCs), (ii) ten healthy subjects (including two scanned during both rest and exercise), and (iii) five patients with PVCs. Phantom results were assessed using peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). In vivo performance was evaluated by comparing left-ventricular function quantification (against 2D real-time cine) and image quality (against 2D real-time cine and binning-based 5D-Cine). Results: In the phantom study, ML-DIP achieved PSNR > 29 dB and SSIM > 0.90 for scan times as short as two minutes, while recovering cardiac motion, respiratory motion, and PVC events. In healthy subjects, ML-DIP yielded functional measurements comparable to 2D cine and higher image quality than 5D-Cine, including during exercise with high heart rates and bulk motion. In PVC patients, ML-DIP preserved beat-to-beat variability and reconstructed irregular beats, whereas 5D-Cine showed motion artifacts and information loss due to binning. Conclusion: ML-DIP enables high-quality 3D real-time CMR with acceleration factors exceeding 1,000 by learning low-rank spatial and temporal representations from undersampled data, without relying on external fully sampled training datasets.

Paper number 33:
Title: Part Segmentation of Human Meshes via Multi-View Human Parsing
Authors: James Dickens, Kamyar Hamad
Abstract: Recent advances in point cloud deep learning have led to models that achieve high per-part labeling accuracy on large-scale point clouds, using only the raw geometry of unordered point sets. In parallel, the field of human parsing focuses on predicting body part and clothing/accessory labels from images. This work aims to bridge these two domains by enabling per-vertex semantic segmentation of large-scale human meshes. To achieve this, a pseudo-ground truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are first aligned to a canonical pose, segmented from multiple viewpoints, and the resulting point-level labels are then backprojected onto the original mesh to produce per-point pseudo ground truth annotations. Subsequently, a novel, memory-efficient sampling strategy is introduced, a windowed iterative farthest point sampling (FPS) with space-filling curve-based serialization to effectively downsample the point clouds. This is followed by a purely geometric segmentation using PointTransformer, enabling semantic parsing of human meshes without relying on texture information. Experimental results confirm the effectiveness and accuracy of the proposed approach.

Paper number 34:
Title: SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence Learning
Authors: Vishakh Begari
Abstract: A curated dataset of Guitar Pro tablature files (.gp5 format), tailored for tasks involving guitar music generation, sequence modeling, and performance-aware learning is provided. The dataset is derived from MIDI notes in MAESTRO and GiantMIDI which have been adapted into rhythm guitar tracks. These tracks are further processed to include a variety of expression settings typical of guitar performance, such as bends, slides, vibrato, and palm muting, to better reflect the nuances of real-world guitar playing.

Paper number 35:
Title: Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems: A Novel Network Deep Q-Learning Approach
Authors: Amir Fard, Arnold X.-X. Yuan
Abstract: Infrastructure asset management is essential for sustaining the performance of public infrastructure such as road networks, bridges, and utility networks. Traditional maintenance and rehabilitation planning methods often face scalability and computational challenges, particularly for large-scale networks with thousands of assets under budget constraints. This paper presents a novel deep reinforcement learning (DRL) framework that optimizes asset management strategies for large infrastructure networks. By decomposing the network-level Markov Decision Process (MDP) into individual asset-level MDPs while using a unified neural network architecture, the proposed framework reduces computational complexity, improves learning efficiency, and enhances scalability. The framework directly incorporates annual budget constraints through a budget allocation mechanism, ensuring maintenance plans are both optimal and cost-effective. Through a case study on a large-scale pavement network of 68,800 segments, the proposed DRL framework demonstrates significant improvements over traditional methods like Progressive Linear Programming and genetic algorithms, both in efficiency and network performance. This advancement contributes to infrastructure asset management and the broader application of reinforcement learning in complex, large-scale environments.

Paper number 36:
Title: KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ
Authors: Tristan Repolusk, Eduardo Veas
Abstract: Optical Music Recognition (OMR) for historical Chinese musical notations, such as suzipu and lülüpu, presents unique challenges due to high class imbalance and limited training data. This paper introduces significant advancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ from 1202. In this work, we develop and evaluate a character recognition model for scarce imbalanced data. We improve upon previous baselines by reducing the Character Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with 77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for lülüpu. Our models outperform human transcribers, with an average human CER of 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve a well-calibrated model with an Expected Calibration Error (ECE) below 0.0162. Using a leave-one-edition-out cross-validation approach, we ensure robust performance across five historical editions. Additionally, we extend the KuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing suzipu, lülüpu, and jianzipu notations. Our findings advance the digitization and accessibility of historical Chinese music, promoting cultural diversity in OMR and expanding its applicability to underrepresented music traditions.

Paper number 37:
Title: CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image Generation
Authors: Hyunwoo Oh, SeungJu Cha, Kwanyoung Lee, Si-Woo Kim, Dong-Jin Kim
Abstract: We propose CatchPhrase, a novel audio-to-image generation framework designed to mitigate semantic misalignment between audio inputs and generated images. While recent advances in multi-modal encoders have enabled progress in cross-modal generation, ambiguity stemming from homographs and auditory illusions continues to hinder accurate alignment. To address this issue, CatchPhrase generates enriched cross-modal semantic prompts (EXPrompt Mining) from weak class labels by leveraging large language models (LLMs) and audio captioning models (ACMs). To address both class-level and instance-level misalignment, we apply multi-modal filtering and retrieval to select the most semantically aligned prompt for each audio sample (EXPrompt Selector). A lightweight mapping network is then trained to adapt pre-trained text-to-image generation models to audio input. Extensive experiments on multiple audio classification datasets demonstrate that CatchPhrase improves audio-to-image alignment and consistently enhances generation quality by mitigating semantic misalignment.

Paper number 38:
Title: HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling
Authors: Rongkun Xue, Yazhe Niu, Shuai Hu, Zixin Yin, Yongqiang Yao, Jing Yang
Abstract: Discrete speech tokenization is a fundamental component in speech codecs. However, in large-scale speech-to-speech systems, the complexity of parallel streams from multiple quantizers and the computational cost of high-time-dimensional codecs pose significant challenges. In this paper, we introduce HH-Codec, a neural codec that achieves extreme compression at 24 tokens per second for 24 kHz audio while relying on single-quantizer inference. Our approach involves a carefully designed Vector Quantization space for Spoken Language Modeling, optimizing compression efficiency while minimizing information loss. Building on this, we propose an asymmetric encoder-decoder architecture (Audio-VQ-Mel-Audio) that leverages dual supervision and progressive training to enhance reconstruction stability and fidelity. HH-Codec achieves state-of-the-art performance in speech reconstruction with an ultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in codebook utilization and generative model adaptation, with extensive ablations validating the necessity of each module. HH-Codec is available at this https URL.

Paper number 39:
Title: Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation
Authors: Guanyi Qin, Ziyue Wang, Daiyun Shen, Haofeng Liu, Hantao Zhou, Junde Wu, Runze Hu, Yueming Jin
Abstract: Given an object mask, Semi-supervised Video Object Segmentation (SVOS) technique aims to track and segment the object across video frames, serving as a fundamental task in computer vision. Although recent memory-based methods demonstrate potential, they often struggle with scenes involving occlusion, particularly in handling object interactions and high feature similarity. To address these issues and meet the real-time processing requirements of downstream applications, in this paper, we propose a novel bOundary Amendment video object Segmentation method with Inherent Structure refinement, hereby named OASIS. Specifically, a lightweight structure refinement module is proposed to enhance segmentation accuracy. With the fusion of rough edge priors captured by the Canny filter and stored object features, the module can generate an object-level structure map and refine the representations by highlighting boundary features. Evidential learning for uncertainty estimation is introduced to further address challenges in occluded regions. The proposed method, OASIS, maintains an efficient design, yet extensive experiments on challenging benchmarks demonstrate its superior performance and competitive inference speed compared to other state-of-the-art methods, i.e., achieving the F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6 (vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive speed of 48 FPS on DAVIS.

Paper number 40:
Title: MLLM-based Speech Recognition: When and How is Multimodality Beneficial?
Authors: Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill
Abstract: Recent advances in multi-modal large language models (MLLMs) have opened new possibilities for unified modeling of speech, text, images, and other modalities. Building on our prior work, this paper examines the conditions and model architectures under which multiple input modalities can improve automatic speech recognition (ASR) accuracy in noisy environments. Through experiments on synthetic and real-world data, we find that (1) harnessing more modalities usually improves ASR accuracy, as each modality provides complementary information, but the improvement depends on the amount of auditory noise. (2) Synchronized modalities (e.g., lip movements) are more useful at high noise levels whereas unsynchronized modalities (e.g., image context) are most helpful at moderate noise levels. (3) Higher-quality visual representations consistently improve ASR accuracy, highlighting the importance of developing more powerful visual encoders. (4) Mamba exhibits similar trends regarding the benefits of multimodality as do Transformers. (5) The input order of modalities as well as their weights in the loss function can significantly impact accuracy. These findings both offer practical insights and help to deepen our understanding of multi-modal speech recognition under challenging conditions.

Paper number 41:
Title: A Distributed Approach for Agile Supply Chain Decision-Making Based on Network Attributes
Authors: Mingjie Bi, Dawn M. Tilbury, Siqian Shen, Kira Barton
Abstract: In recent years, the frequent occurrence of disruptions has had a negative impact on global supply chains. To stay competitive, enterprises strive to remain agile through the implementation of efficient and effective decision-making strategies in reaction to disruptions. A significant effort has been made to develop these agile disruption mitigation approaches, leveraging both centralized and distributed decision-making strategies. Though trade-offs of centralized and distributed approaches have been analyzed in existing studies, no related work has been found on understanding supply chain performance based on the network attributes of the disrupted supply chain entities. In this paper, we characterize supply chains from a capability and network topological perspective and investigate the use of a distributed decision-making approach based on classical multi-agent frameworks. The performance of the distributed framework is evaluated through a comprehensive case study that investigates the performance of the supply chain as a function of the network structure and agent attributes within the network in the presence of a disruption. Comparison to a centralized decision-making approach highlights trade-offs between performance, computation time, and network communication based on the decision-making strategy and network architecture. Practitioners can use the outcomes of our studies to design response strategies based on agent capabilities, network attributes, and desired supply chain performance.

Paper number 42:
Title: Dynamic distributed decision-making for resilient resource reallocation in disrupted manufacturing systems
Authors: Mingjie Bi, Ilya Kovalenko, Dawn M. Tilbury, Kira Barton
Abstract: The COVID-19 pandemic brings many unexpected disruptions, such as frequently shifting markets and limited human workforce, to manufacturers. To stay competitive, flexible and real-time manufacturing decision-making strategies are needed to deal with such highly dynamic manufacturing environments. One essential problem is dynamic resource allocation to complete production tasks, especially when a resource disruption (e.g., machine breakdown) occurs. Though multi-agent methods have been proposed to solve the problem in a flexible and agile manner, the agent internal decision-making process and resource uncertainties have rarely been studied. This work introduces a model-based resource agent (RA) architecture that enables effective agent coordination and dynamic agent decision-making. Based on the RA architecture, a rescheduling strategy that incorporates risk assessment via a clustering agent coordination strategy is also proposed. A simulation-based case study is implemented to demonstrate dynamic rescheduling using the proposed multi-agent framework. The results show that the proposed method reduces the computational efforts while losing some throughput optimality compared to the centralized method. Furthermore, the case study illustrates that incorporating risk assessment into rescheduling decision-making improves the throughput.

Paper number 43:
Title: Heterogeneous Risk Management Using a Multi-Agent Framework for Supply Chain Disruption Response
Authors: Mingjie Bi, Juan-Alberto Estrada-Garcia, Dawn M. Tilbury, Siqian Shen, Kira Barton
Abstract: In the highly complex and stochastic global, supply chain environments, local enterprise agents seek distributed and dynamic strategies for agile responses to disruptions. Existing literature explores both centralized and distributed approaches, while most work neglects temporal dynamics and the heterogeneity of the risk management of individual agents. To address this gap, this letter presents a heterogeneous risk management mechanism to incorporate uncertainties and risk attitudes into agent communication and decision-making strategy. Hence, this approach empowers enterprises to handle disruptions in stochastic environments in a distributed way, and in particular in the context of multi-agent control and management. Through a simulated case study, we showcase the feasibility and effectiveness of the proposed approach under stochastic settings and how the decision of disruption responses changes when agents hold various risk attitudes.

Paper number 44:
Title: From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models
Authors: Zhaoxi Mu, Rilin Chen, Andong Li, Meng Yu, Xinyu Yang, Dong Yu
Abstract: This paper introduces OmniGSE, a novel general speech enhancement (GSE) framework designed to mitigate the diverse distortions that speech signals encounter in real-world scenarios. These distortions include background noise, reverberation, bandwidth limitations, signal clipping, and network packet loss. Existing methods typically focus on optimizing for a single type of distortion, often struggling to effectively handle the simultaneous presence of multiple distortions in complex scenarios. OmniGSE bridges this gap by integrating the strengths of discriminative and generative approaches through a two-stage architecture that enables cross-domain collaborative optimization. In the first stage, continuous features are enhanced using a lightweight channel-split NAC-RoFormer. In the second stage, discrete tokens are generated to reconstruct high-quality speech through language models. Specifically, we designed a hierarchical language model structure consisting of a RootLM and multiple BranchLMs. The RootLM models general acoustic features across codebook layers, while the BranchLMs explicitly capture the progressive relationships between different codebook levels. Experimental results demonstrate that OmniGSE surpasses existing models across multiple benchmarks, particularly excelling in scenarios involving compound distortions. These findings underscore the framework's potential for robust and versatile speech enhancement in real-world applications.

Paper number 45:
Title: Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations
Authors: Taewon Kang, Ji-Wook Kwon, Il Bae, Jin Hyo Kim
Abstract: Localization of mobile robots is crucial for deploying robots in real-world applications such as search and rescue missions. This work aims to develop an accurate localization system applicable to swarm robots equipped only with low-cost monocular vision sensors and visual markers. The system is designed to operate in fully open spaces, without landmarks or support from positioning infrastructures. To achieve this, we propose a localization method based on equilateral triangular formations. By leveraging the geometric properties of equilateral triangles, the accurate two-dimensional position of each participating robot is estimated using one-dimensional lateral distance information between robots, which can be reliably and accurately obtained with a low-cost monocular vision sensor. Experimental and simulation results demonstrate that, as travel time increases, the positioning error of the proposed method becomes significantly smaller than that of a conventional dead-reckoning system, another low-cost localization approach applicable to open environments.

Paper number 46:
Title: Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers
Authors: Chi-Ching Hsu, Gaëtan Frusque, Florent Forest, Felipe Macedo, Christian M. Franck, Olga Fink
Abstract: Commercial high-voltage circuit breaker (CB) condition monitoring systems rely on directly observable physical parameters such as gas filling pressure with pre-defined thresholds. While these parameters are crucial, they only cover a small subset of malfunctioning mechanisms and usually can be monitored only if the CB is disconnected from the grid. To facilitate online condition monitoring while CBs remain connected, non-intrusive measurement techniques such as vibration or acoustic signals are necessary. Currently, CB condition monitoring studies using these signals typically utilize supervised methods for fault diagnostics, where ground-truth fault types are known due to artificially introduced faults in laboratory settings. This supervised approach is however not feasible in real-world applications, where fault labels are unavailable. In this work, we propose a novel unsupervised fault detection and segmentation framework for CBs based on vibration and acoustic signals. This framework can detect deviations from the healthy state. The explainable artificial intelligence (XAI) approach is applied to the detected faults for fault diagnostics. The specific contributions are: (1) we propose an integrated unsupervised fault detection and segmentation framework that is capable of detecting faults and clustering different faults with only healthy data required during training (2) we provide an unsupervised explainability-guided fault diagnostics approach using XAI to offer domain experts potential indications of the aged or faulty components, achieving fault diagnostics without the prerequisite of ground-truth fault labels. These contributions are validated using an experimental dataset from a high-voltage CB under healthy and artificially introduced fault conditions, contributing to more reliable CB system operation.

Paper number 47:
Title: Latent Granular Resynthesis using Neural Audio Codecs
Authors: Nao Tokui, Tom Baker
Abstract: We introduce a novel technique for creative audio resynthesis that operates by reworking the concept of granular synthesis at the latent vector level. Our approach creates a "granular codebook" by encoding a source audio corpus into latent vector segments, then matches each latent grain of a target audio signal to its closest counterpart in the codebook. The resulting hybrid sequence is decoded to produce audio that preserves the target's temporal structure while adopting the source's timbral characteristics. This technique requires no model training, works with diverse audio materials, and naturally avoids the discontinuities typical of traditional concatenative synthesis through the codec's implicit interpolation during decoding. We include supplementary material at this https URL , as well as a proof-of-concept implementation to allow users to experiment with their own sounds at this https URL .

Paper number 48:
Title: Optimal Control of Hybrid Systems via Measure Relaxations
Authors: Etienne Buehrle, Ömer Şahin Taş, Christoph Stiller
Abstract: We propose an approach to trajectory optimization for piecewise polynomial systems based on the recently proposed graphs of convex sets framework. We instantiate the framework with a convex relaxation of optimal control based on occupation measures, resulting in a convex optimization problem resembling the discrete shortest-paths linear program that can be solved efficiently to global optimality. While this approach inherits the limitations of semidefinite programming, scalability to large numbers of discrete modes improves compared to the NP-hard mixed-integer formulation. We use this to plan trajectories under temporal logic specifications, comparing the computed cost lower bound to a nonconvex optimization approach with fixed mode sequence. In our numerical experiments, we find that this bound is typically in the vicinity of the nonconvex solution, while the runtime speedup is significant compared to the often intractable mixed-integer formulation. Our implementation is available at this https URL.

Paper number 49:
Title: Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation
Authors: Fang Kang, Yin Cao, Haoyu Chen
Abstract: Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity \& Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU.

Paper number 50:
Title: The Eloquence team submission for task 1 of MLC-SLM challenge
Authors: Lorenzo Concina, Jordi Luque, Alessio Brutti, Marco Matassoni, Yuchen Zhang
Abstract: In this paper, we present our studies and experiments carried out for the task 1 of the Challenge and Workshop on Multilingual Conversational Speech Language Model (MLC-SLM), which focuses on advancing multilingual conversational speech recognition through the development of speech language models architectures. Given the increasing relevance of real-world conversational data for building robust Spoken Dialogue Systems, we explore three approaches to multilingual ASR. First, we conduct an evaluation of the official baseline to better understand its strengths and limitations, by training two projectors (linear and qformer) with different foundation models. Second we leverage the SLAM-ASR framework to train a custom multilingual linear projector. Finally we investigate the role of contrastive learning and the extended conversational context in enhancing the robustness of recognition.

Paper number 51:
Title: SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models
Authors: Zhen Wan, Chao-Han Huck Yang, Yahan Yu, Jinchuan Tian, Sheng Li, Ke Hu, Zhehuai Chen, Shinji Watanabe, Fei Cheng, Chenhui Chu, Sadao Kurohashi
Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.

Paper number 52:
Title: Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints
Authors: Amir Fard, Arnold X.-X. Yuan
Abstract: Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows.

Paper number 53:
Title: Variational Bayesian Inference for Multiple Extended Targets or Unresolved Group Targets Tracking
Authors: Yuanhao Cheng, Yunhe Cao, Tat-Soon Yeo, Yulin Zhang, Fu Jie
Abstract: In this work, we propose a method for tracking multiple extended targets or unresolvable group targets in a clutter environment. Firstly, based on the Random Matrix Model (RMM), the joint state of the target is modeled as the Gamma Gaussian Inverse Wishart (GGIW) distribution. Considering the uncertainty of measurement origin caused by the clutters, we adopt the idea of probabilistic data association and describe the joint association event as an unknown parameter in the joint prior distribution. Then the Variational Bayesian Inference (VBI) is employed to approximately solve the non-analytical posterior distribution. Furthermore, to ensure the practicability of the proposed method, we further provide two potential lightweight schemes to reduce its computational complexity. One of them is based on clustering, which effectively prunes the joint association events. The other is a simplification of the variational posterior through marginal association probabilities. Finally, the effectiveness of the proposed method is demonstrated by simulation and real data experiments, and we show that the proposed method outperforms current state-of-the-art methods in terms of accuracy and adaptability.

Paper number 54:
Title: Integrating IP Broadcasting with Audio Tags: Workflow and Challenges
Authors: Rhys Burchett-Vass, Arshdeep Singh, Gabriel Bibbó, Mark D. Plumbley
Abstract: The broadcasting industry has adopted IP technologies, revolutionising both live and pre-recorded content production, from news gathering to live music events. IP broadcasting allows for the transport of audio and video signals in an easily configurable way, aligning with modern networking techniques. This shift towards an IP workflow allows for much greater flexibility, not only in routing signals but with the integration of tools using standard web development techniques. One possible tool could include the use of live audio tagging, which has a number of uses in the production of content. These could include adding sound effects to automated closed captioning or identifying unwanted sound events within a scene. In this paper, we describe the process of containerising an audio tagging model into a microservice, a small segregated code module that can be integrated into a multitude of different network setups. The goal is to develop a modular, accessible, and flexible tool capable of seamless deployment into broadcasting workflows of all sizes, from small productions to large corporations. Challenges surrounding latency of the selected audio tagging model and its effect on the usefulness of the end product are discussed.

Paper number 55:
Title: A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level
Authors: Ali AbuSaleh, Mehdi Rahim
Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.

Paper number 56:
Title: GREAT: Grassmannian REcursive Algorithm for Tracking & Online System Identification
Authors: András Sasfi, Alberto Padoan, Ivan Markovsky, Florian Dörfler
Abstract: This paper introduces an online approach for identifying time-varying subspaces defined by linear dynamical systems. The approach of representing linear systems by non-parametric subspace models has received significant interest in the field of data-driven control recently. This system representation enables us to provide rigorous guarantees for linear time-varying systems, which are difficult to obtain for parametric system models. The proposed method leverages optimization on the Grassmann manifold leading to the Grassmannian Recursive Algorithm for Tracking (GREAT). We view subspaces as points on the Grassmann manifold and adapt the estimate based on online data by performing optimization on the manifold. At each time step, a single measurement from the current subspace corrupted by a bounded error is available. The subspace estimate is updated online using Grassmannian gradient descent on a cost function incorporating a window of the most recent data. Under suitable assumptions on the signal-to-noise ratio of the online data and the subspace's rate of change, we establish theoretical guarantees for the resulting algorithm. More specifically, we prove an exponential convergence rate and provide an uncertainty quantification of the estimates in terms of an upper bound on their distance to the true subspace. The applicability of the proposed algorithm is demonstrated by means of numerical examples.

Paper number 57:
Title: Self-Supervised Frameworks for Speaker Verification via Bootstrapped Positive Sampling
Authors: Theo Lepage, Reda Dehak
Abstract: Recent developments in Self-Supervised Learning (SSL) have demonstrated significant potential for Speaker Verification (SV), but closing the performance gap with supervised systems remains an ongoing challenge. SSL frameworks rely on anchor-positive pairs, constructed from segments of the same audio utterance. Hence, positives have channel characteristics similar to those of their corresponding anchors, even with extensive data-augmentation. Therefore, this positive sampling strategy is a fundamental limitation as it encodes too much information regarding the recording source in the learned representations. This article introduces Self-Supervised Positive Sampling (SSPS), a bootstrapped technique for sampling appropriate and diverse positives in SSL frameworks for SV. SSPS samples positives close to their anchor in the representation space, assuming that these pseudo-positives belong to the same speaker identity but correspond to different recording conditions. This method consistently demonstrates improvements in SV performance on VoxCeleb benchmarks when applied to major SSL frameworks, including SimCLR, SwAV, VICReg, and DINO. Using SSPS, SimCLR and DINO achieve 2.57% and 2.53% EER on VoxCeleb1-O, respectively. SimCLR yields a 58% relative reduction in EER, getting comparable performance to DINO with a simpler training framework. Furthermore, SSPS lowers intra-class variance and reduces channel information in speaker representations while exhibiting greater robustness without data-augmentation.

Paper number 58:
Title: Adaptive Self-Improvement for Smarter Energy Systems using Agentic Policy Search
Authors: Alexander Sommer, Peter Bazan, Behnam Babaeian, Jonathan Fellerer, Warren B. Powell, Reinhard German
Abstract: Controlling energy systems usually involves manually designed policies for decision-making, which can be complex and time-consuming to develop. This process requires interdisciplinary collaboration among multiple domain experts, resulting in slow and inflexible adaptation to rapidly changing environments. Large Language Models (LLMs) offer a promising paradigm shift by integrating extensive contextual knowledge with the capability to generate structured, executable code. We present Agentic Policy Search (APS) -- a novel hierarchical optimization framework in which LLMs act as autonomous agents that propose complete control logics, translate them into executable code, and iteratively improve them through direct system feedback. We apply APS to a residential energy system with PV, battery, demand, and dynamic electricity prices. Within just seven simulated days, the method yields a net profit of up to 6.20 EUR compared to the no-battery reference scenario (-10.70 EUR), nearly matching the global optimum of a perfectly informed linear program. By combining LLM-driven policy search with the generation of human-interpretable control logic, APS effectively bridges adaptability and traceability in energy management -- while also offering a transferable framework for agentic optimization in other domains.

Paper number 59:
Title: Modal-based prediction of power system frequency response and frequency nadir
Authors: Francisco Zelaya-Arrazabal, Sebastian Martinez-Lizana, Héctor Pulgar-Painemal
Abstract: This paper introduces a novel approach for predicting system frequency response (SFR) and frequency nadir based on modal analysis. By decomposing the full system dynamic response, the method identifies dominant modes based on their participation in frequency behavior and derives a closed-form expression for the frequency trajectory. Unlike traditional approaches based on the Average System Frequency (ASF) model, this method captures the true system dynamics and avoids oversimplified representations. The dominant modes exhibit low sensitivity to system parameters, enabling robust and accurate estimations across diverse operating conditions. The proposed approach is tested on two benchmark systems as well as the Salvadoran transmission planning network, demonstrating its scalability, precision, and adaptability. This methodology represents a shift from observing a simplified average system frequency response to a more detailed analysis focusing on system dynamics.

Paper number 60:
Title: Integration of a Graph-Based Path Planner and Mixed-Integer MPC for Robot Navigation in Cluttered Environments
Authors: Joshua A. Robbins, Stephen J. Harnett, Andrew F. Thompson, Sean Brennan, Herschel C. Pangborn
Abstract: The ability to update a path plan is a required capability for autonomous mobile robots navigating through uncertain environments. This paper proposes a re-planning strategy using a multilayer planning and control framework for cases where the robot's environment is partially known. A medial axis graph-based planner defines a global path plan based on known obstacles, where each edge in the graph corresponds to a unique corridor. A mixed-integer model predictive control (MPC) method detects if a terminal constraint derived from the global plan is infeasible, subject to a non-convex description of the local environment. Infeasibility detection is used to trigger efficient global re-planning via medial axis graph edge deletion. The proposed re-planning strategy is demonstrated experimentally.

Paper number 61:
Title: A User-centric Game for Balancing V2G Benefits with Battery Degradation of Electric Vehicles
Authors: Arghya Mallick, Georgios Pantazis, Peyman Mohajerin Esfahani, Sergio Grammatico
Abstract: We present a novel user-centric vehicle-to-grid (V2G) framework that enables electric vehicle (EV) users to balance the trade-off between financial benefits from V2G and battery health degradation based on individual preference signals.

Paper number 62:
Title: Quasi Steady-State Frequency
Authors: Joan Gutierrez-Florensa, Alvaro Ortega, Lukas Sigrist, Federico Milano
Abstract: Accurate frequency estimation is critical for the control, monitoring and protection of electrical power systems, in particular, of systems with a high penetration of power electronics. This paper introduces the novel concept of Quasi Steady-State (QSS) frequency as a quantity that fills the gap between stationary and instantaneous frequency. QSS frequency coincides with the fundamental frequency of an AC voltage in any stationary conditions, including unbalanced and non-sinusoidal, and is able to capture the time-varying fundamental frequency in transient conditions. The paper also proposes a metric borrowed from fluid dynamics, namely, the time derivative of the circulation, to define the scope of validity of the QSS frequency. Analytical examples as well as a case study based on a fully-fledged EMT model of the IEEE 39-bus system serve to illustrate, respectively, the properties of the QSS frequency and its behavior in transient conditions.

Paper number 63:
Title: Framework of a multiscale data-driven DT of the musculoskeletal system
Authors: Martina Paccini, Simone Cammarasana, Giuseppe Patanè
Abstract: Musculoskeletal disorders (MSDs) are a leading cause of disability worldwide, requiring advanced diagnostic and therapeutic tools for personalised assessment and treatment. Effective management of MSDs involves the interaction of heterogeneous data sources, making the Digital Twin (DT) paradigm a valuable option. This paper introduces the Musculoskeletal Digital Twin (MS-DT), a novel framework that integrates multiscale biomechanical data with computational modelling to create a detailed, patient-specific representation of the musculoskeletal system. By combining motion capture, ultrasound imaging, electromyography, and medical imaging, the MS-DT enables the analysis of spinal kinematics, posture, and muscle function. An interactive visualisation platform provides clinicians and researchers with an intuitive interface for exploring biomechanical parameters and tracking patient-specific changes. Results demonstrate the effectiveness of MS-DT in extracting precise kinematic and dynamic tissue features, offering a comprehensive tool for monitoring spine biomechanics and rehabilitation. This framework provides high-fidelity modelling and real-time visualization to improve patient-specific diagnosis and intervention planning.

Paper number 64:
Title: Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses
Authors: He-Yang Xu, Hongxiang Gao, Yuwen Li, Xiu-Shen Wei, Chengyu Liu
Abstract: The diagnostic value of electrocardiogram (ECG) lies in its dynamic characteristics, ranging from rhythm fluctuations to subtle waveform deformations that evolve across time and frequency domains. However, supervised ECG models tend to overfit dominant and repetitive patterns, overlooking fine-grained but clinically critical cues, a phenomenon known as Simplicity Bias (SB), where models favor easily learnable signals over subtle but informative ones. In this work, we first empirically demonstrate the presence of SB in ECG analyses and its negative impact on diagnostic performance, while simultaneously discovering that self-supervised learning (SSL) can alleviate it, providing a promising direction for tackling the bias. Following the SSL paradigm, we propose a novel method comprising two key components: 1) Temporal-Frequency aware Filters to capture temporal-frequency features reflecting the dynamic characteristics of ECG signals, and 2) building on this, Multi-Grained Prototype Reconstruction for coarse and fine representation learning across dual domains, further mitigating SB. To advance SSL in ECG analyses, we curate a large-scale multi-site ECG dataset with 1.53 million recordings from over 300 clinical centers. Experiments on three downstream tasks across six ECG datasets demonstrate that our method effectively reduces SB and achieves state-of-the-art performance. Code and dataset will be released publicly.

Paper number 65:
Title: Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation
Authors: Klaus Brümann, Kouei Yamaoka, Nobutaka Ono, Simon Doclo
Abstract: Estimating the position of a speech source based on time-differences-of-arrival (TDOAs) is often adversely affected by background noise and reverberation. A popular method to estimate the TDOA between a microphone pair involves maximizing a generalized cross-correlation with phase transform (GCC-PHAT) function. Since the TDOAs across different microphone pairs satisfy consistency relations, generally only a small subset of microphone pairs are used for source position estimation. Although the set of microphone pairs is often determined based on a reference microphone, recently a more robust method has been proposed to determine the set of microphone pairs by computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT function reliabilities. To reduce the influence of noise and reverberation on the TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT functions of the MST based on an average of multiple cross-power spectral densities (CPSDs) using an incremental method. In each step of the method, we increase the number of CPSDs over which we average by considering CPSDs computed indirectly via other microphones from previous steps. Using signals recorded in a noisy and reverberant laboratory with an array of spatially distributed microphones, the performance of the proposed method is evaluated in terms of TDOA estimation error and 2D source position estimation error. Experimental results for different source and microphone configurations and three reverberation conditions show that the proposed method considering multiple CPSDs improves the TDOA estimation and source position estimation accuracy compared to the reference microphone- and MST-based methods that rely on a single CPSD as well as steered-response power-based source position estimation.

Paper number 66:
Title: P.808 Multilingual Speech Enhancement Testing: Approach and Results of URGENT 2025 Challenge
Authors: Marvin Sach, Yihui Fu, Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Anurag Kumar, Wei Wang, Yanmin Qian, Shinji Watanabe, Tim Fingscheidt
Abstract: In speech quality estimation for speech enhancement (SE) systems, subjective listening tests so far are considered as the gold standard. This should be even more true considering the large influx of new generative or hybrid methods into the field, revealing issues of some objective metrics. Efforts such as the Interspeech 2025 URGENT Speech Enhancement Challenge also involving non-English datasets add the aspect of multilinguality to the testing procedure. In this paper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective listening test method. A first novel contribution is our proposed process of localizing both text and audio components of Naderi and Cutler's implementation of crowdsourced subjective absolute category rating (ACR) listening tests involving text-to-speech (TTS). Further, we provide surprising analyses of and insights into URGENT Challenge results, tackling the reliability of (P.808) ACR subjective testing as gold standard in the age of generative AI. Particularly, it seems that for generative SE methods, subjective (ACR MOS) and objective (DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone fidelity metrics to reliably detect hallucinations. Finally, we will soon release our localization scripts and methods for easy deployment for new multilingual speech enhancement subjective evaluations according to ITU-T P.808.

Paper number 67:
Title: A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT
Authors: Tanjin Taher Toma, Tejas Sudharshan Mathai, Bikash Santra, Pritam Mukherjee, Jianfei Liu, Wesley Jong, Darwish Alabyad, Vivek Batheja, Abhishek Jha, Mayank Patel, Darko Pucar, Jayadira del Rivero, Karel Pacak, Ronald M. Summers
Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is essential for tumor burden estimation, prognosis, and treatment planning. It may also help infer genetic clusters, reducing reliance on expensive testing. This study systematically evaluates anatomical priors to identify configurations that improve deep learning-based PCC segmentation. We employed the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D segmentation of pheochromocytoma, introducing a set of novel multi-class schemes based on organ-specific anatomical priors. These priors were derived from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen, kidney, aorta, adrenal gland, and pancreas), and were compared against a broad body-region prior used in previous work. The framework was trained and tested on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center. Performance was measured using Dice Similarity Coefficient (DSC), Normalized Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation accuracy, significantly outperforming the previously used Tumor + Body (TB) annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84% improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split. The TKA model also showed superior tumor burden quantification (R^2 = 0.968) and strong segmentation across all genetic subtypes. In five-fold cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1 to 0.5), reinforcing its robustness and generalizability. These findings highlight the value of incorporating relevant anatomical context into deep learning models to achieve precise PCC segmentation, offering a valuable tool to support clinical assessment and longitudinal disease monitoring in PCC patients.

Paper number 68:
Title: The Sustainability of the Leo Orbit Capacity via Risk-Driven Active Debris Removal
Authors: Yacob Medhin, Simone Servadio
Abstract: The growing number of space debris in Low Earth Orbit (LEO) jeopardizes long-term orbital sustainability, requiring efficient risk assessment for active debris removal (ADR) missions. This study presents the development and validation of Filtered Modified MITRI (FMM), an enhanced risk index designed to improve the prioritization of high-criticality debris. Leveraging the MOCAT-MC simulation framework, we conducted a comprehensive performance evaluation and sensitivity analysis to probe the robustness of the FMM formulation. The results demonstrate that while the FMM provides superior identification of high-risk targets for annual removal campaigns, a nuanced performance trade-off exists between risk models depending on the operational removal cadence. The analysis also confirms that physically grounded mass terms are indispensable for practical risk assessment. By providing a validated open source tool and critical insights into the dynamics of risk, this research enhances our ability to select optimal ADR targets and ensure the long-term viability of LEO operations.

Paper number 69:
Title: MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for Efficient 3D Medical Image Segmentation
Authors: Nand Kumar Yadav, Rodrigue Rizk, William CW Chen, KC Santosh (AI Research Lab, Department of Computer Science and Biomedical and Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA)
Abstract: Accurate and efficient medical image segmentation is crucial but challenging due to anatomical variability and high computational demands on volumetric data. Recent hybrid CNN-Transformer architectures achieve state-of-the-art results but add significant complexity. In this paper, we propose MLRU++, a Multiscale Lightweight Residual UNETR++ architecture designed to balance segmentation accuracy and computational efficiency. It introduces two key innovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that enhances contextual feature encoding with minimal overhead, and a Multiscale Bottleneck Block (M2B) in the decoder that captures fine-grained details via multi-resolution feature aggregation. Experiments on four publicly available benchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that MLRU++ achieves state-of-the-art performance, with average Dice scores of 87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing leading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and ACDC, respectively, while significantly reducing parameter count and computational cost. Ablation studies evaluating LCBAM and M2B further confirm the effectiveness of the proposed architectural components. Results suggest that MLRU++ offers a practical and high-performing solution for 3D medical image segmentation tasks. Source code is available at: this https URL

Paper number 70:
Title: ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding
Authors: Arindam Ghosh, Mark Fuhs, Bongjun Kim, Anurag Chowdhury, Monika Woszczyna
Abstract: From an application standpoint, speaker-role diarization (RD), such as doctor vs. patient, host vs. guest, etc. is often more useful than traditional speaker diarization (SD), which assigns generic labels like speaker-1, speaker-2 etc. In the context of joint automatic speech recognition (ASR) + SD (who spoke what?), recent end-to-end models employ an auxiliary SD transducer, synchronized with the ASR transducer, to predict speakers per word. In this paper, we extend this framework to RD with three key contributions: (1) we simplify the training via forced alignment and cross-entropy loss instead of RNNT loss, (2) we show that word prediction and role prediction require different amounts of predictor's context, leading to separate task-specific predictors, unlike existing shared-predictor models, and (3) we propose a way to leverage RD posterior activity to influence ASR decoding and reduce small-word deletion errors.

Paper number 71:
Title: Improving Multislice Electron Ptychography with a Generative Prior
Authors: Christian K. Belardi, Chia-Hao Lee, Yingheng Wang, Justin Lovelace, Kilian Q. Weinberger, David A. Muller, Carla P. Gomes
Abstract: Multislice electron ptychography (MEP) is an inverse imaging technique that computationally reconstructs the highest-resolution images of atomic crystal structures from diffraction patterns. Available algorithms often solve this inverse problem iteratively but are both time consuming and produce suboptimal solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion model trained on a large database of crystal structures specifically for MEP to augment existing iterative solvers. MEP-Diffusion is easily integrated as a generative prior into existing reconstruction methods via Diffusion Posterior Sampling (DPS). We find that this hybrid approach greatly enhances the quality of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over existing methods.

Paper number 72:
Title: Benchmarking of Deep Learning Methods for Generic MRI Multi-Organ Abdominal Segmentation
Authors: Deepa Krishnaswamy, Cosmin Ciausu, Steve Pieper, Ron Kikinis, Benjamin Billot, Andrey Fedorov
Abstract: Recent advances in deep learning have led to robust automated tools for segmentation of abdominal computed tomography (CT). Meanwhile, segmentation of magnetic resonance imaging (MRI) is substantially more challenging due to the inherent signal variability and the increased effort required for annotating training datasets. Hence, existing approaches are trained on limited sets of MRI sequences, which might limit their generalizability. To characterize the landscape of MRI abdominal segmentation tools, we present here a comprehensive benchmarking of the three state-of-the-art and open-source models: MRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these models are trained using labor-intensive manual annotation cycles, we also introduce and evaluate ABDSynth, a SynthSeg-based model purely trained on widely available CT segmentations (no real images). More generally, we assess accuracy and generalizability by leveraging three public datasets (not seen by any of the evaluated methods during their training), which span all major manufacturers, five MRI sequences, as well as a variety of subject conditions, voxel resolutions, and fields-of-view. Our results reveal that MRSegmentator achieves the best performance and is most generalizable. In contrast, ABDSynth yields slightly less accurate results, but its relaxed requirements in training data make it an alternative when the annotation budget is limited. The evaluation code and datasets are given for future benchmarking at this https URL, along with inference code and weights for ABDSynth.

Paper number 73:
Title: Modeling Nonlinear Control Systems via Koopman Control Family: Universal Forms and Subspace Invariance Proximity
Authors: Masih Haseli, Jorge Cortés
Abstract: This paper introduces the Koopman Control Family (KCF), a mathematical framework for modeling general (not necessarily control-affine) discrete-time nonlinear control systems with the aim of providing a solid theoretical foundation for the use of Koopman-based methods in systems with inputs. We demonstrate that the concept of KCF captures the behavior of nonlinear control systems on a (potentially infinite-dimensional) function space. By employing a generalized notion of subspace invariance under the KCF, we establish a universal form for finite-dimensional models, which encompasses the commonly used linear, bilinear, and linear switched models as specific instances. In cases where the subspace is not invariant under the KCF, we propose a method for approximating models in general form and characterize the model's accuracy using the concept of invariance proximity. We end by discussing how the proposed framework naturally lends itself to data-driven modeling of control systems.

Paper number 74:
Title: TrafficMCTS: A Closed-Loop Traffic Flow Generation Framework with Group-Based Monte Carlo Tree Search
Authors: Ze Fu, Licheng Wen, Pinlong Cai, Daocheng Fu, Song Mao, Botian Shi
Abstract: Traffic flow simulation within the domain of intelligent transportation systems is garnering significant attention, and generating realistic, diverse, and human-like traffic patterns presents critical challenges that must be addressed. Current approaches often hinge on predefined driver models, objective optimization, or reliance on pre-recorded driving datasets, imposing limitations on their scalability, versatility, and adaptability. In this paper, we introduce TrafficMCTS, an innovative framework that harnesses the synergy of group-based Monte Carlo tree search (MCTS) and Social Value Orientation (SVO) to engender a multifaceted traffic flow with varying driving styles and cooperative tendencies. Anchored by a closed-loop architecture, our framework enables vehicles to dynamically adapt to their environment in real time, and ensure feasible collision-free trajectories. Through comprehensive comparisons with state-of-the-art methods, we illuminate the advantages of our approach in terms of computational efficiency, planning success rate, intention completion time, and diversity metrics. Besides, we simulate multiple scenarios to illustrate the effectiveness of the proposed framework and highlight its ability to induce diverse social behaviors within the traffic flow. Finally, we validate the scalability of TrafficMCTS by demonstrating its capability to efficiently simulate diverse traffic scenarios involving numerous interacting vehicles within a complex road network, capturing the intricate dynamics of human-like driving behaviors.

Paper number 75:
Title: A boostlet transform for wave-based acoustic signal processing in space-time
Authors: Elias Zea, Marco Laudato, Joakim andén
Abstract: Sparse representation systems that encode signal architecture have had a profound impact on sampling and compression paradigms. Remarkable examples are multi-scale directional systems, which, similar to our vision system, encode the underlying architecture of natural images with sparse features. Inspired by this philosophy, we introduce a representation system for wave-based acoustic signal processing in 2D space--time, referred to as the \emph{boostlet transform}, which encodes sparse features of natural acoustic fields using the Poincaré group and isotropic dilations. Boostlets are spatiotemporal functions parametrized with dilations, Lorentz boosts, and translations in space--time. Physically speaking, boostlets are supported away from the acoustic radiation cone, i.e., having broadband frequency with phase velocities other than the speed of sound, resulting in a peculiar scaling function. We formulate a discrete boostlet frame using Meyer wavelets and bump functions and examine its sparsity properties. An analysis with experimentally measured fields indicates that discrete boostlet coefficients decay significantly faster and attain superior reconstruction performance than wavelets, curvelets, shearlets, and wave atoms. The results demonstrate that boostlets provide a natural, compact representation system for acoustic waves in space-time.

Paper number 76:
Title: Multispectral Demosaicing via Dual Cameras
Authors: SaiKiran Tedla, Junyong Lee, Beixuan Yang, Mahmoud Afifi, Michael S. Brown
Abstract: Multispectral (MS) images capture detailed scene information across a wide range of spectral bands, making them invaluable for applications requiring rich spectral data. Integrating MS imaging into multi camera devices, such as smartphones, has the potential to enhance both spectral applications and RGB image quality. A critical step in processing MS data is demosaicing, which reconstructs color information from the mosaic MS images captured by the camera. This paper proposes a method for MS image demosaicing specifically designed for dual-camera setups where both RGB and MS cameras capture the same scene. Our approach leverages co-captured RGB images, which typically have higher spatial fidelity, to guide the demosaicing of lower-fidelity MS images. We introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB and MS mosaiced images with ground-truth demosaiced outputs - that enables training and evaluation of our method. Experimental results demonstrate that our method achieves state-of-the-art accuracy compared to existing techniques.

Paper number 77:
Title: Time-resolved dynamic CBCT reconstruction using prior-model-free spatiotemporal Gaussian representation (PMF-STGR)
Authors: Jiacheng Xie, Hua-Chieh Shao, You Zhang
Abstract: Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a 'one-shot' training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation.

Paper number 78:
Title: HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis
Authors: Kangwei Xu, Bing Li, Grace Li Zhang, Ulf Schlichtmann
Abstract: In high-level synthesis (HLS), C/C++ programs with synthesis directives are used to generate circuits for FPGA implementations. However, hardware-specific and platform-dependent characteristics in these implementations can introduce behavioral discrepancies between the original C/C++ programs and the circuits after high-level synthesis. Existing methods for testing behavioral discrepancies in HLS are still immature, and the testing workflow requires significant human efforts. To address this challenge, we propose HLSTester, a large language model (LLM) aided testing framework that efficiently detects behavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance prompt quality, the testbenches for original C/C++ programs are leveraged to guide LLMs in generating HLS-compatible testbenches, effectively eliminating certain traditional C/C++ constructs that are incompatible with HLS tools. Key variables are pinpointed through a backward slicing technique in both C/C++ and HLS programs to monitor their runtime spectra, enabling an in-depth analysis of the discrepancy symptoms. To reduce test time, a testing input generation mechanism is introduced to integrate dynamic mutation with insights from an LLM-based progressive reasoning chain. In addition, repetitive hardware testing is skipped by a redundancy-aware filtering technique for the generated test inputs. Experimental results demonstrate that the proposed LLM-aided testing framework significantly accelerates the testing workflow while achieving higher testbench simulation pass rates compared with the traditional method and the direct use of LLMs on the same HLS programs.

Paper number 79:
Title: SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model
Authors: Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg
Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility.

Paper number 80:
Title: Collision-free Control Barrier Functions for General Ellipsoids via Separating Hyperplane
Authors: Zeming Wu, Lu Liu
Abstract: This paper presents a novel collision avoidance method for general ellipsoids based on control barrier functions (CBFs) and separating hyperplanes. First, collision-free conditions for general ellipsoids are analytically derived using the concept of dual cones. These conditions are incorporated into the CBF framework by extending the system dynamics of controlled objects with separating hyperplanes, enabling efficient and reliable collision avoidance. The validity of the proposed collision-free CBFs is rigorously proven, ensuring their effectiveness in enforcing safety constraints. The proposed method requires only single-level optimization, significantly reducing computational time compared to state-of-the-art methods. Numerical simulations and real-world experiments demonstrate the effectiveness and practicality of the proposed algorithm.

Paper number 81:
Title: Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems
Authors: Jhen-Ke Lin, Hao-Chien Lu, Chung-Chun Wang, Hong-Yun Lin, Berlin Chen
Abstract: Verbatim transcription for automatic speaking assessment demands accurate capture of disfluencies, crucial for downstream tasks like error analysis and feedback. However, many ASR systems discard or generalize hesitations, losing important acoustic details. We fine-tune Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), without recourse to external audio training data. We compare three annotation schemes: removing hesitations (Pure), generic tags (Rich), and acoustically precise fillers inferred by Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra" scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme (6.2% WER). This demonstrates that explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.

Paper number 82:
Title: AI Flow: Perspectives, Scenarios, and Approaches
Authors: Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.

Paper number 83:
Title: JCAPT: A Joint Modeling Approach for CAPT
Authors: Tzu-Hsuan Yang, Yue-Yang He, Berlin Chen
Abstract: Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.

Paper number 84:
Title: GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness
Authors: Hongjie Chen, Zehan Li, Yaodong Song, Wenming Deng, Yitong Yao, Yuxin Zhang, Hang Lv, Xuechao Zhu, Jian Kang, Jie Lian, Jie Li, Chao Wang, Shuangyong Song, Yongxiang Li, Zhongjiang He, Xuelong Li
Abstract: Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.
    