
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: 3D forest semantic segmentation using multispectral LiDAR and 3D deep learning
Authors: Narges Takhtkeshha, Lauris Bocaux, Lassi Ruoppa, Fabio Remondino, Gottfried Mandlburger, Antero Kukko, Juha Hyypp√§
Abstract: Conservation and decision-making regarding forest resources necessitate regular forest inventory. Light detection and ranging (LiDAR) in laser scanning systems has gained significant attention over the past two decades as a remote and non-destructive solution to streamline the labor-intensive and time-consuming procedure of forest inventory. Advanced multispectral (MS) LiDAR systems simultaneously acquire three-dimensional (3D) spatial and spectral information across multiple wavelengths of the electromagnetic spectrum. Consequently, MS-LiDAR technology enables the estimation of both the biochemical and biophysical characteristics of forests. Forest component segmentation is crucial for forest inventory. The synergistic use of spatial and spectral laser information has proven to be beneficial for achieving precise forest semantic segmentation. Thus, this study aims to investigate the potential of MS-LiDAR data, captured by the HeliALS system, providing high-density multispectral point clouds to segment forests into six components: ground, low vegetation, trunks, branches, foliage, and woody debris. Three point-wise 3D deep learning models and one machine learning model, including kernel point convolution, superpoint transformer, point transformer V3, and random forest, are implemented. Our experiments confirm the superior accuracy of the KPConv model. Additionally, various geometric and spectral feature vector scenarios are examined. The highest accuracy is achieved by feeding all three wavelengths (1550 nm, 905 nm, and 532 nm) as the initial features into the deep learning model, resulting in improvements of 33.73% and 32.35% in mean intersection over union (mIoU) and in mean accuracy (mAcc), respectively. This study highlights the excellent potential of multispectral LiDAR for improving the accuracy in fully automated forest component segmentation.

Paper number 2:
Title: DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation
Authors: Chunxi Wang, Maoshen Jia, Wenyu Jin
Abstract: Room Impulse Responses (RIRs) accurately characterize acoustic properties of indoor environments and play a crucial role in applications such as speech enhancement, speech recognition, and audio rendering in augmented reality (AR) and virtual reality (VR). Existing blind estimation methods struggle to achieve practical accuracy. To overcome this challenge, we propose the dynamic audio-room acoustic synthesis (DARAS) model, a novel deep learning framework that is explicitly designed for blind RIR estimation from monaural reverberant speech signals. First, a dedicated deep audio encoder effectively extracts relevant nonlinear latent space features. Second, the Mamba-based self-supervised blind room parameter estimation (MASS-BRPE) module, utilizing the efficient Mamba state space model (SSM), accurately estimates key room acoustic parameters and features. Third, the system incorporates a hybrid-path cross-attention feature fusion module, enhancing deep integration between audio and room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT) decoder adaptively segments early reflections and late reverberation to improve the realism of synthesized RIRs. Experimental results, including a MUSHRA-based subjective listening study, demonstrate that DARAS substantially outperforms existing baseline models, providing a robust and effective solution for practical blind RIR estimation in real-world acoustic environments.

Paper number 3:
Title: AI-Augmented Visible Light Communication: A Framework for Noise Mitigation and Secure Data Transmission
Authors: A. A. Nutfaji, Moustafa Hassan Elmallah
Abstract: This paper presents a proposed AI Deep Learning model that addresses common challenges encountered in Visible Light Communication (VLC) systems. In this work, we run a Python simulation that models a basic VLC system primarily affected by Additive White Gaussian Noise (AWGN). A Deep Neural Network (DNN) is then trained to equalize the noisy signal received and improve signal integrity. The system evaluates and compares the Bit Error Rate (BER) before and after equalization to demonstrate the effectiveness of the proposed model. This paper starts by introducing the concept of visible light communication, then it dives deep into some details about the process of VLC and the challenges it faces, shortly after we propose our project which helps overcome these challenges. We finally conclude with a lead for future work, highlighting the areas that are most suitable for future improvements.

Paper number 4:
Title: Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance Learning for Whole Slide Image Analysis
Authors: Xiwen Chen, Peijie Qiu, Wenhui Zhu, Hao Wang, Huayu Li, Xuanzhao Dong, Xiaotong Sun, Xiaobing Yu, Yalin Wang, Abolfazl Razi, Aristeidis Sotiras
Abstract: While multiple instance learning (MIL) has shown to be a promising approach for histopathological whole slide image (WSI) analysis, its reliance on permutation invariance significantly limits its capacity to effectively uncover semantic correlations between instances within WSIs. Based on our empirical and theoretical investigations, we argue that approaches that are not permutation-invariant but better capture spatial correlations between instances can offer more effective solutions. In light of these findings, we propose a novel alternative to existing MIL for WSI analysis by learning to restore the order of instances from their randomly shuffled arrangement. We term this task as cracking an instance jigsaw puzzle problem, where semantic correlations between instances are uncovered. To tackle the instance jigsaw puzzles, we propose a novel Siamese network solution, which is theoretically justified by optimal transport theory. We validate the proposed method on WSI classification and survival prediction tasks, where the proposed method outperforms the recent state-of-the-art MIL competitors. The code is available at this https URL.

Paper number 5:
Title: Deep Reinforcement Learning in Applied Control: Challenges, Analysis, and Insights
Authors: Klinsmann Agyei, Pouria Sarhadi, Daniel Polani
Abstract: Over the past decade, remarkable progress has been made in adopting deep neural networks to enhance the performance of conventional reinforcement learning. A notable milestone was the development of Deep Q-Networks (DQN), which achieved human-level performance across a range of Atari games, demonstrating the potential of deep learning to stabilise and scale reinforcement learning. Subsequently, extensions to continuous control algorithms paved the way for a new paradigm in control, one that has attracted broader attention than any classical control approach in recent literature. These developments also demonstrated strong potential for advancing data-driven, model-free algorithms for control and for achieving higher levels of autonomy. However, the application of these methods has remained largely confined to simulated and gaming environments, with ongoing efforts to extend them to real-world applications. Before such deployment can be realised, a solid and quantitative understanding of their performance on applied control problems is necessary. This paper conducts a comparative analysis of these approaches on four diverse benchmark problems with implementation results. This analysis offers a scrutinising and systematic evaluation to shed light on the real-world capabilities and limitations of deep reinforcement learning methods in applied control settings.

Paper number 6:
Title: Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification Mapping on Non-Contrast CT
Authors: Xiangjian Hou, Ebru Yaman Akcicek, Xin Wang, Kazem Hashemizadeh, Scott Mcnally, Chun Yuan, Xiaodong Ma
Abstract: While total intracranial carotid artery calcification (ICAC) volume is an established stroke biomarker, growing evidence shows this aggregate metric ignores the critical influence of plaque location, since calcification in different segments carries distinct prognostic and procedural risks. However, a finer-grained, segment-specific quantification has remained technically infeasible. Conventional 3D models are forced to process downsampled volumes or isolated patches, sacrificing the global context required to resolve anatomical ambiguity and render reliable landmark localization. To overcome this, we reformulate the 3D challenge as a \textbf{Parallel Probabilistic Landmark Localization} task along the 1D axial dimension. We propose the \textbf{Depth-Sequence Transformer (DST)}, a framework that processes full-resolution CT volumes as sequences of 2D slices, learning to predict $N=6$ independent probability distributions that pinpoint key anatomical landmarks. Our DST framework demonstrates exceptional accuracy and robustness. Evaluated on a 100-patient clinical cohort with rigorous 5-fold cross-validation, it achieves a Mean Absolute Error (MAE) of \textbf{0.1 slices}, with \textbf{96\%} of predictions falling within a $\pm1$ slice tolerance. Furthermore, to validate its architectural power, the DST backbone establishes the best result on the public Clean-CC-CCII classification benchmark under an end-to-end evaluation protocol. Our work delivers the first practical tool for automated segment-specific ICAC analysis. The proposed framework provides a foundation for further studies on the role of location-specific biomarkers in diagnosis, prognosis, and procedural planning. Our code will be made publicly available.

Paper number 7:
Title: RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing
Authors: Yang Xiao, Ting Dang, Rohan Kumar Das
Abstract: Automatic speaker verification (ASV) systems are often affected by spoofing attacks. Recent transformer-based models have improved anti-spoofing performance by learning strong feature representations. However, these models usually need high computing power. To address this, we introduce RawTFNet, a lightweight CNN model designed for audio signals. The RawTFNet separates feature processing along time and frequency dimensions, which helps to capture the fine-grained details of synthetic speech. We tested RawTFNet on the ASVspoof 2021 LA and DF evaluation datasets. The results show that RawTFNet reaches comparable performance to that of the state-of-the-art models, while also using fewer computing resources. The code and models will be made publicly available.

Paper number 8:
Title: Maneuver Detection via a Confidence Dominance Maneuver Indicator
Authors: Xingyu Zhou, Roberto Armellin, Laura Pirovano, Dong Qiao, Xiangyu Li
Abstract: Accurate and efficient maneuver detection is critical for ensuring the safety and predictability of spacecraft trajectories. This paper presents a novel maneuver detection approach based on comparing the confidence levels associated with the orbital state estimation and the observation likelihood. First, a confidence-dominance maneuver indicator (CDMI) is proposed by setting a confidence level for the state estimation and computing the maximum likelihood of the observation and its confidence level. The CDMI then flag a maneuver when the observation's confidence level exceeds that of the state estimation, indicating that the observation is unlikely under the no-maneuver hypothesis while maintaining consistency with the prior state estimation confidence. To efficiently compute the maximum likelihood of the observation and obtain the CDMI, a recursive polynomial optimization method is developed, taking advantage of convex optimization and polynomial approximation. In addition, an integrated CDMI approach is developed to eliminate the need to manually select the state confidence level. The integrated CDMI approach maintains high detection accuracy while simultaneously providing an indication of maneuver likelihood, thereby enhancing robustness and practical applicability. The performance of the proposed CDMI-based maneuver detection approaches is evaluated against an optimal control distance metric and two mixture-based approaches. The simulation results demonstrate that the proposed integrated CDMI approach can achieve up to 99.33\% detection accuracy, at least 10% higher than the competing methods, while substantially reducing computational costs.

Paper number 9:
Title: Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models
Authors: Ulzee An, Moonseong Jeong, Simon A. Lee, Aditya Gorla, Yuzhe Yang, Sriram Sankararaman
Abstract: Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of training state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes. To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining semantic information. Extensive experiments on ten diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14% SLIViT), while entirely bypassing the need for costly training. Our results highlight the effectiveness and versatility of Raptor as a foundation for advancing deep learning-based methods for medical volumes.

Paper number 10:
Title: Neural Parameter-varying Data-enabled Predictive Control of Cold Atmospheric Pressure Plasma Jets
Authors: Pegah GhafGhanbari, Mircea Lazar, Javad Mohammadpour Velni
Abstract: Cold Atmospheric Pressure Plasma Jets (APPJs) show significant potential for biomedical applications, but their inherent complexity, characterized by nonlinear dynamics and strong sensitivity to operating conditions like tip-to-surface distance, presents considerable challenges for achieving robust and reliable real-time control. To address these issues, this paper presents the Neural Parameter-Varying Data-enabled Predictive Control (NPV-DeePC) framework. By integrating hyper neural networks (hypernets) into the neural Data-enabled Predictive Control (DeePC) paradigm, the proposed method adaptively captures system nonlinearities and parameter variations, updates the neural feature space accordingly, and enables efficient and accurate trajectory prediction and control. The NPV-DeePC framework is validated through extensive simulations involving surface temperature tracking and thermal dose delivery. The results highlight its ability to outperform existing controllers in terms of accuracy and adaptability. The computational efficiency of the NPV-DeePC approach makes it a viable candidate for real-time applications. These findings underscore its potential to advance the safe and precise control of APPJs and provide a scalable solution for other parameter-varying nonlinear systems.

Paper number 11:
Title: MetaH2: A Snapshot Metasurface HDR Hyperspectral Camera
Authors: Yuxuan Liu, Qi Guo
Abstract: We present a metasurface camera that jointly performs high-dynamic range (HDR) and hyperspectral imaging in a snapshot. The system integrates exposure bracketing and computed tomography imaging spectrometry (CTIS) by simultaneously forming multiple spatially multiplexed projections with unique power ratios and chromatic aberrations on a photosensor. The measurements are subsequently processed through a deep reconstruction model to generate an HDR image and a hyperspectral datacube. Our simulation studies show that the proposed system achieves higher reconstruction accuracy than previous snapshot hyperspectral imaging methods on benchmark datasets. We assemble a working prototype and demonstrate snapshot reconstruction of 60 dB dynamic range and 10 nm spectral resolution from 600 nm to 700 nm on real-world scenes from a monochrome photosensor.

Paper number 12:
Title: Ambiguity Function Analysis of AFDM Signals for Integrated Sensing and Communications
Authors: Haoran Yin, Yanqun Tang, Yuanhan Ni, Zulin Wang, Gaojie Chen, Jun Xiong, Kai Yang, Marios Kountouris, Yong Liang Guan, Yong Zeng
Abstract: Affine frequency division multiplexing (AFDM) is a promising chirp-based waveform with high flexibility and resilience, making it well-suited for next-generation wireless networks, particularly in high-mobility scenarios. In this paper, we investigate the ambiguity functions (AFs) of AFDM signals, which fundamentally characterize their range and velocity estimation capabilities in both monostatic and bistatic settings. Specifically, we first derive the auto-ambiguity function (AAF) of an AFDM chirp subcarrier, revealing its "spike-like" local property and "periodic-like" global property along the rotated delay and Doppler dimensions. This structure naturally forms a parallelogram for each localized pulse of the AAF of the AFDM chirp subcarrier, enabling unambiguous target sensing. Then, we study the cross-ambiguity function (CAF) between two different AFDM chirp subcarriers, which exhibits the same local and global properties as the AAF but with an additional shift along the Doppler dimension. We then extend our analysis to the AF of various typical AFDM frames, considering both deterministic pilot and random data symbols. In particular, we demonstrate that inserting guard symbols in AFDM facilitates interference-free sensing. Simulation results validate our theoretical findings, highlighting AFDM's strong potential for ISAC applications.

Paper number 13:
Title: Two-Level Distributed Interference Management for Large-Scale HAPS-Empowered vHetNets
Authors: Afsoon Alidadi Shamsabadi, Animesh Yadav, Halim Yanikomeroglu
Abstract: Next-generation wireless networks (xG) must provide ubiquitous connectivity while enhancing user experience in both densely populated urban areas and rural regions. To achieve this, a disruptive network architecture is essential, and high altitude platform stations (HAPS) offer a promising solution. By integrating HAPS with terrestrial networks, we can create HAPS-empowered vertical heterogeneous networks (vHetNets), which significantly improve coverage and capacity, as well as support emerging use cases. In HAPS-empowered vHetNets, different tiers can share the same spectrum, forming harmonized spectrum vHetNets that enhance spectral efficiency (SE). However, we face two major challenges: i) co-channel interference in harmonized spectrum vHetNets, and ii) the large-scale nature of the network. To address the first challenge, we adopt a cell-free approach as the underlying network architecture for the HAPS-empowered vHetNet. In this approach, base stations use beamforming to direct high-gain, narrow beams toward users, which helps mitigate interference. However, this creates a nonconvex and high-dimensional optimization problem, which highlights the second challenge of dealing with a large-scale network. Consequently, centralized solutions become impractical due to the computational and communication overhead involved. The standard two-block alternating direction method of multipliers (ADMM) is one option, but nonconvex constraints can hinder its convergence. As an alternative, we have developed a two-level distributed proportional fairness beamforming weight design (PFBWD) algorithm. This algorithm uses a combination of the augmented Lagrangian method (ALM) and a three-block ADMM framework. The proposed method effectively tackles nonconvexity, reduces complexity, and enables scalable, distributed optimization with guaranteed convergence.

Paper number 14:
Title: A Generalized Stability Analysis Method with Dynamic Phasors for LV AC Microgrids
Authors: B√ºlent Daƒü
Abstract: Representation of inductive coupling lines with conventional static phasors is the main reason of inadequacy of the existing phasors based simplified stability analysis methods for microgrids with inductive coupling lines. In the literature, dynamic phasors have been proposed for the dynamic modelling of inductive lines to conserve the simplified structure of the analysis method. In this study a generalized stability analysis method for LV AC microgrids, composed of droop controlled inverters, is presented. The proposed analysis method is based on the inclusion of dynamic phasors for inductive coupling lines into the existing phasors based stability analysis method. The results show that the stability analysis method with dynamic phasors successfully predicts the instability boundaries of LV AC microgrids.

Paper number 15:
Title: PGD-based optimization of 3D bobsleigh track centerlines from 2D centerlines for simulation applications
Authors: Zhe Chen, Huichao Zhao, Yongfeng Jiang, Minghui Bai, Lun Li, Jicheng Chen
Abstract: The centerline of a bobsleigh track defines its geometry and is essential for simulation modeling. To reduce bBobsleigh training costs, leveraging the centerline of the bobsleigh track to construct a virtual environment that closely replicates real competitive settings presents a promising solution. However, publicly available centerline data are typically limited and it is imprecise to construct a training system solely based on 2-dimensional (2D) centerline. To address this practical issue, this paper proposes a method for generating a 3-dimensional (3D) track centerline based on 2D centerline data. Incorporating international track design regulations, the method formulates an optimization problem that considers total track length, height difference, slope constraints, and geometric continuity. A Projected Gradient Descent (PGD) algorithm is used to solve the optimization problem. The generated 3D centerlines are compared with real track data, and the results show that the method can reproduce realistic centerline trends from original or scaled 2D data. For the selected track segment, the relative errors in total length, height difference, and average slope are within 1.7%, 3.2% and 4.1%, respectively, for real 2D data and within 1.1%, 3.5% and 4.3% respectively for scaled data. All slope values remain within the allowable limits. Moreover, by adjusting the segmentation or modifying the weight of height difference in the cost function, various centerline styles applicable to different competitions can be generated. Under different segmentation and weight factors, the maximum errors reach up to 4.4%, 4.8%, and 9.8%, and 4.4%, 4.8%, and 10.0%, respectively. The proposed method provides a flexible and efficient tool for supporting bobsleigh track centerline design.

Paper number 16:
Title: Unobtrusive Reflectance Photoplethysmography for Detecting and Severity Grading of Sleep Apnea via Oxygen Desaturation Index
Authors: Karen Adam, Cl√©mentine Aguet, Patrick Theurillat, Florent Baty, Maximilian Boesch, Damien Ferrario, Mathieu Lemay, Martin Brutsche, Fabian Braun
Abstract: Sleep apnea is a common chronic sleep-related disorder which is known to be a comorbidity for cerebro- and cardio-vascular disease. Diagnosis of sleep apnea usually requires an overnight polysomnography at the sleep laboratory. In this paper, we used a wearable device which measures reflectance photoplethysmography (PPG) at the wrist and upper arm to estimate continuous SpO2 levels during sleep and subsequently derive an oxygen desaturation index (ODI) for each patient. On a cohort of 170 patients undergoing sleep apnea screening, we evaluated whether this ODI value could represent a surrogate marker for the apnea-hypopnea index (AHI) for the diagnosis and severity assessment of sleep apnea. As the ODI was simultaneously obtained at the fingertip, upper arm and wrist, we compared ODI diagnostic performance depending on the measurement location. We then further evaluated the accuracy of ODI as a direct predictor for moderate and severe sleep apnea as defined by established AHI thresholds. We found that ODI values obtained at the upper arm were good predictors for moderate or severe sleep apnea, with 86% accuracy, 96% sensitivity and 70% specificity, whereas ODI values obtained at the wrist were less reliable as a diagnostic tool.

Paper number 17:
Title: Exploiting Cognition in ISAR Processing for Spectral Compatibility Applications
Authors: Massimo Rosamilia, Augusto Aubry, Alessio Balleri, Antonio De Maio, Marco Martorella
Abstract: This paper introduces and analyzes the concept of a cognitive inverse synthetic aperture radar (ISAR) ensuring spectral compatibility in crowded electromagnetic environments. In such a context, the proposed approach alternates between environmental perception, recognizing possible emitters in its frequency range, and an action stage, synthesizing and transmitting a tailored radar waveform to achieve the desired imaging task while guaranteeing spectral coexistence with overlaid emitters. The perception is carried out by a spectrum sensing module providing the true relevant spectral parameters of the sources in the environment. The action stage employs a tailored signal design process, synthesizing a radar waveform with bespoke spectral notches, enabling ISAR imaging over a wide spectral bandwidth without interfering with the other radio frequency (RF) sources. A key enabling requirement for the proposed application is the capability to successfully recover possible missing data in the frequency domain (induced by spectral notches) and in the slow-time dimension (enabling concurrent RF activities still in a cognitive fashion). This process is carried out by resorting to advanced methods based on either the compressed-sensing framework or a rank-minimization recovery strategy. The capabilities of the proposed system are assessed exploiting a dataset of drone measurements in the frequency band between 13 GHz and 15 GHz. Results highlight the effectiveness of the devised architecture to enable spectral compatibility while delivering high-quality ISAR images as well as additional RF activities.

Paper number 18:
Title: A Temporal Gaussian Noise Model for Equalization-enhanced Phase Noise
Authors: Benedikt Geiger, Fred Buchali, Vahid Aref, Laurent Schmalen
Abstract: Equalization-enhanced Phase Noise causes burst-like distortions in high symbol-rate transmission systems. We propose a temporal Gaussian noise model that captures these distortions by introducing a time-varying distortion power. Validated through simulations and experiments, it enables accurate and simple performance prediction for high symbol-rate transmission systems.

Paper number 19:
Title: Onboard Neuromorphic Split Computing via Optical Links for LEO Remote Sensing
Authors: Zihang Song, Petar Popovski
Abstract: Low Earth orbit (LEO) satellite constellations increasingly require onboard intelligence under strict power and communication constraints. This paper proposes a neuromorphic split computing framework tailored for hierarchical LEO systems, where edge satellites perform event-driven sensing using dynamic vision sensors (DVS) and lightweight spiking neural network (SNN) encoders, while core satellites conduct inference using a powerful SNN decoder. A learned spike mapping scheme enables direct transmission over optical inter-satellite links (OISLs) without conventional modulation overhead. Experimental results on synthetic aerial scene classification demonstrate that the proposed architecture achieves accuracy on par with modern large vision-based pipelines, while offering energy efficiency comparable to that of existing lightweight implementations. These findings highlight the potential of neuromorphic computing for energy-efficient inter-satellite split computing in LEO remote sensing missions.

Paper number 20:
Title: Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees
Authors: Berire Gunes Reyhan, Sinem Coleri
Abstract: In Wireless Networked Control Systems (WNCSs), control and communication systems must be co-designed due to their strong interdependence. This paper presents a novel optimization theory-based safe deep reinforcement learning (DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction while optimizing performance, for the first time in the literature. The approach minimizes power consumption under key constraints, including Peak Age of Information (PAoI) violation probability, transmit power, and schedulability in the finite blocklength regime. PAoI violation probability is uniquely derived by combining stochastic maximum allowable transfer interval (MATI) and maximum allowable packet delay (MAD) constraints in a multi-sensor network. The framework consists of two stages: optimization theory and safe DRL. The first stage derives optimality conditions to establish mathematical relationships among variables, simplifying and decomposing the problem. The second stage employs a safe DRL model where a teacher-student framework guides the DRL agent (student). The control mechanism (teacher) evaluates compliance with system constraints and suggests the nearest feasible action when needed. Extensive simulations show that the proposed framework outperforms rule-based and other optimization theory based DRL benchmarks, achieving faster convergence, higher rewards, and greater stability.

Paper number 21:
Title: Multi-Symbol Digital AirComp via Modulation Design and Power Adaptation
Authors: Xiaojing Yan, Saeed Razavikia, Carlo Fischione
Abstract: In this paper, we consider digital over-the-air computation (AirComp) and introduce a new multi-symbol modulation framework called sequential modulation for AirComp (SeMAC). Building upon ChannelComp, a general framework for designing modulation schemes to support arbitrary function computation over a multiple access channel (MAC), SeMAC maps each input value to a sequence of modulated symbols using distinct constellation diagrams across multiple time slots. This extension generalizes ChannelComp by enabling flexible modulation design across multiple transmissions, thereby enhancing reliability against channel noise. We formulate the modulation design as a non-convex optimization problem, apply matrix lifting to relax it into a semidefinite programming (SDP), and recover a feasible modulation solution by solving a low rank approximation. For scenarios where the modulation formats cannot be changed, we further develop a power adaptation scheme that adjusts amplitude and phase of the modulated symbols while preserving the modulation structure. Numerical results show that SeMAC can achieve a reliable computation by reducing the computation error up to 18 dB compared to other existing methods, particularly for the product function.

Paper number 22:
Title: Large-Scale Processing and Validation of Grid Data for Assessing the Fair Spatial Distribution of PV Hosting Capacity
Authors: Ali Mohamed Ali, Yaser Raeisi, Plouton Grammatikos, Davide Pavanello, Pierre Roduit, Fabrizio Sossan
Abstract: The integration of PV systems and increased electrification levels present significant challenges to the traditional design and operation of distribution grids. This paper presents a methodology for extracting, validating, and adapting grid data from a distribution system operator's (DSO) database to facilitate large-scale grid studies, including load flow and optimal power flow analyses. The validation process combines rule-based sanity checks and offline automated power flow analyses to ensure data consistency and detect potential errors in the grid database, allowing for their correction. As a practical application, the paper proposes a method to assess the PV hosting capacity of distribution grids, with a focus on ensuring fairness in their spatial distribution. By incorporating fairness criteria into the analyses, we quantify the costs (in terms of missed revenues from selling PV generation) associated with spatial fairness.

Paper number 23:
Title: Energy Management for Renewable-Colocated Artificial Intelligence Data Centers
Authors: Siying Li, Lang Tong, Timothy D. Mount
Abstract: We develop an energy management system (EMS) for artificial intelligence (AI) data centers with colocated renewable generation. Under a profit-maximizing framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI workload scheduling, on-site renewable utilization, and electricity market participation. Within both wholesale and retail market participation models, the economic benefit of the RCDC operation is maximized. Empirical evaluations using real-world traces of electricity prices, data center power consumption, and renewable generation demonstrate significant profit gains from renewable and AI data center colocations.

Paper number 24:
Title: RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning
Authors: Atli Sigurgeirsson, Simon King
Abstract: A Prompt-based Text-To-Speech model allows a user to control different aspects of speech, such as speaking rate and perceived gender, through natural language instruction. Although user-friendly, such approaches are on one hand constrained: control is limited to acoustic features exposed to the model during training, and too flexible on the other: the same inputs yields uncontrollable variation that are reflected in the corpus statistics. We investigate a novel fine-tuning regime to address both of these issues at the same time by exploiting the uncontrollable variance of the model. Through principal component analysis of thousands of synthesised samples, we determine latent features that account for the highest proportion of the output variance and incorporate them as new labels for secondary fine-tuning. We evaluate the proposed methods on two models trained on an expressive Icelandic speech corpus, one with emotional disclosure and one without. In the case of the model without emotional disclosure, the method yields both continuous and discrete features that improve overall controllability of the model.

Paper number 25:
Title: Mod√®le physique variationnel pour l'estimation de r√©ponses impulsionnelles de salles
Authors: Louis Lalay (LTCI, IP Paris, S2A), Mathieu Fontaine (LTCI, IP Paris, S2A), Roland Badeau (S2A, LTCI, IP Paris)
Abstract: Room impulse response estimation is essential for tasks like speech dereverberation, which improves automatic speech recognition. Most existing methods rely on either statistical signal processing or deep neural networks designed to replicate signal processing principles. However, combining statistical and physical modeling for RIR estimation remains largely unexplored. This paper proposes a novel approach integrating both aspects through a theoretically grounded model. The RIR is decomposed into interpretable parameters: white Gaussian noise filtered by a frequency-dependent exponential decay (e.g. modeling wall absorption) and an autoregressive filter (e.g. modeling microphone response). A variational free-energy cost function enables practical parameter estimation. As a proof of concept, we show that given dry and reverberant speech signals, the proposed method outperforms classical deconvolution in noisy environments, as validated by objective metrics.

Paper number 26:
Title: Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models
Authors: Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang-gil Lee, Chao-Han Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, Bryan Catanzaro
Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.

Paper number 27:
Title: Distilling Spectrograms into Tokens: Fast and Lightweight Bioacoustic Classification for BirdCLEF+ 2025
Authors: Anthony Miyaguchi, Murilo Gustineli, Adrian Cheung
Abstract: The BirdCLEF+ 2025 challenge requires classifying 206 species, including birds, mammals, insects, and amphibians, from soundscape recordings under a strict 90-minute CPU-only inference deadline, making many state-of-the-art deep learning approaches impractical. To address this constraint, the DS@GT BirdCLEF team explored two strategies. First, we establish competitive baselines by optimizing pre-trained models from the Bioacoustics Model Zoo for CPU inference. Using TFLite, we achieved a nearly 10x inference speedup for the Perch model, enabling it to run in approximately 16 minutes and achieve a final ROC-AUC score of 0.729 on the public leaderboard post-competition and 0.711 on the private leaderboard. The best model from the zoo was BirdSetEfficientNetB1, with a public score of 0.810 and a private score of 0.778. Second, we introduce a novel, lightweight pipeline named Spectrogram Token Skip-Gram (STSG) that treats bioacoustics as a sequence modeling task. This method converts audio into discrete "spectrogram tokens" by clustering Mel-spectrograms using Faiss K-means and then learns high-quality contextual embeddings for these tokens in an unsupervised manner with a Word2Vec skip-gram model. For classification, embeddings within a 5-second window are averaged and passed to a linear model. With a projected inference time of 6 minutes for a 700-minute test set, the STSG approach achieved a final ROC-AUC public score of 0.559 and a private score of 0.520, demonstrating the viability of fast tokenization approaches with static embeddings for bioacoustic classification. Supporting code for this paper can be found at this https URL.

Paper number 28:
Title: Active Learning for Text-to-Speech Synthesis with Informative Sample Collection
Authors: Kentaro Seki, Shinnosuke Takamichi, Takaaki Saeki, Hiroshi Saruwatari
Abstract: The construction of high-quality datasets is a cornerstone of modern text-to-speech (TTS) systems. However, the increasing scale of available data poses significant challenges, including storage constraints. To address these issues, we propose a TTS corpus construction method based on active learning. Unlike traditional feed-forward and model-agnostic corpus construction approaches, our method iteratively alternates between data collection and model training, thereby focusing on acquiring data that is more informative for model improvement. This approach enables the construction of a data-efficient corpus. Experimental results demonstrate that the corpus constructed using our method enables higher-quality speech synthesis than corpora of the same size.

Paper number 29:
Title: Audio Inpanting using Discrete Diffusion Model
Authors: Tali Dror, Iftach Shoham, Moshe Buchris, Oren Gal, Haim Permuter, Gilad Katz, Eliya Nachmani
Abstract: Audio inpainting refers to the task of reconstructing missing segments in corrupted audio recordings. While prior approaches-including waveform and spectrogram-based diffusion models-have shown promising results for short gaps, they often degrade in quality when gaps exceed 100 milliseconds (ms). In this work, we introduce a novel inpainting method based on discrete diffusion modeling, which operates over tokenized audio representations produced by a pre-trained audio tokenizer. Our approach models the generative process directly in the discrete latent space, enabling stable and semantically coherent reconstruction of missing audio. We evaluate the method on the MusicNet dataset using both objective and perceptual metrics across gap durations up to 300 ms. We further evaluated our approach on the MTG dataset, extending the gap duration to 500 ms. Experimental results demonstrate that our method achieves competitive or superior performance compared to existing baselines, particularly for longer gaps, offering a robust solution for restoring degraded musical recordings. Audio examples of our proposed method can be found at this https URL

Paper number 30:
Title: Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment
Authors: Yuki Yoshihara, Linjing Jiang, Nihan Karatas, Hitoshi Kanamori, Asuka Harada, Takahiro Tanaka
Abstract: This study investigates the potential of a multimodal large language model (LLM), specifically ChatGPT-4o, to perform human-like interpretations of traffic scenes using static dashcam images. Herein, we focus on three judgment tasks relevant to elderly driver assessments: evaluating traffic density, assessing intersection visibility, and recognizing stop signs recognition. These tasks require contextual reasoning rather than simple object detection. Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated the performance of the model with human annotations serving as the reference standard. Evaluation metrics included precision, recall, and F1-score. Results indicate that prompt design considerably affects performance, with recall for intersection visibility increasing from 21.7% (zero-shot) to 57.0% (multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In stop-sign detection, the model demonstrated high precision (up to 86.3%) but a lower recall (approximately 76.7%), indicating a conservative response tendency. Output stability analysis revealed that humans and the model faced difficulties interpreting structurally ambiguous scenes. However, the model's explanatory texts corresponded with its predictions, enhancing interpretability. These findings suggest that, with well-designed prompts, LLMs hold promise as supportive tools for scene-level driving risk assessments. Future studies should explore scalability using larger datasets, diverse annotators, and next-generation model architectures for elderly driver assessments.

Paper number 31:
Title: Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization
Authors: Nan Li, Qi Sun, Lehan Wang, Xiaofei Xu, Jinri Huang, Chunhui Liu, Jing Gao, Yuhong Huang, Chih-Lin I
Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not natively integrated but rather an add-on feature over existing architecture, 6G shall incorporate AI from the onset to address its complexity and support ubiquitous AI applications. Based on our extensive mobile network operation and standardization experience from 2G to 5G, this paper explores the design and standardization principles of AI-Native radio access networks (RAN) for 6G, with a particular focus on its critical Day 1 architecture, functionalities and capabilities. We investigate the framework of AI-Native RAN and present its three essential capabilities to shed some light on the standardization direction; namely, AI-driven RAN processing/optimization/automation, reliable AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The standardization of AI-Native RAN, in particular the Day 1 features, including an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale field trial with over 5000 5G-A base stations have been built and delivered significant improvements in average air interface latency, root cause identification, and network energy consumption with the proposed architecture and the supporting AI functions. This paper aims to provide a Day 1 framework for 6G AI-Native RAN standardization design, balancing technical innovation with practical deployment.

Paper number 32:
Title: A Dissipativity Framework for Constructing Scaled Graphs
Authors: Timo de Groot, Maurice heemels, Sebastiaan van den Eijnden
Abstract: Scaled relative graphs have been originally introduced in the context of convex optimization and have recently gained attention in the control systems community for the graphical analysis of nonlinear systems. Of particular interest in stability analysis of feedback systems is the scaled graph, a special case of the scaled relative graph. In many ways, scaled graphs can be seen as a generalization of the classical Nyquist plot for linear time-invariant systems, and facilitate a powerful graphical tool for analyzing nonlinear feedback systems. In their current formulation, however, scaled graphs require characterizing the input-output behaviour of a system for an uncountable number of inputs. This poses a practical bottleneck in obtaining the scaled graph of a nonlinear system, and currently limits its use. This paper presents a framework grounded in dissipativity for efficiently computing the scaled graph of several important classes of systems, including multivariable linear time-invariant systems, impulsive systems, and piecewise linear systems. The proposed approach leverages novel connections between linear matrix inequalities, integral quadratic constraints, and scaled graphs, and is shown to be exact for specific linear time-invariant systems. The results are accompanied by several examples illustrating the potential and effectiveness of the presented framework.

Paper number 33:
Title: Enforcing Speech Content Privacy in Environmental Sound Recordings using Segment-wise Waveform Reversal
Authors: Modan Tailleur, Mathieu Lagrange, Pierre Aumond, Vincent Tourre
Abstract: Environmental sound recordings often contain intelligible speech, raising privacy concerns that limit analysis, sharing and reuse of data. In this paper, we introduce a method that renders speech unintelligible while preserving both the integrity of the acoustic scene, and the overall audio quality. Our approach involves reversing waveform segments to distort speech content. This process is enhanced through a voice activity detection and speech separation pipeline, which allows for more precise targeting of speech. In order to demonstrate the effectivness of the proposed approach, we consider a three-part evaluation protocol that assesses: 1) speech intelligibility using Word Error Rate (WER), 2) sound sources detectability using Sound source Classification Accuracy-Drop (SCAD) from a widely used pre-trained model, and 3) audio quality using the Fr√©chet Audio Distance (FAD), computed with our reference dataset that contains unaltered speech. Experiments on this simulated evaluation dataset, which consists of linear mixtures of speech and environmental sound scenes, show that our method achieves satisfactory speech intelligibility reduction (97.9% WER), minimal degradation of the sound sources detectability (2.7% SCAD), and high perceptual quality (FAD of 1.40). An ablation study further highlights the contribution of each component of the pipeline. We also show that incorporating random splicing to our speech content privacy enforcement method can enhance the algorithm's robustness to attempt to recover the clean speech, at a slight cost of audio quality.

Paper number 34:
Title: Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers
Authors: Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun
Abstract: Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.

Paper number 35:
Title: MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling
Authors: Jingjing Tang, Xin Wang, Zhe Zhang, Junichi Yamagishi, Geraint Wiggins, George Fazekas
Abstract: Generating expressive audio performances from music scores requires models to capture both instrument acoustics and human interpretation. Traditional music performance synthesis pipelines follow a two-stage approach, first generating expressive performance MIDI from a score, then synthesising the MIDI into audio. However, the synthesis models often struggle to generalise across diverse MIDI sources, musical styles, and recording environments. To address these challenges, we propose MIDI-VALLE, a neural codec language model adapted from the VALLE framework, which was originally designed for zero-shot personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio synthesis, we improve the architecture to condition on a reference audio performance and its corresponding MIDI. Unlike previous TTS-based systems that rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens, facilitating a more consistent and robust modelling of piano performances. Furthermore, the model's generalisation ability is enhanced by training on an extensive and diverse piano performance dataset. Evaluation results show that MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline, demonstrating improved synthesis quality and generalisation across diverse performance MIDI inputs.

Paper number 36:
Title: FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation
Authors: Yuxuan Jiang, Zehua Chen, Zeqian Ju, Chang Li, Weibei Dou, Jun Zhu
Abstract: Text-to-audio (T2A) generation has achieved promising results with the recent advances in generative models. However, because of the limited quality and quantity of temporally-aligned audio-text pairs, existing T2A methods struggle to handle the complex text prompts that contain precise timing control, e.g., "owl hooted at 2.4s-5.2s". Recent works have explored data augmentation techniques or introduced timing conditions as model inputs to enable timing-conditioned 10-second T2A generation, while their synthesis quality is still limited. In this work, we propose a novel training-free timing-controlled T2A framework, FreeAudio, making the first attempt to enable timing-controlled long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping at 0s-24s". Specifically, we first employ an LLM to plan non-overlapping time windows and recaption each with a refined natural language description, based on the input text and timing prompts. Then we introduce: 1) Decoupling and Aggregating Attention Control for precise timing control; 2) Contextual Latent Composition for local smoothness and Reference Guidance for global consistency. Extensive experiments show that: 1) FreeAudio achieves state-of-the-art timing-conditioned T2A synthesis quality among training-free methods and is comparable to leading training-based methods; 2) FreeAudio demonstrates comparable long-form generation quality with training-based Stable Audio and paves the way for timing-controlled long-form T2A synthesis. Demo samples are available at: this https URL

Paper number 37:
Title: Unlocking Speech Instruction Data Potential with Query Rewriting
Authors: Yonghua Hei, Yibo Yan, Shuliang Liu, Huiyu Zhou, Linfeng Zhang, Xuming Hu
Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models~(\textbf{LLMs}) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating speech instruction datasets by humans, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models. To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 72\% to 93\%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.

Paper number 38:
Title: Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection
Authors: Davide Salvi, Viola Negroni, Sara Mandelli, Paolo Bestagini, Stefano Tubaro
Abstract: Recent advances in generative AI have made the creation of speech deepfakes widely accessible, posing serious challenges to digital trust. To counter this, various speech deepfake detection strategies have been proposed, including Person-of-Interest (POI) approaches, which focus on identifying impersonations of specific individuals by modeling and analyzing their unique vocal traits. Despite their excellent performance, the existing methods offer limited granularity and lack interpretability. In this work, we propose a POI-based speech deepfake detection method that operates at the phoneme level. Our approach decomposes reference audio into phonemes to construct a detailed speaker profile. In inference, phonemes from a test sample are individually compared against this profile, enabling fine-grained detection of synthetic artifacts. The proposed method achieves comparable accuracy to traditional approaches while offering superior robustness and interpretability, key aspects in multimedia forensics. By focusing on phoneme analysis, this work explores a novel direction for explainable, speaker-centric deepfake detection.

Paper number 39:
Title: Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters
Authors: Robert B. Kent, Marios S. Pattichis
Abstract: A new set of hardware merge sort devices are introduced here, which merge multiple sorted input lists into a single sorted output list in a fast and efficient manner. In each merge sorter, the values from the sorted input lists are arranged in an input 2-D setup array, but with the order of each sorted input list offset from the order of each of the other sorted input lists. In these new devices, called List Offset Merge Sorters (LOMS), a minimal set of column sort stages alternating with row sort stages process the input setup array into a final output array, now in the defined sorted order. LOMS 2-way sorters, which merge 2 sorted input lists, require only 2 merge stages and are significantly faster than Kenneth Batcher's previous state-of-the-art 2-way merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS) in their first stage. Both LOMS and S2MS devices can merge any mixture of input list sizes, while Batcher's merge sorters are difficult to design unless the 2 input lists are equal, and a power-of-2. By themselves, S2MS devices are the fastest 2-way merge sorters when implemented in this study's target FPGA devices, but they tend to use a large number of LUT resources. LOMS 2-way devices use fewer resources than comparable S2MS devices, enabling some large LOMS devices to be implemented in a given FPGA when comparable S2MS devices cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with 32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging 3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

Paper number 40:
Title: System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility
Authors: Paul Saves, Jasper Bussemaker, R√©mi Lafage, Thierry Lefebvre, Nathalie Bartoli, Youssef Diouane, Joseph Morlier
Abstract: For developing innovative systems architectures, modeling and optimization techniques have been central to frame the architecting process and define the optimization and modeling problems. In this context, for system-of-systems the use of efficient dedicated approaches (often physics-based simulations) is highly recommended to reduce the computational complexity of the targeted applications. However, exploring novel architectures using such dedicated approaches might pose challenges for optimization algorithms, including increased evaluation costs and potential failures. To address these challenges, surrogate-based optimization algorithms, such as Bayesian optimization utilizing Gaussian process models have emerged.

Paper number 41:
Title: On Barriers to Archival Audio Processing
Authors: Peter Sullivan, Muhammad Abdul-Mageed
Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century radio recordings to probe the robustness of modern off-the-shelf language identification (LID) and speaker recognition (SR) methods, especially with respect to the impact of multilingual speakers and cross-age recordings. Our findings suggest that LID systems, such as Whisper, are increasingly adept at handling second-language and accented speech. However, speaker embeddings remain a fragile component of speech processing pipelines that is prone to biases related to the channel, age, and language. Issues which will need to be overcome should archives aim to employ SR methods for speaker indexing.

Paper number 42:
Title: Electricity-Aware Bid Format for Coordinated Heat and Electricity Market Clearing
Authors: Lesia Mitridati, Jalal Kazempour, Pascal Van Hentenryck
Abstract: Coordination between heat and electricity markets is essential to achieve a cost-effective and efficient operation of the energy system. In the current sequential market practice, the heat market is cleared before the electricity market and has no insight into the impacts of heat dispatch on the electricity market. While preserving this sequential practice, this paper introduces an electricity-aware bid format for the coordination of heat and electricity systems. This novel market mechanism defines heat bids conditionally on the day-ahead electricity prices. Prior to clearing heat and electricity markets, the proposed bid selection mechanism selects the valid bids which minimize the heat system operating cost while anticipating heat and electricity market clearing. This mechanism is modeled as a trilevel optimization problem, which we recast as a mixed-integer linear program using a lexicographic function. We use a realistic case study based on the Danish electricity and heat system and show that the proposed bid selection mechanism yields a 4.5% reduction in the total operating cost of heat and electricity systems compared to the existing market-clearing procedure while reducing the financial losses of combined heat and power plants and heat pumps due to invalid bids by up to 20.3 million euros.

Paper number 43:
Title: Wholesale Market Participation of DERA: Competitive DER Aggregation
Authors: Cong Chen, Ahmed S. Alahmed, Timothy D. Mount, Lang Tong
Abstract: We consider the aggregation of distributed energy resources (DERs) by a profit-seeking aggregator participating directly in wholesale market under distribution network access constraints. We propose a competitive DER aggregator (DERA) model that maximizes the DERA's profit while ensuring each aggregated customer gains no less surplus and pays no higher energy cost than under the regulated retail tariff. The DERA participates in wholesale electricity market as virtual storage with optimized generation offers and consumption bids derived from our competitive aggregation model. Also derived are DERA's bid curves for the distribution network access and DERA's profitability when competing with the regulated retail tariff. We show that, with the same distribution network access, the proposed DERA's wholesale market participation achieves the same welfare-maximizing outcome as when its customers participate directly in the wholesale market. Numerical studies compare the proposed DERA with existing methods in terms of customer surplus and DERA profit. We empirically evaluate how many DERAs can survive in the competition at long-run equilibrium, and assess the impacts of DER adoption levels and distribution network access on short-run market outcomes.

Paper number 44:
Title: A Preventive-Corrective Scheme for Ensuring Power System Security During Active Wildfire Risks
Authors: Satyaprajna Sahoo, Anamitra Pal
Abstract: The focus of this paper is on operating the electric power grid in a secure manner when wildfire risks are high. This is a challenging problem because of the uncertain ways in which the fires can impact the operation of the power system. To address this challenge, we propose a novel preventive-corrective coordinated decision-making scheme that quickly mitigates both static and dynamic insecurities given the risk of active wildfires in a region. The scheme utilizes a comprehensive contingency analysis tool for multi-asset outages that leverages: (i) a Feasibility Test algorithm which exhaustively desaturates overloaded cut-sets to prevent cascading line outages, and (ii) a data-driven transient stability analyzer which alleviates dynamic instabilities. This tool is then used to operate a coordinated unit commitment/optimal power flow model that is designed to adapt to varying risk levels associated with wildfires. Depending on the allowed risk, the model balances economical operation and grid robustness. The results obtained using the IEEE 118-bus system indicate that the proposed approach alleviates system vulnerabilities to wildfires while also minimizing operational cost.

Paper number 45:
Title: Online convex optimization for constrained control of nonlinear systems
Authors: Marko Nonhoff, Johannes K√∂hler, Matthias A. M√ºller
Abstract: This paper proposes a modular approach that combines the online convex optimization framework and reference governors to solve a constrained control problem featuring time-varying and a priori unknown cost functions. Compared to existing results, the proposed framework is uniquely applicable to nonlinear dynamical systems subject to state and input constraints. Furthermore, our method is general in the sense that we do not limit our analysis to a specific choice of online convex optimization algorithm or reference governor. We show that the dynamic regret of the proposed framework is bounded linearly in both the dynamic regret and the path length of the chosen online convex optimization algorithm, even though the online convex optimization algorithm does not account for the underlying dynamics. We prove that a linear bound with respect to the online convex optimization algorithm's dynamic regret is optimal, i.e., cannot be improved upon. Furthermore, for a standard class of online convex optimization algorithms, our proposed framework attains a bound on its dynamic regret that is linear only in the variation of the cost functions, which is known to be an optimal bound. Finally, we demonstrate implementation and flexibility of the proposed framework by comparing different combinations of online convex optimization algorithms and reference governors to control a nonlinear chemical reactor in a numerical experiment.

Paper number 46:
Title: A Review on Deep Learning Autoencoder in the Design of Next-Generation Communication Systems
Authors: Omar Alnaseri, Laith Alzubaidi, Yassine Himeur, Mohammed Alaa Ala'anzy, Jens Timmermann, Mohammed S. M. Gismalla
Abstract: Traditional mathematical models used in designing next-generation communication systems often fall short due to inherent simplifications, narrow scope, and computational limitations. In recent years, the incorporation of deep learning (DL) methodologies into communication systems has made significant progress in system design and performance optimisation. Autoencoders (AEs) have become essential, enabling end-to-end learning that allows for the combined optimisation of transmitters and receivers. Consequently, AEs offer a data-driven methodology capable of bridging the gap between theoretical models and real-world complexities. The paper presents a comprehensive survey of the application of AEs within communication systems, with a particular focus on their architectures, associated challenges, and future directions. We examine 120 recent studies across wireless, optical, semantic, and quantum communication fields, categorising them according to transceiver design, channel modelling, digital signal processing, and computational complexity. This paper further examines the challenges encountered in the implementation of AEs, including the need for extensive training data, the risk of overfitting, and the requirement for differentiable channel models. Through data-driven approaches, AEs provide robust solutions for end-to-end system optimisation, surpassing traditional mathematical models confined by simplifying assumptions. This paper also summarises the computational complexity associated with AE-based systems by conducting an in-depth analysis employing the metric of floating-point operations per second (FLOPS). This analysis encompasses the evaluation of matrix multiplications, bias additions, and activation functions. This survey aims to establish a roadmap for future research, emphasising the transformative potential of AEs in the formulation of next-generation communication systems.

Paper number 47:
Title: Physically Large Apertures for Wireless Power Transfer: Performance and Regulatory Aspects
Authors: Benjamin J. B. Deutschmann, Ulrich Muehlmann, Ahmet Kaplan, Gilles Callebaut, Thomas Wilding, Bert Cox, Liesbet Van der Perre, Fredrik Tufvesson, Erik G. Larsson, Klaus Witrisal
Abstract: Wireless power transfer (WPT) is a promising service for the Internet of Things, providing a cost-effective and sustainable solution to deploy so-called energy-neutral devices on a massive scale. The power received at the device side from a conventional transmit antenna with a physically small aperture decays rapidly with the distance. New opportunities arise from the transition from conventional far-field beamforming to near-field beam focusing. We argue that a physically large aperture, i.e., large with respect to the distance to the receiver, enables a power budget that remains practically independent of distance. Distance-dependent array gain patterns allow focusing the power density maximum precisely at the device location, while reducing the power density near the infrastructure. Physical aperture size is a key resource in enabling efficient yet regulatory-compliant WPT. We use real-world measurements to demonstrate that a regulatory-compliant system operating at sub-10GHz frequencies can increase the power received at the device into the milliwatt range. Our empirical demonstration shows that power-optimal near-field beam focusing inherently exploits multipath propagation, yielding both increased WPT efficiency and improved human exposure safety.

Paper number 48:
Title: UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift
Authors: Antoine Schieb, Bilal Hadjadji, Natalia Fernanda Valderrama, Daniel Tshokola Mweze, Valentin Derang√®re, Laurent Arnould, Sylvain Ladoire, Alain Lalande, Alessio Fiorin, Carlos L√≥pez Pablo, No√®lia Gallardo Borr√†s, Shrief Abdelazeez, Vincenzo Della Mea, Anna Korzynska, Louis-Oscar Morel, Nathan Vin√ßon
Abstract: Histopathology slide digitization introduces scanner-induced domain shift that can significantly impact computational pathology models based on deep learning methods. In the state-of-the-art, this shift is often characterized at a broad scale (slide-level or dataset-level) but not patch-level, which limits our comprehension of the impact of localized tissue characteristics on the accuracy of the deep learning models. To address this challenge, we present a domain shift analysis framework based on UWarp, a novel registration tool designed to accurately align histological slides scanned under varying conditions. UWarp employs a hierarchical registration approach, combining global affine transformations with fine-grained local corrections to achieve robust tissue patch alignment. We evaluate UWarp using two private datasets, CypathLung and BosomShieldBreast, containing whole slide images scanned by multiple devices. Our experiments demonstrate that UWarp outperforms existing open-source registration methods, achieving a median target registration error (TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while significantly reducing computational time. Additionally, we apply UWarp to characterize scanner-induced local domain shift in the predictions of Breast-NEOprAIdict, a deep learning model for breast cancer pathological response prediction. We find that prediction variability is strongly correlated with tissue density on a given patch. Our findings highlight the importance of localized domain shift analysis and suggest that UWarp can serve as a valuable tool for improving model robustness and domain adaptation strategies in computational pathology.

Paper number 49:
Title: A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration
Authors: Meiyan Kang, Shizuo Kaji, Sang-Yun Lee, Taegon Kim, Hee-Hwan Ryu, Suyoung Choi
Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT) technique for subsurface exploration, particularly in infrastructure inspection and maintenance. However, conventional interpretation methods are often limited by noise sensitivity and a lack of structural awareness. This study presents a novel framework that enhances the detection of underground utilities, especially pipelines, by integrating shape-aware topological features derived from B-scan GPR images using Topological Data Analysis (TDA), with the spatial detection capabilities of the YOLOv5 deep neural network (DNN). We propose a novel shape-aware topological representation that amplifies structural features in the input data, thereby improving the model's responsiveness to the geometrical features of buried objects. To address the scarcity of annotated real-world data, we employ a Sim2Real strategy that generates diverse and realistic synthetic datasets, effectively bridging the gap between simulated and real-world domains. Experimental results demonstrate significant improvements in mean Average Precision (mAP), validating the robustness and efficacy of our approach. This approach underscores the potential of TDA-enhanced learning in achieving reliable, real-time subsurface object detection, with broad applications in urban planning, safety inspection, and infrastructure management.

Paper number 50:
Title: Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases
Authors: Axel TahmasebiMoradi, Vincent Ren, Benjamin Le-Creurer, Chetra Mang, Mouadh Yagoubi
Abstract: Aiming to reduce the computational cost of numerical simulations, a convolutional neural network (CNN) and a multi-layer perceptron (MLP) are introduced to build a surrogate model to approximate radiative heat transfer solutions in a 2-D walled domain with participative gases. The originality of this work lays in the adaptation of the inputs of the problem (gas and wall properties) in order to fit with the CNN architecture, more commonly used for image processing. Two precision datasets have been created with the classical solver, ICARUS2D, that uses the discrete transfer radiation method with the statistical narrow bands model. The performance of the CNN architecture is compared to a more classical MLP architecture in terms of speed and accuracy. Thanks to Optuna, all results are obtained using the optimized hyper parameters networks. The results show a significant speedup with industrially acceptable relative errors compared to the classical solver for both architectures. Additionally, the CNN outperforms the MLP in terms of precision and is more robust and stable to changes in hyper-parameters. A performance analysis on the dataset size of the samples have also been carried out to gain a deeper understanding of the model behavior.

Paper number 51:
Title: Computationally Efficient Information-Driven Optical Design with Interchanging Optimization
Authors: Eric Markley, Henry Pinkard, Leyla Kabuli, Nalini Singh, Laura Waller
Abstract: Recent work has demonstrated that imaging systems can be evaluated through the information content of their measurements alone, enabling application-agnostic optical design that avoids computational decoding challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed to automate this process through gradient-based optimization. In this work, we study IDEAL across diverse imaging systems and find that it suffers from high memory usage, long runtimes, and a potentially mismatched objective function due to end-to-end differentiability requirements. We introduce IDEAL with Interchanging Optimization (IDEAL-IO), a method that decouples density estimation from optical parameter optimization by alternating between fitting models to current measurements and updating optical parameters using fixed models for information estimation. This approach reduces runtime and memory usage by up to 6x while enabling more expressive density models that guide optimization toward superior designs. We validate our method on diffractive optics, lensless imaging, and snapshot 3D microscopy applications, establishing information-theoretic optimization as a practical, scalable strategy for real-world imaging system design.

Paper number 52:
Title: MuCodec: Ultra Low-Bitrate Music Codec
Authors: Yaoxun Xu, Hangting Chen, Jianwei Yu, Wei Tan, Rongzhi Gu, Shun Lei, Zhiwei Lin, Zhiyong Wu
Abstract: Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to extract both acoustic and semantic features, discretizes them with RVQ, and obtains Mel-VAE features via flow-matching. The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. Code and Demo: this https URL.

Paper number 53:
Title: Ultrasound Autofocusing: Common Midpoint Phase Error Optimization via Differentiable Beamforming
Authors: Walter Simson, Louise Zhuang, Benjamin N. Frey, Sergio J. Sanabria, Jeremy J. Dahl, Dongwoon Hyun
Abstract: In ultrasound imaging, propagation of an acoustic wavefront through heterogeneous media causes phase aberrations that degrade the coherence of the reflected wavefront, leading to reduced image resolution and contrast. Adaptive imaging techniques attempt to correct this phase aberration and restore coherence, leading to improved focusing of the image. We propose an autofocusing paradigm for aberration correction in ultrasound imaging by fitting an acoustic velocity field to pressure measurements, via optimization of the common midpoint phase error (CMPE), using a straight-ray wave propagation model for beamforming in diffusely scattering media. We show that CMPE induced by heterogeneous acoustic velocity is a robust measure of phase aberration that can be used for acoustic autofocusing. CMPE is optimized iteratively using a differentiable beamforming approach to simultaneously improve the image focus while estimating the acoustic velocity field of the interrogated medium. The approach relies solely on wavefield measurements using a straight-ray integral solution of the two-way time-of-flight without explicit numerical time-stepping models of wave propagation. We demonstrate method performance through in silico simulations, in vitro phantom measurements, and in vivo mammalian models, showing practical applications in distributed aberration quantification, correction, and velocity estimation for medical ultrasound autofocusing.

Paper number 54:
Title: End-to-end multi-channel speaker extraction and binaural speech synthesis
Authors: Cheng Chi, Xiaoyu Li, Yuxuan Ke, Qunping Ni, Yao Ge, Xiaodong Li, Chengshi Zheng
Abstract: Speech clarity and spatial audio immersion are the two most critical factors in enhancing remote conferencing experiences. Existing methods are often limited: either due to the lack of spatial information when using only one microphone, or because their performance is highly dependent on the accuracy of direction-of-arrival estimation when using microphone array. To overcome this issue, we introduce an end-to-end deep learning framework that has the capacity of mapping multi-channel noisy and reverberant signals to clean and spatialized binaural speech directly. This framework unifies source extraction, noise suppression, and binaural rendering into one network. In this framework, a novel magnitude-weighted interaural level difference loss function is proposed that aims to improve the accuracy of spatial rendering. Extensive evaluations show that our method outperforms established baselines in terms of both speech quality and spatial fidelity.

Paper number 55:
Title: Improvement of Spiking Neural Network with Bit Planes and Color Models
Authors: Nhan T. Luu, Duong T. Luu, Nam N. Pham, Thang C. Truong
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.

Paper number 56:
Title: Biomechanics-Aware Trajectory Optimization for Online Navigation during Robotic Physiotherapy
Authors: Italo Belli, Florian van Melis, J. Micah Prendergast, Ajay Seth, Luka Peternel
Abstract: Robotic devices provide a great opportunity to assist in delivering physical therapy and rehabilitation movements, yet current robot-assisted methods struggle to incorporate biomechanical metrics essential for safe and effective therapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization approach to online robotic Navigation of human musculoskeletal loads for rotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the human shoulder into an optimal control framework, generating strain-minimizing trajectories for real-time control of therapeutic movements. \addedText{Its core strength lies in the ability to adapt biomechanics-informed trajectories online to unpredictable volitional human actions or reflexive reactions during physical human-robot interaction based on robot-sensed motion and forces. BATON's adaptability is enabled by a real-time, model-based estimator that infers changes in muscle activity via a rapid redundancy solver driven by robot pose and force/torque sensor data. We validated BATON through physical human-robot interaction experiments, assessing response speed, motion smoothness, and interaction forces.

Paper number 57:
Title: A Stability Condition for Online Feedback Optimization without Timescale Separation
Authors: Mattia Bianchi, Florian D√∂rfler
Abstract: Online Feedback Optimization (OFO) is a control approach to drive a dynamical plant to an optimal steady state. By interconnecting optimization algorithms with real-time plant measurements, OFO provides all the benefits of feedback control, yet without requiring exact knowledge of plant dynamics for computing a setpoint. On the downside, existing stability guarantees for OFO require the controller to evolve on a sufficiently slower timescale than the plant, possibly affecting transient performance and responsiveness to disturbances. In this paper, we prove that, under suitable conditions, OFO ensures stability without any timescale separation. In particular, the condition we propose is independent of the time constant of the plant, hence it is scaling-invariant. Our analysis leverages a composite Lyapunov function, which is the $\max$ of plant-related and controller-related components. We corroborate our theoretical results with numerical examples.

Paper number 58:
Title: Movable Antenna-Aided Near-Field Integrated Sensing and Communication
Authors: Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang
Abstract: Integrated sensing and communication (ISAC) is emerging as a pivotal technology for next-generation wireless networks. However, existing ISAC systems are based on fixed-position antennas (FPAs), which inevitably incur a loss in performance when balancing the trade-off between sensing and communication. Movable antenna (MA) technology offers promising potential to enhance ISAC performance by enabling flexible antenna movement. Nevertheless, exploiting more spatial channel variations requires larger antenna moving regions, which may invalidate the conventional far-field assumption for channels between transceivers. Therefore, this paper utilizes the MA to enhance sensing and communication capabilities in near-field ISAC systems, where a full-duplex base station (BS) is equipped with multiple transmit and receive MAs movable in large-size regions to simultaneously sense multiple targets and serve multiple uplink (UL) and downlink (DL) users for communication. We aim to maximize the weighted sum of sensing and communication rates (WSR) by jointly designing the transmit beamformers, sensing signal covariance matrices, receive beamformers, and MA positions at the BS, as well as the UL power allocation. The resulting optimization problem is challenging to solve. Thus, we propose an efficient two-layer random position (RP) algorithm to tackle it. In addition, to reduce movement delay and cost, we design an antenna position matching (APM) algorithm based on the greedy strategy to minimize the total MA movement distance. Extensive simulation results demonstrate the substantial performance improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the results show the effectiveness of the proposed APM algorithm in reducing the antenna movement distance, which is helpful for energy saving and time overhead reduction for MA-aided near-field ISAC systems with large moving regions.

Paper number 59:
Title: Optimizing wheel loader performance -- an end-to-end approach
Authors: Koji Aoshima, Eddie Wadbro, Martin Servin
Abstract: Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. Automating this task presents a challenging planning problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.

Paper number 60:
Title: Token Communications: A Unified Framework for Cross-modal Context-aware Semantic Communications
Authors: Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Rahim Tafazolli, Mehdi Bennis, Dusit Niyato
Abstract: In this paper, we introduce token communications (TokCom), a large model-driven framework to leverage cross-modal context information in generative semantic communications (GenSC). TokCom is a new paradigm, motivated by the recent success of generative foundation models and multimodal large language models (GFM/MLLMs), where the communication units are tokens, enabling efficient transformer-based token processing at the transmitter and receiver. In this paper, we introduce the potential opportunities and challenges of leveraging context in GenSC, explore how to integrate GFM/MLLMs-based token processing into semantic communication systems to leverage cross-modal context effectively at affordable complexity, present the key principles for efficient TokCom at various layers in future wireless networks. In a typical image semantic communication setup, we demonstrate a significant improvement of the bandwidth efficiency, achieved by TokCom by leveraging the context information among tokens. Finally, the potential research directions are identified to facilitate adoption of TokCom in future wireless networks.

Paper number 61:
Title: QubitLens: An Interactive Learning Tool for Quantum State Tomography
Authors: Mohammad Aamir Sohail, Ranga Sudharshan, S. Sandeep Pradhan, Arvind Rao
Abstract: Quantum state tomography is a fundamental task in quantum computing, involving the reconstruction of an unknown quantum state from measurement outcomes. Although essential, it is typically introduced at the graduate level due to its reliance on advanced concepts such as the density matrix formalism, tensor product structures, and partial trace operations. This complexity often creates a barrier for students and early learners. In this work, we introduce QubitLens, an interactive visualization tool designed to make quantum state tomography more accessible and intuitive. QubitLens leverages maximum likelihood estimation (MLE), a classical statistical method, to estimate pure quantum states from projective measurement outcomes in the X, Y, and Z bases. The tool emphasizes conceptual clarity through visual representations, including Bloch sphere plots of true and reconstructed qubit states, bar charts comparing parameter estimates, and fidelity gauges that quantify reconstruction accuracy. QubitLens offers a hands-on approach to learning quantum tomography without requiring deep prior knowledge of density matrices or optimization theory. The tool supports both single- and multi-qubit systems and is intended to bridge the gap between theory and practice in quantum computing education.

Paper number 62:
Title: Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia
Authors: Katelyn Xiaoying Mei, Anna Seo Gyeong Choi, Hilke Schellmann, Mona Sloane, Allison Koenecke
Abstract: Automatic Speech Recognition (ASR) has transformed daily tasks from video transcription to workplace hiring. ASR systems' growing use warrants robust and standardized auditing approaches to ensure automated transcriptions of high and equitable quality. This is especially critical for people with speech and language disorders (such as aphasia) who may disproportionately depend on ASR systems to navigate everyday life. In this work, we identify three pitfalls in existing standard ASR auditing procedures, and demonstrate how addressing them impacts audit results via a case study of six popular ASR systems' performance for aphasia speakers. First, audits often adhere to a single method of text standardization during data pre-processing, which (a) masks variability in ASR performance from applying different standardization methods, and (b) may not be consistent with how users - especially those from marginalized speech communities - would want their transcriptions to be standardized. Second, audits often display high-level demographic findings without further considering performance disparities among (a) more nuanced demographic subgroups, and (b) relevant covariates capturing acoustic information from the input audio. Third, audits often rely on a single gold-standard metric -- the Word Error Rate -- which does not fully capture the extent of errors arising from generative AI models, such as transcription hallucinations. We propose a more holistic auditing framework that accounts for these three pitfalls, and exemplify its results in our case study, finding consistently worse ASR performance for aphasia speakers relative to a control group. We call on practitioners to implement these robust ASR auditing practices that remain flexible to the rapidly changing ASR landscape.

Paper number 63:
Title: UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching
Authors: Neta Glazer, Aviv Navon, Yael Segal, Aviv Shamsian, Hilit Segev, Asaf Buchnick, Menachem Pirchi, Gil Hetz, Joseph Keshet
Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech synthesis, yet integrating speech with complex background environments remains challenging. We introduce UmbraTTS, a flow-matching based TTS model that jointly generates both speech and environmental audio, conditioned on text and acoustic context. Our model allows fine-grained control over background volume and produces diverse, coherent, and context-aware audio scenes. A key challenge is the lack of data with speech and background audio aligned in natural context. To overcome the lack of paired training data, we propose a self-supervised framework that extracts speech, background audio, and transcripts from unannotated recordings. Extensive evaluations demonstrate that UmbraTTS significantly outperformed existing baselines, producing natural, high-quality, environmentally aware audios.

Paper number 64:
Title: Lighting the Night with Generative Artificial Intelligence
Authors: Tingting Zhou, Feng Zhang, Haoyang Fu, Baoxiang Pan, Renhe Zhang, Feng Lu, Zhixin Yang
Abstract: The visible light reflectance data from geostationary satellites is crucial for meteorological observations and plays an important role in weather monitoring and forecasting. However, due to the lack of visible light at night, it is impossible to conduct continuous all-day weather observations using visible light reflectance data. This study pioneers the use of generative diffusion models to address this limitation. Based on the multi-band thermal infrared brightness temperature data from the Advanced Geostationary Radiation Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we developed a high-precision visible light reflectance generative model, called Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m}, 0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance generation at night. Compared to the classical models, RefDiff not only significantly improves accuracy through ensemble averaging but also provides uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90, with particularly significant improvements in areas with complex cloud structures and thick clouds. The model's nighttime generation capability was validated using VIIRS nighttime product, demonstrating comparable performance to its daytime counterpart. In summary, this research has made substantial progress in the ability to generate visible light reflectance at night, with the potential to expand the application of nighttime visible light data.

Paper number 65:
Title: Introducing Image-Space Preconditioning in the Variational Formulation of MRI Reconstructions
Authors: Bastien Milani, Jean-Baptist Ledoux, Berk Can Acikgoz, Xavier Richard
Abstract: The aim of the present article is to enrich the comprehension of iterative magnetic resonance imaging (MRI) reconstructions, including compressed sensing (CS) and iterative deep learning (DL) reconstructions, by describing them in the general framework of finite-dimensional inner-product spaces. In particular, we show that image-space preconditioning (ISP) and data-space preconditioning (DSP) can be formulated as non-conventional inner-products. The main gain of our reformulation is an embedding of ISP in the variational formulation of the MRI reconstruction problem (in an algorithm-independent way) which allows in principle to naturally and systematically propagate ISP in all iterative reconstructions, including many iterative DL and CS reconstructions where preconditioning is lacking. The way in which we apply linear algebraic tools to MRI reconstructions as presented in this article is a novelty. A secondary aim of our article is to offer a certain didactic material to scientists who are new in the field of MRI reconstruction. Since we explore here some mathematical concepts of reconstruction, we take that opportunity to recall some principles that may be understood for experts, but which may be hard to find in the literature for beginners. In fact, the description of many mathematical tools of MRI reconstruction is fragmented in the literature or sometimes missing because considered as a general knowledge. Further, some of those concepts can be found in mathematic manuals, but not in a form that is oriented toward MRI. For example, we think of the conjugate gradient descent, the notion of derivative with respect to non-conventional inner products, or simply the notion of adjoint. The authors believe therefore that it is beneficial for their field of research to dedicate some space to such a didactic material.

Paper number 66:
Title: LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification
Authors: Changheon Han, Yun Seok Kang, Yuseop Sim, Hyung Wook Park, Martin Byung-Guk Jun
Abstract: Deep learning-based machine listening is broadening the scope of industrial acoustic analysis for applications like anomaly detection and predictive maintenance, thereby improving manufacturing efficiency and reliability. Nevertheless, its reliance on large, task-specific annotated datasets for every new task limits widespread implementation on shop floors. While emerging sound foundation models aim to alleviate data dependency, they are too large and computationally expensive, requiring cloud infrastructure or high-end hardware that is impractical for on-site, real-time deployment. We address this gap with LISTEN (Lightweight Industrial Sound-representable Transformer for Edge Notification), a kilobyte-sized industrial sound foundation model. Using knowledge distillation, LISTEN runs in real-time on low-cost edge devices. On benchmark downstream tasks, it performs nearly identically to its much larger parent model, even when fine-tuned with minimal datasets and training resource. Beyond the model itself, we demonstrate its real-world utility by integrating LISTEN into a complete machine monitoring framework on an edge device with an Industrial Internet of Things (IIoT) sensor and system, validating its performance and generalization capabilities on a live manufacturing shop floor.
    