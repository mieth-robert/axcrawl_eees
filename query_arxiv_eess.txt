
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A New Underdetermined Framework for Sparse Estimation of Fault Location for Transmission Lines Using Limited Current Measurements
Authors: Guangxiao Zhang, Gaoxi Xiao, Xinghua Liu, Yan Xu, Peng Wang
Abstract: This letter proposes an alternative underdetermined framework for fault location that utilizes current measurements along with the branch-bus matrix, providing another option besides the traditional voltage-based methods. To enhance fault location accuracy in the presence of multiple outliers, the robust YALL1 algorithm is used to resist outlier interference and accurately recover the sparse vector, thereby pinpointing the fault precisely. The results on the IEEE 39-bus test system demonstrate the effectiveness and robustness of the proposed method.

Paper number 2:
Title: Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa
Authors: Rancy Chepchirchir, Jill Sunday, Raymond Confidence, Dong Zhang, Talha Chaudhry, Udunna C. Anazodo, Kendi Muchungi, Yujing Zou
Abstract: In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic Resonance Imaging (MRI) technology raises questions about the applicability of machine learning methods for clinical tasks. This study aims to provide a robust deep learning-based brain tumor segmentation (BraTS) method tailored for the SSA population using a threefold approach. Firstly, the impact of domain shift from the SSA training data on model efficacy was examined, revealing no significant effect. Secondly, a comparative analysis of 3D and 2D full-resolution models using the nnU-Net framework indicates similar performance of both the models trained for 300 epochs achieving a five-fold cross-validation score of 0.93. Lastly, addressing the performance gap observed in SSA validation as opposed to the relatively larger BraTS glioma (GLI) validation set, two strategies are proposed: fine-tuning SSA cases using the GLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel neural style transfer-based data augmentation technique for the SSA cases. This investigation underscores the potential of enhancing brain tumor prediction within SSA's unique healthcare landscape.

Paper number 3:
Title: Topology-based deep-learning segmentation method for deep anterior lamellar keratoplasty (DALK) surgical guidance using M-mode OCT data
Authors: J. Yu, H. Yi, Y. Wang, J. D. Opfermann, W. G. Gensheimer, A. Krieger, J. U. Kang
Abstract: Deep Anterior Lamellar Keratoplasty (DALK) is a partial-thickness corneal transplant procedure used to treat corneal stromal diseases. A crucial step in this procedure is the precise separation of the deep stroma from Descemet's membrane (DM) using the Big Bubble technique. To simplify the tasks of needle insertion and pneumo-dissection in this technique, we previously developed an Optical Coherence Tomography (OCT)-guided, eye-mountable robot that uses real-time tracking of corneal layers from M-mode OCT signals for control. However, signal noise and instability during manipulation of the OCT fiber sensor-integrated needle have hindered the performance of conventional deep-learning segmentation methods, resulting in rough and inaccurate detection of corneal layers. To address these challenges, we have developed a topology-based deep-learning segmentation method that integrates a topological loss function with a modified network architecture. This approach effectively reduces the effects of noise and improves segmentation speed, precision, and stability. Validation using in vivo, ex vivo, and hybrid rabbit eye datasets demonstrates that our method outperforms traditional loss-based techniques, providing fast, accurate, and robust segmentation of the epithelium and DM to guide surgery.

Paper number 4:
Title: Color Correction Meets Cross-Spectral Refinement: A Distribution-Aware Diffusion for Underwater Image Restoration
Authors: Laibin Chang, Yunke Wang, Bo Du, Chang Xu
Abstract: Underwater imaging often suffers from significant visual degradation, which limits its suitability for subsequent applications. While recent underwater image enhancement (UIE) methods rely on the current advances in deep neural network architecture designs, there is still considerable room for improvement in terms of cross-scene robustness and computational efficiency. Diffusion models have shown great success in image generation, prompting us to consider their application to UIE tasks. However, directly applying them to UIE tasks will pose two challenges, \textit{i.e.}, high computational budget and color unbalanced perturbations. To tackle these issues, we propose DiffColor, a distribution-aware diffusion and cross-spectral refinement model for efficient UIE. Instead of diffusing in the raw pixel space, we transfer the image into the wavelet domain to obtain such low-frequency and high-frequency spectra, it inherently reduces the image spatial dimensions by half after each transformation. Unlike single-noise image restoration tasks, underwater imaging exhibits unbalanced channel distributions due to the selective absorption of light by water. To address this, we design the Global Color Correction (GCC) module to handle the diverse color shifts, thereby avoiding potential global degradation disturbances during the denoising process. For the sacrificed image details caused by underwater scattering, we further present the Cross-Spectral Detail Refinement (CSDR) to enhance the high-frequency details, which are integrated with the low-frequency signal as input conditions for guiding the diffusion. This way not only ensures the high-fidelity of sampled content but also compensates for the sacrificed details. Comprehensive experiments demonstrate the superior performance of DiffColor over state-of-the-art methods in both quantitative and qualitative evaluations.

Paper number 5:
Title: Rethinking domain generalization in medical image segmentation: One image as one domain
Authors: Jin Hong, Bo Liu, Guoli Long
Abstract: Domain shifts in medical image segmentation, particularly when data comes from different centers, pose significant challenges. Intra-center variability, such as differences in scanner models or imaging protocols, can cause domain shifts as large as, or even larger than, those between centers. To address this, we propose the "one image as one domain" (OIOD) hypothesis, which treats each image as a unique domain, enabling flexible and robust domain generalization. Based on this hypothesis, we develop a unified disentanglement-based domain generalization (UniDDG) framework, which simultaneously handles both multi-source and single-source domain generalization without requiring explicit domain labels. This approach simplifies training with a fixed architecture, independent of the number of source domains, reducing complexity and enhancing scalability. We decouple each input image into content representation and style code, then exchange and combine these within the batch for segmentation, reconstruction, and further disentanglement. By maintaining distinct style codes for each image, our model ensures thorough decoupling of content representations and style codes, improving domain invariance of the content representations. Additionally, we enhance generalization with expansion mask attention (EMA) for boundary preservation and style augmentation (SA) to simulate diverse image styles, improving robustness to domain shifts. Extensive experiments show that our method achieves Dice scores of 84.43% and 88.91% for multi-source to single-center and single-center generalization in optic disc and optic cup segmentation, respectively, and 86.96% and 88.56% for prostate segmentation, outperforming current state-of-the-art domain generalization methods, offering superior performance and adaptability across clinical settings.

Paper number 6:
Title: Meta-learning-based percussion transcription and $t\bar{a}la$ identification from low-resource audio
Authors: Rahul Bapusaheb Kodag, Vipul Arora
Abstract: This study introduces a meta-learning-based approach for low-resource Tabla Stroke Transcription (TST) and $t\bar{a}la$ identification in Hindustani classical music. Using Model-Agnostic Meta-Learning (MAML), we address the challenge of limited annotated datasets, enabling rapid adaptation to new tasks with minimal data. The method is validated across various datasets, including tabla solo and concert recordings, demonstrating robustness in polyphonic audio scenarios. We propose two novel $t\bar{a}la$ identification techniques based on stroke sequences and rhythmic patterns. Additionally, the approach proves effective for Automatic Drum Transcription (ADT), showcasing its flexibility for Indian and Western percussion music. Experimental results show that the proposed method outperforms existing techniques in low-resource settings, significantly contributing to music transcription and studying musical traditions through computational tools.

Paper number 7:
Title: Development of an Adaptive Sliding Mode Controller using Neural Networks for Trajectory Tracking of a Cylindrical Manipulator
Authors: TieuNien Le, VanCuong Pham, NgocSon Vu
Abstract: Cylindrical manipulators are extensively used in industrial automation, especially in emerging technologies like 3D printing, which represents a significant future trend. However, controlling the trajectory of nonlinear models with system uncertainties remains a critical challenge, often leading to reduced accuracy and reliability. To address this, the study develops an Adaptive Sliding Mode Controller (ASMC) integrated with Neural Networks (NNs) to improve trajectory tracking for cylindrical manipulators. The ASMC leverages the robustness of sliding mode control and the adaptability of neural networks to handle uncertainties and dynamic variations effectively. Simulation results validate that the proposed ASMC-NN achieves high trajectory tracking accuracy, fast response time, and enhanced reliability, making it a promising solution for applications in 3D printing and beyond.

Paper number 8:
Title: Optimize the parameters of the PID Controller using Genetic Algorithm for Robot Manipulators
Authors: Vu Ngoc Son, Pham Van Cuong, Nguyen Duy Minh, Phi Hoang Nha
Abstract: This paper presents the design a Proportional-Integral-Derivative (PID) controller with optimized parameters for a two-degree-of-freedom robotic arm. A genetic algorithm (GA) is proposed to optimize the controller parameters, addressing the challenges in determining PID controller parameters for highly nonlinear systems like robotic arms compared to traditional methods. The GA-optimized PID controller significantly improves control accuracy and performance over traditional control methods. Simulation results demonstrate that the robotic arm system operates with high precision and stability. Additionally, the shortened trajectory tracking response time enhances the feasibility of applying this control algorithm in realworld scenarios. This research not only confirms the suitability of PID-GA for robotic arms and similar systems but also opens new avenues for applying this algorithm to real physical systems.

Paper number 9:
Title: Comparison of fundamental frequency estimators with subharmonic voice signals
Authors: Takeshi Ikuma, Melda Kunduk, Andrew J. McWhorter
Abstract: In clinical voice signal analysis, mishandling of subharmonic voicing may cause an acoustic parameter to signal false negatives. As such, the ability of a fundamental frequency estimator to identify speaking fundamental frequency is critical. This paper presents a sustained-vowel study, which used a quality-of-estimate classification to identify subharmonic errors and subharmonics-to-harmonics ratio (SHR) to measure the strength of subharmonic voicing. Five estimators were studied with a sustained vowel dataset: Praat, YAAPT, Harvest, CREPE, and FCN-F0. FCN-F0, a deep-learning model, performed the best both in overall accuracy and in correctly resolving subharmonic signals. CREPE and Harvest are also highly capable estimators for sustained vowel analysis.

Paper number 10:
Title: A Novel Observer Design for LuGre Friction Estimation and Control
Authors: Caner Odabaş, Ömer Morgül
Abstract: Dynamic components of the friction may directly impact the stability and performance of the motion control systems. The LuGre model is a prevalent friction model utilized to express this dynamic behavior. Since the LuGre model is very comprehensive, friction compensation based on it might be challenging. Inspired by this, we develop a novel observer to estimate and compensate for LuGre friction. Furthermore, we present a Lyapunov stability analysis to show that observer dynamics are asymptotically stable under certain conditions. Compared to its counterparts, the proposed observer constitutes a simple and standalone scheme that can be utilized with arbitrary control inputs in a straightforward way. As a primary difference, the presented observer estimates velocity and uses the velocity error to estimate friction in addition to control input. The extensive simulations revealed that the introduced observer enhances position and velocity tracking performance in the presence of friction.

Paper number 11:
Title: A Steerable Deep Network for Model-Free Diffusion MRI Registration
Authors: Gianfranco Cortes, Baba C. Vemuri
Abstract: Nonrigid registration is vital to medical image analysis but remains challenging for diffusion MRI (dMRI) due to its high-dimensional, orientation-dependent nature. While classical methods are accurate, they are computationally demanding, and deep neural networks, though efficient, have been underexplored for nonrigid dMRI registration compared to structural imaging. We present a novel, deep learning framework for model-free, nonrigid registration of raw diffusion MRI data that does not require explicit reorientation. Unlike previous methods relying on derived representations such as diffusion tensors or fiber orientation distribution functions, in our approach, we formulate the registration as an equivariant diffeomorphism of position-and-orientation space. Central to our method is an $\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while preserving the geometric properties of a raw dMRI's domain. We introduce a new loss function based on the maximum mean discrepancy in Fourier space, implicitly matching ensemble average propagators across images. Experimental results on Human Connectome Project dMRI data demonstrate competitive performance compared to state-of-the-art approaches, with the added advantage of bypassing the overhead for estimating derived representations. This work establishes a foundation for data-driven, geometry-aware dMRI registration directly in the acquisition space.

Paper number 12:
Title: A Deep Learning-Based Method for Power System Resilience Evaluation
Authors: Xuesong Wang, Caisheng Wang
Abstract: Power systems are critical infrastructure in modern society, and power outages can cause significant disruptions to communities and individuals' daily lives. The resilience of a power system measures its ability to maintain power supply during highly disruptive events such as hurricanes, earthquakes, and thunderstorms. Traditional methods for quantifying power system resilience include statistics-based and simulation-based approaches. Statistics-based methods offer a retrospective analysis of system performance without requiring a physical model, while simulation-based methods necessitate detailed physical system information and often simplify real-world scenarios. This paper introduces a deep learning-based method for evaluating power system resilience using historical power outage data. The method leverages the generalization capabilities of deep learning models and incorporates socio-economic and demographic factors as weighting terms to highlight the impacts on vulnerable demographic groups. The effectiveness of the proposed method is demonstrated through two case studies: one with real historical outage data and the other with simulated outage records. This approach provides valuable insights into measuring power system resilience against hazardous weather events without requiring a physical model of the target systems. The evaluation results can further guide the planning of distributed energy resources for resilience enhancement.

Paper number 13:
Title: DRL-Based Medium-Term Planning of Renewable-Integrated Self-Scheduling Cascaded Hydropower to Guide Wholesale Market Participation
Authors: Xianbang Chen, Yikui Liu, Neng Fan, Lei Wu
Abstract: For self-scheduling cascaded hydropower (S-CHP) facilities, medium-term planning is a critical step that coordinates water availability over the medium-term horizon, providing water usage guidance for their short-term operations in wholesale market participation. Typically, medium-term planning strategies (e.g., reservoir storage targets at the end of each short-term period) are determined by either optimization methods or rules of thumb. However, with the integration of variable renewable energy sources (VRESs), optimization-based methods suffer from deviations between the anticipated and actual reservoir storage, while rules of thumb could be financially conservative, thereby compromising short-term operating profitability in wholesale market participation. This paper presents a deep reinforcement learning (DRL)-based framework to derive medium-term planning policies for VRES-integrated S-CHPs (VS-CHPs), which can leverage contextual information underneath individual short-term periods and train planning policies by their induced short-term operating profits in wholesale market participation. The proposed DRL-based framework offers two practical merits. First, its planning strategies consider both seasonal requirements of reservoir storage and needs for short-term operating profits. Second, it adopts a multi-parametric programming-based strategy to accelerate the expensive training process associated with multi-step short-term operations. Finally, the DRL-based framework is evaluated on a real-world VS-CHP, demonstrating its advantages over current practice.

Paper number 14:
Title: Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction
Authors: Jihwan Lee, Tiantian Feng, Aditya Kommineni, Sudarsana Reddy Kadiri, Shrikanth Narayanan
Abstract: Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available.

Paper number 15:
Title: Secure Beamforming for Continuous Aperture Array (CAPA) Systems
Authors: Mingjun Sun, Chongjun Ouyang, Zhaolin Wang, Shaochuan Wu, Yuanwei Liu
Abstract: Continuous aperture array (CAPA) is considered a promising technology for 6G networks, offering the potential to fully exploit spatial DoFs and achieve the theoretical limits of channel capacity. This paper investigates the performance gain of a CAPA-based downlink secure transmission system, where multiple legitimate user terminals (LUTs) coexist with multiple eavesdroppers (Eves). The system's secrecy performance is evaluated using a weighted secrecy sum-rate (WSSR) under a power constraint. We then propose two solutions for the secure current pattern design. The first solution is a block coordinate descent (BCD) optimization method based on fractional programming, which introduces a continuous-function inversion theory corresponding to matrix inversion in the discrete domain. This approach derives a closed-form expression for the optimal source current pattern. Based on this, it can be found that the optimal current pattern is essentially a linear combination of the channel spatial responses, thus eliminating the need for complex integration operations during the algorithm's optimization process. The second solution is a heuristic algorithm based on Zero-Forcing (ZF), which constructs a zero-leakage current pattern using the channel correlation matrix. It further employs a water-filling approach to design an optimal power allocation scheme that maximizes the WSSR. In high SNR regions, this solution gradually approaches the first solution, ensuring zero leakage while offering lower computational complexity. Simulation results demonstrate that: 1) CAPA-based systems achieve better WSSR compared to discrete multiple-input multiple-output systems. 2) The proposed methods, whether optimization-based or heuristic, provide significant performance improvements over existing state-of-the-art Fourier-based discretization methods, while considerably reducing computational complexity.

Paper number 16:
Title: FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with Single-Step Flow Matching
Authors: Jun-Hak Yun, Seung-Bin Kim, Seong-Whan Lee
Abstract: Audio super-resolution is challenging owing to its ill-posed nature. Recently, the application of diffusion models in audio super-resolution has shown promising results in alleviating this challenge. However, diffusion-based models have limitations, primarily the necessity for numerous sampling steps, which causes significantly increased latency when synthesizing high-quality audio samples. In this paper, we propose FLowHigh, a novel approach that integrates flow matching, a highly efficient generative model, into audio super-resolution. We also explore probability paths specially tailored for audio super-resolution, which effectively capture high-resolution audio distributions, thereby enhancing reconstruction quality. The proposed method generates high-fidelity, high-resolution audio through a single-step sampling process across various input sampling rates. The experimental results on the VCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art performance in audio super-resolution, as evaluated by log-spectral distance and ViSQOL while maintaining computational efficiency with only a single-step sampling process.

Paper number 17:
Title: Promoting Shared Energy Storage Aggregation among High Price-Tolerance Prosumer: An Incentive Deposit and Withdrawal Service
Authors: Xin Lu, Jing Qiu, Cuo Zhang, Gang Lei, Jianguo Zhu
Abstract: Many residential prosumers exhibit a high price-tolerance for household electricity bills and a low response to price incentives. This is because the household electricity bills are not inherently high, and the potential for saving on electricity bills through participation in conventional Shared Energy Storage (SES) is limited, which diminishes their motivation to actively engage in SES. Additionally, existing SES models often require prosumers to take additional actions, such as optimizing rental capacity and bidding prices, which happen to be capabilities that typical household prosumers do not possess. To incentivize these high price-tolerance residential prosumers to participate in SES, a novel SES aggregation framework is proposed, which does not require prosumers to take additional actions and allows them to maintain existing energy storage patterns. Compared to conventional long-term operation of SES, the proposed framework introduces an additional short-term construction step during which the energy service provider (ESP) acquires control of the energy storage systems (ESS) and offers electricity deposit and withdrawal services (DWS) with dynamic coefficients, enabling prosumers to withdraw more electricity than they deposit without additional actions. Additionally, a matching mechanism is proposed to align prosumers' electricity consumption behaviors with ESP's optimization strategies. Finally, the dynamic coefficients in DWS and trading strategies are optimized by an improved deep reinforcement learning (DRL) algorithm. Case studies are conducted to verify the effectiveness of the proposed SES aggregation framework with DWS and the matching mechanism.

Paper number 18:
Title: Infinite Factorial Linear Dynamical Systems for Transient Signal Detection
Authors: Jiadi Bao, Yatong Wang, Yunjie Li, Mengtao Zhu, Shafei Wang
Abstract: Accurately detecting the transient signal of interest from the background signal is one of the fundamental tasks in signal processing. The most recent approaches assume the existence of a single background source and represent the background signal using a linear dynamical system (LDS). This assumption might fail to capture the complexities of modern electromagnetic environments with multiple sources. To address this limitation, this paper proposes a method for detecting the transient signal in a background composed of an unknown number of emitters. The proposed method consists of two main tasks. First, a Bayesian nonparametric model called the infinite factorial linear dynamical system (IFLDS) is developed. The developed model is based on the sticky Indian buffet process and enables the representation and parameter learning of the unbounded number of background sources. This study also designs a parameter learning method for the IFLDS using slice sampling and particle Gibbs with ancestor sampling. Second, the finite moving average (FMA) stopping time is introduced to minimize the worst-case probability of missed detection, and the statistical performance of the stopping time is investigated. To facilitate the computation of the FMA stopping time, this study derives the factorial Kalman forward filtering (FKFF) method and designs a dependence structure for the underlying model, allowing the stopping time to be defined by a recursive function. Numerical simulations demonstrate the effectiveness of the proposed method and the validity of the theoretical results. The experimental results of the pulse signal detection under the condition of communication interference confirm the effectiveness and superiority of the proposed method.

Paper number 19:
Title: Coordinated Control of Deformation and Flight for Morphing Aircraft via Meta-Learning and Coupled State-Dependent Riccati Equations
Authors: Hao-Chi Che, Huai-Ning Wu
Abstract: In this paper, the coordinated control problem of deformation and flight for morphing aircraft (MA) is studied by using meta-learning (ML) and coupled state-dependent Riccati equations (CSDREs). Our method is built on two principal observations that dynamic models of MA under varying morphing conditions share a morphing condition independent representation function and that the specific morphing condition part lies in a set of linear coefficients. To that end, the domain adversarially invariant meta-learning (DAIML) is employed to learn the shared representation with offline flight data. Based on the learned representation function, the coordinated control of the deformation and flight for MA is formulated as a non-cooperative differential game. The state-dependent feedback control solutions can be derived by addressing a pair of CSDREs. For this purpose, Lyapunov iterations are extended to obtain the positive semidefinite (definite) stabilizing solutions of the CSDREs, and the convergence proof of the proposed algorithm is provided. Finally, a simulation study is carried out to validate the efficacy of the developed coordinated game control strategies.

Paper number 20:
Title: Improving the U-Net Configuration for Automated Delineation of Head and Neck Cancer on MRI
Authors: Andrei Iantsen
Abstract: Tumor volume segmentation on MRI is a challenging and time-consuming process that is performed manually in typical clinical settings. This work presents an approach to automated delineation of head and neck tumors on MRI scans, developed in the context of the MICCAI Head and Neck Tumor Segmentation for MR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new, task-specific convolutional neural network, the focus of this research was to propose improvements to the configuration commonly used in medical segmentation tasks, relying solely on the traditional U-Net architecture. The empirical results presented in this article suggest the superiority of patch-wise normalization used for both training and sliding window inference. They also indicate that the performance of segmentation models can be enhanced by applying a scheduled data augmentation policy during training. Finally, it is shown that a small improvement in quality can be achieved by using Gaussian weighting to combine predictions for individual patches during sliding window inference. The model with the best configuration obtained an aggregated Dice Similarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five cross-validation folds. The ensemble of five models (one best model per validation fold) showed consistent results on a private test set of 50 patients with an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name: this http URL). The source code and model weights are freely available at this http URL.

Paper number 21:
Title: Joint Detection and Angle Estimation for Multiple Jammers in Beamspace Massive MIMO
Authors: Pengguang Du, Cheng Zhang, Changwei Zhang, Zhilei Zhang, Yongming Huang
Abstract: In this paper, we study the joint detection and angle estimation problem for beamspace multiple-input multiple-output (MIMO) systems with multiple random jamming targets. An iterative low-complexity generalized likelihood ratio test (GLRT) is proposed by transforming the composite multiple hypothesis test on the projected vector into a series of binary hypothesis tests based on the spatial covariance matrix. In each iteration, the detector implicitly inhibits the mainlobe effects of the previously detected jammers by utilizing the estimated angles and average jamming-to-signal ratios. This enables the detection of a new potential jammer and the identification of its corresponding spatial covariance. Simulation results demonstrate that the proposed method outperforms existing benchmarks by suppressing sidelobes of the detected jammers and interference from irrelevant angles, especially in medium-to-high jamming-to-noise ratio scenarios.

Paper number 22:
Title: Explainable AI based System for Supply Air Temperature Forecast
Authors: Marika Eik, Ahmet Kose, Hossein Nourollahi Hokmabad, Juri Belikov
Abstract: This paper explores the application of Explainable AI (XAI) techniques to improve the transparency and understanding of predictive models in control of automated supply air temperature (ASAT) of Air Handling Unit (AHU). The study focuses on forecasting of ASAT using a linear regression with Huber loss. However, having only a control curve without semantic and/or physical explanation is often not enough. The present study employs one of the XAI methods: Shapley values, which allows to reveal the reasoning and highlight the contribution of each feature to the final ASAT forecast. In comparison to other XAI methods, Shapley values have solid mathematical background, resulting in interpretation transparency. The study demonstrates the contrastive explanations--slices, for each control value of ASAT, which makes it possible to give the client objective justifications for curve changes.

Paper number 23:
Title: Holographic Metasurfaces Enabling Wave Computing for 6G: Status Overview, Challenges, and Future Research Trends
Authors: Zahra Rahimian Omam, Hamidreza Taghvaee, Ali Araghi, Maria Garcia-Fernandez, Guillermo Alvarez-Narciandi, George C. Alexandropoulos, Okan Yurduseven, Mohsen Khalily
Abstract: Recent advancements in wave computing using metasurfaces are poised to transform wireless communications by enabling high-speed, energy-efficient, and highly parallelized signal processing. These capabilities are essential to meet the ultra-high data rates of up to 1 terabit per second and minimal latency as low as 1 millisecond required by next-generation wireless networks. Diverging from traditional digital processing, wave computing adopts continuous analog signals to foster innovative functions such as over-the-air computation, integrated sensing and communications, computational electromagnetic imaging, and physical-layer security. This article explores the potential of reconfigurable multi-functional metasurfaces in wave computing, emphasizing their pivotal role in facilitating seamless communications and addressing the escalating computational demands for sixth generation (6G) networks. As artificial intelligence has become one of the most prominent and rapidly advancing fields of research over the last decade, we also introduce a wave-domain-based machine learning approach aimed at achieving power-efficient, fast training and computation. Future research directions are discussed, underscoring how metasurface-based systems can merge computation with communication to innovate components of 6G networks, thus creating smarter, faster, and more adaptable wireless infrastructures.

Paper number 24:
Title: Hyperdimensional Computing for ADHD Classification using EEG Signals
Authors: Federica Colonnese, Antonello Rosato, Francesco Di Luzio, Massimo Panella
Abstract: Following the recent interest in applying the Hyperdimensional Computing paradigm in medical context to power up the performance of general machine learning applied to biomedical data, this study represents the first attempt at employing such techniques to solve the problem of classification of Attention Deficit Hyperactivity Disorder using electroencephalogram signals. Making use of a spatio-temporal encoder, and leveraging the properties of HDC, the proposed model achieves an accuracy of 88.9%, outperforming traditional Deep Neural Networks benchmark models. The core of this research is not only to enhance the classification accuracy of the model but also to explore its efficiency in terms of the required training data: a critical finding of the study is the identification of the minimum number of patients needed in the training set to achieve a sufficient level of accuracy. To this end, the accuracy of our model trained with only $7$ of the $79$ patients is comparable to the one from benchmarks trained on the full dataset. This finding underscores the model's efficiency and its potential for quick and precise ADHD diagnosis in medical settings where large datasets are typically unattainable.

Paper number 25:
Title: RadioTransformer: Accurate Radio Map Construction and Coverage Prediction
Authors: Yuxuan Li, Cheng Zhang, Wen Wang, Yongming Huang
Abstract: Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RadioTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches.

Paper number 26:
Title: Joint Communications and Sensing for 6G Satellite Networks: Use Cases and Challenges
Authors: Chandan Kumar Sheemar, Prabhu Thiruvasagam, Wali Ullah Khan, Sourabh Solanki, George C. Alexandropoulos, Jorge Querol, Jan Plachy, Oliver Holschke, Symeon Chatzinotas
Abstract: Satellite Networks (SN) have traditionally been instrumental in providing two key services: communications and sensing. Communications satellites enable global connectivity, while sensing satellites facilitate applications such as Earth observation, navigation, and disaster management. However, the emergence of novel use cases and the exponential growth in service demands make the independent evolution of communication and sensing payloads increasingly impractical. Addressing this challenge requires innovative approaches to optimize satellite resources. Joint Communications and Sensing (JCAS) technology represents a transformative paradigm for SN. By integrating communication and sensing functionalities into unified hardware platforms, JCAS enhances spectral efficiency, reduces operational costs, and minimizes hardware redundancies. This paper explores the potential of JCAS in advancing the next-generation space era, highlighting its role in emerging applications. Furthermore, it identifies critical challenges, such as waveform design, Doppler effect mitigation, and multi-target detection, that remain open for future research. Through these discussions, we aim to stimulate further research into the transformative potential of JCAS in addressing the demands of 6G and beyond SN.

Paper number 27:
Title: Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast Fourier Transforms
Authors: Talha Sultan, Alex Bocchieri, Chaoying Gu, Xiaochun Liu, Pavel Polynkin, Andreas Velten
Abstract: Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay surface and input this measurement into computational algorithms that output a 3D volumetric reconstruction. These algorithms utilize the Fast Fourier Transform (FFT) to accelerate the reconstruction process but require both input and output to be sampled spatially with uniform grids. However, the geometry of NLOS imaging inherently results in non-uniform sampling on the relay surface when using multi-pixel detector arrays, even though such arrays significantly reduce acquisition times. Furthermore, using these arrays increases the data rate required for sensor readout, posing challenges for real-world deployment. In this work, we utilize the phasor field framework to demonstrate that existing NLOS imaging setups typically oversample the relay surface spatially, explaining why the measurement can be compressed without significantly sacrificing reconstruction quality. This enables us to utilize the Non-Uniform Fast Fourier Transform (NUFFT) to reconstruct from sparse measurements acquired from irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we utilize the NUFFT to reconstruct at arbitrary locations in the hidden volume, ensuring flexible sampling schemes for both the input and output. Finally, we utilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes without increasing the number of samples stored in memory. All algorithms introduced in this paper preserve the computational complexity of FFT-based methods, ensuring scalability for practical NLOS imaging applications.

Paper number 28:
Title: Pitch Plane Trajectory Tracking Control for Sounding Rockets via Adaptive Feedback Linearization
Authors: Pedro dos Santos, Paulo Oliveira
Abstract: This paper proposes a pitch plane trajectory tacking control solution for suborbital launch vehicles relying on adaptive feedback linearization. Initially, the 2D dynamics and kinematics for a single-engine, thrust-vector-controlled sounding rocket are obtained for control design purposes. Then, an inner-outer control strategy, which simultaneously tackles attitude and position control, is adopted, with the inner-loop comprising the altitude and pitch control and the outer-loop addressing the horizontal (downrange) position control. Feedback linearization is used to cancel out the non-linearities in both the inner and outer dynamics. Making use of Lyapunov stability theory, an adaptation law, which provides online estimates on the inner-loop aerodynamic uncertainty, is jointly designed with the output tracking controller via adaptive backstepping, ensuring global reference tracking in the region where the feedback linearization is well-defined. The zero dynamics of the inner-stabilized system are then exploited to obtain the outerloop dynamics and derive a Linear Quadratic Regulator (LQR) with integral action, which can stabilize them as well as reject external disturbances. In the outermost loop, the estimate on the correspondent aerodynamic uncertainty is indirectly obtained by using the inner loop estimates together with known aerodynamics relations. The resulting inner-outer position control solution is proven to be asymptotically stable in the region of interest. Using a single-stage sounding rocket, propelled by a liquid engine, as reference vehicle, different mission scenarios are tested in a simulation environment to verify the adaptability of the proposed control strategy. The system is able to track the requested trajectories while rejecting external wind disturbances. Furthermore, the need to re-tune the control gains in between different mission scenarios is minimal to none.

Paper number 29:
Title: Timing Properties of the Starlink Ku-Band Downlink
Authors: Wenkai Qin, Andrew M. Graff, Zachary L. Clements, Zacharias M. Komodromos, Todd E. Humphreys
Abstract: We develop signal capture and analysis techniques for precisely extracting and characterizing the frame timing of the Starlink constellation's Ku-band downlink transmissions. The aim of this work is to determine whether Starlink frame timing has sufficient short-term stability to support pseudorange-based opportunistic positioning, navigation, and timing (PNT). A second goal is to determine whether frame timing is disciplined to a common time scale such as GPS time. Our analysis reveals several timing characteristics not previously known that carry strong implications for PNT. On the favorable side, periods of ns-level jitter in frame arrival times across all satellite versions indicate that Starlink hardware is fundamentally capable of the short-term stability required to support GPS-like PNT. But there are several unfavorable characteristics that, if not addressed, will make GPS-like PNT impractical: (1) The v1.0 and v1.5 Starlink satellites exhibit once-per-second abrupt frame timing adjustments whose magnitude (as large as 100s of ns) and sign appear unpredictable. Similar discontinuities are also present in the v2.0-Mini frame timing, though smaller and irregularly spaced. (2) Episodic 15-s periods of high frame jitter routinely punctuate the nominal low-jitter frame arrival timing. (3) Starlink frame timing is disciplined to GPS time, but only loosely: to within a few ms by adjustments occurring every 15 s; otherwise exhibiting drift that can exceed 20 ppm. These unfavorable characteristics are essentially incompatible with accurate PNT. Fortunately, they appear to be a consequence of software design choices, not hardware limitations. Moreover, they could be compensated with third-party-provided corrections.

Paper number 30:
Title: Probing Speaker-specific Features in Speaker Representations
Authors: Aemon Yat Fei Chiu, Paco Kei Ching Fung, Roger Tsz Yeung Li, Jingyu Li, Tan Lee
Abstract: This study explores speaker-specific features encoded in speaker embeddings and intermediate layers of speech self-supervised learning (SSL) models. By utilising a probing method, we analyse features such as pitch, tempo, and energy across prominent speaker embedding models and speech SSL models, including HuBERT, WavLM, and Wav2vec 2.0. The results reveal that speaker embeddings like CAM++ excel in energy classification, while speech SSL models demonstrate superior performance across multiple features due to their hierarchical feature encoding. Intermediate layers effectively capture a mix of acoustic and para-linguistic information, with deeper layers refining these representations. This investigation provides insights into model design and highlights the potential of these representations for downstream applications, such as speaker verification and text-to-speech synthesis, while laying the groundwork for exploring additional features and advanced probing methods.

Paper number 31:
Title: Reducing Inter-user Interference: Precoding over OFDM for Enhanced MTC
Authors: Karim A. Said, A. A. Louis Beex, Elizabeth Bentley, Lingjia Liu
Abstract: In the physical layer (PHY) of modern cellular systems, information is transmitted as a sequence of resource blocks (RBs) across various domains with each resource block limited to a certain time and frequency duration. In the PHY of 4G/5G systems, data is transmitted in the unit of transport block (TB) across a fixed number of physical RBs based on resource allocation decisions. Using sharp band-limiting in the frequency domain can provide good separation between different resource allocations without wasting resources in guard bands. However, using sharp filters comes at the cost of elongating the overall system impulse response which can accentuate inter-symbol interference (ISI). In a multi-user setup, such as in Machine Type Communication (MTC), different users are allocated resources across time and frequency, and operate at different power levels. If strict band-limiting separation is used, high power user signals can leak in time into low power user allocations. The ISI extent, i.e., the number of neighboring symbols that contribute to the interference, depends both on the channel delay spread and the spectral concentration properties of the signaling waveforms. We hypothesize that using a precoder that effectively transforms an OFDM waveform basis into a basis comprised of prolate spheroidal sequences (DPSS) can minimize the ISI extent when strictly confined frequency allocations are used. Analytical expressions for upper bounds on ISI are derived. In addition, simulation results support our hypothesis.

Paper number 32:
Title: Towards resilient cities: A hybrid simulation framework for risk mitigation through data driven decision making
Authors: David Carraminana, Ana M. Bernardos, Juan A. Besada, Jose R. Casar
Abstract: Providing a comprehensive view of the city operation and offering useful metrics for decision making is a well known challenge for urban risk analysis systems. Existing systems are, in many cases, generalizations of previous domain specific tools and or methodologies that may not cover all urban interdependencies and makes it difficult to have homogeneous indicators. In order to overcome this limitation while seeking for effective support to decision makers, this article introduces a novel hybrid simulation framework for risk mitigation. The framework is built on a proposed city concept that considers urban space as a Complex Adaptive System composed by interconnected Critical Infrastructures. In this concept, a Social System, which models daily patterns and social interactions of the citizens in the Urban Landscape, drives the CIs demand to configure the full city picture. The frameworks hybrid design integrates agent based and network based modeling by breaking down city agents into system dependent subagents, to enable both inter and intra system interaction simulation, respectively. A layered structure of indicators at different aggregation levels is also developed, to ensure that decisions are not only data driven but also explainable. Therefore, the proposed simulation framework can serve as a DSS tool that allows the quantitative analysis of the impact of threats at different levels. First, system level metrics can be used to get a broad view on the city resilience. Then, agent level metrics back those figures and provide better explainability. On implementation, the proposed framework enables component reusability (for eased coding), simulation federation (enabling the integration of existing system oriented simulators), discrete simulation in accelerated time (for rapid scenario simulation) and decision oriented visualization (for informed outputs).

Paper number 33:
Title: Traffic Simulations: Multi-City Calibration of Metropolitan Highway Networks
Authors: Chao Zhang, Yechen Li, Neha Arora, Damien Pierce, Carolina Osorio
Abstract: This paper proposes an approach to perform travel demand calibration for high-resolution stochastic traffic simulators. It employs abundant travel times at the path-level, departing from the standard practice of resorting to scarce segment-level sensor counts. The proposed approach is shown to tackle high-dimensional instances in a sample-efficient way. For the first time, case studies on 6 metropolitan highway networks are carried out, considering a total of 54 calibration scenarios. This is the first work to show the ability of a calibration algorithm to systematically scale across networks. Compared to the state-of-the-art simultaneous perturbation stochastic approximation (SPSA) algorithm, the proposed approach enhances fit to field data by an average 43.5% with a maximum improvement of 80.0%, and does so within fewer simulation calls.

Paper number 34:
Title: Democratic Resilience and Sociotechnical Shocks
Authors: M. Amin Rahimian, Michael P. Colaresi
Abstract: We focus on the potential fragility of democratic elections given modern information-communication technologies (ICT) in the Web 2.0 era. Our work provides an explanation for the cascading attrition of public officials recently in the United States and offers potential policy interventions from a dynamic system's perspective. We propose that micro-level heterogeneity across individuals within crucial institutions leads to vulnerabilities of election support systems at the macro scale. Our analysis provides comparative statistics to measure the fragility of systems against targeted harassment, disinformation campaigns, and other adversarial manipulations that are now cheaper to scale and deploy. Our analysis also informs policy interventions that seek to retain public officials and increase voter turnout. We show how limited resources (for example, salary incentives to public officials and targeted interventions to increase voter turnout) can be allocated at the population level to improve these outcomes and maximally enhance democratic resilience. On the one hand, structural and individual heterogeneity cause systemic fragility that adversarial actors can exploit, but also provide opportunities for effective interventions that offer significant global improvements from limited and localized actions.

Paper number 35:
Title: JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for Conversational Speech Synthesis
Authors: Jun-Hyeok Cha, Seung-Bin Kim, Hyung-Seok Oh, Seong-Whan Lee
Abstract: Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context. To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules. We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech. The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech. The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation. Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets.

Paper number 36:
Title: Generalized Linear Models with 1-Bit Measurements: Asymptotics of the Maximum Likelihood Estimator
Authors: Jaimin Shah, Martina Cardone, Cynthia Rush, Alex Dytso
Abstract: This work establishes regularity conditions for consistency and asymptotic normality of the multiple parameter maximum likelihood estimator(MLE) from censored data, where the censoring mechanism is in the form of $1$-bit measurements. The underlying distribution of the uncensored data is assumed to belong to the exponential family, with natural parameters expressed as a linear combination of the predictors, known as generalized linear model (GLM). As part of the analysis, the Fisher information matrix is also derived for both censored and uncensored data, which helps to quantify the impact of censoring and assess the performance of the MLE. The choice of GLM allows one to consider a variety of practical examples where 1-bit estimation is of interest. In particular, it is shown how the derived results can be used to analyze two practically relevant scenarios: the Gaussian model with both unknown mean and variance, and the Poisson model with an unknown mean.

Paper number 37:
Title: Vision Graph Non-Contrastive Learning for Audio Deepfake Detection with Limited Labels
Authors: Falih Gozi Febrinanto, Kristen Moore, Chandra Thapa, Jiangang Ma, Vidya Saikrishna, Feng Xia
Abstract: Recent advancements in audio deepfake detection have leveraged graph neural networks (GNNs) to model frequency and temporal interdependencies in audio data, effectively identifying deepfake artifacts. However, the reliance of GNN-based methods on substantial labeled data for graph construction and robust performance limits their applicability in scenarios with limited labeled data. Although vast amounts of audio data exist, the process of labeling samples as genuine or fake remains labor-intensive and costly. To address this challenge, we propose SIGNL (Spatio-temporal vIsion Graph Non-contrastive Learning), a novel framework that maintains high GNN performance in low-label settings. SIGNL constructs spatio-temporal graphs by representing patches from the audio's visual spectrogram as nodes. These graph structures are modeled using vision graph convolutional (GC) encoders pre-trained through graph non-contrastive learning, a label-free that maximizes the similarity between positive pairs. The pre-trained encoders are then fine-tuned for audio deepfake detection, reducing reliance on labeled data. Experiments demonstrate that SIGNL outperforms state-of-the-art baselines across multiple audio deepfake detection datasets, achieving the lowest Equal Error Rate (EER) with as little as 5% labeled data. Additionally, SIGNL exhibits strong cross-domain generalization, achieving the lowest EER in evaluations involving diverse attack types and languages in the In-The-Wild dataset.

Paper number 38:
Title: VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models
Authors: Wenqian Cui, Xiaoqi Jiao, Ziqiao Meng, Irwin King
Abstract: With the growing demand for developing speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. When engaging in conversations with humans, it is essential for these models to comprehend a wide range of world knowledge. In this paper, we introduce VoxEval, a novel speech question-answering benchmark specifically designed to assess SLMs' knowledge understanding through purely speech-based interactions. Unlike existing AudioQA benchmarks, VoxEval maintains speech format for both questions and answers, evaluates model robustness across diverse audio conditions (varying timbres, audio qualities, and speaking styles), and pioneers the assessment of challenging domains like mathematical problem-solving in spoken format. Our comprehensive evaluation of recent SLMs using VoxEval reveals significant performance limitations in current models, highlighting crucial areas for future improvements.

Paper number 39:
Title: D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription
Authors: Hounsu Kim, Taegyun Kwon, Juhan Nam
Abstract: Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in this https URL.

Paper number 40:
Title: End-to-End Deep Learning for Interior Tomography with Low-Dose X-ray CT
Authors: Yoseob Han, Dufan Wu, Kyungsang Kim, Quanzheng Li
Abstract: Objective: There exist several X-ray computed tomography (CT) scanning strategies to reduce a radiation dose, such as (1) sparse-view CT, (2) low-dose CT, and (3) region-of-interest (ROI) CT (called interior tomography). To further reduce the dose, the sparse-view and/or low-dose CT settings can be applied together with interior tomography. Interior tomography has various advantages in terms of reducing the number of detectors and decreasing the X-ray radiation dose. However, a large patient or small field-of-view (FOV) detector can cause truncated projections, and then the reconstructed images suffer from severe cupping artifacts. In addition, although the low-dose CT can reduce the radiation exposure dose, analytic reconstruction algorithms produce image noise. Recently, many researchers have utilized image-domain deep learning (DL) approaches to remove each artifact and demonstrated impressive performances, and the theory of deep convolutional framelets supports the reason for the performance improvement. Approach: In this paper, we found that the image-domain convolutional neural network (CNN) is difficult to solve coupled artifacts, based on deep convolutional framelets. Significance: To address the coupled problem, we decouple it into two sub-problems: (i) image domain noise reduction inside truncated projection to solve low-dose CT problem and (ii) extrapolation of projection outside truncated projection to solve the ROI CT problem. The decoupled sub-problems are solved directly with a novel proposed end-to-end learning using dual-domain CNNs. Main results: We demonstrate that the proposed method outperforms the conventional image-domain deep learning methods, and a projection-domain CNN shows better performance than the image-domain CNNs which are commonly used by many researchers.

Paper number 41:
Title: Hierarchical Decomposed Dual-domain Deep Learning for Sparse-View CT Reconstruction
Authors: Yoseob Han
Abstract: Objective: X-ray computed tomography employing sparse projection views has emerged as a contemporary technique to mitigate radiation dose. However, due to the inadequate number of projection views, an analytic reconstruction method utilizing filtered backprojection results in severe streaking artifacts. Recently, deep learning strategies employing image-domain networks have demonstrated remarkable performance in eliminating the streaking artifact caused by analytic reconstruction methods with sparse projection views. Nevertheless, it is difficult to clarify the theoretical justification for applying deep learning to sparse view CT reconstruction, and it has been understood as restoration by removing image artifacts, not reconstruction. Approach: By leveraging the theory of deep convolutional framelets and the hierarchical decomposition of measurement, this research reveals the constraints of conventional image- and projection-domain deep learning methodologies, subsequently, the research proposes a novel dual-domain deep learning framework utilizing hierarchical decomposed measurements. Specifically, the research elucidates how the performance of the projection-domain network can be enhanced through a low-rank property of deep convolutional framelets and a bowtie support of hierarchical decomposed measurement in the Fourier domain. Main Results: This study demonstrated performance improvement of the proposed framework based on the low-rank property, resulting in superior reconstruction performance compared to conventional analytic and deep learning methods. Significance: By providing a theoretically justified deep learning approach for sparse-view CT reconstruction, this study not only offers a superior alternative to existing methods but also opens new avenues for research in medical imaging.

Paper number 42:
Title: DiffAttack: Diffusion-based Timbre-reserved Adversarial Attack in Speaker Identification
Authors: Qing Wang, Jixun Yao, Zhaokai Sun, Pengcheng Guo, Lei Xie, John H.L. Hansen
Abstract: Being a form of biometric identification, the security of the speaker identification (SID) system is of utmost importance. To better understand the robustness of SID systems, we aim to perform more realistic attacks in SID, which are challenging for both humans and machines to detect. In this study, we propose DiffAttack, a novel timbre-reserved adversarial attack approach that exploits the capability of a diffusion-based voice conversion (DiffVC) model to generate adversarial fake audio with distinct target speaker attribution. By introducing adversarial constraints into the generative process of the diffusion-based voice conversion model, we craft fake samples that effectively mislead target models while preserving speaker-wise characteristics. Specifically, inspired by the use of randomly sampled Gaussian noise in conventional adversarial attacks and diffusion processes, we incorporate adversarial constraints into the reverse diffusion process. These constraints subtly guide the reverse diffusion process toward aligning with the target speaker distribution. Our experiments on the LibriTTS dataset indicate that DiffAttack significantly improves the attack success rate compared to vanilla DiffVC and other methods. Moreover, objective and subjective evaluations demonstrate that introducing adversarial constraints does not compromise the speech quality generated by the DiffVC model.

Paper number 43:
Title: Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs
Authors: Artem Fedorchenko, Tanel Alumäe
Abstract: This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We fine-tune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.

Paper number 44:
Title: Is Your Autonomous Vehicle Safe? Understanding the Threat of Electromagnetic Signal Injection Attacks on Traffic Scene Perception
Authors: Wenhao Liao, Sineng Yan, Youqian Zhang, Xinwei Zhai, Yuanyuan Wang, Eugene Yujun Fu
Abstract: Autonomous vehicles rely on camera-based perception systems to comprehend their driving environment and make crucial decisions, thereby ensuring vehicles to steer safely. However, a significant threat known as Electromagnetic Signal Injection Attacks (ESIA) can distort the images captured by these cameras, leading to incorrect AI decisions and potentially compromising the safety of autonomous vehicles. Despite the serious implications of ESIA, there is limited understanding of its impacts on the robustness of AI models across various and complex driving scenarios. To address this gap, our research analyzes the performance of different models under ESIA, revealing their vulnerabilities to the attacks. Moreover, due to the challenges in obtaining real-world attack data, we develop a novel ESIA simulation method and generate a simulated attack dataset for different driving scenarios. Our research provides a comprehensive simulation and evaluation framework, aiming to enhance the development of more robust AI models and secure intelligent systems, ultimately contributing to the advancement of safer and more reliable technology across various fields.

Paper number 45:
Title: Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image Generation
Authors: Darius Petermann, Mahdi M. Kalayeh
Abstract: Training audio-to-image generative models requires an abundance of diverse audio-visual pairs that are semantically aligned. Such data is almost always curated from in-the-wild videos, given the cross-modal semantic correspondence that is inherent to them. In this work, we hypothesize that insisting on the absolute need for ground truth audio-visual correspondence, is not only unnecessary, but also leads to severe restrictions in scale, quality, and diversity of the data, ultimately impairing its use in the modern generative models. That is, we propose a scalable image sonification framework where instances from a variety of high-quality yet disjoint uni-modal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern vision-language models. To demonstrate the efficacy of this approach, we use our sonified images to train an audio-to-image generative model that performs competitively against state-of-the-art. Finally, through a series of ablation studies, we exhibit several intriguing auditory capabilities like semantic mixing and interpolation, loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process.

Paper number 46:
Title: LSTM Framework for Classification of Radar and Communications Signals
Authors: Victoria Clerico, Jorge Gonzalez-Lopez, Gady Agam, Jesus Grajal
Abstract: Although radar and communications signal classification are usually treated separately, they share similar characteristics, and methods applied in one domain can be potentially applied in the other. We propose a simple and unified scheme for the classification of radar and communications signals using Long Short-Term Memory (LSTM) neural networks. This proposal provides an improvement of the state of the art on radar signals where LSTM models are starting to be applied within schemes of higher complexity. To date, there is no standard public dataset for radar signals. Therefore, we propose DeepRadar2022, a radar dataset used in our systematic evaluations that is available publicly and will facilitate a standard comparison between methods.

Paper number 47:
Title: Learning Disentangled Speech Representations
Authors: Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann
Abstract: Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity. In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.

Paper number 48:
Title: Understanding Concepts in Graph Signal Processing for Neurophysiological Signal Analysis
Authors: Stephan Goerttler, Fei He, Min Wu
Abstract: Multivariate signals, which are measured simultaneously over time and acquired by sensor networks, are becoming increasingly common. The emerging field of graph signal processing (GSP) promises to analyse spectral characteristics of these multivariate signals, while at the same time taking the spatial structure between the time signals into account. A central idea in GSP is the graph Fourier transform, which projects a multivariate signal onto frequency-ordered graph Fourier modes, and can therefore be regarded as a spatial analog of the temporal Fourier transform. This chapter derives and discusses key concepts in GSP, with a specific focus on how the various concepts relate to one another. The experimental section focuses on the role of graph frequency in data classification, with applications to neuroimaging. To address the limited sample size of neurophysiological datasets, we introduce a minimalist simulation framework that can generate arbitrary amounts of data. Using this artificial data, we find that lower graph frequency signals are less suitable for classifying neurophysiological data as compared to higher graph frequency signals. Finally, we introduce a baseline testing framework for GSP. Employing this framework, our results suggest that GSP applications may attenuate spectral characteristics in the signals, highlighting current limitations of GSP for neuroimaging.

Paper number 49:
Title: Reconfigurable Intelligent Surface-Assisted Localization in OFDM Systems with Carrier Frequency Offset and Phase Noise
Authors: Hanfu Zhang, Erwu Liu, Rui Wang, Wei Ni, Zhe Xing, Yan Liu, Abbas Jamalipour
Abstract: Reconfigurable intelligent surface (RIS)-assisted communication systems have been extensively studied for providing high-precision location services. However, most studies have overlooked the impact of carrier frequency offset (CFO) and phase noise (PN) resulting from hardware impairments on localization. This paper presents a novel, alternating optimization (AO)-based algorithm to jointly estimate the CFO, PN, and user position, where, provided the user position, closed-form expressions for the CFO and PN are derived per iteration, significantly reducing the complexity and enhancing the stability of the algorithm. Another important aspect is a new RIS phase shift optimization algorithm developed to minimize the analytical lower bound of localization accuracy, hence benefiting localization. The semidefinite relaxation method and Schur complement are utilized to convexify this challenging non-convex optimization problem to a semidefinite program. Simulations demonstrate the effectiveness of the proposed algorithms, with the localization accuracy enhanced by two orders of magnitude. The localization accuracy of the proposed algorithm is close to the analytical lower bound, with a root mean square error of lower than $\rm 10^{-2} \: m$.

Paper number 50:
Title: HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model for Hearing Aids
Authors: Dyah A. M. G. Wisnu, Stefano Rini, Ryandhimas E. Zezario, Hsin-Min Wang, Yu Tsao
Abstract: This paper introduces HAAQI-Net, a non-intrusive deep learning-based music audio quality assessment model for hearing aid users. Unlike traditional methods like the Hearing Aid Audio Quality Index (HAAQI) that require intrusive reference signal comparisons, HAAQI-Net offers a more accessible and computationally efficient alternative. By utilizing a Bidirectional Long Short-Term Memory (BLSTM) architecture with attention mechanisms and features extracted from the pre-trained BEATs model, it can predict HAAQI scores directly from music audio clips and hearing loss patterns. Experimental results demonstrate HAAQI-Net's effectiveness, achieving a Linear Correlation Coefficient (LCC) of 0.9368 , a Spearman's Rank Correlation Coefficient (SRCC) of 0.9486 , and a Mean Squared Error (MSE) of 0.0064 and inference time significantly reduces from 62.52 to 2.54 seconds. To address computational overhead, a knowledge distillation strategy was applied, reducing parameters by 75.85% and inference time by 96.46%, while maintaining strong performance (LCC: 0.9071 , SRCC: 0.9307 , MSE: 0.0091 ). To expand its capabilities, HAAQI-Net was adapted to predict subjective human scores like the Mean Opinion Score (MOS) through fine-tuning. This adaptation significantly improved prediction accuracy, validated through statistical analysis. Furthermore, the robustness of HAAQI-Net was evaluated under varying Sound Pressure Level (SPL) conditions, revealing optimal performance at a reference SPL of 65 dB, with accuracy gradually decreasing as SPL deviated from this point. The advancements in subjective score prediction, SPL robustness, and computational efficiency position HAAQI-Net as a scalable solution for music audio quality assessment in hearing aid applications, contributing to efficient and accurate models in audio signal processing and hearing aid technology.

Paper number 51:
Title: LUPET: Incorporating Hierarchical Information Path into Multilingual ASR
Authors: Wei Liu, Jingyong Hou, Dong Yang, Muyong Cao, Tan Lee
Abstract: Toward high-performance multilingual automatic speech recognition (ASR), various types of linguistic information and model design have demonstrated their effectiveness independently. They include language identity (LID), phoneme information, language-specific processing modules, and cross-lingual self-supervised speech representation. It is expected that leveraging their benefits synergistically in a unified solution would further improve the overall system performance. This paper presents a novel design of a hierarchical information path, named LUPET, which sequentially encodes, from the shallow layers to deep layers, multiple aspects of linguistic and acoustic information at diverse granularity scales. The path starts from LID prediction, followed by acoustic unit discovery, phoneme sharing, and finally token recognition routed by a mixture-of-expert. ASR experiments are carried out on 10 languages in the Common Voice corpus. The results demonstrate the superior performance of LUPET as compared to the baseline systems. Most importantly, LUPET effectively mitigates the issue of performance compromise of high-resource languages with low-resource ones in the multilingual setting.

Paper number 52:
Title: Low-Complexity Control for a Class of Uncertain MIMO Nonlinear Systems under Generalized Time-Varying Output Constraints (extended version)
Authors: Farhad Mehdifar, Lars Lindemann, Charalampos P. Bechlioulis, Dimos V. Dimarogonas
Abstract: This paper introduces a novel control framework to address the satisfaction of multiple time-varying output constraints in uncertain high-order MIMO nonlinear control systems. Unlike existing methods, which often assume that the constraints are always decoupled and feasible, our approach can handle coupled time-varying constraints even in the presence of potential infeasibilities. First, it is shown that satisfying multiple constraints essentially boils down to ensuring the positivity of a scalar variable, representing the signed distance from the boundary of the time-varying output-constrained set. To achieve this, a single consolidating constraint is designed that, when satisfied, guarantees convergence to and invariance of the time-varying output-constrained set within a user-defined finite time. Next, a novel robust and low-complexity feedback controller is proposed to ensure the satisfaction of the consolidating constraint. Additionally, we provide a mechanism for online modification of the consolidating constraint to find a least violating solution when the constraints become mutually infeasible for some time. Finally, simulation examples of trajectory and region tracking for a mobile robot validate the proposed approach.

Paper number 53:
Title: Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs)
Authors: Buxin She, Hisham Mahmood, Marcelo Elizondo, Veronica Adetola, Yuqing Dong
Abstract: As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior. Accordingly, electromagnetic transient (EMT) simulation is essential to assess high frequency dynamics of the WECC system with integrated OWFs. Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software. The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT simulation. The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs. This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy.

Paper number 54:
Title: Systematic interval observer design for linear systems
Authors: Thach Ngoc Dinh, Gia Quoc Bao Tran
Abstract: We first propose systematic and comprehensive interval observer designs for linear time-invariant systems, under standard assumptions involving observability and interval bounds on the initial condition and disturbances. Historically, such designs rely on transformations with certain limitations into a form that is Metzler (for continuous time) or non-negative (for discrete time). We show that they can be effectively replaced with a linear time-invariant transformation that can be easily computed offline. Next, we propose an extension to the time-varying setting, addressing the limitations of conventional transformations that lack guaranteed outcomes. We employ dynamical transformations into higher-dimensional target forms for which an interval observer can always be constructed. These transformations become left-invertible after a certain time, provided observability conditions are met and the target dynamics are sufficiently high-dimensional and fast, thus enabling the reconstruction of bounds in the original coordinates in finite time. Academic examples are presented to illustrate our methods.

Paper number 55:
Title: Batteryless BLE and Light-based IoT Sensor Nodes for Reliable Environmental Sensing
Authors: Jimmy Fernandez Landivar, Khojiakbar Botirov, Hazem Sallouha, Marcos Katz, Sofie Pollin
Abstract: The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data. Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU). Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features. The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs). Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries. Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation. In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner.

Paper number 56:
Title: Decentralized Singular Value Decomposition for Large-scale Distributed Sensor Networks
Authors: Yufan Fan, Marius Pesavento
Abstract: This article studies the problem of decentralized Singular Value Decomposition (d-SVD), which is fundamental in various signal processing applications. Two scenarios are considered depending on the availability of the data matrix under consideration. In the first scenario, the matrix of interest is row-wisely available in each local node in the network. In the second scenario, the matrix of interest implicitly forms an outer product from two different series of measurements. By combining the lightweight local rational function approximation approach with parallel averaging consensus algorithms, two d-SVD algorithms are proposed to cope with the two aforementioned scenarios. We evaluate the proposed algorithms using two application examples: decentralized sensor localization via low-rank matrix completion and decentralized passive radar detection. Moreover, a novel and non-trivial truncation technique, which employs a representative vector that is orthonormal to the principal signal subspace, is proposed to further reduce the communication cost associated with the d-SVD algorithms. Simulation results show that the proposed d-SVD algorithms converge to the centralized solution with reduced communication cost compared to those facilitated with the state-of-the-art decentralized power method.

Paper number 57:
Title: Adaptive Probabilistic Planning for the Uncertain and Dynamic Orienteering Problem
Authors: Qiuchen Qian, Yanran Wang, David Boyle
Abstract: The Orienteering Problem (OP) is a well-studied routing problem that has been extended to incorporate uncertainties, reflecting stochastic or dynamic travel costs, prize-collection costs, and prizes. Existing approaches may, however, be inefficient in real-world applications due to insufficient modeling knowledge and initially unknowable parameters in online scenarios. Thus, we propose the Uncertain and Dynamic Orienteering Problem (UDOP), modeling travel costs as distributions with unknown and time-variant parameters. UDOP also associates uncertain travel costs with dynamic prizes and prize-collection costs for its objective and budget constraints. To address UDOP, we develop an ADaptive Approach for Probabilistic paThs - ADAPT, that iteratively performs 'execution' and 'online planning' based on an initial 'offline' solution. The execution phase updates system status and records online cost observations. The online planner employs a Bayesian approach to adaptively estimate power consumption and optimize path sequence based on safety beliefs. We evaluate ADAPT in a practical Unmanned Aerial Vehicle (UAV) charging scheduling problem for Wireless Rechargeable Sensor Networks. The UAV must optimize its path to recharge sensor nodes efficiently while managing its energy under uncertain conditions. ADAPT maintains comparable solution quality and computation time while offering superior robustness. Extensive simulations show that ADAPT achieves a 100% Mission Success Rate (MSR) across all tested scenarios, outperforming comparable heuristic-based and frequentist approaches that fail up to 70% (under challenging conditions) and averaging 67% MSR, respectively. This work advances the field of OP with uncertainties, offering a reliable and efficient approach for real-world applications in uncertain and dynamic environments.

Paper number 58:
Title: A Carryover Storage Valuation Framework for Medium-Term Cascaded Hydropower Planning: A Portland General Electric System Study
Authors: Xianbang Chen, Yikui Liu, Zhiming Zhong, Neng Fan, Zhechong Zhao, Lei Wu
Abstract: Medium-term planning of cascaded hydropower (CHP) determines appropriate carryover storage levels in reservoirs to optimize the usage of available water resources. This optimization seeks to maximize the hydropower generated in the current period (i.e., immediate benefit) plus the potential hydropower generation in the future period (i.e., future value). Thus, in the medium-term CHP planning, properly quantifying the future value deposited in carryover storage is essential to achieve a balanced trade-off between immediate benefit and future value. To this end, this paper presents a framework to quantify the future value of carryover storage, which consists of three major steps: i) constructing a model to calculate the maximum possible hydropower generation that a given level of carryover storage can deliver in the future period; ii) extracting the implicit locational marginal water value (LMWV) of carryover storage for each reservoir by applying a partition-then-extract algorithm to the constructed model; and iii) developing a set of analytical rules based on the extracted LMWV to effectively calculate the future value. These rules can be seamlessly integrated into medium-term CHP planning models as tractable mixed-integer linear constraints to quantify the future value properly, and can be easily visualized to offer valuable insights for CHP operators. Finally, numerical results on a CHP system of Portland General Electric demonstrate the effectiveness of the presented framework in determining proper carryover storage values to facilitate medium-term CHP planning.

Paper number 59:
Title: Comparison and calibration of MP2RAGE quantitative T1 values to multi-TI inversion recovery T1 values
Authors: Adam M. Saunders, Michael E. Kim, Chenyu Gao, Lucas W. Remedios, Aravind R. Krishnan, Kurt G. Schilling, Kristin P. O'Grady, Seth A. Smith, Bennett A. Landman
Abstract: While typical qualitative T1-weighted magnetic resonance images reflect scanner and protocol differences, quantitative T1 mapping aims to measure T1 independent of these effects. Changes in T1 in the brain reflect structural changes in brain tissue. Magnetization-prepared two rapid acquisition gradient echo (MP2RAGE) is an acquisition protocol that allows for efficient T1 mapping with a much lower scan time per slab compared to multi-TI inversion recovery (IR) protocols. We collect and register B1-corrected MP2RAGE acquisitions with an additional inversion time (MP3RAGE) alongside multi-TI selective inversion recovery acquisitions for four subjects. We use a maximum a posteriori (MAP) T1 estimation method for both MP2RAGE and compare to typical point estimate MP2RAGE T1 mapping, finding no bias from MAP MP2RAGE but a sensitivity to B1 inhomogeneities with MAP MP3RAGE. We demonstrate a tissue-dependent bias between MAP MP2RAGE T1 estimates and the multi-TI inversion recovery T1 values. To correct this bias, we train a patch-based ResNet-18 to calibrate the MAP MP2RAGE T1 estimates to the multi-TI IR T1 values. Across four folds, our network reduces the RMSE significantly (white matter: from 0.30 +/- 0.01 seconds to 0.11 +/- 0.02 seconds, subcortical gray matter: from 0.26 +/- 0.02 seconds to 0.10 +/- 0.02 seconds, cortical gray matter: from 0.36 +/- 0.02 seconds to 0.17 +/- 0.03 seconds). Using limited paired training data from both sequences, we can reduce the error between quantitative imaging methods and calibrate to one of the protocols with a neural network.

Paper number 60:
Title: Handover_Management_in_UAV_Networks_with_Blockages
Authors: Neetu R R, Gourab Ghatak, Vivek Ashok Bohara
Abstract: We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.

Paper number 61:
Title: Quantifying Metrics for Wildfire Ignition Risk from Geographic Data in Power Shutoff Decision-Making
Authors: Ryan Piansky, Sofia Taylor, Noah Rhodes, Daniel K. Molzahn, Line A. Roald, Jean-Paul Watson
Abstract: Faults on power lines and other electric equipment are known to cause wildfire ignitions. To mitigate the threat of wildfire ignitions from electric power infrastructure, many utilities preemptively de-energize power lines, which may result in power shutoffs. Data regarding wildfire ignition risks are key inputs for effective planning of power line de-energizations. However, there are multiple ways to formulate risk metrics that spatially aggregate wildfire risk map data, and there are different ways of leveraging this data to make decisions. The key contribution of this paper is to define and compare the results of employing six metrics for quantifying the wildfire ignition risks of power lines from risk maps, considering both threshold- and optimization-based methods for planning power line de-energizations. The numeric results use the California Test System (CATS), a large-scale synthetic grid model with power line corridors accurately representing California infrastructure, in combination with real Wildland Fire Potential Index data for a full year. This is the first application of optimal power shutoff planning on such a large and realistic test case. Our results show that the choice of risk metric significantly impacts the lines that are de-energized and the resulting load shed. We find that the optimization-based method results in significantly less load shed than the threshold-based method while achieving the same risk reduction.

Paper number 62:
Title: EndoPerfect: A Hybrid NeRF-Stereo Vision Approach Pioneering Monocular Depth Estimation and 3D Reconstruction in Endoscopy
Authors: Pengcheng Chen, Wenhao Li, Nicole Gunderson, Jeremy Ruthberg, Randall Bly, Zhenglong Sun, Waleed M. Abuzeid, Eric J. Seibel
Abstract: 3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional accuracy, with the mean error and standard deviation necessitating within the range of a single CT slice (0.625 mm), as the critical structures in the nasal cavity are situated within submillimeter distances from surgical instruments. This poses a formidable challenge when using conventional monocular endoscopes. Depth estimation is crucial for 3D reconstruction, yet existing depth estimation methodologies either suffer from inherent accuracy limitations or, in the case of learning-based approaches, perform poorly when applied to ESS despite succeeding on their original datasets. In this study, we present a novel, highly generalizable method that combines Neural Radiance Fields (NeRF) and stereo depth estimation for 3D reconstruction that can derive metric monocular depth. Our approach begins with an initial NeRF reconstruction yielding a coarse 3D scene, the subsequent creation of binocular pairs within coarse 3D scene, and generation of depth maps through stereo vision, These depth maps are used to supervise subsequent NeRF iteration, progressively refining NeRF and binocular depth, the refinement process continues until the depth maps converged. This recursive process generates high-accuracy depth maps from monocular endoscopic video. Evaluation in synthetic endoscopy shows a depth accuracy of 0.125 $\pm$ 0.443 mm, well within the 0.625 mm threshold. Further clinical experiments with real endoscopic data demonstrate a mean distance to CT mesh of 0.269 mm, representing the highest accuracy among monocular 3D reconstruction methods in ESS.

Paper number 63:
Title: Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint Localization and Mask Estimation
Authors: Jakob Kienegger, Alina Mannanova, Timo Gerkmann
Abstract: Due to their robustness and flexibility, neural-driven beamformers are a popular choice for speech separation in challenging environments with a varying amount of simultaneous speakers alongside noise and reverberation. Time-frequency masks and relative directions of the speakers regarding a fixed spatial grid can be used to estimate the beamformer's parameters. To some degree, speaker-independence is achieved by ensuring a greater amount of spatial partitions than speech sources. In this work, we analyze how to encode both mask and positioning into such a grid to enable joint estimation of both quantities. We propose mask-weighted spatial likelihood coding and show that it achieves considerable performance in both tasks compared to baseline encodings optimized for either localization or mask estimation. In the same setup, we demonstrate superiority for joint estimation of both quantities. Conclusively, we propose a universal approach which can replace an upstream sound source localization system solely by adapting the training framework, making it highly relevant in performance-critical scenarios.

Paper number 64:
Title: Optimal Routing and Link Configuration for Covert Heterogeneous Wireless Networks
Authors: Amna Gillani, Beatriz Lorenzo, Majid Ghaderi, Fikadu Dagefu, Dennis Goeckel
Abstract: Nodes in contemporary radio networks often have multiple interfaces available for communication: WiFi, cellular, LoRa, Zigbee, etc. This motivates understanding both link and network configuration when multiple communication modalities with vastly different capabilities are available to each node. In conjunction, covertness or the hiding of radio communications is often a significant concern in both commercial and military wireless networks. We consider the optimal routing problem in wireless networks when nodes have multiple interfaces available and intend to hide the presence of the transmission from attentive and capable adversaries. We first consider the maximization of the route capacity given an end-to-end covertness constraint against a single adversary and we find a polynomial-time algorithm for optimal route selection and link configuration. We further provide optimal polynomial-time algorithms for two important extensions: (i) statistical uncertainty during optimization about the channel state information for channels from system nodes to the adversary; and, (ii) maintaining covertness against multiple adversaries. Numerical results are included to demonstrate the gains of employing heterogeneous radio resources and to compare the performance of the proposed approach versus alternatives.

Paper number 65:
Title: Automotive Speed Estimation: Sensor Types and Error Characteristics from OBD-II to ADAS
Authors: Hany Ragab (1), Sidney Givigi (2), Aboelmagd Noureldin (1 and 2) ((1) Dept. of Electrical and Computer Engineering at Queens University and the NavINST Lab at the Royal Military College of Canada, (2) School of Computing at Queens University)
Abstract: Modern on-road navigation systems heavily depend on integrating speed measurements with inertial navigation systems (INS) and global navigation satellite systems (GNSS). Telemetry-based applications typically source speed data from the On-Board Diagnostic II (OBD-II) system. However, the method of deriving speed, as well as the types of sensors used to measure wheel speed, differs across vehicles. These differences result in varying error characteristics that must be accounted for in navigation and autonomy applications. This paper addresses this gap by examining the diverse speed-sensing technologies employed in standard automotive systems and alternative techniques used in advanced systems designed for higher levels of autonomy, such as Advanced Driver Assistance Systems (ADAS), Autonomous Driving (AD), or surveying applications. We propose a method to identify the type of speed sensor in a vehicle and present strategies for accurately modeling its error characteristics. To validate our approach, we collected and analyzed data from three long real road trajectories conducted in urban environments in Toronto and Kingston, Ontario, Canada. The results underscore the critical role of integrating multiple sensor modalities to achieve more accurate speed estimation, thus improving automotive navigation state estimation, particularly in GNSS-denied environments.

Paper number 66:
Title: Equity Impacts of Public Transit Network Redesign with Shared Autonomous Mobility Services
Authors: Max T.M. Ng, Meredith Raymer, Hani S. Mahmassani, Omer Verbas, Taner Cokyasar
Abstract: This study examines the equity impacts of integrating shared autonomous mobility services (SAMS) into transit system redesign. Using the Greater Chicago area as a case study, we compare two optimization objectives in multimodal transit network redesign: minimizing total generalized costs (equity-agnostic) versus prioritizing service in low-income areas (equity-focused). We evaluate the achieved accessibility of clustered zones with redesigned transit networks under two objectives, compared to driving and the existing transit network. The transit access gaps across zones and between transit and driving are found to be generally reduced with the introduction of SAMS, but less so with the subsequent improved infrastructure under budget. Differential improvement in equity is seen across suburbs and areas of the city, reflecting the disparity in current transit access and improvement potential. In particular, SAMS bridges the transit access gaps in suburban and city areas currently underserved by transit. The City of Chicago, which is also disproportionately home to vulnerable populations, offers an avenue to improve vertical equity. These findings demonstrate that SAMS can enhance both horizontal and vertical equity in transit systems, particularly when equity is explicitly incorporated into the design objective.

Paper number 67:
Title: tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation and Its Application in Medical Image Segmentation
Authors: Guanghua He, Wangang Cheng, Hancan Zhu, Xiaohao Cai, Gaohang Yu
Abstract: Transfer learning, by leveraging knowledge from pre-trained models, has significantly enhanced the performance of target tasks. However, as deep neural networks scale up, full fine-tuning introduces substantial computational and storage challenges in resource-constrained environments, limiting its widespread adoption. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed to reduce computational complexity and storage requirements by minimizing the number of updated parameters. While matrix decomposition-based PEFT methods, such as LoRA, show promise, they struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-dimensional tensors offer a more natural representation of neural network weights, allowing for a more comprehensive capture of higher-order features and multi-dimensional interactions. In this paper, we propose tCURLoRA, a novel fine-tuning method based on tensor CUR decomposition. By concatenating pre-trained weight matrices into a three-dimensional tensor and applying tensor CUR decomposition, we update only the lower-order tensor components during fine-tuning, effectively reducing computational and storage overhead. Experimental results demonstrate that tCURLoRA outperforms existing PEFT methods in medical image segmentation tasks.

Paper number 68:
Title: Gaming on Coincident Peak Shaving: Equilibrium and Strategic Behavior
Authors: Liudong Chen, Bolun Xu
Abstract: Coincident peak demand charges are imposed by power system operators or electric utilities when the overall system demand, aggregated across multiple consumers, reaches its peak. These charges incentivize consumers to reduce their demand during peak periods, a practice known as coincident peak shaving. In this paper, we analyze the coincident peak shaving problem through the lens of game theory, developing a theoretical model to examine the impact of strategic consumer behavior on system efficiency. We demonstrate that the game structure exhibits varying characteristics - concave, quasiconcave/discontinuous, or non-concave/discontinuous - depending on the extent of consumers demand-shifting capabilities. For a two-agent, two-period setting, we derive closed-form Nash equilibrium solutions under each condition and generalize our findings to cases with multiple agents. We prove the stability of the equilibrium points and present an algorithm for computing equilibrium outcomes across all game scenarios. We also show that the peak-shaving effectiveness of the game model matches that of the centralized peak-shaving model but with increased levels of anarchy. In the cases of quasiconcave and non-concave game conditions, we analytically demonstrate in the two-agent setting that anarchy increases with consumers' flexibility and inequity, as measured by their marginal shifting costs, and we also analyze the influence of the number of agents on anarchy. Finally, we provide numerical simulations to validate our theoretical results.

Paper number 69:
Title: Collaborative Spacecraft Servicing under Partial Feedback using Lyapunov-based Deep Neural Networks
Authors: Cristian F. Nino, Omkar Sudhir Patil, Christopher D. Petersen, Sean Phillips, Warren E. Dixon
Abstract: Multi-agent systems are increasingly applied in space missions, including distributed space systems, resilient constellations, and autonomous rendezvous and docking operations. A critical emerging application is collaborative spacecraft servicing, which encompasses on-orbit maintenance, space debris removal, and swarm-based satellite repositioning. These missions involve servicing spacecraft interacting with malfunctioning or defunct spacecraft under challenging conditions, such as limited state information, measurement inaccuracies, and erratic target behaviors. Existing approaches often rely on assumptions of full state knowledge or single-integrator dynamics, which are impractical for real-world applications involving second-order spacecraft dynamics. This work addresses these challenges by developing a distributed state estimation and tracking framework that requires only relative position measurements and operates under partial state information. A novel $\rho$-filter is introduced to reconstruct unknown states using locally available information, and a Lyapunov-based deep neural network adaptive controller is developed that adaptively compensates for uncertainties stemming from unknown spacecraft dynamics. To ensure the collaborative spacecraft regulation problem is well-posed, a trackability condition is defined. A Lyapunov-based stability analysis is provided to ensure exponential convergence of errors in state estimation and spacecraft regulation to a neighborhood of the origin under the trackability condition. The developed method eliminates the need for expensive velocity sensors or extensive pre-training, offering a practical and robust solution for spacecraft servicing in complex, dynamic environments.

Paper number 70:
Title: Regret Analysis: a control perspective
Authors: Travis E. Gibson, Sawal Acharya
Abstract: Online learning and model reference adaptive control have many interesting intersections. One area where they differ however is in how the algorithms are analyzed and what objective or metric is used to discriminate "good" algorithms from "bad" algorithms. In adaptive control there are usually two objectives: 1) prove that all time varying parameters/states of the system are bounded, and 2) that the instantaneous error between the adaptively controlled system and a reference system converges to zero over time (or at least a compact set). For online learning the performance of algorithms is often characterized by the regret the algorithm incurs. Regret is defined as the cumulative loss (cost) over time from the online algorithm minus the cumulative loss (cost) of the single optimal fixed parameter choice in hindsight. Another significant difference between the two areas of research is with regard to the assumptions made in order to obtain said results. Adaptive control makes assumptions about the input-output properties of the control problem and derives solutions for a fixed error model or optimization task. In the online learning literature results are derived for classes of loss functions (i.e. convex) while a priori assuming that all time varying parameters are bounded, which for many optimization tasks is not unrealistic, but is a non starter in control applications. In this work we discuss these differences in detail through the regret based analysis of gradient descent for convex functions and the control based analysis of a streaming regression problem. We close with a discussion about the newly defined paradigm of online adaptive control and ask the following question "Are regret optimal control strategies deployable?"

Paper number 71:
Title: COCOLA: Coherence-Oriented Contrastive Learning of Musical Audio Representations
Authors: Ruben Ciranni, Giorgio Mariani, Michele Mancusi, Emilian Postolache, Giorgio Fabbro, Emanuele Rodolà, Luca Cosmo
Abstract: We present COCOLA (Coherence-Oriented Contrastive Learning for Audio), a contrastive learning method for musical audio representations that captures the harmonic and rhythmic coherence between samples. Our method operates at the level of the stems composing music tracks and can input features obtained via Harmonic-Percussive Separation (HPS). COCOLA allows the objective evaluation of generative models for music accompaniment generation, which are difficult to benchmark with established metrics. In this regard, we evaluate recent music accompaniment generation models, demonstrating the effectiveness of the proposed method. We release the model checkpoints trained on public datasets containing separate stems (MUSDB18-HQ, MoisesDB, Slakh2100, and CocoChorales).

Paper number 72:
Title: Generative manufacturing systems using diffusion models and ChatGPT
Authors: Xingyu Li, Fei Tao, Wei Ye, Aydin Nassehi, John W. Sutherland
Abstract: In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.

Paper number 73:
Title: Formalising the intentional stance 1: attributing goals and beliefs to stochastic processes
Authors: Simon McGregor, timorl, Nathaniel Virgo
Abstract: This article presents a formalism inspired by Dennett's notion of the intentional stance. Whereas Dennett's treatment of these concepts is informal, we aim to provide a more formal analogue. We introduce a framework based on stochastic processes with inputs and outputs, in which we can talk precisely about *interpreting* systems as having *normative-epistemic states*, which combine belief-like and desire-like features. Our framework is based on optimality but nevertheless allows us to model some forms of bounded cognition. One might expect that the systems that can be described in normative-epistemic terms would be some special subset of all systems, but we show that this is not the case: every system admits a (possibly trivial) normative-epistemic interpretation, and those that can be *uniquely specified* by a normative-epistemic description are exactly the deterministic ones. Finally, we show that there is a suitable notion of Bayesian updating for normative-epistemic states, which we call *value-laden filtering*, since it involves both normative and epistemic elements. For unbounded cognition it is always permissible to attribute beliefs that update in this way. This is not always the case for bounded cognition, but we give a sufficient condition under which it is. This paper gives an overview of our framework aimed at cognitive scientists, with a formal mathematical treatment given in a companion paper.

Paper number 74:
Title: Convex Optimization of Initial Perturbations toward Quantitative Weather Control
Authors: Toshiyuki Ohtsuka, Atsushi Okazaki, Masaki Ogura, Shunji Kotsuki
Abstract: This study proposes introducing convex optimization to find initial perturbations of atmospheric states to realize specified changes in subsequent weather. In the proposed method, we formulate and solve an inverse problem to find effective perturbations in atmospheric variables so that controlled variables satisfy specified changes at a specified time. The proposed method first constructs a sensitivity matrix of controlled variables, such as accumulated precipitation, to the initial atmospheric variables, such as temperature and humidity, through sensitivity analysis using a numerical weather prediction (NWP) model. Then a convex optimization problem is formulated to achieve various control specifications involving not only quadratic functions but also absolute values and maximum values of the controlled variables and initial atmospheric variables in the cost function and constraints. The proposed method was validated through a benchmark warm bubble experiment using the NWP model. The experiments showed that the identified perturbations successfully realized specified spatial distributions of accumulated precipitation.

Paper number 75:
Title: Towards Unsupervised Speech Recognition Without Pronunciation Models
Authors: Junrui Ni, Liming Wang, Yang Zhang, Kaizhi Qian, Heting Gao, Mark Hasegawa-Johnson, Chang D. Yoo
Abstract: Recent advancements in supervised automatic speech recognition (ASR) have achieved remarkable performance, largely due to the growing availability of large transcribed speech corpora. However, most languages lack sufficient paired speech and text data to effectively train these systems. In this article, we tackle the challenge of developing ASR systems without paired speech and text corpora by proposing the removal of reliance on a phoneme lexicon. We explore a new research direction: word-level unsupervised ASR, and experimentally demonstrate that an unsupervised speech recognizer can emerge from joint speech-to-speech and text-to-text masked token-infilling. Using a curated speech corpus containing a fixed number of English words, our system iteratively refines the word segmentation structure and achieves a word error rate of between 20-23%, depending on the vocabulary size, without parallel transcripts, oracle word boundaries, or a pronunciation lexicon. This innovative model surpasses the performance of previous unsupervised ASR models under the lexicon-free setting.

Paper number 76:
Title: EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech Recognition Architecture with High Accuracy and Inference Speed
Authors: Ziyang Zhuang, Chenfeng Miao, Kun Zou, Ming Fang, Tao Wei, Zijian Li, Ning Cheng, Wei Hu, Shaojun Wang, Jing Xiao
Abstract: Non-autoregressive (NAR) automatic speech recognition (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. In this paper, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EffectiveASR. It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference. It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss. The proposed EffectiveASR achieves competitive results on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the leading models. Specifically, it achieves character error rates (CER) of 4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR Conformer with about 30x inference speedup.

Paper number 77:
Title: Parameter Training Efficiency Aware Resource Allocation for AIGC in Space-Air-Ground Integrated Networks
Authors: Liangxin Qian, Jun Zhao
Abstract: With the evolution of artificial intelligence-generated content (AIGC) techniques and the development of space-air-ground integrated networks (SAGIN), there will be a growing opportunity to enhance more users' mobile experience with customized AIGC applications. This is made possible through the use of parameter-efficient fine-tuning (PEFT) training alongside mobile edge computing. In this paper, we formulate the optimization problem of maximizing the parameter training efficiency of the SAGIN system over wireless networks under limited resource constraints. We propose the Parameter training efficiency Aware Resource Allocation (PARA) technique to jointly optimize user association, data offloading, and communication and computational resource allocation. Solid proofs are presented to solve this difficult sum of ratios problem based on quadratically constrained quadratic programming (QCQP), semidefinite programming (SDP), graph theory, and fractional programming (FP) techniques. Our proposed PARA technique is effective in finding a stationary point of this non-convex problem. The simulation results demonstrate that the proposed PARA method outperforms other baselines.

Paper number 78:
Title: Audio-Language Datasets of Scenes and Events: A Survey
Authors: Gijs Wijngaard, Elia Formisano, Michele Esposito, Michel Dumontier
Abstract: Audio-language models (ALMs) generate linguistic descriptions of sound-producing events and scenes. Advances in dataset creation and computational power have led to significant progress in this domain. This paper surveys 69 datasets used to train ALMs, covering research up to September 2024 (this https URL). It provides a comprehensive analysis of datasets origins, audio and linguistic characteristics, and use cases. Key sources include YouTube-based datasets like AudioSet with over two million samples, and community platforms like Freesound with over 1 million samples. Through principal component analysis of audio and text embeddings, the survey evaluates the acoustic and linguistic variability across datasets. It also analyzes data leakage through CLAP embeddings, and examines sound category distributions to identify imbalances. Finally, the survey identifies key challenges in developing large, diverse datasets to enhance ALM performance, including dataset overlap, biases, accessibility barriers, and the predominance of English-language content, while highlighting opportunities for improvement.

Paper number 79:
Title: GSVD-NMF: Recovering Missing Features in Non-negative Matrix Factorization
Authors: Youdong Guo, Timothy E. Holy
Abstract: Non-negative matrix factorization (NMF) is an important tool in signal processing and widely used to separate mixed sources into their components. Algorithms for NMF require that the user choose the number of components in advance, and if the results are unsatisfying one typically needs to start again with a different number of components. To make NMF more interactive and incremental, here we introduce GSVD-NMF, a method that proposes new components based on the generalized singular value decomposition (GSVD) to address discrepancies between the initial under-complete NMF results and the SVD of the original matrix. Simulation and experimental results demonstrate that GSVD-NMF often effectively recovers multiple missing components in under-complete NMF, with the recovered NMF solutions frequently reaching better local optima. The results further show that GSVD-NMF is compatible with various NMF algorithms and that directly augmenting components is more efficient than rerunning NMF from scratch with additional components. By deliberately starting from under-complete NMF, GSVD-NMF has the potential to be a recommended approach for a range of general NMF applications.

Paper number 80:
Title: Deep Learning-Based Automatic Multi-Level Airway Collapse Monitoring on Obstructive Sleep Apnea Patients
Authors: Ying-Chieh Hsu, Stanley Yung-Chuan Liu, Chao-Jung Huang, Chi-Wei Wu, Ren-Kai Cheng, Jane Yung-Jen Hsu, Shang-Ran Huang, Yuan-Ren Cheng, Fu-Shun Hsu
Abstract: This study investigated the use of deep learning to identify multi-level upper airway collapses in obstructive sleep apnea (OSA) patients based on snoring sounds. We fi-ne-tuned ResNet-50 and Audio Spectrogram Transformer (AST) models using snoring recordings from 37 subjects undergoing drug-induced sleep endoscopy (DISE) between 2020 and 2021. Snoring sounds were labeled according to the VOTE (Velum, Orophar-ynx, Tongue Base, Epiglottis) classification, resulting in 259 V, 403 O, 77 T, 13 E, 1016 VO, 46 VT, 140 OT, 39 OE, 30 VOT, and 3150 non-snoring (N) 0.5-second clips. The models were trained for two multi-label classification tasks: identifying obstructions at V, O, T, and E levels, and identifying retropalatal (RP) and retroglossal (RG) obstruc-tions. Results showed AST slightly outperformed ResNet-50, demonstrating good abil-ity to identify V (F1-score: 0.71, MCC: 0.61, AUC: 0.89), O (F1-score: 0.80, MCC: 0.72, AUC: 0.94), and RP obstructions (F1-score: 0.86, MCC: 0.77, AUC: 0.97). However, both models struggled with T, E, and RG classifications due to limited data. Retrospective analysis of a full-night recording showed the potential to profile airway obstruction dynamics. We expect this information, combined with polysomnography and other clinical parameters, can aid clinical triage and treatment planning for OSA patients.

Paper number 81:
Title: LSTM-QGAN: Scalable NISQ Generative Adversarial Network
Authors: Cheng Chu, Aishwarya Hastak, Fan Chen
Abstract: Current quantum generative adversarial networks (QGANs) still struggle with practical-sized data. First, many QGANs use principal component analysis (PCA) for dimension reduction, which, as our studies reveal, can diminish the QGAN's effectiveness. Second, methods that segment inputs into smaller patches processed by multiple generators face scalability issues. In this work, we propose LSTM-QGAN, a QGAN architecture that eliminates PCA preprocessing and integrates quantum long short-term memory (QLSTM) to ensure scalable performance. Our experiments show that LSTM-QGAN significantly enhances both performance and scalability over state-of-the-art QGAN models, with visual data improvements, reduced Frechet Inception Distance scores, and reductions of 5x in qubit counts, 5x in single-qubit gates, and 12x in two-qubit gates.

Paper number 82:
Title: FlowSep: Language-Queried Sound Separation with Rectified Flow Matching
Authors: Yi Yuan, Xubo Liu, Haohe Liu, Mark D. Plumbley, Wenwu Wang
Abstract: Language-queried audio source separation (LASS) focuses on separating sounds using textual descriptions of the desired sources. Current methods mainly use discriminative approaches, such as time-frequency masking, to separate target sounds and minimize interference from other sources. However, these models face challenges when separating overlapping soundtracks, which may lead to artifacts such as spectral holes or incomplete separation. Rectified flow matching (RFM), a generative model that establishes linear relations between the distribution of data and noise, offers superior theoretical properties and simplicity, but has not yet been explored in sound separation. In this work, we introduce FlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space. During inference, the RFM-generated latent features are reconstructed into a mel-spectrogram via the pre-trained VAE decoder, followed by a pre-trained vocoder to synthesize the waveform. Trained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art models across multiple benchmarks, as evaluated with subjective and objective metrics. Additionally, our results show that FlowSep surpasses a diffusion-based LASS model in both separation quality and inference efficiency, highlighting its strong potential for audio source separation tasks. Code, pre-trained models and demos can be found at: this https URL .

Paper number 83:
Title: AccentBox: Towards High-Fidelity Zero-Shot Accent Generation
Authors: Jinzuomu Zhong, Korin Richmond, Zhiba Su, Siqi Sun
Abstract: While recent Zero-Shot Text-to-Speech (ZS-TTS) models have achieved high naturalness and speaker similarity, they fall short in accent fidelity and control. To address this issue, we propose zero-shot accent generation that unifies Foreign Accent Conversion (FAC), accented TTS, and ZS-TTS, with a novel two-stage pipeline. In the first stage, we achieve state-of-the-art (SOTA) on Accent Identification (AID) with 0.56 f1 score on unseen speakers. In the second stage, we condition a ZS-TTS system on the pretrained speaker-agnostic accent embeddings extracted by the AID model. The proposed system achieves higher accent fidelity on inherent/cross accent generation, and enables unseen accent generation.

Paper number 84:
Title: MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder
Authors: Khai Le-Duc, Phuc Phan, Tan-Hanh Pham, Bach Phan Tat, Minh-Huong Ngo, Truong-Son Hy
Abstract: Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study, a layer-wise ablation study for the AED, and a linguistic analysis for multilingual medical ASR. All code, data, and models are available online: this https URL

Paper number 85:
Title: Multi-class Decoding of Attended Speaker Direction Using Electroencephalogram and Audio Spatial Spectrum
Authors: Yuanming Zhang, Jing Lu, Fei Chen, Haoliang Du, Xia Gao, Zhibin Lin
Abstract: Decoding the directional focus of an attended speaker from listeners' electroencephalogram (EEG) signals is essential for developing brain-computer interfaces to improve the quality of life for individuals with hearing impairment. Previous works have concentrated on binary directional focus decoding, i.e., determining whether the attended speaker is on the left or right side of the listener. However, a more precise decoding of the exact direction of the attended speaker is necessary for effective speech processing. Additionally, audio spatial information has not been effectively leveraged, resulting in suboptimal decoding results. In this paper, it is found that on the recently presented dataset with 14-class directional focus, models relying exclusively on EEG inputs exhibit significantly lower accuracy when decoding the directional focus in both leave-one-subject-out and leave-one-trial-out scenarios. By integrating audio spatial spectra with EEG features, the decoding accuracy can be effectively improved. The CNN, LSM-CNN, and Deformer models are employed to decode the directional focus from listeners' EEG signals and audio spatial spectra. The proposed Sp-EEG-Deformer model achieves notable 14-class decoding accuracies of 55.35% and 57.19% in leave-one-subject-out and leave-one-trial-out scenarios with a decision window of 1 second, respectively. Experiment results indicate increased decoding accuracy as the number of alternative directions reduces. These findings suggest the efficacy of our proposed dual modal directional focus decoding strategy.

Paper number 86:
Title: CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for Code-Switching Speech Recognition
Authors: He Wang, Xucheng Wan, Naijun Zheng, Kai Liu, Huan Zhou, Guojian Li, Lei Xie
Abstract: Code-switching automatic speech recognition (ASR) aims to transcribe speech that contains two or more languages accurately. To better capture language-specific speech representations and address language confusion in code-switching ASR, the mixture-of-experts (MoE) architecture and an additional language diarization (LD) decoder are commonly employed. However, most researches remain stagnant in simple operations like weighted summation or concatenation to fuse languagespecific speech representations, leaving significant opportunities to explore the enhancement of integrating language bias information. In this paper, we introduce CAMEL, a cross-attention-based MoE and language bias approach for code-switching ASR. Specifically, after each MoE layer, we fuse language-specific speech representations with cross-attention, leveraging its strong contextual modeling abilities. Additionally, we design a source attention-based mechanism to incorporate the language information from the LD decoder output into text embeddings. Experimental results demonstrate that our approach achieves state-of-the-art performance on the SEAME, ASRU200, and ASRU700+LibriSpeech460 Mandarin-English code-switching ASR datasets.

Paper number 87:
Title: Feasibility of short blocklength Reed-Muller codes for physical layer security in real environment
Authors: Md Munibun Billah, Tyler Sweat, Willie K. Harrison
Abstract: In this paper, we investigate the application of Reed-Muller (RM) codes for Physical-layer security in a real world wiretap channel scenario. Utilizing software-defined radios (SDRs) in a real indoor environment, we implement a coset coding scheme that leverages the hierarchical structure of RM codes to secure data transmission. The generator matrix of the RM code is used to partition codewords into cosets in the usual way, where each message corresponds to a unique coset, and auxiliary bits select specific codewords within each coset. This approach enables the legitimate receiver (Bob) can decode the transmitted message with minimal information leakage to eavesdropper (Eve) thus protecting the confidentiality of the communication with the help of coset structure. Mutual information neural estimation (MINE) is used to quantify information leakage and validate the effectiveness of the scheme. Experimental results indicate that RM codes can achieve robust security even in practical environments affected by real-world channel impairments. These findings demonstrate the potential of RM codes as an efficient solution for physical-layer security, particularly for applications that require low latency and short blocklengths.

Paper number 88:
Title: Right Label Context in End-to-End Training of Time-Synchronous ASR Models
Authors: Tina Raissi, Ralf Schlüter, Hermann Ney
Abstract: Current time-synchronous sequence-to-sequence automatic speech recognition (ASR) models are trained by using sequence level cross-entropy that sums over all alignments. Due to the discriminative formulation, incorporating the right label context into the training criterion's gradient causes normalization problems and is not mathematically well-defined. The classic hybrid neural network hidden Markov model (NN-HMM) with its inherent generative formulation enables conditioning on the right label context. However, due to the HMM state-tying the identity of the right label context is never modeled explicitly. In this work, we propose a factored loss with auxiliary left and right label contexts that sums over all alignments. We show that the inclusion of the right label context is particularly beneficial when training data resources are limited. Moreover, we also show that it is possible to build a factored hybrid HMM system by relying exclusively on the full-sum criterion. Experiments were conducted on Switchboard 300h and LibriSpeech 960h.
    