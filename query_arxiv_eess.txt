
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation
Authors: Daniel Cieślak, Miriam Reca, Olena Onyshchenko, Jacek Rumiński
Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa significant challenge, crucial for effective wound management and automatedassessment. In this study, we propose a novel dual-attention U-Net++ archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms toaddress severe class imbalance and variability in medical images this http URL, extensive benchmarking across diverse architectures and encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal encoder this http URL, we independently trained two class-specific models with tailoredpreprocessing, extensive data augmentation, and Bayesian hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test Time Augmentationto further enhance prediction reliability. Our approach was evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition. Segmentationperformance was quantified using a weighted F1-score (75% wounds, 25% scalemarkers), calculated externally by competition organizers on undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640, underscoring itseffectiveness for complex medical segmentation tasks.

Paper number 2:
Title: PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT
Authors: Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu
Abstract: Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM.

Paper number 3:
Title: Comparative Analysis of Finite Difference and Finite Element Method for Audio Waveform Simulation
Authors: Juliette Florin
Abstract: In many industries, including aerospace and defense, waveform analysis is commonly conducted to compute the resonance of physical objects, with the Finite Element Method (FEM) being the standard approach. The Finite Difference Method (FDM) is seldom used, and this preference is often stated without formal justification in the literature. In this work, the accuracy, feasibility, and time of simulation of FEM and FDM are compared by simulating the vibration of a guitar string. Python simulations for both methods are implemented, and their results are compared against analytical solutions and experimental data. Additionally, FDM is applied to analyze the sound of a cycling bell to assess its reliability compared to a real cycling bell. Final results show that both FEM and FDM yield similar error margins and accurately predict the system's behavior. Moreover, the errors from FEM and FDM follow the same periodicity with a phase shift when varying the assumed analytical tension and without a phase shift when changing the time interval. However, FEM converges faster with increasing mesh complexity, whereas FDM demonstrates quicker computational performance and achieves stable solutions even with bigger time intervals. Despite this FDM is limited to simpler configurations and often demands extensive mathematical formulation, which can become cumbersome for intricate shapes. For example, modeling a hemispherical object using FDM results in significant simulation times and big calculations. In conclusion, while FDM may offer faster convergence and computation time in certain cases, FEM remains the preferred method in industrial contexts due to its flexibility, scalability, and ease of implementation for complex geometries.

Paper number 4:
Title: Sample Rate Offset Compensated Acoustic Echo Cancellation For Multi-Device Scenarios
Authors: Srikanth Korse, Oliver Thiergart, Emanuel A. P. Habets
Abstract: Acoustic echo cancellation (AEC) in multi-device scenarios is a challenging problem due to sample rate offset (SRO) between devices. The SRO hinders the convergence of the AEC filter, diminishing its performance. To address this , we approach the multi-device AEC scenario as a multi-channel AEC problem involving a multi-channel Kalman filter, SRO estimation, and resampling of far-end signals. Experiments in a two-device scenario show that our system mitigates the divergence of the multi-channel Kalman filter in the presence of SRO for both correlated and uncorrelated playback signals during echo-only and double-talk. Additionally, for devices with correlated playback signals, an independent single-channel AEC filter is crucial to ensure fast convergence of SRO estimation.

Paper number 5:
Title: Stereo Reproduction in the Presence of Sample Rate Offsets
Authors: Srikanth Korse, Andreas Walther, Emanuel A. P. Habets
Abstract: One of the main challenges in synchronizing wirelessly connected loudspeakers for spatial audio reproduction is clock skew. Clock skew arises from sample rate offsets ( SROs) between the loudspeakers, caused by the use of independent device clocks. While network-based protocols like Precision Time Protocol (PTP) and Network Time Protocol (NTP) are explored, the impact of SROs on spatial audio reproduction and its perceptual consequences remains underexplored. We propose an audio-domain SRO compensation method using spatial filtering to isolate loudspeaker contributions. These filtered signals, along with the original playback signal, are used to estimate the SROs, and their influence is compensated for prior to spatial audio reproduction. We evaluate the effect of the compensation method in a subjective listening test. The results of these tests as well as objective metrics demonstrate that the proposed method mitigates the perceptual degradation introduced by SROs by preserving the spatial cues.

Paper number 6:
Title: Parametric Object Coding in IVAS: Efficient Coding of Multiple Audio Objects at Low Bit Rates
Authors: Andrea Eichenseer, Srikanth Korse, Guillaume Fuchs, Markus Multrus
Abstract: The recently standardized 3GPP codec for Immersive Voice and Audio Services (IVAS) includes a parametric mode for efficiently coding multiple audio objects at low bit rates. In this mode, parametric side information is obtained from both the object metadata and the input audio objects. The side information comprises directional information, indices of two dominant objects, and the power ratio between these two dominant objects. It is transmitted to the decoder along with a stereo downmix. In IVAS, parametric object coding allows for transmitting three or four arbitrarily placed objects at bit rates of 24.4 or 32 kbit/s and faithfully reconstructing the spatial image of the original audio scene. Subjective listening tests confirm that IVAS provides a comparable immersive experience at lower bit rate and complexity compared to coding the audio objects independently using Enhanced Voice Services (EVS).

Paper number 7:
Title: A Compositional Approach to Diagnosing Faults in Cyber-Physical Systems
Authors: Josefine B. Graebener, Inigo Incer, Richard M. Murray
Abstract: Identifying the cause of a system-level failure in a cyber-physical system (CPS) can be like tracing a needle in a haystack. This paper approaches the problem by assuming that the CPS has been designed compositionally and that each component in the system is associated with an assume-guarantee contract. We exploit recent advances in contract-based design that show how to compute the contract for the entire system using the component-level contracts. When presented with a system-level failure, our approach is able to efficiently identify the components that are responsible for the system-level failure together with the specific predicates in those components' specifications that are involved in the fault. We implemented this approach using Pacti and demonstrate it through illustrative examples inspired by an autonomous vehicle in the DARPA urban challenge.

Paper number 8:
Title: Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging
Authors: Lijie Huang, Jingyi Yin, Jingke Zhang, U-Wai Lok, Ryan M. DeRuiter, Jieyang Jin, Kate M. Knoll, Kendra E. Petersen, James D. Krier, Xiang-yang Zhu, Gina K. Hesley, Kathryn A. Robinson, Andrew J. Bentall, Thomas D. Atwell, Andrew D. Rule, Lilach O. Lerman, Shigao Chen, Chengwu Huang
Abstract: Ultrasound microvascular imaging (UMI) is often hindered by low signal-to-noise ratio (SNR), especially in contrast-free or deep tissue scenarios, which impairs subsequent vascular quantification and reliable disease diagnosis. To address this challenge, we propose Half-Angle-to-Half-Angle (HA2HA), a self-supervised denoising framework specifically designed for UMI. HA2HA constructs training pairs from complementary angular subsets of beamformed radio-frequency (RF) blood flow data, across which vascular signals remain consistent while noise varies. HA2HA was trained using in-vivo contrast-free pig kidney data and validated across diverse datasets, including contrast-free and contrast-enhanced data from pig kidneys, as well as human liver and kidney. An improvement exceeding 15 dB in both contrast-to-noise ratio (CNR) and SNR was observed, indicating a substantial enhancement in image quality. In addition to power Doppler imaging, denoising directly in the RF domain is also beneficial for other downstream processing such as color Doppler imaging (CDI). CDI results of human liver derived from the HA2HA-denoised signals exhibited improved microvascular flow visualization, with a suppressed noisy background. HA2HA offers a label-free, generalizable, and clinically applicable solution for robust vascular imaging in both contrast-free and contrast-enhanced UMI.

Paper number 9:
Title: Risk-Aware Aerocapture Guidance Through a Probabilistic Indicator Function
Authors: Grace E. Calkins, Jay W. McMahon, Alireza Doostan, David C. Woffinden
Abstract: Aerocapture is sensitive to trajectory errors, particularly for low-cost missions with imprecise navigation. For such missions, considering the probability of each failure mode when computing guidance commands can increase performance. A risk-aware aerocapture guidance algorithm is proposed that uses a generative-modeling-based probabilistic indicator function to estimate escape, impact, or capture probabilities. The probability of each mode is incorporated into corrective guidance commands to increase the likelihood of successful capture. The proposed method is evaluated against state-of-the-art numeric predictor-corrector guidance algorithms in high-uncertainty scenarios where entry interface dispersions lead to nontrivial failure probabilities. When using a probabilistic indicator function in guidance, 69% to 100% of recoverable cases are saved in near-escape and near-impact scenarios. In addition, the probabilistic indicator is compared to a first-order fading memory filter for density estimation, showing improvements in apoapsis error even when a fading filter is included. The probabilistic indicator function can also accurately predict failure probability for dispersions outside its training data, showing generalizability. The proposed risk-aware aerocapture guidance algorithm improves capture performance and robustness to entry interface state dispersions, especially for missions with high navigation uncertainty.

Paper number 10:
Title: Constraint Hypergraphs as a Unifying Framework for Digital Twins
Authors: John Morris, Douglas L. Van Bossuyt, Edward Louis, Gregory Mocko, John Wagner
Abstract: Digital twins, used to represent physical systems, have been lauded as tools for understanding reality. Complex system behavior is typically captured in domain-specific models crafted by subject experts. Contemporary methods for employing models in a digital twin require prescriptive interfaces, resulting in twins that are difficult to connect, redeploy, and modify. The limited interoperability of these twins has prompted calls for a universal framework enabling observability across model aggregations. Here we show how a new mathematical formalism called a constraint hypergraph serves as such a framework by representing system behavior as the composition of set-based functions. A digital twin is shown to be the second of two coupled systems where both adhere to the same constraint hypergraph, permitting the properties of the first to be observable from the second. Interoperability is given by deconstructing models into a structure enabling autonomous, white-box simulation of system properties. The resulting digital twins can interact immediately with both human and autonomous agents. This is demonstrated in a case study of a microgrid, showing how both measured and simulated data from the aggregated twins can be provided regardless of the operating environment. By connecting models, constraint hypergraphs supply scientists and modelers robust means to capture, communicate, and combine digital twins across all fields of study. We expect this framework to expand the use of digital twins, enriching scientific insights and collaborations by providing a structure for characterizing complex systems.

Paper number 11:
Title: Basic Computations in Fault Tree Analysis
Authors: Hamid Jahanian
Abstract: Fault Tree Analysis (FTA) is a well-established method in failure analysis and is widely used in safety and reliability assessments. While FTA tools enable users to manage complex analyses effectively, they can sometimes obscure the underlying calculation processes. As a result, the soundness of FTA results often hinges on the user's expertise and familiarity with the methodology and the tool. This paper aims to explore the fundamental principles underlying both qualitative and quantitative FTA analyses, while addressing broader conceptual considerations such as coherence and consensus. By developing a deeper understanding of these concepts, engineers can improve their ability to interpret, verify, and make informed use of the outputs generated by FTA tools. This paper does not propose a novel concept in FTA but aims to compile and present a concise overview of the fundamental computations in FTA.

Paper number 12:
Title: Learning Segmentation from Radiology Reports
Authors: Pedro R. A. S. Bassi, Wenxuan Li, Jieneng Chen, Zheren Zhu, Tianyu Lin, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou
Abstract: Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis, yet segmentation masks are scarce because their creation requires time and expertise. Public abdominal CT datasets have from dozens to a couple thousand tumor masks, but hospitals have hundreds of thousands of tumor CTs with radiology reports. Thus, leveraging reports to improve segmentation is key for scaling. In this paper, we propose a report-supervision loss (R-Super) that converts radiology reports into voxel-wise supervision for tumor segmentation AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital), and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our R-Super to train with these masks and reports, and strongly improved tumor segmentation in internal and external validation--F1 Score increased by up to 16% with respect to training with masks only. By leveraging readily available radiology reports to supplement scarce segmentation masks, R-Super strongly improves AI performance both when very few training masks are available (e.g., 50), and when many masks were available (e.g., 1.7K). Project: this https URL

Paper number 13:
Title: MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses
Authors: Yang Liu, Li Wan, Yiteng Huang, Yong Xu, yangyang shi, Saurabh Adya, ming sun, Florian Metze
Abstract: Smart glasses are increasingly positioned as the next-generation interface for ubiquitous access to large language models (LLMs). Nevertheless, achieving reliable interaction in real-world noisy environments remains a major challenge, particularly due to interference from side speech. In this work, we introduce a novel side-talk rejection multi-microphone Whisper (MMW) framework for smart glasses, incorporating three key innovations. First, we propose a Mix Block based on a Tri-Mamba architecture to effectively fuse multi-channel audio at the raw waveform level, while maintaining compatibility with streaming processing. Second, we design a Frame Diarization Mamba Layer to enhance frame-level side-talk suppression, facilitating more efficient fine-tuning of Whisper models. Third, we employ a Multi-Scale Group Relative Policy Optimization (GRPO) strategy to jointly optimize frame-level and utterance-level side speech suppression. Experimental evaluations demonstrate that the proposed MMW system can reduce the word error rate (WER) by 4.95\% in noisy conditions.

Paper number 14:
Title: Iterative Sparse Asymptotic Minimum Variance Based Channel Estimation in Fluid Antenna System
Authors: Zhen Chen, Jianqing Li, Xiu Yin Zhang, Kai-Kit Wong, Chan-Byoung Chae, Yangyang Zhang
Abstract: With fluid antenna system (FAS) gradually establishing itself as a possible enabling technology for next generation wireless communications, channel estimation for FAS has become a pressing issue. Existing methodologies however face limitations in noise suppression. To overcome this, in this paper, we propose a maximum likelihood (ML)-based channel estimation approach tailored for FAS systems, designed to mitigate noise interference and enhance estimation accuracy. By capitalizing on the inherent sparsity of wireless channels, we integrate an ML-based iterative tomographic algorithm to systematically reduce noise perturbations during the channel estimation process. Furthermore, the proposed approach leverages spatial correlation within the FAS channel to optimize estimation accuracy and spectral efficiency. Simulation results confirm the efficacy of the proposed method, demonstrating superior channel estimation accuracy and robustness compared to existing benchmark techniques.

Paper number 15:
Title: Frequency-Specific Neural Response and Cross-Correlation Analysis of Envelope Following Responses to Native Speech and Music Using Multichannel EEG Signals: A Case Study
Authors: Md. Mahbub Hasan, Md Rakibul Hasan, Md Zakir Hossain, Tom Gedeon
Abstract: Although native speech and music envelope following responses (EFRs) play a crucial role in auditory processing and cognition, their frequency profile, such as the dominating frequency and spectral coherence, is largely unknown. We have assumed that the auditory pathway - which transmits envelope components of speech and music to the scalp through time-varying neurophysiological processes - is a linear time-varying system, with the envelope and the multi-channel EEG responses as excitation and response, respectively. This paper investigates the transfer function of this system through two analytical techniques - time-averaged spectral responses and cross-spectral density - in the frequency domain at four different positions of the human scalp. Our findings suggest that alpha (8-11 Hz), lower gamma (53-56 Hz), and higher gamma (78-81 Hz) bands are the peak responses of the system. These frequently appearing dominant frequency responses may be the key components of familiar speech perception, maintaining attention, binding acoustic features, and memory processing. The cross-spectral density, which reflects the spatial neural coherence of the human brain, shows that 10-13 Hz, 27-29 Hz, and 62-64 Hz are common for all channel pairs. As neural coherences are frequently observed in these frequencies among native participants, we suggest that these distributed neural processes are also dominant in native speech and music perception.

Paper number 16:
Title: Diffusion-Based Limited-Angle CT Reconstruction under Noisy Conditions
Authors: Jiaqi Guo, Santiago López-Tapia
Abstract: Limited-Angle Computed Tomography (LACT) is a challenging inverse problem where missing angular projections lead to incomplete sinograms and severe artifacts in the reconstructed images. While recent learning-based methods have demonstrated effectiveness, most of them assume ideal, noise-free measurements and fail to address the impact of measurement noise. To overcome this limitation, we treat LACT as a sinogram inpainting task and propose a diffusion-based framework that completes missing angular views using a Mean-Reverting Stochastic Differential Equation (MR-SDE) formulation. To improve robustness under realistic noise, we propose RNSD$^+$, a novel noise-aware rectification mechanism that explicitly models inference-time uncertainty, enabling reliable and robust reconstruction. Extensive experiments demonstrate that our method consistently surpasses baseline models in data consistency and perceptual quality, and generalizes well across varying noise intensity and acquisition scenarios.

Paper number 17:
Title: ADPv2: A Hierarchical Histological Tissue Type-Annotated Dataset for Potential Biomarker Discovery of Colorectal Disease
Authors: Zhiyuan Yang, Kai Li, Sophia Ghamoshi Ramandi, Patricia Brassard, Hakim Khellaf, Vincent Quoc-Huy Trinh, Jennifer Zhang, Lina Chen, Corwyn Rowsell, Sonal Varma, Kostas Plataniotis, Mahdi S. Hosseini
Abstract: Computational pathology (CoPath) leverages histopathology images to enhance diagnostic precision and reproducibility in clinical pathology. However, publicly available datasets for CoPath that are annotated with extensive histological tissue type (HTT) taxonomies at a granular level remain scarce due to the significant expertise and high annotation costs required. Existing datasets, such as the Atlas of Digital Pathology (ADP), address this by offering diverse HTT annotations generalized to multiple organs, but limit the capability for in-depth studies on specific organ diseases. Building upon this foundation, we introduce ADPv2, a novel dataset focused on gastrointestinal histopathology. Our dataset comprises 20,004 image patches derived from healthy colon biopsy slides, annotated according to a hierarchical taxonomy of 32 distinct HTTs of 3 levels. Furthermore, we train a multilabel representation learning model following a two-stage training procedure on our ADPv2 dataset. We leverage the VMamba architecture and achieving a mean average precision (mAP) of 0.88 in multilabel classification of colon HTTs. Finally, we show that our dataset is capable of an organ-specific in-depth study for potential biomarker discovery by analyzing the model's prediction behavior on tissues affected by different colon diseases, which reveals statistical patterns that confirm the two pathological pathways of colon cancer development. Our dataset is publicly available here: Part 1 at this https URL, Part 2 at this https URL and Part 3 at this https URL

Paper number 18:
Title: Robust One-step Speech Enhancement via Consistency Distillation
Authors: Liang Xu, Longfei Felix Yan, W. Bastiaan Kleijn
Abstract: Diffusion models have shown strong performance in speech enhancement, but their real-time applicability has been limited by multi-step iterative sampling. Consistency distillation has recently emerged as a promising alternative by distilling a one-step consistency model from a multi-step diffusion-based teacher model. However, distilled consistency models are inherently biased towards the sampling trajectory of the teacher model, making them less robust to noise and prone to inheriting inaccuracies from the teacher model. To address this limitation, we propose ROSE-CD: Robust One-step Speech Enhancement via Consistency Distillation, a novel approach for distilling a one-step consistency model. Specifically, we introduce a randomized learning trajectory to improve the model's robustness to noise. Furthermore, we jointly optimize the one-step model with two time-domain auxiliary losses, enabling it to recover from teacher-induced errors and surpass the teacher model in overall performance. This is the first pure one-step consistency distillation model for diffusion-based speech enhancement, achieving 54 times faster inference speed and superior performance compared to its 30-step teacher model. Experiments on the VoiceBank-DEMAND dataset demonstrate that the proposed model achieves state-of-the-art performance in terms of speech quality. Moreover, its generalization ability is validated on both an out-of-domain dataset and real-world noisy recordings.

Paper number 19:
Title: Sparsity-Promoting Dynamic Mode Decomposition Applied to Sea Surface Temperature Fields
Authors: Zhicheng Zhang, Yoshihiko Susuki, Atsushi Okazaki
Abstract: In this paper, we leverage Koopman mode decomposition to analyze the nonlinear and high-dimensional climate systems acting on the observed data space. The dynamics of atmospheric systems are assumed to be equation-free, with the linear evolution of observables derived from measured historical long-term time-series data snapshots, such as monthly sea surface temperature records, to construct a purely data-driven climate dynamics. In particular, sparsity-promoting dynamic mode decomposition is exploited to extract the dominant spatial and temporal modes, which are among the most significant coherent structures underlying climate variability, enabling a more efficient, interpretable, and low-dimensional representation of the system dynamics. We hope that the combined use of Koopman modes and sparsity-promoting techniques will provide insights into the significant climate modes, enabling reduced-order modeling of the climate system and offering a potential framework for predicting and controlling weather and climate variability.

Paper number 20:
Title: ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark
Authors: He Wang, Linhan Ma, Dake Guo, Xiong Wang, Lei Xie, Jin Xu, Junyang Lin
Abstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet prior evaluative efforts have largely been restricted to contextless paradigms. This constraint stems from the limited proficiency of conventional ASR models in context modeling and their deficiency in memory and reasoning based on world knowledge. Recent breakthroughs in the development of Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of general artificial intelligence capabilities. Consequently, there exists a compelling need for a benchmark that can evaluate both the generality and intelligence of ASR systems. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess contextual speech recognition. This benchmark encompasses up to 40,000 data entries across over 10 domains, enabling a thorough evaluation of model performance in scenarios that omit or incorporate coarse-grained or fine-grained contextual information. Moreover, diverging from conventional ASR evaluations, our benchmark includes an analysis of model efficacy in recognizing named entities mentioned within the auditory input. Our extensive evaluation highlights that LALMs, with strong world knowledge and context learning capabilities, outperform conventional ASR models by a large margin. The dataset and evaluation code have been released at this https URL.

Paper number 21:
Title: Tissue Concepts v2: a Supervised Foundation Model for whole slide images
Authors: Till Nicke, Daniela Scharcherer, Jan Raphael Schäfer, Natalia Artysh, Antje Prasse, André Homeyer, Andrea Schenk, Henning Höfener, Johannes Lotz
Abstract: Foundation models (FMs) are transforming the field of computational pathology by offering new approaches to analyzing histopathology images. Typically relying on weeks of training on large databases, the creation of FMs is a resource-intensive process in many ways. In this paper, we introduce the extension of our supervised foundation model, Tissue Concepts, to whole slide images, called Tissue Concepts v2 (TCv2), a supervised foundation model for whole slide images to address the issue above. TCv2 uses supervised, end-to-end multitask learning on slide-level labels. Training TCv2 uses a fraction of the training resources compared to self-supervised training. The presented model shows superior performance compared to SSL-trained models in cancer subtyping benchmarks and is fully trained on freely available data. Furthermore, a shared trained attention module provides an additional layer of explainability across different tasks.

Paper number 22:
Title: PSAT: Pediatric Segmentation Approaches via Adult Augmentations and Transfer Learning
Authors: Tristan Kirscher (ICube, ICANS), Sylvain Faisan (ICube), Xavier Coubez (ICANS), Loris Barrier (ICANS), Philippe Meyer (ICube, ICANS)
Abstract: Pediatric medical imaging presents unique challenges due to significant anatomical and developmental differences compared to adults. Direct application of segmentation models trained on adult data often yields suboptimal performance, particularly for small or rapidly evolving structures. To address these challenges, several strategies leveraging the nnU-Net framework have been proposed, differing along four key axes: (i) the fingerprint dataset (adult, pediatric, or a combination thereof) from which the Training Plan -including the network architecture-is derived; (ii) the Learning Set (adult, pediatric, or mixed), (iii) Data Augmentation parameters, and (iv) the Transfer learning method (finetuning versus continual learning). In this work, we introduce PSAT (Pediatric Segmentation Approaches via Adult Augmentations and Transfer learning), a systematic study that investigates the impact of these axes on segmentation performance. We benchmark the derived strategies on two pediatric CT datasets and compare them with state-of-theart methods, including a commercial radiotherapy solution. PSAT highlights key pitfalls and provides actionable insights for improving pediatric segmentation. Our experiments reveal that a training plan based on an adult fingerprint dataset is misaligned with pediatric anatomy-resulting in significant performance degradation, especially when segmenting fine structures-and that continual learning strategies mitigate institutional shifts, thus enhancing generalization across diverse pediatric datasets. The code is available at this https URL.

Paper number 23:
Title: Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning
Authors: Jian Kai, Tianwei Zhang, Zihan Ling, Yang Cao, Can Shen
Abstract: Accurate bandwidth estimation (BWE) is critical for real-time communication (RTC) systems. Traditional heuristic approaches offer limited adaptability under dynamic networks, while online reinforcement learning (RL) suffers from high exploration costs and potential service disruptions. Offline RL, which leverages high-quality data collected from real-world environments, offers a promising alternative. However, challenges such as out-of-distribution (OOD) actions, policy extraction from behaviorally diverse datasets, and reliable deployment in production systems remain unsolved. We propose RBWE, a robust bandwidth estimation framework based on offline RL that integrates Q-ensemble (an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD risks and enhance policy learning. A fallback mechanism ensures deployment stability by switching to heuristic methods under high uncertainty. Experimental results show that RBWE reduces overestimation errors by 18% and improves the 10th percentile Quality of Experience (QoE) by 18.6%, demonstrating its practical effectiveness in real-world RTC applications.

Paper number 24:
Title: Just Say Better or Worse: A Human-AI Collaborative Framework for Medical Image Segmentation Without Manual Annotations
Authors: Yizhe Zhang
Abstract: Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback -- simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images.

Paper number 25:
Title: Assessing Linear Control Strategies for Zero-Speed Fin Roll Damping
Authors: Nikita Savin, Elena Ambrosovskaya, Dmitry Romaev, Anton Proskurnikov
Abstract: Roll stabilization is a critical aspect of ship motion control, particularly for vessels operating in low-speed or zero-speed conditions, where traditional hydrodynamic fins lose their effectiveness. In this paper, we consider a roll damping system, developed by Navis JSC, based on two actively controlled zero-speed fins. Unlike conventional fin stabilizers, zero-speed fins employ a drag-based mechanism and active oscillations to generate stabilizing forces even when the vessel is stationary. We propose a simple linear control architecture that, however, accounts for nonlinear drag forces and actuator limitations. Simulation results on a high-fidelity vessel model used for HIL testing demonstrate the effectiveness of the proposed approach.

Paper number 26:
Title: A novel framework for fully-automated co-registration of intravascular ultrasound and optical coherence tomography imaging data
Authors: Xingwei He, Kit Mills Bransby, Ahmet Emir Ulutas, Thamil Kumaran, Nathan Angelo Lecaros Yap, Gonul Zeren, Hesong Zeng, Yaojun Zhang, Andreas Baumbach, James Moon, Anthony Mathur, Jouke Dijkstra, Qianni Zhang, Lorenz Raber, Christos V Bourantas
Abstract: Aims: To develop a deep-learning (DL) framework that will allow fully automated longitudinal and circumferential co-registration of intravascular ultrasound (IVUS) and optical coherence tomography (OCT) images. Methods and results: Data from 230 patients (714 vessels) with acute coronary syndrome that underwent near-infrared spectroscopy (NIRS)-IVUS and OCT imaging in their non-culprit vessels were included in the present analysis. The lumen borders annotated by expert analysts in 61,655 NIRS-IVUS and 62,334 OCT frames, and the side branches and calcific tissue identified in 10,000 NIRS-IVUS frames and 10,000 OCT frames, were used to train DL solutions for the automated extraction of these features. The trained DL solutions were used to process NIRS-IVUS and OCT images and their output was used by a dynamic time warping algorithm to co-register longitudinally the NIRS-IVUS and OCT images, while the circumferential registration of the IVUS and OCT was optimized through dynamic programming. On a test set of 77 vessels from 22 patients, the DL method showed high concordance with the expert analysts for the longitudinal and circumferential co-registration of the two imaging sets (concordance correlation coefficient >0.99 for the longitudinal and >0.90 for the circumferential co-registration). The Williams Index was 0.96 for longitudinal and 0.97 for circumferential co-registration, indicating a comparable performance to the analysts. The time needed for the DL pipeline to process imaging data from a vessel was <90s. Conclusion: The fully automated, DL-based framework introduced in this study for the co-registration of IVUS and OCT is fast and provides estimations that compare favorably to the expert analysts. These features renders it useful in research in the analysis of large-scale data collected in studies that incorporate multimodality imaging to characterize plaque composition.

Paper number 27:
Title: Performance Analysis of Linear Detection under Noise-Dependent Fast-Fading Channels
Authors: Almutasem Bellah Enad, Jihad Fahs, Hadi Sarieddeen, Hakim Jemaa, Tareq Y. Al-Naffouri
Abstract: This paper presents a performance analysis framework for linear detection in fast-fading channels with possibly correlated channel and noise. The framework is both accurate and adaptable, making it well-suited for analyzing a wide range of channel and noise models. As such, it serves as a valuable tool for the design and evaluation of detection algorithms in next-generation wireless communication systems. By characterizing the distribution of the effective noise after zero-forcing filtering, we derive a semi-analytical and asymptotic expression for the symbol error rate under Rayleigh fading and channel-dependent additive circular complex Gaussian noise. The proposed approach demonstrates excellent agreement with integration-based benchmarks as confirmed by numerical simulations thus validating its accuracy. The framework is flexible and can be extended to various channel and noise models, offering a valuable tool for the design and analysis of detection algorithms in next-generation communication systems.

Paper number 28:
Title: Low voltage user phase reconfiguration as a planning problem
Authors: Sari Kerckhove, Marta Vanin, Reinhilde D'hulst, Dirk Van Hertem
Abstract: Considerable levels of phase imbalance in low voltage (LV) distribution networks imply that grid assets are suboptimally utilized and can cause additional losses, equipment failure and degradation. With the ongoing energy transition, the installation of additional single-phase distributed energy resources may further increase the phase imbalance if no countermeasures are taken. Phase reconfiguration is a cost-effective solution to reduce imbalance. However, dynamic reconfiguration, through real-time phase swapping of loads using remotely controlled switches, is often impractical because these switches are too costly for widespread installation at LV users. Approaching phase reconfiguration as a planning problem, i.e. static reconfiguration, is an underaddressed but promising alternative. Effective static approaches that allow appropriate imbalance objectives are currently lacking. This paper presents reliable and expressive static phase reconfiguration methods that grid operators can easily integrate into routine maintenance for effective phase balancing. We present and compare three static methods, an exact mixed-integer nonlinear formulation (MINLP), a mixed-integer quadratic approximation (MIQP), and a genetic algorithm (GA), each supporting different imbalance objectives. The MIQP approach, despite using proxy objectives, efficiently mitigates the different types of imbalance considered, and outperforms both MINLP and GA in scalability and consistency.

Paper number 29:
Title: Optimal Placement of Smart Hybrid Transformers in Distribution Networks
Authors: Samuel Hayward, Martin Doff-Sotta, Michael Merlin, Matthew Williams, Thomas Morstyn
Abstract: Hybrid transformers are a relatively new technology that combine conventional power transformers with power electronics to provide voltage and reactive power control capabilities in distribution networks. This paper proposes a novel method of determining the optimal location and utilisation of hybrid transformers in 3-phase distribution networks to maximise the net present value of hybrid transformers based on their ability to increase the export of power produced by distributed generators over their operational lifespan. This has been accomplished through sequential linear programming, a key feature of which is the consideration of nonlinear characteristics and constraints relating to hybrid transformer power electronics and control capabilities. Test cases were carried out in a modified version of the Cigre European Low Voltage Distribution Network Benchmark, which has been extended by connecting it with two additional low voltage distribution test networks. All test case results demonstrate that the installation and utilisation of hybrid transformers can improve the income earned from exporting excess active power, justifying their installation cost (with the highest net present value being £6.56 million, resulting from a 45.53 percent increase in estimated annual profits due to coordinated HT compensation).

Paper number 30:
Title: A Differential Evolution Algorithm with Neighbor-hood Mutation for DOA Estimation
Authors: Bo Zhou, Kaijie Xu, Yinghui Quan, Mengdao Xing
Abstract: Two-dimensional (2D) Multiple Signal Classification algorithm is a powerful technique for high-resolution direction-of-arrival (DOA) estimation in array signal processing. However, the exhaustive search over the 2D an-gular domain leads to high computa-tional cost, limiting its applicability in real-time scenarios. In this work, we reformulate the peak-finding process as a multimodal optimization prob-lem, and propose a Differential Evolu-tion algorithm with Neighborhood Mutation (DE-NM) to efficiently lo-cate multiple spectral peaks without requiring dense grid sampling. Simu-lation results demonstrate that the proposed method achieves comparable estimation accuracy to the traditional grid search, while significantly reduc-ing computation time. This strategy presents a promising solution for real-time, high-resolution DOA estimation in practical applications. The imple-mentation code is available at this https URL.

Paper number 31:
Title: RIS-Enabled Transmitter Design for Joint Radar and Communication
Authors: Emanuele Grossi, Marco Lops, Luca Venturino
Abstract: Achieving efficient and cost-effective transmit beampattern control for integrated sensing and communication (ISAC) systems is a significant challenge. This paper addresses this by proposing a dual-function radar communication (DFRC) transmitter based on a reconfigurable intelligent surface (RIS) illuminated by a limited number of active sources. We formulate and solve the joint design of source waveforms and RIS phase shifts to match a desired space-frequency radiation pattern, and we evaluate the resulting ISAC system's performance in terms of radar detection probability and data transmission rate. Numerical results demonstrate the promising capabilities of this RIS-enabled transmitter for ISAC applications.

Paper number 32:
Title: Secure Communication of UAV-mounted STAR-RIS under Phase Shift Errors
Authors: Aseel Qsibat, Habiba Akhleifa, Abdelhamid Salem, Khaled Rabie, Xingwang Li, Thokozani Shongwe, Mohamad A. Alawad, Yazeed Alkhrijah
Abstract: This paper investigates the secure communication capabilities of a non-orthogonal multiple access (NOMA) network supported by a STAR-RIS (simultaneously transmitting and reflecting reconfigurable intelligent surface) deployed on an unmanned aerial vehicle (UAV), in the presence of passive eavesdroppers. The STAR-RIS facilitates concurrent signal reflection and transmission, allowing multiple legitimate users-grouped via NOMA-to be served efficiently, thereby improving spectral utilization. Each user contends with an associated eavesdropper, creating a stringent security scenario. Under Nakagami fading conditions and accounting for phase shift inaccuracies in the STAR-RIS, closed-form expressions for the ergodic secrecy rates of users in both transmission and reflection paths are derived. An optimization framework is then developed to jointly adjust the UAV's positioning and the STAR-RIS power splitting coefficient, aiming to maximize the system's secrecy rate. The proposed approach enhances secure transmission in STAR-RIS-NOMA configurations under realistic hardware constraints and offers valuable guidance for the design of future 6G wireless networks.

Paper number 33:
Title: AI-based Environment-Aware XL-MIMO Channel Estimation with Location-Specific Prior Knowledge Enabled by CKM
Authors: Yuelong Qiu, Di Wu, Yong Zeng, Yanqun Tang, Nan Cheng, Chenhao Qi
Abstract: Accurate and efficient acquisition of wireless channel state information (CSI) is crucial to enhance the communication performance of wireless systems. However, with the continuous densification of wireless links, increased channel dimensions, and the use of higher-frequency bands, channel estimation in the sixth generation (6G) and beyond wireless networks faces new challenges, such as insufficient orthogonal pilot sequences, inadequate signal-to-noise ratio (SNR) for channel training, and more sophisticated channel statistical distributions in complex environment. These challenges pose significant difficulties for classical channel estimation algorithms like least squares (LS) and maximum a posteriori (MAP). To address this problem, we propose a novel environment-aware channel estimation framework with location-specific prior channel distribution enabled by the new concept of channel knowledge map (CKM). To this end, we propose a new type of CKM called channel score function map (CSFM), which learns the channel probability density function (PDF) using artificial intelligence (AI) techniques. To fully exploit the prior information in CSFM, we propose a plug-and-play (PnP) based algorithm to decouple the regularized MAP channel estimation problem, thereby reducing the complexity of the optimization process. Besides, we employ Tweedie's formula to establish a connection between the channel score function, defined as the logarithmic gradient of the channel PDF, and the channel denoiser. This allows the use of the high-precision, environment-aware channel denoiser from the CSFM to approximate the channel score function, thus enabling efficient processing of the decoupled channel statistical components. Simulation results show that the proposed CSFM-PnP based channel estimation technique significantly outperforms the conventional techniques in the aforementioned challenging scenarios.

Paper number 34:
Title: Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration
Authors: Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr
Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative imaging due to its rapid acquisition and low radiation dose. However, CBCT images typically suffer from artifacts and lower visual quality compared to conventional Computed Tomography (CT). A promising solution is synthetic CT (sCT) generation, where CBCT volumes are translated into the CT domain. In this work, we enhance sCT generation through multimodal learning by jointly leveraging intraoperative CBCT and preoperative CT data. To overcome the inherent misalignment between modalities, we introduce an end-to-end learnable registration module within the sCT pipeline. This model is evaluated on a controlled synthetic dataset, allowing precise manipulation of data quality and alignment parameters. Further, we validate its robustness and generalizability on two real-world clinical datasets. Experimental results demonstrate that integrating registration in multimodal sCT generation improves sCT quality, outperforming baseline multimodal methods in 79 out of 90 evaluation settings. Notably, the improvement is most significant in cases where CBCT quality is low and the preoperative CT is moderately misaligned.

Paper number 35:
Title: LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models
Authors: Zhihao Chen, Tao Chen, Chenhui Wang, Qi Gao, Huidong Xie, Chuang Niu, Ge Wang, Hongming Shan
Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on this https URL.

Paper number 36:
Title: Dynamic Slimmable Networks for Efficient Speech Separation
Authors: Mohamed Elminshawi, Srikanth Raj Chetupalli, Emanuël A. P. Habets
Abstract: Recent progress in speech separation has been largely driven by advances in deep neural networks, yet their high computational and memory requirements hinder deployment on resource-constrained devices. A significant inefficiency in conventional systems arises from using static network architectures that maintain constant computational complexity across all input segments, regardless of their characteristics. This approach is sub-optimal for simpler segments that do not require intensive processing, such as silence or non-overlapping speech. To address this limitation, we propose a dynamic slimmable network (DSN) for speech separation that adaptively adjusts its computational complexity based on the input signal. The DSN combines a slimmable network, which can operate at different network widths, with a lightweight gating module that dynamically determines the required width by analyzing the local input characteristics. To balance performance and efficiency, we introduce a signal-dependent complexity loss that penalizes unnecessary computation based on segmental reconstruction error. Experiments on clean and noisy two-speaker mixtures from the WSJ0-2mix and WHAM! datasets show that the DSN achieves a better performance-efficiency trade-off than individually trained static networks of different sizes.

Paper number 37:
Title: Cross-Subject DD: A Cross-Subject Brain-Computer Interface Algorithm
Authors: Xiaoyuan Li, Xinru Xue, Bohan Zhang, Ye Sun, Shoushuo Xi, Gang Liu
Abstract: Brain-computer interface (BCI) based on motor imagery (MI) enables direct control of external devices by decoding the electroencephalogram (EEG) generated in the brain during imagined movements. However, due to inter-individual variability in brain activity, existing BCI models exhibit poor adaptability across subjects, thereby limiting their generalizability and widespread application. To address this issue, this paper proposes a cross-subject BCI algorithm named Cross-Subject DD (CSDD), which constructs a universal BCI model by extracting common features across subjects. The specific methods include: 1) training personalized models for each subject; 2) transforming personalized models into relation spectrums; 3) identifying common features through statistical analysis; and 4) constructing a cross-subject universal model based on common features. The experiments utilized the BCIC IV 2a dataset, involving nine subjects. Eight of these subjects were selected for training and extracing the common features, and the cross-subject decoding performance of the model was validated on the remaining subject. The results demonstrate that, compared with existing similar methods, our approach achieves a 3.28% improvement in performance. This paper introduces for the first time a novel method for extracting pure common features and constructing a universal cross-subject BCI model, thereby facilitating broader applications of BCI technology.

Paper number 38:
Title: Introducing Image-Space Preconditioning in the Variational Formulation of MRI Reconstructions
Authors: Bastien Milani, Jean-Baptist Ledoux, Berk Can Acikgoz, Xavier Richard
Abstract: The aim of the present article is to enrich the comprehension of iterative magnetic resonance imaging (MRI) reconstructions, including compressed sensing (CS) and iterative deep learning (DL) reconstructions, by describing them in the general framework of finite-dimensional inner-product spaces. In particular, we show that image-space preconditioning (ISP) and data-space preconditioning (DSP) can be formulated as non-conventional inner-products. The main gain of our reformulation is an embedding of ISP in the variational formulation of the MRI reconstruction problem (in an algorithm-independent way) which allows in principle to naturally and systematically propagate ISP in all iterative reconstructions, including many iterative DL and CS reconstructions where preconditioning is lacking. The way in which we apply linear algebraic tools to MRI reconstructions as presented in this article is a novelty. A secondary aim of our article is to offer a certain didactic material to scientists who are new in the field of MRI reconstruction. Since we explore here some mathematical concepts of reconstruction, we take that opportunity to recall some principles that may be understood for experts, but which may be hard to find in the literature for beginners. In fact, the description of many mathematical tools of MRI reconstruction is fragmented in the literature or sometimes missing because considered as a general knowledge. Further, some of those concepts can be found in mathematic manuals, but not in a form that is oriented toward MRI. For example, we think of the conjugate gradient descent, the notion of derivative with respect to non-conventional inner products, or simply the notion of adjoint. The authors believe therefore that it is beneficial for their field of research to dedicate some space to such a didactic material.

Paper number 39:
Title: From General to Specialized: The Need for Foundational Models in Agriculture
Authors: Vishal Nedungadi, Xingguo Xiong, Aike Potze, Ron Van Bree, Tao Lin, Marc Rußwurm, Ioannis N. Athanasiadis
Abstract: Food security remains a global concern as population grows and climate change intensifies, demanding innovative solutions for sustainable agricultural productivity. Recent advances in foundation models have demonstrated remarkable performance in remote sensing and climate sciences, and therefore offer new opportunities for agricultural monitoring. However, their application in challenges related to agriculture-such as crop type mapping, crop phenology estimation, and crop yield estimation-remains under-explored. In this work, we quantitatively evaluate existing foundational models to assess their effectivity for a representative set of agricultural tasks. From an agricultural domain perspective, we describe a requirements framework for an ideal agricultural foundation model (CropFM). We then survey and compare existing general-purpose foundational models in this framework and empirically evaluate two exemplary of them in three representative agriculture specific tasks. Finally, we highlight the need for a dedicated foundational model tailored specifically to agriculture.

Paper number 40:
Title: Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration
Authors: Jose M. Montero, Jose-Luis Lisani
Abstract: Recent advances in deep learning, particularly neural networks, have significantly impacted a wide range of fields, including the automatic enhancement of underwater images. This paper presents a deep learning-based approach to improving underwater image quality by integrating human subjective assessments into the training process. To this end, we utilize publicly available datasets containing underwater images labeled by experts as either high or low quality. Our method involves first training a classifier network to distinguish between high- and low-quality images. Subsequently, generative adversarial networks (GANs) are trained using various enhancement criteria to refine the low-quality images. The performance of the GAN models is evaluated using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through qualitative analysis. Results demonstrate that the proposed model -- particularly when incorporating criteria such as color fidelity and image sharpness -- achieves substantial improvements in both perceived and measured image quality.

Paper number 41:
Title: Baton: Compensate for Missing Wi-Fi Features for Practical Device-free Tracking
Authors: Yiming Zhao, Xuanqi Meng, Xinyu Tong, Xiulong Liu, Xin Xie, Wenyu Qu
Abstract: Wi-Fi contact-free sensing systems have attracted widespread attention due to their ubiquity and convenience. The integrated sensing and communication (ISAC) technology utilizes off-the-shelf Wi-Fi communication signals for sensing, which further promotes the deployment of intelligent sensing applications. However, current Wi-Fi sensing systems often require prolonged and unnecessary communication between transceivers, and brief communication interruptions will lead to significant performance degradation. This paper proposes Baton, the first system capable of accurately tracking targets even under severe Wi-Fi feature deficiencies. To be specific, we explore the relevance of the Wi-Fi feature matrix from both horizontal and vertical dimensions. The horizontal dimension reveals feature correlation across different Wi-Fi links, while the vertical dimension reveals feature correlation among different time slots. Based on the above principle, we propose the Simultaneous Tracking And Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi features over time and across different links, akin to passing a baton. We implement the system on commercial devices, and the experimental results show that our system outperforms existing solutions with a median tracking error of 0.46m, even when the communication duty cycle is as low as 20.00%. Compared with the state-of-the-art, our system reduces the tracking error by 79.19% in scenarios with severe Wi-Fi feature deficiencies.

Paper number 42:
Title: Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration
Authors: Yuyang Hu, Kangfu Mei, Mojtaba Sahraee-Ardakan, Ulugbek S. Kamilov, Peyman Milanfar, Mauricio Delbracio
Abstract: Diffusion models show promise for image restoration, but existing methods often struggle with inconsistent fidelity and undesirable artifacts. To address this, we introduce Kernel Density Steering (KDS), a novel inference-time framework promoting robust, high-fidelity outputs through explicit local mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples, computing patch-wise kernel density estimation gradients from their collective outputs. These gradients steer patches in each particle towards shared, higher-density regions identified within the ensemble. This collective local mode-seeking mechanism, acting as "collective wisdom", steers samples away from spurious modes prone to artifacts, arising from independent sampling or model imperfections, and towards more robust, high-fidelity structures. This allows us to obtain better quality samples at the expense of higher compute by simultaneously sampling multiple particles. As a plug-and-play framework, KDS requires no retraining or external verifiers, seamlessly integrating with various diffusion samplers. Extensive numerical validations demonstrate KDS substantially improves both quantitative and qualitative performance on challenging real-world super-resolution and image inpainting tasks.

Paper number 43:
Title: Adaptive Linearly Constrained Minimum Variance Volumetric Active Noise Control
Authors: Manan Mittal, Ryan M. Corey, Andrew C. Singer
Abstract: Traditional volumetric noise control typically relies on multipoint error minimization to suppress sound energy across a region, but offers limited flexibility in shaping spatial responses. This paper introduces a time-domain formulation for linearly constrained minimum variance active noise control (LCMV ANC) for spatial control filter design. We demonstrate how the LCMV ANC optimization framework allows system designers to prioritize noise reduction at specific spatial locations through strategically defined linear constraints, providing a more flexible alternative to uniformly weighted multipoint error minimization. An adaptive algorithm based on filtered-X least mean squares (FxLMS) is derived for online adaptation of filter coefficients. Simulation and experimental results validate the proposed method's noise reduction and constraint adherence, demonstrating effective, spatially selective, and broadband noise control compared to multipoint volumetric noise control.

Paper number 44:
Title: Beamforming with Random Projections: Upper and Lower Bounds
Authors: Manan Mittal, Ryan M. Corey, Andrew C. Singer
Abstract: Beamformers often trade off white noise gain against the ability to suppress interferers. With distributed microphone arrays, this trade-off becomes crucial as different arrays capture vastly different magnitude and phase differences for each source. We propose the use of multiple random projections as a first-stage preprocessing scheme in a data-driven approach to dimensionality reduction and beamforming. We show that a mixture beamformer derived from the use of multiple such random projections can effectively outperform the minimum variance distortionless response (MVDR) beamformer in terms of signal-to-noise ratio (SNR) and signal-to-interferer-and-noise ratio (SINR) gain. Moreover, our method introduces computational complexity as a trade-off in the design of adaptive beamformers, alongside noise gain and interferer suppression. This added degree of freedom allows the algorithm to better exploit the inherent structure of the received signal and achieve better real-time performance while requiring fewer computations. Finally, we derive upper and lower bounds for the output power of the compressed beamformer when compared to the full complexity MVDR beamformer.

Paper number 45:
Title: Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain
Authors: Junfei Shi, Yu Cheng, Haiyan Jin, Junhuai Li, Zhaolin Xiao, Maoguo Gong, Weisi Lin
Abstract: Diffusion models have demonstrated exceptional performance across various domains due to their ability to model and generate complicated data distributions. However, when applied to PolSAR data, traditional real-valued diffusion models face challenges in capturing complex-valued phase this http URL, these models often struggle to preserve fine structural details. To address these limitations, we leverage the Contourlet transform, which provides rich multiscale and multidirectional representations well-suited for PolSAR imagery. We propose a structural knowledge-guided complex diffusion model for PolSAR image classification in the Contourlet domain. Specifically, the complex Contourlet transform is first applied to decompose the data into low- and high-frequency subbands, enabling the extraction of statistical and boundary features. A knowledge-guided complex diffusion network is then designed to model the statistical properties of the low-frequency components. During the process, structural information from high-frequency coefficients is utilized to guide the diffusion process, improving edge preservation. Furthermore, multiscale and multidirectional high-frequency features are jointly learned to further boost classification accuracy. Experimental results on three real-world PolSAR datasets demonstrate that our approach surpasses state-of-the-art methods, particularly in preserving edge details and maintaining region homogeneity in complex terrain.

Paper number 46:
Title: Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition
Authors: Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly
Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.

Paper number 47:
Title: Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners
Authors: Katsuhiko Yamamoto, Koichi Miyazaki
Abstract: Speech intelligibility prediction (SIP) models have been used as objective metrics to assess intelligibility for hearing-impaired (HI) listeners. In the Clarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based on transformers showed high prediction accuracy. However, the self-attention mechanism theoretically incurs high computational and memory costs, making it a bottleneck for low-latency, power-efficient devices. This may also degrade the temporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP models instead of transformers for the temporal processing blocks. Experimental results show that our proposed SIP model achieves competitive performance compared to the baseline while maintaining a relatively small number of parameters. Our analysis suggests that the SIP model based on bidirectional Mamba effectively captures contextual and spatial speech information from binaural signals.

Paper number 48:
Title: Automated Reasoning for Vulnerability Management by Design
Authors: Avi Shaked, Nan Messe
Abstract: For securing systems, it is essential to manage their vulnerability posture and design appropriate security controls. Vulnerability management allows to proactively address vulnerabilities by incorporating pertinent security controls into systems designs. Current vulnerability management approaches do not support systematic reasoning about the vulnerability postures of systems designs. To effectively manage vulnerabilities and design security controls, we propose a formally grounded automated reasoning mechanism. We integrate the mechanism into an open-source security design tool and demonstrate its application through an illustrative example driven by real-world challenges. The automated reasoning mechanism allows system designers to identify vulnerabilities that are applicable to a specific system design, explicitly specify vulnerability mitigation options, declare selected controls, and thus systematically manage vulnerability postures.

Paper number 49:
Title: Adaptive Communication Through Exploiting RIS, SSK, and CIM for Improved Reliability and Efficiency
Authors: Ferhat Bayar, Onur Salan, Erdogan Aydin, Haci Ilhan
Abstract: In this paper, we present a novel communication system model that integrates reconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and code index modulation (CIM) based on Hadamard coding called RIS based transmit SSK-CIM (RIS-CIM-TSSK). By leveraging RIS, the system adapts rapidly to dynamic environments, enhancing error rates and overall reliability. SSK facilitates the transmission of additional passive information while eliminating the need for multiple radio frequency (RF) chains, thereby reducing complexity. CIM enhances passive information transmission through frequency domain spreading, which may increase signal obfuscation. This proposed scheme not only improves energy efficiency but also offers a robust solution for reliable communication in modern wireless networks, paving the way for smarter and more adaptable implementations. We consider a suboptimal, low-complexity detector for the proposed scheme and also address the blind case for phase adjustment of the RIS. Finally, we present the simulation results for the proposed system model across various configurations, including different numbers of receive and transmit antennas, varying reflecting elements of the RIS, and different code lengths.

Paper number 50:
Title: Robust Power System State Estimation using Physics-Informed Neural Networks
Authors: Solon Falas, Markos Asprou, Charalambos Konstantinou, Maria K. Michael
Abstract: Modern power systems face significant challenges in state estimation and real-time monitoring, particularly regarding response speed and accuracy under faulty conditions or cyber-attacks. This paper proposes a hybrid approach using physics-informed neural networks (PINNs) to enhance the accuracy and robustness, of power system state estimation. By embedding physical laws into the neural network architecture, PINNs improve estimation accuracy for transmission grid applications under both normal and faulty conditions, while also showing potential in addressing security concerns such as data manipulation attacks. Experimental results show that the proposed approach outperforms traditional machine learning models, achieving up to 83% higher accuracy on unseen subsets of the training dataset and 65% better performance on entirely new, unrelated datasets. Experiments also show that during a data manipulation attack against a critical bus in a system, the PINN can be up to 93% more accurate than an equivalent neural network.

Paper number 51:
Title: An Effective Equivalence Model of Analyzing PLS of Multiple Eavesdroppers Facing Low-altitude Communication Systems
Authors: Yujia Zhao, Zhiyong Feng, Kan Yu, Qixun Zhang, Dong Li
Abstract: In low-altitude wireless communications, the increased complexity of wireless channels and the uncertainty of eavesdroppers (Eves)--caused by diverse altitudes, speeds, and obstacles--pose significant challenges to physical layer security (PLS) technologies based on fixed-position antennas (FPAs), particularly in terms of beamforming capabilities and spatial efficiency. In contrast, movable antennas (MAs) offer a flexible solution by enabling channel reconstruction through antenna movement, effectively compensating for the limitations of FPAs. In this paper, we aim to derive a closed-form expression for the secrecy rate, a key metric in PLS, which is often unattainable in current studies due to the uncertainty of Eves. We construct an equivalent model that leverages the reconfigurable nature of MAs, equating the secrecy rates obtained by multiple Eves with single FPAs to those achieved by a single virtual Eve equipped with an MA array. To minimize the gap between these two types of secrecy rates, we formulate and solve an optimization problem by jointly designing the equivalent distance between the transmitter and the virtual Eve} and the antenna positions of MAs at the virtual Eve. Numerical simulations validate the effectiveness of the proposed equivalent model, offering a new perspective for PLS strategies. This work provides significant insights for network designers on how system parameters affect PLS performance.

Paper number 52:
Title: How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures
Authors: Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg
Abstract: There is increasingly more evidence that automatic speech recognition (ASR) systems are biased against different speakers and speaker groups, e.g., due to gender, age, or accent. Research on bias in ASR has so far primarily focused on detecting and quantifying bias, and developing mitigation approaches. Despite this progress, the open question is how to measure the performance and bias of a system. In this study, we compare different performance and bias measures, from literature and proposed, to evaluate state-of-the-art end-to-end ASR systems for Dutch. Our experiments use several bias mitigation strategies to address bias against different speaker groups. The findings reveal that averaged error rates, a standard in ASR research, alone is not sufficient and should be supplemented by other measures. The paper ends with recommendations for reporting ASR performance and bias to better represent a system's performance for diverse speaker groups, and overall system bias.

Paper number 53:
Title: Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning
Authors: Zengjing Chen, Lu Wang, Chengzhi Xing
Abstract: This study addresses the problem of stable acoustic relay assignment in an underwater acoustic network. Unlike the objectives of most existing literature, two distinct objectives, namely classical stable arrangement and ambiguous stable arrangement, are considered. To achieve these stable arrangements, a laser chaos-based multi-processing learning (LC-ML) method is introduced to efficiently obtain high throughput and rapidly attain stability. In order to sufficiently explore the relay's decision-making, this method uses random numbers generated by laser chaos to learn the assignment of relays to multiple source nodes. This study finds that the laser chaos-based random number and multi-processing in the exchange process have a positive effect on higher throughput and strong adaptability with environmental changing over time. Meanwhile, ambiguous cognitions result in the stable configuration with less volatility compared to accurate ones. This provides a practical and useful method and can be the basis for relay selection in complex underwater environments.

Paper number 54:
Title: Differentiable Reward Optimization for LLM based TTS system
Authors: Changfeng Gao, Zhihao Du, Shiliang Zhang
Abstract: This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system's capability to follow instructions this http URL results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner.

Paper number 55:
Title: Experimental Investigation of Availability in a 4.6 km Terrestrial Urban Coherent Free-Space Optical Communications Link
Authors: Vincent van Vliet, Menno van den Hout, Kadir Gümüş, Eduward Tangdiongga, Chigo Okonkwo
Abstract: We measured the outage probability of a 4.6-km urban free-space optical communication link over six days. High-speed power measurements reveal slow and fast fading effects, with link availabilities of 92% including and 99% excluding slow fades for 500 Gb/s transmission.

Paper number 56:
Title: Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol
Authors: Christos Nikou, Theodoros Giannakopoulos
Abstract: Recent advances in song identification leverage deep neural networks to learn compact audio fingerprints directly from raw waveforms. While these methods perform well under controlled conditions, their accuracy drops significantly in real-world scenarios where the audio is captured via mobile devices in noisy environments. In this paper, we introduce a novel evaluation protocol designed to better reflect such real-world conditions. We generate three recordings of the same audio, each with increasing levels of noise, captured using a mobile device's microphone. Our results reveal a substantial performance drop for two state-of-the-art CNN-based models under this protocol, compared to previously reported benchmarks. Additionally, we highlight the critical role of the augmentation pipeline during training with contrastive loss. By introduction low pass and high pass filters in the augmentation pipeline we significantly increase the performance of both systems in our proposed evaluation. Furthermore, we develop a transformer-based model with a tailored projection module and demonstrate that transferring knowledge from a semantically relevant domain yields a more robust solution. The transformer architecture outperforms CNN-based models across all noise levels, and query durations. In low noise conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in finding the correct song, surpassing by 14%, and by 18.5% the second-best performing model, respectively, Under heavy noise levels, we achieve a detection rate 56.5% for 15-second query duration. All experiments are conducted on public large-scale dataset of over 100K songs, with queries matched against a database of 56 million vectors.

Paper number 57:
Title: Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis
Authors: Xintong Hu, Yixuan Chen, Rui Yang, Wenxiang Guo, Changhao Pan
Abstract: Automatic speech quality assessment plays a crucial role in the development of speech synthesis systems, but existing models exhibit significant performance variations across different granularity levels of prediction tasks. This paper proposes an enhanced MOS prediction system based on self-supervised learning speech models, incorporating a Mixture of Experts (MoE) classification head and utilizing synthetic data from multiple commercial generation models for data augmentation. Our method builds upon existing self-supervised models such as wav2vec2, designing a specialized MoE architecture to address different types of speech quality assessment tasks. We also collected a large-scale synthetic speech dataset encompassing the latest text-to-speech, speech conversion, and speech enhancement systems. However, despite the adoption of the MoE architecture and expanded dataset, the model's performance improvements in sentence-level prediction tasks remain limited. Our work reveals the limitations of current methods in handling sentence-level quality assessment, provides new technical pathways for the field of automatic speech quality assessment, and also delves into the fundamental causes of performance differences across different assessment granularities.

Paper number 58:
Title: Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model
Authors: Koki Yamane, Yunhan Li, Masashi Konosu, Koki Inami, Junji Oaki, Sho Sakaino, Toshiaki Tsuji
Abstract: In recent years, the advancement of imitation learning has led to increased interest in teleoperating low-cost manipulators to collect demonstration data. However, most existing systems rely on unilateral control, which only transmits target position values. While this approach is easy to implement and suitable for slow, non-contact tasks, it struggles with fast or contact-rich operations due to the absence of force feedback. This work demonstrates that fast teleoperation with force feedback is feasible even with force-sensorless, low-cost manipulators by leveraging 4-channel bilateral control. Based on accurately identified manipulator dynamics, our method integrates nonlinear terms compensation, velocity and external force estimation, and variable gain corresponding to inertial variation. Furthermore, using data collected by 4-channel bilateral control, we show that incorporating force information into both the input and output of learned policies improves performance in imitation learning. These results highlight the practical effectiveness of our system for high-fidelity teleoperation and data collection on affordable hardware.

Paper number 59:
Title: An Optimal Transport Perspective on Unpaired Image Super-Resolution
Authors: Milena Gazdieva, Petr Mokrov, Litu Rout, Alexander Korotin, Andrey Kravchenko, Alexander Filippov, Evgeny Burnaev
Abstract: Real-world image super-resolution (SR) tasks often do not have paired datasets, which limits the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs), which yield complex training losses with several regularization terms, e.g., content or identity losses. While GANs usually provide good practical performance, they are used heuristically, i.e., theoretical understanding of their behaviour is yet rather limited. We theoretically investigate optimization problems which arise in such models and find two surprising observations. First, the learned SR map is always an optimal transport (OT) map. Second, we theoretically prove and empirically show that the learned map is biased, i.e., it does not actually transform the distribution of low-resolution images to high-resolution ones. Inspired by these findings, we investigate recent advances in neural OT field to resolve the bias issue. We establish an intriguing connection between regularized GANs and neural OT approaches. We show that unlike the existing GAN-based alternatives, these algorithms aim to learn an unbiased OT map. We empirically demonstrate our findings via a series of synthetic and real-world unpaired SR experiments. Our source code is publicly available at this https URL.

Paper number 60:
Title: Distributed Estimation with Decentralized Control for Quadruple-Tank Process
Authors: Moh Kamalul Wafi, Bambang L. Widjiantoro
Abstract: This paper presents a unified modeling, control, and estimation framework for the quadruple-tank process, a benchmark multivariable system that exhibits either minimum phase or nonminimum phase behavior depending on valve flow ratios. A decentralized PI control strategy is employed to regulate water levels, while a distributed state estimation scheme is developed using local Luenberger observers and inter-agent communication. Each observer uses only local output measurements and exchanges information with neighboring nodes over a strongly connected communication graph. To address the limitations of partial observability, the observer design incorporates an observability decomposition and consensus-based coupling that ensures convergence to the true system state. Simulation results validate the effectiveness of the proposed framework, demonstrating accurate state reconstruction and stable closed loop performance under both minimum-phase and nonminimum phase configurations. These results highlight the potential of combining decentralized control with distributed estimation for scalable, networked control of complex multivariable systems.

Paper number 61:
Title: AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ Segmentation in Three Weeks
Authors: Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, Zongwei Zhou
Abstract: Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes an active learning method to expedite the annotation process for organ segmentation and creates the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintaining a similar or even better annotation quality. This achievement is attributed to three unique properties of our method: (1) label bias reduction using multiple pre-trained segmentation models, (2) effective error detection in the model predictions, and (3) attention guidance for annotators to make corrections on the most salient errors. Furthermore, we summarize the taxonomy of common errors made by AI algorithms and annotators. This allows for continuous revision of both AI and annotations and significantly reduces the annotation costs required to create large-scale datasets for a wider variety of medical imaging tasks.

Paper number 62:
Title: Identifying visible tissue in intraoperative ultrasound: a method and application
Authors: Alistair Weld, Luke Dixon, Giulio Anichini, Michael Dyck, Alex Ranne, Sophie Camp, Stamatia Giannarou
Abstract: Purpose: Intraoperative ultrasound scanning is a demanding visuotactile task. It requires operators to simultaneously localise the ultrasound perspective and manually perform slight adjustments to the pose of the probe, making sure not to apply excessive force or breaking contact with the tissue, whilst also characterising the visible tissue. Method: To analyse the probe-tissue contact, an iterative filtering and topological method is proposed to identify the underlying visible tissue, which can be used to detect acoustic shadow and construct confidence maps of perceptual salience. Results: For evaluation, datasets containing both in vivo and medical phantom data are created. A suite of evaluations is performed, including an evaluation of acoustic shadow classification. Compared to an ablation, deep learning, and statistical method, the proposed approach achieves superior classification on in vivo data, achieving an F_beta score of 0.864, in comparison to 0.838, 0.808, 0.808. A novel framework for evaluating the confidence estimation of probe tissue contact is created. The phantom data is captured specifically for this, and comparison is made against two established methods. The proposed method produced the superior response, achieving an average normalised root mean square error of 0.168, in comparison to 1.836 and 4.542. Evaluation is also extended to determine the algorithm's robustness to parameter perturbation, speckle noise, data distribution shift, and capability for guiding a robotic scan. Conclusion: The results of this comprehensive set of experiments justify the potential clinical value of the proposed algorithm, which can be used to support clinical training and robotic ultrasound automation.

Paper number 63:
Title: Bi-Linear Homogeneity Enforced Calibration for Pipelined ADCs
Authors: Matthias Wagner, Oliver Lang, Esmaeil Kavousi Ghafi, Arianit Preniqi, Andreas Schwarz, Mario Huemer
Abstract: Pipelined analog-to-digital converters (ADCs) are key enablers in many state-of-the-art signal processing systems with high sampling rates. In addition to high sampling rates, such systems often demand a high linearity. To meet these challenging linearity requirements, ADC calibration techniques were heavily investigated throughout the past decades. One limitation in ADC calibration is the need for a precisely known test signal. In our previous work, we proposed the homogeneity enforced calibration (HEC) approach, which circumvents this need by consecutively feeding a test signal and a scaled version of it into the ADC. The calibration itself is performed using only the corresponding output samples, such that the test signal can remain unknown. On the downside, the HEC approach requires to accurately scale the test signal, impeding an on-chip implementation. In this work, we provide a thorough analysis of the HEC approach, including limitations such as the effects of an inaccurately scaled test signal. Furthermore, the bi-linear homogeneity enforced calibration (BL-HEC) approach is introduced and suggested to account for an inaccurate scaling and, therefore, to facilitate an on-chip implementation. In addition, a comprehensive stability analysis of the BL-HEC approach is carried out. Finally, we verify our concept with behavioral Matlab simulations and measurements conducted on 24 integrated ADCs.

Paper number 64:
Title: Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image Segmentation?
Authors: Pallabi Dutta, Soham Bose, Swalpa Kumar Roy, Sushmita Mitra
Abstract: The development of efficient segmentation strategies for medical images has evolved from its initial dependence on Convolutional Neural Networks (CNNs) to the current investigation of hybrid models that combine CNNs with Vision Transformers (ViTs). There is an increasing focus on creating architectures that are both high-performing and computationally efficient, capable of being deployed on remote systems with limited resources. Although transformers can capture global dependencies in the input space, they face challenges from the corresponding high computational and storage expenses involved. This research investigates the integration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s by introducing the novel U-VixLSTM. The Vision-xLSTM blocks capture the temporal and global relationships within the patches extracted from the CNN feature maps. The convolutional feature reconstruction path upsamples the output volume from the Vision-xLSTM blocks to produce the segmentation output. Our primary objective is to propose that Vision-xLSTM forms an appropriate backbone for medical image segmentation, offering excellent performance with reduced computational costs. The U-VixLSTM exhibits superior performance compared to the state-of-the-art networks in the publicly available Synapse, ISIC and ACDC datasets. Code provided: this https URL

Paper number 65:
Title: Real-Time Stochastic Terrain Mapping and Processing for Autonomous Safe Landing
Authors: Kento Tomita, Koki Ho
Abstract: Onboard terrain sensing and mapping for safe planetary landings often suffer from missed hazardous features, e.g., small rocks, due to the large observational range and the limited resolution of the obtained terrain data. To this end, this paper develops a novel real-time stochastic terrain mapping algorithm that accounts for topographic uncertainty between the sampled points, or the uncertainty due to the sparse 3D terrain measurements. We introduce a Gaussian digital elevation map that is efficiently constructed using the combination of Delauney triangulation and local Gaussian process regression. The geometric investigation of the lander-terrain interaction is exploited to efficiently evaluate the marginally conservative local slope and roughness while avoiding the costly computation of the local plane. The conservativeness is proved in the paper. The developed real-time uncertainty quantification pipeline enables stochastic landing safety evaluation under challenging operational conditions, such as a large observational range or limited sensor capability, which is a critical stepping stone for the development of predictive guidance algorithms for safe autonomous planetary landing. Detailed reviews on background and related works are also presented.

Paper number 66:
Title: Compressive radio-interferometric sensing with random beamforming as rank-one signal covariance projections
Authors: Olivier Leblanc, Yves Wiaux, Laurent Jacques
Abstract: Radio-interferometry (RI) observes the sky at unprecedented angular resolutions, enabling the study of several far-away galactic objects such as galaxies and black holes. In RI, an array of antennas probes cosmic signals coming from the observed region of the sky. The covariance matrix of the vector gathering all these antenna measurements offers, by leveraging the Van Cittert-Zernike theorem, an incomplete and noisy Fourier sensing of the image of interest. The number of noisy Fourier measurements -- or visibilities -- scales as $\mathcal O(Q^2B)$ for $Q$ antennas and $B$ short-time integration (STI) intervals. We address the challenges posed by this vast volume of data, which is anticipated to increase significantly with the advent of large antenna arrays, by proposing a compressive sensing technique applied directly at the level of the antenna measurements. First, this paper shows that beamforming -- a common technique of dephasing antenna signals -- usually used to focus some region of the sky, is equivalent to sensing a rank-one projection (ROP) of the signal covariance matrix. We build upon our recent work arXiv:2306.12698v3 [eess.IV] to propose a compressive sensing scheme relying on random beamforming, trading the $Q^2$-dependence of the data size for a smaller number $P$ ROPs. We provide image recovery guarantees for sparse image reconstruction. Secondly, the data size is made independent of $B$ by applying $M$ Bernoulli modulations of the ROP vectors obtained for the STI. The resulting sample complexities, theoretically derived in a simpler case without modulations and numerically obtained in phase transition diagrams, are shown to scale as $\mathcal O(K)$ where $K$ is the image sparsity. This illustrates the potential of the approach.

Paper number 67:
Title: Physics-Driven Autoregressive State Space Models for Medical Image Reconstruction
Authors: Bilal Kabas, Fuat Arslan, Valiyeh A. Nezhad, Saban Ozturk, Emine U. Saritas, Tolga Çukur
Abstract: Medical image reconstruction from undersampled acquisitions is an ill-posed problem involving inversion of the imaging operator linking measurement and image domains. Physics-driven (PD) models have gained prominence in reconstruction tasks due to their desirable performance and generalization. These models jointly promote data fidelity and artifact suppression, typically by combining data-consistency mechanisms with learned network modules. Artifact suppression depends on the network's ability to disentangle artifacts from true tissue signals, both of which can exhibit contextual structure across diverse spatial scales. Convolutional neural networks (CNNs) are strong in capturing local correlations, albeit relatively insensitive to non-local context. While transformers promise to alleviate this limitation, practical implementations frequently involve design compromises to reduce computational cost by balancing local and non-local sensitivity, occasionally resulting in performance comparable to or trailing that of CNNs. To enhance contextual sensitivity without incurring high complexity, we introduce a novel physics-driven autoregressive state-space model (MambaRoll) for medical image reconstruction. In each cascade of its unrolled architecture, MambaRoll employs a physics-driven state-space module (PD-SSM) to aggregate contextual features efficiently at a given spatial scale, and autoregressively predicts finer-scale feature maps conditioned on coarser-scale features to capture multi-scale context. Learning across scales is further enhanced via a deep multi-scale decoding (DMSD) loss tailored to the autoregressive prediction task. Demonstrations on accelerated MRI and sparse-view CT reconstructions show that MambaRoll consistently outperforms state-of-the-art data-driven and physics-driven methods based on CNN, transformer, and SSM backbones.

Paper number 68:
Title: TimeFlow: Longitudinal Brain Image Registration and Aging Progression Analysis
Authors: Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger
Abstract: Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. TimeFlow leverages a U-Net architecture with temporal conditioning inspired by diffusion models, enabling accurate registration using only two images as input and facilitating prospective analyses through future image prediction. Unlike traditional methods, TimeFlow eliminates the demand for explicit smoothness regularizers and dense sequential data while maintaining temporal consistency and continuity. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging, all without requiring segmentation, thus avoiding non-trivial annotation and inconsistent segmentation flaws. This framework paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases.

Paper number 69:
Title: Data-Driven Mean Field Equilibrium Computation in Large-Population LQG Games
Authors: Zhenhui Xu, Jiayu Chen, Bing-Chang Wang, Tielong Shen
Abstract: This paper presents a novel data-driven approach for approximating the $\varepsilon$-Nash equilibrium in continuous-time linear quadratic Gaussian (LQG) games, where multiple agents interact with each other through their dynamics and infinite horizon discounted costs. The core of our method involves solving two algebraic Riccati equations (AREs) and an ordinary differential equation (ODE) using state and input samples collected from agents, eliminating the need for a priori knowledge of their dynamical models. The standard ARE is addressed through an integral reinforcement learning (IRL) technique, while the nonsymmetric ARE and the ODE are resolved by identifying the drift coefficients of the agents' dynamics under general conditions. Moreover, by imposing specific conditions on models, we extend the IRL-based approach to approximately solve the nonsymmetric ARE. Numerical examples are given to demonstrate the effectiveness of the proposed algorithms.

Paper number 70:
Title: Multi-model Stochastic Particle-based Variational Bayesian Inference for Multiband Delay Estimation
Authors: Zhixiang Hu, An Liu, Minjian Zhao
Abstract: Joint utilization of multiple discrete frequency bands can enhance the accuracy of delay estimation. Although some unique challenges of multiband fusion, such as phase distortion, oscillation phenomena, and high-dimensional search, have been partially addressed, further challenges remain. Specifically, under conditions of low signal-to-noise ratio (SNR), insufficient data, and closely spaced delay paths, accurately determining the model order-the number of delay paths-becomes difficult. Misestimating the model order can significantly degrade the estimation performance of traditional methods. To address joint model selection and parameter estimation under such harsh conditions, we propose a multi-model stochastic particle-based variational Bayesian inference (MM-SPVBI) framework, capable of exploring multiple high-dimensional parameter spaces. Initially, we split potential overlapping primary delay paths based on coarse estimates, generating several parallel candidate models. Then, an auto-focusing sampling strategy is employed to quickly identify the optimal model. Additionally, we introduce a hybrid posterior approximation to improve the original single-model SPVBI, ensuring overall complexity does not increase significantly with parallelism. Simulations demonstrate that our algorithm offers substantial advantages over existing methods.

Paper number 71:
Title: A Quantum-Empowered SPEI Drought Forecasting Algorithm Using Spatially-Aware Mamba Network
Authors: Po-Wei Tang, Chia-Hsiang Lin, Jian-Kai Huang, Alfredo R. Huete
Abstract: Due to the intensifying impacts of extreme climate changes, drought forecasting (DF), which aims to predict droughts from historical meteorological data, has become increasingly critical for monitoring and managing water resources. Though drought conditions often exhibit spatial climatic coherence among neighboring regions, benchmark deep learning-based DF methods overlook this fact and predict the conditions on a region-by-region basis. Using the Standardized Precipitation Evapotranspiration Index (SPEI), we designed and trained a novel and transformative spatially-aware DF neural network, which effectively captures local interactions among neighboring regions, resulting in enhanced spatial coherence and prediction accuracy. As DF also requires sophisticated temporal analysis, the Mamba network, recognized as the most accurate and efficient existing time-sequence modeling, was adopted to extract temporal features from short-term time frames. We also adopted quantum neural networks (QNN) to entangle the spatial features of different time instances, leading to refined spatiotemporal features of seven different meteorological variables for effectively identifying short-term climate fluctuations. In the last stage of our proposed SPEI-driven quantum spatially-aware Mamba network (SQUARE-Mamba), the extracted spatiotemporal features of seven different meteorological variables were fused to achieve more accurate DF. Validation experiments across El Niño, La Niña, and normal years demonstrated the superiority of the proposed SQUARE-Mamba, remarkably achieving an average improvement of more than 9.8% in the coefficient of determination index (R^2) compared to baseline methods, thereby illustrating the promising roles of the temporal quantum entanglement and Mamba temporal analysis to achieve more accurate DF.

Paper number 72:
Title: Distributionally Robust Predictive Runtime Verification under Spatio-Temporal Logic Specifications
Authors: Yiqi Zhao, Emily Zhu, Bardh Hoxha, Georgios Fainekos, Jyotirmoy V. Deshmukh, Lars Lindemann
Abstract: Cyber-physical systems (CPS) designed in simulators, often consisting of multiple interacting agents (e.g. in multi-agent formations), behave differently in the real-world. We want to verify these systems during runtime when they are deployed. We thus propose robust predictive runtime verification (RPRV) algorithms for: (1) general stochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic multi-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem presents the following challenges: (1) there may not be sufficient data on the behavior of the deployed CPS, (2) predictive models based on design phase system trajectories may encounter distribution shift during real-world deployment, and (3) the algorithms need to scale to the complexity of MAS and be applicable to spatio-temporal logic tasks. To address the challenges, we assume knowledge of an upper bound on the statistical distance between the trajectory distributions of the system at deployment and design time. We are motivated by our prior work [1, 2] where we proposed an accurate and an interpretable RPRV algorithm for general CPS, which we here extend to the MAS setting and spatio-temporal logic tasks. Specifically, we use a learned predictive model to estimate the system behavior at runtime and robust conformal prediction to obtain probabilistic guarantees by accounting for distribution shifts. Building on [1], we perform robust conformal prediction over the robust semantics of spatio-temporal reach and escape logic (STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate our results in a drone swarm simulator, where we show the scalability of our RPRV algorithms to MAS and analyze the impact of different trajectory predictors on the verification result. To the best of our knowledge, these are the first statistically valid algorithms for MAS under distribution shift.

Paper number 73:
Title: Lightweight Medical Image Restoration via Integrating Reliable Lesion-Semantic Driven Prior
Authors: Pengcheng Zheng, Kecheng Chen, Jiaxin Huang, Bohao Chen, Ju Liu, Yazhou Ren, Xiaorong Pu
Abstract: Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency.

Paper number 74:
Title: Discrete-Time CRLB-based Power Allocation for CF MIMO-ISAC with Joint Localization and Velocity Sensing
Authors: Guoqing Xia, Pei Xiao, Qu Luo, Bing Ji, Yue Zhang, Huiyu Zhou
Abstract: In this paper, we investigate integrated sensing and communication (ISAC) in a cell-free (CF) multiple-input multiple-output (MIMO) network, where each access point functions either as an ISAC transmitter or as a sensing receiver. We devote into the ISAC sensing metric using the discrete-time signal-based Cramer-Rao lower bounds (CRLBs) for joint location and velocity estimation under arbitrary power allocation ratios under the deterministic radar cross section assumption (RCS). Then, we consider the power allocation optimization problem for the CF MIMO-ISAC as the maximization of the communication signal-to-interference-plus-noise ratio (SINR), subject to CRLB-based sensing constraints and per-transmitter power limits. To solve the resulting nonlinear and non-convex problem, we propose a penalty function and projection-based modified conjugate gradient algorithm with inexact line search (PP-MCG-ILS), and an alternative method based on a modified steepest descent approach (PP-MSD-ILS). We show that the proposed algorithms are scalable and can be extended to a broad class of optimization problems involving nonlinear inequality constraints and affine equality constraints. In addition, we extend the PP-MCG-ILS algorithm to the pure sensing scenario, where a penalty function-based normalized conjugate gradient algorithm (P-NCG-ILS) is developed for sensing power minimization. Finally, we analyze the convergence behavior and qualitatively compare the computational complexity of the proposed algorithms. Simulation results confirm the accuracy of the derived CRLBs and demonstrate the effectiveness of the proposed power allocation strategies in enhancing both sensing and overall ISAC performance.

Paper number 75:
Title: On data usage and predictive behavior of data-driven predictive control with 1-norm regularization
Authors: Manuel Klädtke, Moritz Schulze Darup
Abstract: We investigate the data usage and predictive behavior of data-driven predictive control (DPC) with 1-norm regularization. Our analysis enables the offline removal of unused data and facilitates a comparison between the identified symmetric structure and data usage against prior knowledge of the true system. This comparison helps assess the suitability of the DPC scheme for effective control.

Paper number 76:
Title: Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge
Authors: Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik
Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research.

Paper number 77:
Title: Theoretical Foundations of Waste Factor and Waste Figure with Applications to Fixed Wireless Access and Relay Systems
Authors: Nurullah Sevim, Mostafa Ibrahim, Sabit Ekin, Theodore S. Rappaport
Abstract: The exponential rise in energy consumption across wireless communication systems, particularly in anticipation of next-generation wireless systems, necessitates rigorous frameworks for evaluating and optimizing energy efficiency. This paper revisits and expands the concept of the Waste Factor (W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures both utilized and wasted power in cascaded communication systems. Building upon its foundation in system-level power modeling, we integrate the Waste Factor into a refined formulation of the Consumption Factor (CF), the ratio of data rate to total consumed power, linking it directly to Shannon's theoretical limit on energy per bit. This analysis introduces additive energy waste into the classical energy-per-bit derivation through the Waste Factor term. We derive closed-form expressions for energy-per-bit expenditure in both direct and relay-assisted links and develop a decision rule to determine which communication path is more energy efficient under given conditions. While not modeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as a special case of relay-based architectures within this unified formulation, suggesting broader applicability of the Waste Factor framework to emerging 6G use cases. The framework is then extended to a Fixed Wireless Access (FWA) scenario, where uplink and downlink asymmetries, traffic directionality, and component inefficiencies are jointly considered to analyze energy-optimal deployment strategies.

Paper number 78:
Title: S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation Framework via Seamless Speech-Text Alignment and Progressive Fine-tuning
Authors: Yu Pan, Yuguang Yang, Yanni Hu, Jianhao Ye, Xiang Zhang, Hongbin Zhou, Lei Ma, Jianjun Zhao
Abstract: Despite recent advances in multilingual speech-to-speech translation (S2ST), several critical challenges persist: 1) achieving high-quality translation remains a major hurdle, and 2) most existing methods heavily rely on large-scale parallel speech corpora, which are costly and difficult to obtain. To address these issues, we propose \textit{S2ST-Omni}, an efficient and scalable framework for multilingual S2ST. Specifically, we decompose the S2ST task into speech-to-text translation (S2TT) and text-to-speech synthesis (TTS). For S2TT, we propose an effective speech language model that integrates the pretrained Whisper encoder for robust audio understanding and Qwen 3.0 for advanced text comprehension. A lightweight speech adapter is employed to bridge the modality gap between speech and text representations. To further facilitate the multimodal knowledge learning, a two-stage fine-tuning strategy is introduced. In the TTS stage, we adopt a streaming autoregressive generation approach to produce natural and fluent target speech. Experiments on the CVSS benchmark show that S2ST-Omni consistently outperforms existing state-of-the-art S2ST systems in translation quality, highlighting its effectiveness and superiority.

Paper number 79:
Title: Evaluating Logit-Based GOP Scores for Mispronunciation Detection
Authors: Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik
Abstract: Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment.

Paper number 80:
Title: SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures
Authors: Fengyi Jiang, Xiaorui Zhang, Lingbo Jin, Ruixing Liang, Yuxin Chen, Adi Chola Venkatesh, Jason Culman, Tiantian Wu, Lirong Shao, Wenqing Sun, Cong Gao, Hallie McNamara, Jingpei Lu, Omid Mohareri
Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries.

Paper number 81:
Title: Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models
Authors: Yi Zhang, Yidong Zhao, Qian Tao
Abstract: Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods.

Paper number 82:
Title: SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech
Authors: Zhuangfei Cheng, Guangyan Zhang, Zehai Tu, Yangyang Song, Shuiyang Mao, Xiaoqi Jiao, Jingyu Li, Yiwen Guo, Jiasong Wu
Abstract: Foreign accent conversion (FAC) in speech processing remains a challenging task. Building on the remarkable success of large language models (LLMs) in Text-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based techniques for FAC, which we term SpeechAccentLLM. At the core of this framework, we introduce SpeechCodeVAE, the first model to integrate connectionist temporal classification (CTC) directly into codebook discretization for speech content tokenization. This novel architecture generates tokens with a unique "locality" property, as validated by experiments demonstrating optimal trade-offs among content faithfulness, temporal coherence, and structural recoverability. Then, to address data scarcity for the FAC module, we adopted a multitask learning strategy that jointly trains the FAC and TTS modules. Beyond mitigating data limitations, this approach yielded accelerated convergence and superior speech quality compared to standalone FAC training. Moreover, leveraging the salient properties of our discrete speech representations, we introduce SpeechRestorer, a postprocessing architecture designed to refine LLM-generated outputs. This module effectively mitigates stochastic errors prevalent in LLM inference pipelines while enhancing prosodic continuity, as validated by ablation experiments.

Paper number 83:
Title: Copula Density Neural Estimation
Authors: Nunzio A. Letizia, Nicola Novello, Andrea M. Tonello
Abstract: Probability density estimation from observed data constitutes a central task in statistics. In this brief, we focus on the problem of estimating the copula density associated to any observed data, as it fully describes the dependence between random variables. We separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and can be applied for mutual information estimation and data generation.

Paper number 84:
Title: On the Robotic Uncertainty of Fully Autonomous Traffic: From Stochastic Car-Following to Mobility-Safety Trade-Offs
Authors: Hangyu Li, Xiaotong Sun, Chenglin Zhuang, Xiaopeng Li
Abstract: Recent transportation research highlights the potential of autonomous vehicles (AV) to improve traffic flow mobility as they are able to maintain smaller car-following distances. However, as a unique class of ground robots, AVs are susceptible to robotic errors, particularly in their perception and control modules with imperfect sensors and actuators, leading to uncertainties in their movements and an increased risk of collisions. Consequently, conservative operational strategies, such as larger headway and slower speeds, are implemented to prioritize safety over mobility in real-world operations. To reconcile the inconsistency, this paper presents an analytical model framework that delineates the endogenous reciprocity between traffic safety and mobility that arises from AVs' robotic uncertainties. Using both realistic car-following data and a stochastic intelligent driving model (IDM), the stochastic car-following distance is derived as a key parameter, enabling analysis of single-lane capacity and collision probability. A semi-Markov process is then employed to model the dynamics of the lane capacity, and the resulting collision-inclusive capacity, representing expected lane capacity under stationary conditions, serves as the primary performance metric for fully autonomous traffic. The analytical results are further utilized to investigate the impacts of critical parameters in AV and roadway designs on traffic performance, as well as the properties of optimal speed and headway under mobility-targeted or safety-dominated management objectives. Extensions to scenarios involving multiple non-independent collisions or multi-lane traffic scenarios are also discussed, which demonstrates the robustness of the theoretical results and their practical applications.

Paper number 85:
Title: Robustness in Wireless Distributed Learning: An Information-Theoretic Analysis
Authors: Yangshuo He, Guanding Yu, Huaiyu Dai
Abstract: In recent years, the application of artificial intelligence (AI) in wireless communications has demonstrated inherent robustness against wireless channel distortions. Most existing works empirically leverage this robustness to yield considerable performance gains through AI architectural designs. However, there is a lack of direct theoretical analysis of this robustness and its potential to enhance communication efficiency, which restricts the full exploitation of these advantages. In this paper, we adopt an information-theoretic approach to evaluate the robustness in wireless distributed learning by deriving an upper bound on the task performance loss due to imperfect wireless channels. Utilizing this insight, we define task outage probability and characterize the maximum transmission rate under task accuracy guarantees, referred to as the task-aware $\epsilon$-capacity resulting from the robustness. To achieve the utility of the theoretical results in practical settings, we present an efficient algorithm for the approximation of the upper bound. Subsequently, we devise a robust training framework that optimizes the trade-off between robustness and task accuracy, enhancing the robustness against channel distortions. Extensive experiments validate the effectiveness of the proposed upper bound and task-aware $\epsilon$-capacity and demonstrate that the proposed robust training framework achieves high robustness, thus ensuring a high transmission rate while maintaining inference performance.

Paper number 86:
Title: MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation
Authors: Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Jing Xiong, Rossella Arcucci, Huaxiu Yao, Mi Zhang
Abstract: Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, resilience to signal perturbation, and alignment with human expert evaluation. These findings emphasize the efficacy of MEIT and its potential for real-world clinical application.

Paper number 87:
Title: CFMW: Cross-modality Fusion Mamba for Robust Object Detection under Adverse Weather
Authors: Haoyuan Li, Qi Hu, Binjia Zhou, You Yao, Jiacheng Lin, Kailun Yang, Peng Chen
Abstract: Visible-infrared image pairs provide complementary information, enhancing the reliability and robustness of object detection applications in real-world scenarios. However, most existing methods face challenges in maintaining robustness under complex weather conditions, which limits their applicability. Meanwhile, the reliance on attention mechanisms in modality fusion introduces significant computational complexity and storage overhead, particularly when dealing with high-resolution images. To address these challenges, we propose the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment stability and cost-effectiveness under adverse weather conditions. Leveraging the proposed Perturbation-Adaptive Diffusion Model (PADM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to reconstruct visual features affected by adverse weather, enriching the representation of image details. With efficient architecture design, CFMW is 3 times faster than Transformer-style fusion (e.g., CFT). To bridge the gap in relevant datasets, we construct a new Severe Weather Visible-Infrared (SWVI) dataset, encompassing diverse adverse weather scenarios such as rain, haze, and snow. The dataset contains 64,281 paired visible-infrared images, providing a valuable resource for future research. Extensive experiments on public datasets (i.e., M3FD and LLVIP) and the newly constructed SWVI dataset conclusively demonstrate that CFMW achieves state-of-the-art detection performance. Both the dataset and source code will be made publicly available at this https URL.

Paper number 88:
Title: Policy Verification in Stochastic Dynamical Systems Using Logarithmic Neural Certificates
Authors: Thom Badings, Wietze Koops, Sebastian Junges, Nils Jansen
Abstract: We consider the verification of neural network policies for discrete-time stochastic systems with respect to reach-avoid specifications. We use a learner-verifier procedure that learns a certificate for the specification, represented as a neural network. Verifying that this neural network certificate is a so-called reach-avoid supermartingale (RASM) proves the satisfaction of a reach-avoid specification. Existing approaches for such a verification task rely on computed Lipschitz constants of neural networks. These approaches struggle with large Lipschitz constants, especially for reach-avoid specifications with high threshold probabilities. We present two key contributions to obtain smaller Lipschitz constants than existing approaches. First, we introduce logarithmic RASMs (logRASMs), which take exponentially smaller values than RASMs and hence have lower theoretical Lipschitz constants. Second, we present a fast method to compute tighter upper bounds on Lipschitz constants based on weighted norms. Our empirical evaluation shows we can consistently verify the satisfaction of reach-avoid specifications with probabilities as high as 99.9999%.

Paper number 89:
Title: Robust Score-Based Quickest Change Detection
Authors: Sean Moushegian, Suya Wu, Enmao Diao, Jie Ding, Taposh Banerjee, Vahid Tarokh
Abstract: Methods in the field of quickest change detection rapidly detect in real-time a change in the data-generating distribution of an online data stream. Existing methods have been able to detect this change point when the densities of the pre- and post-change distributions are known. Recent work has extended these results to the case where the pre- and post-change distributions are known only by their score functions. This work considers the case where the pre- and post-change score functions are known only to correspond to distributions in two disjoint sets. This work selects a pair of least-favorable distributions from these sets to robustify the existing score-based quickest change detection algorithm, the properties of which are studied. This paper calculates the least-favorable distributions for specific model classes and provides methods of estimating the least-favorable distributions for common constructions. Simulation results are provided demonstrating the performance of our robust change detection algorithm.

Paper number 90:
Title: A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs
Authors: Ehsan Kabir, Austin R.J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang
Abstract: Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some state-of-the-art FPGA-based accelerators.

Paper number 91:
Title: Random Walks with Tweedie: A Unified View of Score-Based Diffusion Models
Authors: Chicago Y. Park, Michael T. McCann, Cristina Garcia-Cardona, Brendt Wohlberg, Ulugbek S. Kamilov
Abstract: We present a concise derivation for several influential score-based diffusion models that relies on only a few textbook results. Diffusion models have recently emerged as powerful tools for generating realistic, synthetic signals -- particularly natural images -- and often play a role in state-of-the-art algorithms for inverse problems in image processing. While these algorithms are often surprisingly simple, the theory behind them is not, and multiple complex theoretical justifications exist in the literature. Here, we provide a simple and largely self-contained theoretical justification for score-based diffusion models that is targeted towards the signal processing community. This approach leads to generic algorithmic templates for training and generating samples with diffusion models. We show that several influential diffusion models correspond to particular choices within these templates and demonstrate that alternative, more straightforward algorithmic choices can provide comparable results. This approach has the added benefit of enabling conditional sampling without any likelihood approximation.

Paper number 92:
Title: Iterative Importance Fine-tuning of Diffusion Models
Authors: Alexander Denker, Shreyas Padhy, Francisco Vargas, Johannes Hertrich
Abstract: Diffusion models are an important tool for generative modelling, serving as effective priors in applications such as imaging and protein design. A key challenge in applying diffusion models for downstream tasks is efficiently sampling from resulting posterior distributions, which can be addressed using the $h$-transform. This work introduces a self-supervised algorithm for fine-tuning diffusion models by estimating the $h$-transform, enabling amortised conditional sampling. Our method iteratively refines the $h$-transform using a synthetic dataset resampled with path-based importance weights. We demonstrate the effectiveness of this framework on class-conditional sampling, inverse problems and reward fine-tuning for text-to-image diffusion models.

Paper number 93:
Title: Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions
Authors: Ji Yin, Oswin So, Eric Yang Yu, Chuchu Fan, Panagiotis Tsiotras
Abstract: A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ``black-box'' dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments. Project website: this https URL.

Paper number 94:
Title: Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation
Authors: Qiuming Zhao, Guangzhi Sun, Chao Zhang
Abstract: Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-lingual multi-task training approaches aim to address this by jointly optimising multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language interference, and enhancing extensibility. Experimental results across 10 languages demonstrate that LoRS-Merging significantly outperforms multi-lingual multi-task training, sequential training, and other merging methods, achieving over 20% improvement in normalised performance. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.

Paper number 95:
Title: The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government
Authors: Zeynep Engin, Jon Crowcroft, David Hand, Philip Treleaven
Abstract: As artificial intelligence transforms public sector operations, governments struggle to integrate technological innovations into coherent systems for effective service delivery. This paper introduces the Algorithmic State Architecture (ASA), a novel four-layer framework conceptualising how Digital Public Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and GovTech interact as an integrated system in AI-enabled states. Unlike approaches that treat these as parallel developments, ASA positions them as interdependent layers with specific enabling relationships and feedback mechanisms. Through comparative analysis of implementations in Estonia, Singapore, India, and the UK, we demonstrate how foundational digital infrastructure enables systematic data collection, which powers algorithmic decision-making processes, ultimately manifesting in user-facing services. Our analysis reveals that successful implementations require balanced development across all layers, with particular attention to integration mechanisms between them. The framework contributes to both theory and practice by bridging previously disconnected domains of digital government research, identifying critical dependencies that influence implementation success, and providing a structured approach for analysing the maturity and development pathways of AI-enabled government systems.

Paper number 96:
Title: Gradient Field-Based Dynamic Window Approach for Collision Avoidance in Complex Environments
Authors: Ze Zhang, Yifan Xue, Nadia Figueroa, Knut Åkesson
Abstract: For safe and flexible navigation in multi-robot systems, this paper presents an enhanced and predictive sampling-based trajectory planning approach in complex environments, the Gradient Field-based Dynamic Window Approach (GF-DWA). Building upon the dynamic window approach, the proposed method utilizes gradient information of obstacle distances as a new cost term to anticipate potential collisions. This enhancement enables the robot to improve awareness of obstacles, including those with non-convex shapes. The gradient field is derived from the Gaussian process distance field, which generates both the distance field and gradient field by leveraging Gaussian process regression to model the spatial structure of the environment. Through several obstacle avoidance and fleet collision avoidance scenarios, the proposed GF-DWA is shown to outperform other popular trajectory planning and control methods in terms of safety and flexibility, especially in complex environments with non-convex obstacles.

Paper number 97:
Title: ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection
Authors: Hao Gu, Jiangyan Yi, Chenglong Wang, Jianhua Tao, Zheng Lian, Jiayi He, Yong Ren, Yujie Chen, Zhengqi Wen
Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the rise of high-fidelity audio generative models and their potential for misuse. Given that audio large language models (ALLMs) have made significant progress in various audio processing tasks, a heuristic question arises: \textit{Can ALLMs be leveraged to solve ADD?}. In this paper, we first conduct a comprehensive zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness. To this end, we propose ALLM4ADD, an ALLM-driven framework for ADD. Specifically, we reformulate ADD task as an audio question answering problem, prompting the model with the question: ``Is this audio fake or real?''. We then perform supervised fine-tuning to enable the ALLM to assess the authenticity of query audio. Extensive experiments are conducted to demonstrate that our ALLM-based method can achieve superior performance in fake audio detection, particularly in data-scarce scenarios. As a pioneering study, we anticipate that this work will inspire the research community to leverage ALLMs to develop more effective ADD systems. Code is available at this https URL

Paper number 98:
Title: MARS: Radio Map Super-resolution and Reconstruction Method under Sparse Channel Measurements
Authors: Chuyun Deng, Na Liu, Wei Xie, Lianming Xu, Li Wang
Abstract: Radio maps reflect the spatial distribution of signal strength and are essential for applications like smart cities, IoT, and wireless network planning. However, reconstructing accurate radio maps from sparse measurements remains challenging. Traditional interpolation and inpainting methods lack environmental awareness, while many deep learning approaches depend on detailed scene data, limiting generalization. To address this, we propose MARS, a Multi-scale Aware Radiomap Super-resolution method that combines CNNs and Transformers with multi-scale feature fusion and residual connections. MARS focuses on both global and local feature extraction, enhancing feature representation across different receptive fields and improving reconstruction accuracy. Experiments across different scenes and antenna locations show that MARS outperforms baseline models in both MSE and SSIM, while maintaining low computational cost, demonstrating strong practical potential.

Paper number 99:
Title: A Novel Hybrid Grey Wolf Differential Evolution Algorithm
Authors: Ioannis D. Bougas, Pavlos Doanis, Maria S. Papadopoulou, Achilles D. Boursianis, Sotirios P. Sotiroudis, Zaharias D. Zaharis, George Koudouridis, Panagiotis Sarigiannidis, Mohammad Abdul Matint, George Karagiannidis, Sotirios K. Goudos
Abstract: Grey wolf optimizer (GWO) is a nature-inspired stochastic meta-heuristic of the swarm intelligence field that mimics the hunting behavior of grey wolves. Differential evolution (DE) is a popular stochastic algorithm of the evolutionary computation field that is well suited for global optimization. In this part, we introduce a new algorithm based on the hybridization of GWO and two DE variants, namely the GWO-DE algorithm. We evaluate the new algorithm by applying various numerical benchmark functions. The numerical results of the comparative study are quite satisfactory in terms of performance and solution quality.

Paper number 100:
Title: SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge
Authors: Yuxiang Mei, Yuang Zheng, Dongxing Xu, Yanhua Long
Abstract: This paper describes SHNU multilingual conversational speech recognition system (SHNU-mASR, team name-"maybe"), submitted to Track 1 of the INTERSPEECH 2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder architecture with a large language model (LLM) to form a unified multilingual ASR framework. The parallel-speech-encoder consists of two pre-trained encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output embeddings are concatenated and fed into the LLM, enabling the model to leverage complementary acoustic and linguistic knowledge and achieve competitive performance. Moreover, we adopt a tri-stage training strategy to jointly update the low-rank adaptation modules and projector parameters of both the speech encoders and the LLM. In addition, we incorporate an additional language-aware prompt at the LLM input to enhance language-specific text generation. The SHNU-mASR system achieves an overall character/word error rate (CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming the official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the baseline training data.

Paper number 101:
Title: Movable-Antenna-Enhanced Physical-Layer Service Integration: Performance Analysis and Optimization
Authors: Xuanlin Shen, Xin Wei, Weidong Mei, Zhi Chen, Jun Fang, Boyu Ning
Abstract: Movable antennas (MAs) have drawn increasing attention in wireless communications due to their capability to create favorable channel conditions via local movement within a confined region. In this letter, we investigate its application in physical-layer service integration (PHY-SI), where a multi-MA base station (BS) simultaneously transmits both confidential and multicast messages to two users. The multicast message is intended for both users, while the confidential message is intended only for one user and must remain perfectly secure from the other. Our goal is to jointly optimize the secrecy and multicast beamforming, as well as the MAs' positions at the BS to maximize the secrecy rate for one user while satisfying the multicast rate requirement for both users. To gain insights, we first conduct performance analysis of this MA-enhanced PHY-SI system in two special cases, revealing its unique characteristics compared to conventional PHY-SI with fixed-position antennas (FPAs). To address the secrecy rate maximization problem, we propose a two-layer optimization framework that integrates the semidefinite relaxation (SDR) technique and a discrete sampling algorithm. Numerical results demonstrate that MAs can greatly enhance the achievable secrecy rate region for PHY-SI compared to FPAs.

Paper number 102:
Title: StreamDiT: Real-Time Streaming Text-to-Video Generation
Authors: Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, Yue Zhao
Abstract: Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: this https URL

Paper number 103:
Title: Self-supervised learning of speech representations with Dutch archival data
Authors: Nik Vaessen, Roeland Ordelman, David A. van Leeuwen
Abstract: This paper explores the use of Dutch archival television broadcast data for self-supervised learning of speech foundation models, specifically wav2vec 2.0. We first study data quality assumptions for pre-training, and show how music, noise and speaker overlap affect SSL convergence and downstream fine-tuning performance. Secondly, we explore effectively pre-processing strategies to convert the noisy broadcast dataset into a qualitative dataset for pre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual and multi-lingual pre-training with equivalent amounts of data, and show that mono-lingual pre-training is more robust to out-of-domain data. Lastly, we achieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a continuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k hour archival dataset.

Paper number 104:
Title: What's Making That Sound Right Now? Video-centric Audio-Visual Localization
Authors: Hahyeon Choi, Junhoo Lee, Nojun Kwak
Abstract: Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization.

Paper number 105:
Title: OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model
Authors: Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang
Abstract: Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at this https URL
    