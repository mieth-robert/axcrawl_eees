
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A Remark on the AAA Method for Secret-Key Generation in Mobile Networks
Authors: Yingbo Hua
Abstract: A broadly applicable method for secret-key generation is named for its accumulative, adaptable and additive (AAA) properties. This paper first shows a robustness of its performance. Namely, even if there is an inter correlation or a leakage caused intra correlation among the superimposed packets, provided there is a nonzero probability for each packet to be missed in full or in part by Eve, then the equivocation of the key generated by the AAA method always becomes perfect as the number of superpositions becomes infinite. Also shown in this paper is a comparison between the AAA method and an ideal method based on reciprocal channel estimation, which reveals several advantages of the AAA method.

Paper number 2:
Title: A United Framework for Planning Electric Vehicle Charging Accessibility
Authors: Tony Kinchen, Panagiotis Typaldos, Andreas A. Malikopoulos
Abstract: The shift towards electric vehicles (EVs) is crucial for establishing sustainable and low-emission urban transportation systems. However, the success of this transition depends on the strategic placement of the charging infrastructure. This paper addresses the challenge of optimizing charging station locations in dense urban environments while balancing efficiency with spatial accessibility. We propose an optimization framework that integrates traffic simulation, energy consumption modeling, and a mobility equity measure to evaluate the social reach of each potential charging station. Using New York City as a case study, we demonstrate consistent improvements in accessibility (15-20% reduction in travel time variability). Our results provide a scalable methodology for incorporating equity considerations into EV infrastructure planning, although economic factors and grid integration remain important areas for future development.

Paper number 3:
Title: NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference
Authors: Edresson Casanova, Paarth Neekhara, Ryan Langman, Shehzeen Hussain, Subhankar Ghosh, Xuesong Yang, Ante JukiÄ‡, Jason Li, Boris Ginsburg
Abstract: Large Language Models (LLMs) have significantly advanced audio processing by leveraging audio codecs to discretize audio into tokens, enabling the application of language modeling techniques to speech data. However, existing audio codecs often operate at high frame rates, leading to slow training and inference, particularly for autoregressive models. To address this, there is growing interest in low frame-rate audio codecs, which reduce the number of autoregressive steps required to generate one second of audio. In this paper, we conduct ablation studies to examine the impact of frame rate, bitrate, and causality on codec reconstruction quality. Based on our findings, we introduce NanoCodec, a state-of-the-art audio codec that achieves high-quality compression at just 12.5 frames per second (FPS). NanoCodec outperforms related works across various bitrate ranges, establishing a new benchmark for low-latency and efficient Speech LLM training and inference.

Paper number 4:
Title: STEEP -- An Alternative To Quantum Key Distribution
Authors: Yingbo Hua
Abstract: Secret-message transmission by echoing encrypted probes (STEEP) is discussed as an alternative to quantum key distribution (QKD). The former only needs classic or non-quantum channels while the latter needs both quantum and classic channels for secret-key generation. STEEP is shown to yield a secrecy rate sufficient for one-time pads encryption in many practical situations including in-air channels or undersea optical cables. Other advantages of STEEP over QKD include cost, complexity, compatibility, and robustness against constant eavesdropping.

Paper number 5:
Title: Distributed Optimization and Learning for Automated Stepsize Selection with Finite Time Coordination
Authors: Apostolos I. Rikos, Nicola Bastianello, Themistoklis Charalambous, Karl H. Johansson
Abstract: Distributed optimization and learning algorithms are designed to operate over large scale networks enabling processing of vast amounts of data effectively and efficiently. One of the main challenges for ensuring a smooth learning process in gradient-based methods is the appropriate selection of a learning stepsize. Most current distributed approaches let individual nodes adapt their stepsizes locally. However, this may introduce stepsize heterogeneity in the network, thus disrupting the learning process and potentially leading to divergence. In this paper, we propose a distributed learning algorithm that incorporates a novel mechanism for automating stepsize selection among nodes. Our main idea relies on implementing a finite time coordination algorithm for eliminating stepsize heterogeneity among nodes. We analyze the operation of our algorithm and we establish its convergence to the optimal solution. We conclude our paper with numerical simulations for a linear regression problem, showcasing that eliminating stepsize heterogeneity enhances convergence speed and accuracy against current approaches.

Paper number 6:
Title: Distributed Quantized Average Consensus in Open Multi-Agent Systems with Dynamic Communication Links
Authors: Jiaqi Hu, Karl H. Johansson, Apostolos I. Rikos
Abstract: In this paper, we focus on the distributed quantized average consensus problem in open multi-agent systems consisting of communication links that change dynamically over time. Open multi-agent systems exhibiting the aforementioned characteristic are referred to as \textit{open dynamic multi-agent systems} in this work. We present a distributed algorithm that enables active nodes in the open dynamic multi-agent system to calculate the quantized average of their initial states. Our algorithm consists of the following advantages: (i) ensures efficient communication by enabling nodes to exchange quantized valued messages, and (ii) exhibits finite time convergence to the desired solution. We establish the correctness of our algorithm and we present necessary and sufficient topological conditions for it to successfully solve the quantized average consensus problem in an open dynamic multi-agent system. Finally, we illustrate the performance of our algorithm with numerical simulations.

Paper number 7:
Title: IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis
Authors: Amirhossein Taherpour, Somayeh Khani, Abbas Taherpour, Tamer Khattab
Abstract: This paper addresses the problem of activity detection in distributed Internet of Things (IoT) networks, where devices employ asynchronous transmissions with heterogeneous power levels to report their local observations. The system leverages an intelligent reflecting surface (IRS) to enhance detection reliability, with optional incorporation of a direct line-of-sight (LoS) path. We formulate the detection problem as a binary hypothesis test and develop four detectors: an optimal detector alongside three computationally efficient detectors designed for practical scenarios with different levels of prior knowledge about noise variance, channel state information, and device transmit powers. For each detector, we derive closed-form expressions for both detection and false alarm probabilities, establishing theoretical performance benchmarks. Extensive simulations validate our analytical results and systematically evaluate the impact of key system parameters including the number of antennas, samples, users, and IRS elements on detection performance. The proposed framework effectively bridges theoretical optimality with implementation practicality, providing a scalable solution for IRS-assisted IoT networks in emerging 6G systems.

Paper number 8:
Title: Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives
Authors: Zeping Sui, Qu Luo, Zilong Liu, Murat Temiz, Leila Musavian, Christos Masouros, Yong Liang Guan, Pei Xiao, Lajos Hanzo
Abstract: To meet the increasingly demanding quality-of-service requirements of the next-generation multi-carrier mobile networks, it is essential to design multi-functional signalling schemes facilitating efficient, flexible, and reliable communication and sensing in complex wireless environments. As a compelling candidate, we advocate chirp signalling, beneficially amalgamating sequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread spectrum and frequency-modulated continuous wave (FMCW) radar), given their resilience against doubly selective channels. Besides chirp sequences, a wide range of chirp waveforms is considered, ranging from FMCW to affine frequency-division multiplexing (AFDM), to create a promising chirp multicarrier waveform. This study also highlights the advantages of such waveforms in supporting reliable high-mobility communications, plus integrated sensing and communications (ISAC). Finally, we outline several emerging research directions for chirp signalling designs.

Paper number 9:
Title: Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models
Authors: Tien Ngoc Ha, Daniel Romero
Abstract: Radio map estimation (RME) is the problem of inferring the value of a certain metric (e.g. signal power) across an area of interest given a collection of measurements. While most works tackle this problem from a purely non-Bayesian perspective, some Bayesian estimators have been proposed. However, the latter focus on estimating the map itself, the Bayesian standpoint is adopted mainly to exploit prior information or to capture uncertainty. This paper pursues a more general formulation, where the goal is to determine the posterior distribution of the map given the measurements. Besides handling uncertainty and allowing standard Bayesian estimates, solving this problem is seen to enable minimum mean square error estimation of arbitrary map functionals (e.g. capacity, bit error rate, or coverage area to name a few) while training only for power estimation. A general Bayesian estimator is proposed based on conditional diffusion models and both the Bayesian and non-Bayesian paradigms are compared analytically and numerically to determine when the Bayesian approach is preferable.

Paper number 10:
Title: Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling
Authors: Yiheng Wang, Shutao Zhang, Ye Xue, Tsung-Hui Chang
Abstract: This paper presents MM-LSCM, a self-supervised multi-modal neural radio radiance field framework for localized statistical channel modeling (LSCM) for next-generation network optimization. Traditional LSCM methods rely solely on RSRP data, limiting their ability to model environmental structures that affect signal propagation. To address this, we propose a dual-branch neural architecture that integrates RSRP data and LiDAR point cloud information, enhancing spatial awareness and predictive accuracy. MM-LSCM leverages volume-rendering-based multi-modal synthesis to align radio propagation with environmental obstacles and employs a self-supervised training approach, eliminating the need for costly labeled data. Experimental results demonstrate that MM-LSCM significantly outperforms conventional methods in channel reconstruction accuracy and robustness to noise, making it a promising solution for real-world wireless network optimization.

Paper number 11:
Title: Panel-Scale Reconfigurable Photonic Interconnects for Scalable AI Computation
Authors: Tzu-Chien Hsueh, Bill Lin, Zijun Chen, Yeshaiahu Fainman
Abstract: Panel-scale reconfigurable photonic interconnects on a glass substrate up to 500-mm x 500-mm or larger are envisioned by proposing a novel photonic switch fabric that enables all directional panel-edge-to-panel-edge reach without the need for active repeaters while offering high communication bandwidth, planar-direction reconfigurability, low energy consumption, and compelling data bandwidth density for heterogeneous integration of an in-package AI computing system on a single glass-substrate photonic interposer exceeding thousands of centimeters square. The proposed approach focuses on reconfigurable photonic interconnects, which are integration-compatible with commercial processor chiplets and 3D high-bandwidth memory (HBM) stacks on a large-area glass substrate, to create a novel panel-scale heterogeneously integrated interposer or package enabling low-energy and high-capacity wavelength-division-multiplexing (WDM) optical data links using advanced high-speed optical modulators, broadband photodetectors, novel optical crossbar switches with multi-layer waveguides, and in-package frequency comb sources.

Paper number 12:
Title: Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework
Authors: Ojonugwa Oluwafemi Ejiga Peter, Daniel Emakporuena, Bamidele Dayo Tunde, Maryam Abdulkarim, Abdullahi Bn Umar
Abstract: Breast cancer detection through mammography interpretation remains difficult because of the minimal nature of abnormalities that experts need to identify alongside the variable interpretations between readers. The potential of CNNs for medical image analysis faces two limitations: they fail to process both local information and wide contextual data adequately, and do not provide explainable AI (XAI) operations that doctors need to accept them in clinics. The researcher developed the MammoFormer framework, which unites transformer-based architecture with multi-feature enhancement components and XAI functionalities within one framework. Seven different architectures consisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were tested alongside four enhancement techniques, including original images, negative transformation, adaptive histogram equalization, and histogram of oriented gradients. The MammoFormer framework addresses critical clinical adoption barriers of AI mammography systems through: (1) systematic optimization of transformer architectures via architecture-specific feature enhancement, achieving up to 13% performance improvement, (2) comprehensive explainable AI integration providing multi-perspective diagnostic interpretability, and (3) a clinically deployable ensemble system combining CNN reliability with transformer global context modeling. The combination of transformer models with suitable feature enhancements enables them to achieve equal or better results than CNN approaches. ViT achieves 98.3% accuracy alongside AHE while Swin Transformer gains a 13.0% advantage through HOG enhancements

Paper number 13:
Title: Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks
Authors: Marco Bertuletti, Yichao Zhang, Mahdi Abdollahpour, Samuel Riedel, Alessandro Vanelli-Coralli
Abstract: The fast-rising demand for wireless bandwidth requires rapid evolution of high-performance baseband processing infrastructure. Programmable many-core processors for software-defined radio (SDR) have emerged as high-performance baseband processing engines, offering the flexibility required to capture evolving wireless standards and technologies. This trend must be supported by a design framework enabling functional validation and end-to-end performance analysis of SDR hardware within realistic radio environment models. We propose a static binary translation based simulator augmented with a fast, approximate timing model of the hardware and coupled to wireless channel models to simulate the most performance-critical physical layer functions implemented in software on a many (1024) RISC-V cores cluster customized for SDR. Our framework simulates the detection of a 5G OFDM-symbol on a server-class processor in 9.5s-3min, on a single thread, depending on the input MIMO size (three orders of magnitude faster than RTL simulation). The simulation is easily parallelized to 128 threads with 73-121x speedup compared to a single thread.

Paper number 14:
Title: A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks
Authors: Marco Bertuletti, Yichao Zhang, Alessandro Vanelli-Coralli, Luca Benini
Abstract: Following the scale-up of new radio (NR) complexity in 5G and beyond, the physical layer's computing load on base stations is increasing under a strictly constrained latency and power budget; base stations must process > 20-Gb/s uplink wireless data rate on the fly, in < 10 W. At the same time, the programmability and reconfigurability of base station components are the key requirements; it reduces the time and cost of new networks' deployment, it lowers the acceptance threshold for industry players to enter the market, and it ensures return on investments in a fast-paced evolution of standards. In this article, we present the design of a many-core cluster for 5G and beyond base station processing. Our design features 1024, streamlined RISC-V cores with domain-specific FP extensions, and 4-MiB shared memory. It provides the necessary computational capabilities for software-defined processing of the lower physical layer of 5G physical uplink shared channel (PUSCH), satisfying high-end throughput requirements (66 Gb/s for a transition time interval (TTI), 9.4-302 Gb/s depending on the processing stage). The throughput metrics for the implemented functions are ten times higher than in state-of-the-art (SoTA) application-specific instruction processors (ASIPs). The energy efficiency on key NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 Â°C, and 0.8 V, on a placed and routed instance in 12-nm CMOS technology, is competitive with SoTA architectures. The PUSCH processing runs end-to-end on a single cluster in 1.7 ms, at <6-W average power consumption, achieving 12 Gb/s/W.

Paper number 15:
Title: Clinically-guided Data Synthesis for Laryngeal Lesion Detection
Authors: Chiara Baldini, Kaisar Kushibar, Richard Osuala, Simone Balocco, Oliver Diaz, Karim Lekadir, Leonardo S. Mattos
Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have made significant progress in various medical domains, their application is still limited in specialized fields such as otorhinolaryngology. In the latter, current assessment methods heavily depend on operator expertise, and the high heterogeneity of lesions complicates diagnosis, with biopsy persisting as the gold standard despite its substantial costs and risks. A critical bottleneck for specialized endoscopic CADx/e systems is the lack of well-annotated datasets with sufficient variability for real-world generalization. This study introduces a novel approach that exploits a Latent Diffusion Model (LDM) coupled with a ControlNet adapter to generate laryngeal endoscopic image-annotation pairs, guided by clinical observations. The method addresses data scarcity by conditioning the diffusion process to produce realistic, high-quality, and clinically relevant image features that capture diverse anatomical conditions. The proposed approach can be leveraged to expand training datasets for CADx/e models, empowering the assessment process in laryngology. Indeed, during a downstream task of detection, the addition of only 10% synthetic data improved the detection rate of laryngeal lesions by 9% when the model was internally tested and 22.1% on out-of-domain external data. Additionally, the realism of the generated images was evaluated by asking 5 expert otorhinolaryngologists with varying expertise to rate their confidence in distinguishing synthetic from real images. This work has the potential to accelerate the development of automated tools for laryngeal disease diagnosis, offering a solution to data scarcity and demonstrating the applicability of synthetic data in real-world scenarios.

Paper number 16:
Title: EchoFree: Towards Ultra Lightweight and Efficient Neural Acoustic Echo Cancellation
Authors: Xingchen Li, Boyi Kang, Ziqian Wang, Zihan Zhang, Mingshuai Liu, Zhonghua Fu, Lei Xie
Abstract: In recent years, neural networks (NNs) have been widely applied in acoustic echo cancellation (AEC). However, existing approaches struggle to meet real-world low-latency and computational requirements while maintaining performance. To address this challenge, we propose EchoFree, an ultra lightweight neural AEC framework that combines linear filtering with a neural post filter. Specifically, we design a neural post-filter operating on Bark-scale spectral features. Furthermore, we introduce a two-stage optimization strategy utilizing self-supervised learning (SSL) models to improve model performance. We evaluate our method on the blind test set of the ICASSP 2023 AEC Challenge. The results demonstrate that our model, with only 278K parameters and 30 MMACs computational complexity, outperforms existing low-complexity AEC models and achieves performance comparable to that of state-of-the-art lightweight model DeepVQE-S. The audio examples are available.

Paper number 17:
Title: Efficient Deep Neural Receiver with Post-Training Quantization
Authors: SaiKrishna Saketh Yellapragada, Esa Ollila, Mario Costa
Abstract: Deep learning has recently garnered significant interest in wireless communications due to its superior performance compared to traditional model-based algorithms. Deep convolutional neural networks (CNNs) have demonstrated notable improvements in block error rate (BLER) under various channel models and mobility scenarios. However, the high computational complexity and resource demands of deep CNNs pose challenges for deployment in resource-constrained edge systems. The 3rd Generation Partnership Project (3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI) integration in enabling advanced radio-access networks for 6G systems. The hard real-time processing demands of 5G and 6G require efficient techniques such as post-training quantization (PTQ), quantization-aware training (QAT), pruning, and hybrid approaches to meet latency requirements. In this paper, we focus on PTQ to reduce model complexity by lowering the bit-width of weights, thereby enhancing computational efficiency. Our analysis employs symmetric uniform quantization, applying both per-tensor and per-channel PTQ to a neural receiver achieving performance comparable to full-precision models. Specifically, 8-bit per-channel quantization maintains BLER performance with minimal degradation, while 4-bit quantization shows great promise but requires further optimization to achieve target BLER levels. These results highlight the potential of ultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.

Paper number 18:
Title: Deep Learning Based Reconstruction Methods for Electrical Impedance Tomography
Authors: Alexander Denker, Fabio Margotti, Jianfeng Ning, Kim Knudsen, Derick Nganyu Tanyu, Bangti Jin, Andreas Hauptmann, Peter Maass
Abstract: Electrical Impedance Tomography (EIT) is a powerful imaging modality widely used in medical diagnostics, industrial monitoring, and environmental studies. The EIT inverse problem is about inferring the internal conductivity distribution of the concerned object from the voltage measurements taken on its boundary. This problem is severely ill-posed, and requires advanced computational approaches for accurate and reliable image reconstruction. Recent innovations in both model-based reconstruction and deep learning have driven significant progress in the field. In this review, we explore learned reconstruction methods that employ deep neural networks for solving the EIT inverse problem. The discussion focuses on the complete electrode model, one popular mathematical model for real-world applications of EIT. We compare a wide variety of learned approaches, including fully-learned, post-processing and learned iterative methods, with several conventional model-based reconstruction techniques, e.g., sparsity regularization, regularized Gauss-Newton iteration and level set method. The evaluation is based on three datasets: a simulated dataset of ellipses, an out-of-distribution simulated dataset, and the KIT4 dataset, including real-world measurements. Our results demonstrate that learned methods outperform model-based methods for in-distribution data but face challenges in generalization, where hybrid methods exhibit a good balance of accuracy and adaptability.

Paper number 19:
Title: Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment
Authors: Fredrik Cumlin, Xinyu Liang, Anubhab Ghosh, Saikat Chatterjee
Abstract: Non-intrusive speech quality assessment (SQA) systems suffer from limited training data and costly human annotations, hindering their generalization to real-time conferencing calls. In this work, we propose leveraging large language models (LLMs) as pseudo-raters for speech quality to address these data bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129 speech clips with simulated degradations labeled by a fine-tuned auditory LLM (Vicuna-7b-v1.5). We compare three training strategies: using human-labeled data, using LLM-labeled data, and a two-stage approach (pretraining on LLM labels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS. We test on several datasets across languages and quality degradations. While LLM-labeled training yields mixed results compared to human-labeled training, we provide empirical evidence that the two-stage approach improves the generalization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on NISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings demonstrate the potential of using LLMs as scalable pseudo-raters for speech quality assessment, offering a cost-effective solution to the data limitation problem.

Paper number 20:
Title: Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification
Authors: Mobarak Abumohsen, Enrique Costa-Montenegro, Silvia GarcÃ­a-MÃ©ndez, Amani Yousef Owda, Majdi Owda
Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.

Paper number 21:
Title: Egonoise Resilient Source Localization and Speech Enhancement for Drones Using a Hybrid Model and Learning-Based Approach
Authors: Yihsuan Wu, Yukai Chiu, Michael Anthony, Mingsian R. Bai
Abstract: Drones are becoming increasingly important in search and rescue missions, and even military operations. While the majority of drones are equipped with camera vision capabilities, the realm of drone audition remains underexplored due to the inherent challenge of mitigating the egonoise generated by the rotors. In this paper, we present a novel technique to address this extremely low signal-to-noise ratio (SNR) problem encountered by the microphone-embedded drones. The technique is implemented using a hybrid approach that combines Array Signal Processing (ASP) and Deep Neural Networks (DNN) to enhance the speech signals captured by a six-microphone uniform circular array mounted on a quadcopter. The system performs localization of the target speaker through beamsteering in conjunction with speech enhancement through a Generalized Sidelobe Canceller-DeepFilterNet 2 (GSC-DF2) system. To validate the system, the DREGON dataset and measured data are employed. Objective evaluations of the proposed hybrid approach demonstrated its superior performance over four baseline methods in the SNR condition as low as -30 dB.

Paper number 22:
Title: MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications
Authors: Danish Mehmood Mughal, Daniyal Munir, Qazi Arbab Ahmed, Hans D. Schotten, Thorsten Jungeblut, Sang-Hyo Kim, Min Young Chung
Abstract: Reconfigurable intelligent surfaces (RIS) enhance wireless communication by dynamically shaping the propagation environment, but their integration introduces hardware-level security risks. This paper presents the concept of Malicious RIS (MALRIS), where compromised components behave adversarially, even under passive operation. The focus of this work is on practical threats such as manufacturing time tampering, malicious firmware, and partial element control. Two representative attacks, power-splitting and element-splitting, are modeled to assess their impact. Simulations in a RIS-assisted system reveal that even a limited hardware compromise can significantly degrade performance metrics such as bit error rate, throughput, and secrecy metrics. By exposing this overlooked threat surface, this work aims to promote awareness and support secure, trustworthy RIS deployment in future wireless networks.

Paper number 23:
Title: Use Cases for Voice Anonymization
Authors: Sarina Meyer, Ngoc Thang Vu
Abstract: The performance of a voice anonymization system is typically measured according to its ability to hide the speaker's identity and keep the data's utility for downstream tasks. This means that the requirements the anonymization should fulfill depend on the context in which it is used and may differ greatly between use cases. However, these use cases are rarely specified in research papers. In this paper, we study the implications of use case-specific requirements on the design of voice anonymization methods. We perform an extensive literature analysis and user study to collect possible use cases and to understand the expectations of the general public towards such tools. Based on these studies, we propose the first taxonomy of use cases for voice anonymization, and derive a set of requirements and design criteria for method development and evaluation. Using this scheme, we propose to focus more on use case-oriented research and development of voice anonymization systems.

Paper number 24:
Title: Acoustic Non-Stationarity Objective Assessment with Hard Label Criteria for Supervised Learning Models
Authors: Guilherme Zucatelli, Ricardo Barioni, Gabriela Dantas
Abstract: Objective non-stationarity measures are resource intensive and impose critical limitations for real-time processing solutions. In this paper, a novel Hard Label Criteria (HLC) algorithm is proposed to generate a global non-stationarity label for acoustic signals, enabling supervised learning strategies to be trained as stationarity estimators. The HLC is first evaluated on state-of-the-art general-purpose acoustic models, demonstrating that these models encode stationarity information. Furthermore, the first-of-its-kind HLC-based Network for Acoustic Non-Stationarity Assessment (NANSA) is proposed. NANSA models outperform competing approaches, achieving up to 99\% classification accuracy, while solving the computational infeasibility of traditional objective measures.

Paper number 25:
Title: Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation
Authors: Zhiwen Zhou
Abstract: Low-altitude unmanned aerial vehicles (UAVs) are expected to play an important role for low-altitude economy with a wide range of applications like precise agriculture, aerial delivery and surveillance. Integrated sensing and communication (ISAC) is a key technology to enable the large-scale deployment and routine usage of UAVs by providing both communication and sensing services efficiently. For UAV ISAC systems, as UAV often acts as both a communication user equipment (UE) and a sensing target, traditional ISAC systems that usually allocate dedicated TF resources for sensing are inefficient due to the severe degradation of communication spectral efficiency. To address this issue, in this paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM)-based ISAC framework for UAVs that eliminates the need for dedicated sensing TF resources, achieving zero TF sensing overhead. By designing the transmit beamforming to meet the requirements for both communication and sensing tasks, our proposed approach enables the communication TF resources to be fully reused for sensing, thereby enhancing both the communication sum rate and the sensing performance in terms of resolution, unambiguous range, and accuracy. Additionally, we introduce a low-complexity target searching beamforming algorithm and a two-stage super-resolution sensing algorithm, which ensure efficient implementation. Simulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not only improves the communication sum rate but also outperforms traditional ISAC systems in sensing performance, making it a promising solution for future ISAC systems to support low-altitude UAVs.

Paper number 26:
Title: Multivariate Fields of Experts
Authors: Stanislas Ducotterd, Michael Unser
Abstract: We introduce the multivariate fields of experts, a new framework for the learning of image priors. Our model generalizes existing fields of experts methods by incorporating multivariate potential functions constructed via Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of our proposal across a range of inverse problems that include image denoising, deblurring, compressed-sensing magnetic-resonance imaging, and computed tomography. The proposed approach outperforms comparable univariate models and achieves performance close to that of deep-learning-based regularizers while being significantly faster, requiring fewer parameters, and being trained on substantially fewer data. In addition, our model retains a relatively high level of interpretability due to its structured design.

Paper number 27:
Title: Random Walk Learning and the Pac-Man Attack
Authors: Xingran Chen, Parimal Parag, Rohit Bhagat, Zonghong Liu, Salim El Rouayheb
Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the Average Crossing (AC) algorithm--a fully decentralized mechanism for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our theoretical analysis establishes that (i) the RW population remains almost surely bounded under AC and (ii) RW-based stochastic gradient descent remains convergent under AC, even in the presence of Pac-Man, with a quantifiable deviation from the true optimum. Our extensive empirical results on both synthetic and real-world datasets corroborate our theoretical findings. Furthermore, they uncover a phase transition in the extinction probability as a function of the duplication threshold. We offer theoretical insights by analyzing a simplified variant of the AC, which sheds light on the observed phase transition.

Paper number 28:
Title: Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control
Authors: Yan Zhang
Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread adoption of Electric Vehicles (EVs), and the growing frequency of extreme weather events have significantly increased the complexity of power grid planning, operation, and management. Traditional rule-based systems and numerical optimization approaches often struggle with the scale, dynamics, and adaptability required by modern power networks. This paper introduces Grid-Agent, an autonomous, AI-driven framework that combines Large Language Models (LLMs) with multi-agent reinforcement learning to detect and remediate grid violations in real time. Grid-Agent integrates semantic reasoning with numerical precision through a modular agent architecture: a planning agent generates coordinated action sequences using numerical power flow solvers, while a validation agent evaluates system stability and action effectiveness via sandboxed execution with safety rollbacks. To ensure scalability, Grid-Agent incorporates an adaptive multiscale network representation that dynamically selects optimal encoding schemes based on network size and complexity. The framework enables coordinated violation resolution through optimizing switch configurations, battery deployment, and load curtailment strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE 69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation performance. Additionally, the framework's built-in data collection and learning capabilities enable continuous learning and adaptation to diverse network topologies. The autonomous nature of the framework makes it particularly suitable for modern smart grid applications requiring rapid response to dynamic operating conditions.

Paper number 29:
Title: Optimizing MV CBCT Imaging Protocols Using NTCP and Secondary Cancer Risk: A Multi-Site Study in Breast, Pelvic, and Head & Neck Radiotherapy
Authors: Thanh Tai Duong, Tien Phat Luong, Trung Kien Tran, Tuan Linh Duong, Ngoc Anh Nguyen, Quang Hung Nguyen, Peter Sandwall, Parham Alaei, David Bradley, James C. L. Chow
Abstract: Purpose: To evaluate the cumulative radiobiological impact of daily Megavoltage Cone-Beam Computed Tomography (MV-CBCT) imaging dose based on Normal Tissue Complication Probability (NTCP) and Excess Absolute Risk (EAR) of secondary malignancies among radiotherapy patients treated for breast, pelvic, and head and neck cancers. This study investigated whether MV-CBCT imaging dose warrants protocol personalization according to patient age, anatomical treatment site, and organ-specific radiosensitivity. Methods: This retrospective study included cohorts of breast (n=30), pelvic (n=17), and head and neck (n=20) cancer patients undergoing radiotherapy with daily MV-CBCT. Imaging plans using two common protocols (5 MU and 10 MU per fraction) were analyzed. NTCP values were estimated using logistic and Lyman-Kutcher-Burman (LKB) models, while EAR was calculated using Schneider's Organ Equivalent Dose (OED)-based model. Statistical analysis used paired t-tests, and results were further stratified by age (under 40, 40 to 60, over 60 years). Results: In breast cancer patients, NTCP for lung increased significantly under the 10 MU protocol (p<0.001). EAR was elevated in younger breast patients (under 40 years), with some exceeding 15 cases per 10,000 person-years. In pelvic and head and neck groups, NTCP and EAR remained low (under 1 percent), with no clinically meaningful differences between protocols. Across all sites, younger age correlated with higher secondary cancer risk. Conclusion: Daily 10 MU MV-CBCT presents minimal additional risk in pelvic and head and neck radiotherapy. For breast cancer patients under 40, however, it significantly increases secondary cancer risk and lung NTCP. Personalized imaging protocols are recommended based on age, treatment site, and radiosensitivity.

Paper number 30:
Title: A Framework for Inherently Safer AGI through Language-Mediated Active Inference
Authors: Bo Wen
Abstract: This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations. We present an architecture where safety guarantees are integrated into the system's core design through transparent belief representations and hierarchical value alignment. Our framework leverages natural language as a medium for representing and manipulating beliefs, enabling direct human oversight while maintaining computational tractability. The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures. The paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.

Paper number 31:
Title: GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems
Authors: Keyvan Majd, Hardik Parwana, Bardh Hoxha, Steven Hong, Hideki Okamoto, Georgios Fainekos
Abstract: Articulated vehicles such as tractor-trailers, yard trucks, and similar platforms must often reverse and maneuver in cluttered spaces where pedestrians are present. We present how Barrier-Rate guided Model Predictive Path Integral (BR-MPPI) control can solve navigation in such challenging environments. BR-MPPI embeds Control Barrier Function (CBF) constraints directly into the path-integral update. By steering the importance-sampling distribution toward collision-free, dynamically feasible trajectories, BR-MPPI enhances the exploration strength of MPPI and improves robustness of resulting trajectories. The method is evaluated in the high-fidelity CarMaker simulator on a 12 [m] tractor-trailer tasked with reverse and forward parking in a parking lot. BR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for scenarios with eight obstacles) and maintains better parking clearance than a standard MPPI baseline and an MPPI with collision cost baseline.

Paper number 32:
Title: Progress and new challenges in image-based profiling
Authors: Erik Serrano, John Peters, Jesko Wagner, Rebecca E. Graham, Zhenghao Chen, Brian Feng, Gisele Miranda, Alexandr A. Kalinin, Loan Vulliard, Jenna Tomkinson, Cameron Mattson, Michael J. Lippincott, Ziqi Kang, Divya Sitani, Dave Bunten, Srijit Seal, Neil O. Carragher, Anne E. Carpenter, Shantanu Singh, Paula A. Marin Zapata, Juan C. Caicedo, Gregory P. Way
Abstract: For over two decades, image-based profiling has revolutionized cellular phenotype analysis. Image-based profiling processes rich, high-throughput, microscopy data into unbiased measurements that reveal phenotypic patterns powerful for drug discovery, functional genomics, and cell state classification. Here, we review the evolving computational landscape of image-based profiling, detailing current procedures, discussing limitations, and highlighting future development directions. Deep learning has fundamentally reshaped image-based profiling, improving feature extraction, scalability, and multimodal data integration. Methodological advancements such as single-cell analysis and batch effect correction, drawing inspiration from single-cell transcriptomics, have enhanced analytical precision. The growth of open-source software ecosystems and the development of community-driven standards have further democratized access to image-based profiling, fostering reproducibility and collaboration across research groups. Despite these advancements, the field still faces significant challenges requiring innovative solutions. By focusing on the technical evolution of image-based profiling rather than the wide-ranging biological applications, our aim with this review is to provide researchers with a roadmap for navigating the progress and new challenges in this rapidly advancing domain.

Paper number 33:
Title: Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction
Authors: Ahmad Farooq, Kamran Iqbal
Abstract: This paper presents a novel approach that integrates vision foundation models with reinforcement learning to enhance object interaction capabilities in simulated environments. By combining the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the AI2-THOR simulation environment, we enable the agent to perceive and interact with objects more effectively. Our comprehensive experiments, conducted across four diverse indoor kitchen settings, demonstrate significant improvements in object interaction success rates and navigation efficiency compared to a baseline agent without advanced perception. The results show a 68% increase in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency. These findings highlight the potential of integrating foundation models with reinforcement learning for complex robotic tasks, paving the way for more sophisticated and capable autonomous agents.

Paper number 34:
Title: Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance
Authors: Haruto Nakashima, Siddhartha Ganguly, Kenji Kashima
Abstract: We tackle the data-driven chance-constrained density steering problem using the Gromov-Wasserstein metric. The underlying dynamical system is an unknown linear controlled recursion, with the assumption that sufficiently rich input-output data from pre-operational experiments are available. The initial state is modeled as a Gaussian mixture, while the terminal state is required to match a specified Gaussian distribution. We reformulate the resulting optimal control problem as a difference-of-convex program and show that it can be efficiently and tractably solved using the DC algorithm. Numerical results validate our approach through various data-driven schemes.

Paper number 35:
Title: Decentralized Optimization via RC-ALADIN with Efficient Quantized Communication
Authors: Xu Du, Karl H. Johansson, Apostolos I. Rikos
Abstract: In this paper, we investigate the problem of decentralized consensus optimization over directed graphs with limited communication bandwidth. We introduce a novel decentralized optimization algorithm that combines the Reduced Consensus Augmented Lagrangian Alternating Direction Inexact Newton (RC-ALADIN) method with a finite time quantized coordination protocol, enabling quantized information exchange among nodes. Assuming the nodes' local objective functions are $\mu$-strongly convex and simply smooth, we establish global convergence at a linear rate to a neighborhood of the optimal solution, with the neighborhood size determined by the quantization level. Additionally, we show that the same convergence result also holds for the case where the local objective functions are convex and $L$-smooth. Numerical experiments demonstrate that our proposed algorithm compares favorably against algorithms in the current literature while exhibiting communication efficient operation.

Paper number 36:
Title: SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems
Authors: Ioan-Sorin Comsa, Purav Shah, Karthik Vaidhyanathan, Deepak Gangadharan, Christof Imhof, Per Bergamin, Aryan Kaushik, Gabriel-Miro Muntean, Ramona Trestian
Abstract: The advent of 6G networks opens new possibilities for connected infotainment services in vehicular environments. However, traditional Radio Resource Management (RRM) techniques struggle with the increasing volume and complexity of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To address this, we propose SCAR (State-Space Compression for AI-Driven Resource Management), an Edge AI-assisted framework that optimizes scheduling and fairness in vehicular infotainment. SCAR employs ML-based compression techniques (e.g., clustering and RBF networks) to reduce CQI data size while preserving essential features. These compressed states are used to train 6G-enabled Reinforcement Learning policies that maximize throughput while meeting fairness objectives defined by the NGMN. Simulations show that SCAR increases time in feasible scheduling regions by 14\% and reduces unfair scheduling time by 15\% compared to RL baselines without CQI compression. Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based clustering reduces CQI clustering distortion by 10\%, confirming its efficiency. These results demonstrate SCAR's scalability and fairness benefits for dynamic vehicular networks.

Paper number 37:
Title: Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis
Authors: Wenjie Tian, Xinfa Zhu, Hanke Xie, Zhen Ye, Wei Xue, Lei Xie
Abstract: Recent progress in text-to-speech (TTS) has achieved impressive naturalness and flexibility, especially with the development of large language model (LLM)-based approaches. However, existing autoregressive (AR) structures and large-scale models, such as Llasa, still face significant challenges in inference latency and streaming synthesis. To deal with the limitations, we introduce Llasa+, an accelerated and streaming TTS model built on Llasa. Specifically, to accelerate the generation process, we introduce two plug-and-play Multi-Token Prediction (MTP) modules following the frozen backbone. These modules allow the model to predict multiple tokens in one AR step. Additionally, to mitigate potential error propagation caused by inaccurate MTP, we design a novel verification algorithm that leverages the frozen backbone to validate the generated tokens, thus allowing Llasa+ to achieve speedup without sacrificing generation quality. Furthermore, we design a causal decoder that enables streaming speech reconstruction from tokens. Extensive experiments show that Llasa+ achieves a 1.48X speedup without sacrificing generation quality, despite being trained only on LibriTTS. Moreover, the MTP-and-verification framework can be applied to accelerate any LLM-based model. All codes and models are publicly available at this https URL.

Paper number 38:
Title: A "good regulator theorem" for embodied agents
Authors: Nathaniel Virgo, Martin Biehl, Manuel Baltieri, Matteo Capucci
Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a system must be a model of that system." Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup. Nevertheless, here we show that a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable. However, it necessitates a change in perspective, in that the observer plays an essential role in the theory: models are not a mere property of the system but are imposed on it from outside. Our theorem holds regardless of whether the system is regulating its environment in a classic control theory setup, or whether it's regulating its own internal state; the model is of its environment either way. The model might be trivial, however, and this is how the apparent counterexamples are resolved.

Paper number 39:
Title: Arbitrarily-high-dimensional reconciliation via cross-rotation for continuous-variable quantum key distribution
Authors: Jisheng Dai, Xue-Qin Jiang, Tao Wang, Peng Huang, Guihua Zeng
Abstract: Multidimensional rotation serves as a powerful tool for enhancing information reconciliation and extending the transmission distance in continuous-variable quantum key distribution (CV-QKD). However, the lack of closed-form orthogonal transformations for high-dimensional rotations has limited the maximum reconciliation efficiency to channels with 8 dimensions over the past decade. This paper presents a cross-rotation scheme to overcome this limitation and enable reconciliation in arbitrarily high dimensions, constrained to even multiples of 8. The key treatment involves reshaping the string vector into matrix form and applying orthogonal transformations to its columns and rows in a cross manner, thereby increasing the reconciliation dimension by one order per cross-rotation while significantly reducing the communication overhead over the classical channel. A rigorous performance analysis is also presented from the perspective of achievable sum-rate. Simulation results demonstrate that 64-dimensional cross-rotation nearly approaches the upper bound, making it a recommended choice for practical implementations.

Paper number 40:
Title: A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery
Authors: Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Oktay Karakus
Abstract: High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.

Paper number 41:
Title: MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan
Authors: Muhammad Usman, Azka Rehman, Abd Ur Rehman, Abdullah Shahid, Tariq Mahmood Khan, Imran Razzak, Minyoung Chung, Yeong Gil Shin
Abstract: Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation of lung nodules, focusing on the nodule region in each slice to generate 3D volumetric segmentation of lung nodules. The proposed framework has been comprehensively evaluated on the LIDC-IDRI dataset, the largest publicly available dataset for lung nodule segmentation. The results demonstrate that our approach is highly robust for various lung nodule types, outperforming previous state-of-the-art techniques in terms of segmentation accuracy and computational complexity, rendering it suitable for real-time clinical implementation.

Paper number 42:
Title: A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities segmentation
Authors: Yin Li, Qi Chen, Kai Wang, Meige Li, Liping Si, Yingwei Guo, Yu Xiong, Qixing Wang, Yang Qin, Ling Xu, Patrick van der Smagt, Jun Tang, Nutan Chen
Abstract: Multi-modality magnetic resonance imaging(MRI) data facilitate the early diagnosis, tumor segmentation, and disease staging in the management of nasopharyngeal carcinoma (NPC). The lack of publicly available, comprehensive datasets limits advancements in diagnosis, treatment planning, and the development of machine learning algorithms for NPC. Addressing this critical need, we introduce the first comprehensive NPC MRI dataset, encompassing MR axial imaging of 277 primary NPC patients. This dataset includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences, totaling 831 scans. In addition to the corresponding clinical data, manually annotated and labeled segmentations by experienced radiologists offer high-quality data resources from untreated primary NPC.

Paper number 43:
Title: A Self-Attention-Driven Deep Denoiser Model for Real Time Lung Sound Denoising in Noisy Environments
Authors: Samiul Based Shuvo, Syed Samiul Alam, Taufiq Hasan
Abstract: Objective: Lung auscultation is a valuable tool in diagnosing and monitoring various respiratory diseases. However, lung sounds (LS) are significantly affected by numerous sources of contamination, especially when recorded in real-world clinical settings. Conventional denoising models prove impractical for LS denoising, primarily owing to spectral overlap complexities arising from diverse noise sources. To address this issue, we propose a specialized deep-learning model (Uformer) for lung sound denoising. Methods: The proposed Uformer model is constituted of three modules: a Convolutional Neural Network (CNN) encoder module, dedicated to extracting latent features; a Transformer encoder module, employed to further enhance the encoding of unique LS features and effectively capture intricate long-range dependencies; and a CNN decoder module, employed to generate the denoised signals. An ablation study was performed in order to find the most optimal architecture. Results: The performance of the proposed Uformer model was evaluated on lung sounds induced with different types of synthetic and real-world noises. Lung sound signals of -12 dB to 15 dB signal-to-noise ratio (SNR) were considered in testing experiments. The proposed model showed an average SNR improvement of 16.51 dB when evaluated with -12 dB LS signals. Our end-to-end model, with an average SNR improvement of 19.31 dB, outperforms the existing model when evaluated with ambient noise and fewer parameters. Conclusion: Based on the qualitative and quantitative findings in this study, it can be stated that Uformer is robust and generalized to be used in assisting the monitoring of respiratory conditions.

Paper number 44:
Title: A Versatile Framework for Data-Driven Control of Nonlinear Systems
Authors: Nima Monshizadeh, Claudio De Persis, Pietro Tesi
Abstract: This note aims to provide a systematic investigation of direct data-driven control, enriching the existing literature not by adding another isolated result, but rather by offering a unifying, versatile, and broad framework that enables the generation of novel results in this domain. We formulate the nonlinear design problem from a high-level perspective as a set of desired controlled systems and propose systematic procedures to synthesize data-driven control algorithms that meet the specified design requirements. Various examples are presented to demonstrate the applicability of the proposed approach and its ability to derive new insights and results, illustrating the novel contributions enabled by the framework.

Paper number 45:
Title: Equilibrium Selection in Replicator Equations Using Adaptive-Gain Control
Authors: Lorenzo Zino, Mengbin Ye, Giuseppe Carlo Calafiore, Alessandro Rizzo
Abstract: In this paper, we deal with the equilibrium selection problem, which amounts to steering a population of individuals engaged in strategic game-theoretic interactions to a desired collective behavior. In the literature, this problem has been typically tackled by means of open-loop strategies, whose applicability is however limited by the need of accurate a priori information on the game and scarce robustness to uncertainty and noise. Here, we overcome these limitations by adopting a closed-loop approach using an adaptive-gain control scheme within a replicator equation -a nonlinear ordinary differential equation that models the evolution of the collective behavior of the population. For most classes of 2-action matrix games we establish sufficient conditions to design a controller that guarantees convergence of the replicator equation to the desired equilibrium, requiring limited a-priori information on the game. Numerical simulations corroborate and expand our theoretical findings.

Paper number 46:
Title: MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation
Authors: Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang
Abstract: Segmenting anatomical structures and lesions from ultrasound images contributes to disease assessment. Weakly supervised learning (WSL) based on sparse annotation has achieved encouraging performance and demonstrated the potential to reduce annotation costs. This study attempts to introduce scribble-based WSL into ultrasound image segmentation tasks. However, ultrasound images often suffer from poor contrast and unclear edges, coupled with insufficient supervison signals for edges, posing challenges to edge prediction. Uncertainty modeling has been proven to facilitate models in dealing with these issues. Nevertheless, existing uncertainty estimation paradigms are not robust enough and often filter out predictions near decision boundaries, resulting in unstable edge predictions. Therefore, we propose leveraging predictions near decision boundaries effectively. Specifically, we introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided Consistency strategy. This strategy utilizes high-evidence predictions, which are more likely to occur near high-density regions, to guide the optimization of low-evidence predictions that may appear near decision boundaries. Furthermore, the diverse sizes and locations of lesions in ultrasound images pose a challenge for CNNs with local receptive fields, as they struggle to model global information. Therefore, we introduce Visual Mamba based on structured state space sequence models, which achieves long-range dependency with linear computational complexity, and we construct a novel hybrid CNN-Mamba framework. During training, the collaboration between the CNN branch and the Mamba branch in the proposed framework draws inspiration from each other based on the EGC strategy. Experiments demonstrate the competitiveness of the proposed method. Dataset and code will be available on this https URL.

Paper number 47:
Title: Koopman-based control using sum-of-squares optimization: Improved stability guarantees and data efficiency
Authors: Robin StrÃ¤sser, Julian Berberich, Frank AllgÃ¶wer
Abstract: In this paper, we propose a novel controller design approach for unknown nonlinear systems using the Koopman operator. In particular, we use the recently proposed stability- and certificate-oriented extended dynamic mode decomposition (SafEDMD) architecture to generate a data-driven bilinear surrogate model with certified error bounds. Then, by accounting for the obtained error bounds in a controller design based on the bilinear system, one can guarantee closed-loop stability for the true nonlinear system. While existing approaches over-approximate the bilinearity of the surrogate model, thus introducing conservatism and providing only local guarantees, we explicitly account for the bilinearity by using sum-of-squares (SOS) optimization in the controller design. More precisely, we parametrize a rational controller stabilizing the error-affected bilinear surrogate model and, consequently, the underlying nonlinear system. The resulting SOS optimization problem provides explicit data-driven controller design conditions for unknown nonlinear systems based on semidefinite programming. Our approach significantly reduces conservatism by establishing a larger region of attraction and improved data efficiency. The proposed method is evaluated using numerical examples, demonstrating its advantages over existing approaches.

Paper number 48:
Title: Observability and Generalized Sensor Placement for Nonlinear Quality Models in Drinking Water Networks
Authors: Mohamad H. Kazma, Salma M. Elsherif, Ahmad F. Taha
Abstract: This paper studies the problem of optimal placement of water quality (WQ) sensors in water distribution networks (WDNs), with a focus on chlorine transport, decay, and reaction models. Such models are traditionally used as suitable proxies for WQ. The literature on this topic is inveterate, but has a key limitation: it utilizes simplified single-species decay and reaction models that do not capture WQ transients for nonlinear, multi-species interactions. This results in sensor placements (SP) that do not account for nonlinear WQ dynamics. Furthermore, as WQ simulations are parameterized by hydraulic profiles and demand patterns, the placement of sensors are often hydraulics-dependent. This study produces a greedy algorithm that addresses the two aforementioned limitations. The algorithm is grounded in nonlinear dynamic systems and observability theory, and yields SPs that are submodular and robust to hydraulic changes. Case studies on benchmark water networks are provided. The key findings provide practical recommendations for WDN operators.

Paper number 49:
Title: Koopman-based control of nonlinear systems with closed-loop guarantees
Authors: Robin StrÃ¤sser, Julian Berberich, Manuel Schaller, Karl Worthmann, Frank AllgÃ¶wer
Abstract: In this paper, we provide a tutorial overview and an extension of a recently developed framework for data-driven control of unknown nonlinear systems with rigorous closed-loop guarantees. The proposed approach relies on the Koopman operator representation of the nonlinear system, for which a bilinear surrogate model is estimated based on data. In contrast to existing Koopman-based estimation procedures, we state guaranteed bounds on the approximation error using the stability- and certificate-oriented extended dynamic mode decomposition (SafEDMD) framework. The resulting surrogate model and the uncertainty bounds allow us to design controllers via robust control theory and sum-of-squares optimization, guaranteeing desirable properties for the closed-loop system. We present results on stabilization both in discrete and continuous time, and we derive a method for controller design with performance objectives. The benefits of the presented framework over established approaches are demonstrated with a numerical example.

Paper number 50:
Title: A Linear Differential Inclusion for Contraction Analysis to Known Trajectories
Authors: Akash Harapanahalli, Samuel Coogan
Abstract: Infinitesimal contraction analysis provides exponential convergence rates between arbitrary pairs of trajectories of a system by studying the system's linearization. An essentially equivalent viewpoint arises through stability analysis of a linear differential inclusion (LDI) encompassing the incremental behavior of the system. In this note, we use contraction tools to study the exponential stability of a system to a particular known trajectory, deriving a new LDI characterizing the error between arbitrary trajectories and this known trajectory. As with classical contraction analysis, this new inclusion is constructed via first partial derivatives of the system's vector field, and convergence rates are obtained with familiar tools: uniform bounding of the logarithmic norm and LMI-based Lyapunov conditions. Our LDI is guaranteed to outperform a usual contraction analysis in two special circumstances: i) when the bound on the logarithmic norm arises from an interval overapproximation of the Jacobian matrix, and ii) when the norm considered is the $\ell_1$ norm. Finally, we demonstrate how the proposed approach strictly improves an existing framework for ellipsoidal reachable set computation.

Paper number 51:
Title: CDI: Blind Image Restoration Fidelity Evaluation based on Consistency with Degraded Image
Authors: Xiaojun Tang, Jingru Wang, Guangwei Huang, Guannan Chen, Rui Zheng, Lian Huai, Yuyu Liu, Xingqun Jiang
Abstract: Recent advancements in Blind Image Restoration (BIR) methods, based on Generative Adversarial Networks and Diffusion Models, have significantly improved visual quality. However, they present significant challenges for Image Quality Assessment (IQA), as the existing Full-Reference IQA methods often rate images with high perceptual quality poorly. In this paper, we reassess the Solution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and propose constructing a specific BIR IQA system. In stead of directly comparing a restored image with a reference image, the BIR IQA evaluates fidelity by calculating the Consistency with Degraded Image (CDI). Specifically, we propose a wavelet domain Reference Guided CDI algorithm, which can acquire the consistency with a degraded image for various types without requiring knowledge of degradation parameters. The supported degradation types include down sampling, blur, noise, JPEG and complex combined degradations etc. In addition, we propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without reference images. Finally, in order to validate the rationality of CDI, we create a new Degraded Images Switch Display Comparison Dataset (DISDCD) for subjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify that CDI is markedly superior to common Full Reference IQA methods for BIR fidelity evaluation. The source code and the DISDCD dataset will be publicly available shortly.

Paper number 52:
Title: Continuous-time Data-driven Barrier Certificate Synthesis
Authors: Luke Rickard, Alessandro Abate, Kostas Margellos
Abstract: We consider the problem of verifying safety for continuous-time dynamical systems. Developing upon recent advancements in data-driven verification, we use only a finite number of sampled trajectories to learn a barrier certificate, namely a function which verifies safety. We train a safety-informed neural network to act as this certificate, with an appropriately designed loss function to encompass the safety conditions. In addition, we provide probabilistic generalisation guarantees from discrete samples of continuous trajectories, to unseen continuous ones. Numerical investigations demonstrate the efficacy of our approach and contrast it with related results in the literature.

Paper number 53:
Title: POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction
Authors: Songyan Zhang, Yongtao Ge, Jinyuan Tian, Guangkai Xu, Hao Chen, Chen Lv, Chunhua Shen
Abstract: 3D reconstruction in dynamic scenes primarily relies on the combination of geometry estimation and matching modules where the latter task is pivotal for distinguishing dynamic regions which can help to mitigate the interference introduced by camera and object motion. Furthermore, the matching module explicitly models object motion, enabling the tracking of specific targets and advancing motion understanding in complex scenarios. Recently, the proposed representation of pointmap in DUSt3R suggests a potential solution to unify both geometry estimation and matching in 3D space, but it still struggles with ambiguous matching in dynamic regions, which may hamper further improvement. In this work, we present POMATO, a unified framework for dynamic 3D reconstruction by marrying pointmap matching with temporal motion. Specifically, our method first learns an explicit matching relationship by mapping RGB pixels from both dynamic and static regions across different views to 3D pointmaps within a unified coordinate system. Furthermore, we introduce a temporal motion module for dynamic motions that ensures scale consistency across different frames and enhances performance in tasks requiring both precise geometry and reliable matching, most notably 3D point tracking. We show the effectiveness of the proposed pointmap matching and temporal fusion paradigm by demonstrating the remarkable performance across multiple downstream tasks, including video depth estimation, 3D point tracking, and pose estimation. Code and models are publicly available at this https URL.

Paper number 54:
Title: Diversity Analysis for Terahertz Communication Systems under Small-Scale Fading
Authors: Almutasem Bellah Enad, Jihad Fahs, Hakim Jemaa, Hadi Sarieddeen, Tareq Y. Al-Naffouri
Abstract: The terahertz (THz) band is a key enabler for future wireless systems, promising ultra-high data rates and dense spatial reuse. However, the reliability of THz links remains a major challenge due to severe path loss and small-scale fading effects, particularly in dynamic indoor and outdoor environments. This paper presents a comprehensive diversity analysis framework for THz communication systems under realistic small-scale fading conditions. We model fading statistically using the generalized $\alpha$-$\mu$ distribution for indoor scenarios and the mixture of gamma (MG) model for outdoor propagation. We complement previous works that analyzed diversity under $\alpha$-$\mu$ channels [1],[2]. In particular, we present new insights on diversity for the MG channel in addition to recovering the results of [2] using a different approach. Moreover, we derive asymptotic expressions for the bit error rate as a function of the inverse signal-to-noise ratio, recovering all of the $\alpha$-$\mu$ diversity results using a simpler approximation method. The analytical results are extensively validated through Monte Carlo simulations, demonstrating excellent agreement. Our findings show that the achievable diversity in THz systems is strongly influenced by the number of independent paths, the severity of fading, and frequency selectivity. The proposed framework provides system designers with clear guidelines to quantify and optimize diversity gains under emerging channel models, paving the way for more reliable high-frequency wireless links in next-generation networks.

Paper number 55:
Title: Integrative, Scalable Modeling of Hydrological Systems with MBSE and HFGT
Authors: Megan Harris, Ehsanoddin Ghorbanichemazkati, Mohammad Mahdi Naderi, John C. Little, Amro M. Farid
Abstract: Worsening global challenges in the Anthropocene demand complex, adaptive solutions grounded in a systems-level understanding of coupled social and environmental dynamics. However, existing modeling approaches often fall short due to disciplinary silos, limited scalability, and the absence of shared ontological frameworks. Model-Based Systems Engineering (MBSE), when integrated with Hetero-functional Graph Theory (HFGT), offers a powerful methodology for modeling systems of systems while preserving subsystem heterogeneity and enabling cross-disciplinary integration. This paper presents the first application of the MBSE-HFGT methodology to environmental systems, using a series of worked examples involving flow through lake and land segments. These examples demonstrate how the approach enables consistent, scalable, and integrative modeling of complex environmental processes.

Paper number 56:
Title: Spatial Association Between Near-Misses and Accident Blackspots in Sydney, Australia: A Getis-Ord $G_i^*$ Analysis
Authors: Artur Grigorev, David Lillo-Trynes, Adriana-Simona Mihaita
Abstract: Conventional road safety management is inherently reactive, relying on analysis of sparse and lagged historical crash data to identify hazardous locations, or crash blackspots. The proliferation of vehicle telematics presents an opportunity for a paradigm shift towards proactive safety, using high-frequency, high-resolution near-miss data as a leading indicator of crash risk. This paper presents a spatial-statistical framework to systematically analyze the concordance and discordance between official crash records and near-miss events within urban environment. A Getis-Ord statistic is first applied to both reported crashes and near-miss events to identify statistically significant local clusters of each type. Subsequently, Bivariate Local Moran's I assesses spatial relationships between crash counts and High-G event counts, classifying grid cells into distinct profiles: High-High (coincident risk), High-Low and Low-High. Our analysis reveals significant amount of Low-Crash, High-Near-Miss clusters representing high-risk areas that remain unobservable when relying solely on historical crash data. Feature importance analysis is performed using contextual Point of Interest data to identify the different infrastructure factors that characterize difference between spatial clusters. The results provide a data-driven methodology for transport authorities to transition from a reactive to a proactive safety management strategy, allowing targeted interventions before severe crashes occur.

Paper number 57:
Title: Post-training for Deepfake Speech Detection
Authors: Wanying Ge, Xin Wang, Xuechen Liu, Junichi Yamagishi
Abstract: We introduce a post-training approach that adapts self-supervised learning (SSL) models for deepfake speech detection by bridging the gap between general pre-training and domain-specific fine-tuning. We present AntiDeepfake models, a series of post-trained models developed using a large-scale multilingual speech dataset containing over 56,000 hours of genuine speech and 18,000 hours of speech with various artifacts in over one hundred languages. Experimental results show that the post-trained models already exhibit strong robustness and generalization to unseen deepfake speech. When they are further fine-tuned on the Deepfake-Eval-2024 dataset, these models consistently surpass existing state-of-the-art detectors that do not leverage post-training. Model checkpoints and source code are available online.

Paper number 58:
Title: Latency-Optimal File Assignment in Geo-Distributed Storage with Preferential Demands
Authors: Srivathsa Acharya, P. Vijay Kumar, Viveck R. Cadambe
Abstract: We consider the problem of data storage in a geographically distributed (or geo-distributed) network of servers (or nodes) where inter-node communication incurs certain round-trip delays. Every node serves a set of users who can request any file in the network. If the requested file is not available at the node, it communicates with other nodes to obtain the file, thus causing the user to experience latency in obtaining the file. The files can be placed uncoded, where each node stores exact copies of the files, or in coded fashion, where certain linear combination of files are placed at each node. We aim to obtain an optimal file placement on the nodes with respect to minimizing the worst-case latency at each node, as well as the system-average latency. The prior literature considered the case of equiprobable file demands at the nodes. In this paper, we investigate the generic case of non-uniform file-demand probabilities at each node. The scheme presented here is optimal within the family of uncoded schemes. It is obtained first by modeling the worst-case latency constraint as a vertex coloring problem, and then converting the system-average latency optimization to a problem of balanced-assignment.

Paper number 59:
Title: Adaptive Network Security Policies via Belief Aggregation and Rollout
Authors: Kim Hammar, Yuchao Li, Tansu Alpcan, Emil C. Lupu, Dimitri Bertsekas
Abstract: Evolving security vulnerabilities and shifting operational conditions require frequent updates to network security policies. These updates include adjustments to incident response procedures and modifications to access controls, among others. Reinforcement learning methods have been proposed for automating such policy adaptations, but most of the methods in the research literature lack performance guarantees and adapt slowly to changes. In this paper, we address these limitations and present a method for computing security policies that is scalable, offers theoretical guarantees, and adapts quickly to changes. It assumes a model or simulator of the system and comprises three components: belief estimation through particle filtering, offline policy computation through aggregation, and online policy adaptation through rollout. Central to our method is a new feature-based aggregation technique, which improves scalability and flexibility. We analyze the approximation error of aggregation and show that rollout efficiently adapts policies to changes under certain conditions. Simulations and testbed results demonstrate that our method outperforms state-of-the-art methods on several benchmarks, including CAGE-2.

Paper number 60:
Title: Attitude Determination and Control of GPS Satellites: Stabilization, Orbital Insertion, and Operational Control Mechanisms
Authors: Oliullah Samir
Abstract: Global Positioning System (GPS) satellites are essential for providing accurate navigation and timing information worldwide. Operating in medium Earth orbit (MEO), these satellites must maintain precise Earth-pointing attitudes to transmit signals effectively. This paper presents a comprehensive review of the operational dynamics, attitude determination and control systems (ADCS), and orbital insertion techniques for GPS satellites. We explore the integration of sensors and actuators, control algorithms, stabilization strategies, and the launch procedures required to deploy these satellites. Key equations related to orbital mechanics and attitude control are discussed, and references to recent technical literature are included.

Paper number 61:
Title: Measuring Dependencies between Biological Signals with Self-supervision, and its Limitations
Authors: Evangelos Sariyanidi, John D. Herrington, Lisa Yankowitz, Pratik Chaudhari, Theodore D. Satterthwaite, Casey J. Zampella, Robert T. Schultz, Russell T. Shinohara, Birkan Tunc
Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge regarding the nature of dependence. We introduce a self-supervised approach, concurrence, which is inspired by the observation that if two signals are dependent, then one should be able to distinguish between temporally aligned vs. misaligned segments extracted from them. Experiments with fMRI, physiological and behavioral signals show that, to our knowledge, concurrence is the first approach that can expose relationships across such a wide spectrum of signals and extract scientifically relevant differences without ad-hoc parameter tuning or reliance on a priori information, providing a potent tool for scientific discoveries across fields. However, dependencies caused by extraneous factors remain an open problem, thus researchers should validate that exposed relationships truly pertain to the question(s) of interest.

Paper number 62:
Title: Modeling and Simulation of an Active Quarter Car Suspension with a Robust LQR Controller under Road Disturbance and Parameter Uncertainty
Authors: Mehmet Karahan
Abstract: Vehicle suspension is important for passengers to travel comfortably and to be less exposed to effects such as vibration and shock. A good suspension system increases the road holding of vehicles, allows them to take turns safely, and reduces the risk of traffic accidents. A passive suspension system is the most widely used suspension system in vehicles due to its simple structure and low cost. Passive suspension systems do not have an actuator and therefore do not have a controller. Active suspension systems have an actuator and a controller. Although their structures are more complex and costly, they are safer. PID controller is widely used in active suspension systems due to its simple structure, reasonable cost, and easy adjustment of coefficients. In this study, a more robust LQR-controlled active suspension was designed than a passive suspension and a PID-controlled active suspension. Robustness analyses were performed for passive suspension, PID-controlled active suspension, and LQR-controlled active suspension. Suspension travel, sprung mass acceleration, and sprung mass motion simulations were performed for all 3 suspensions under road disturbance and under simultaneous road disturbance and parameter uncertainty. A comparative analysis was performed by obtaining the suspension rise time, overshoot, and settling time data. It was observed that the LQR-controlled active suspension showed the least overshoot and had the shortest settling time. In this case, it was proven that the LQR controlled active suspension provided a more comfortable and safe ride compared to the other two suspension systems.

Paper number 63:
Title: How to Proactively Monitor Untrusted Communications with Cell-Free Massive MIMO?
Authors: Isabella W. G. da Silva, Zahra Mobini, Hien Q. Ngo, Hyundong Shin, Michail Matthaiou
Abstract: This paper studies a cell-free massive multiple-input multiple-output (CF-mMIMO) proactive monitoring system in which multiple multi-antenna monitoring nodes (MNs) are assigned to either observe the transmissions from an untrusted transmitter (UT) or to jam the reception at the untrusted receiver (UR). We propose an effective channel state information (CSI) acquisition scheme for the monitoring system. In our approach, the MNs leverage the pilot signals transmitted during the uplink and downlink phases of the untrusted link and estimate the effective channels corresponding to the UT and UR via a minimum mean-squared error (MMSE) estimation scheme. We derive new spectral efficiency (SE) expressions for the untrusted link and the monitoring system. For the latter, the SE is derived for two CSI availability cases at the central processing unit (CPU); namely case-1: imperfect CSI knowledge at both MNs and CPU, case-2: imperfect CSI knowledge at the MNs and no CSI knowledge at the CPU. To improve the monitoring performance, we propose a novel joint mode assignment and jamming power control optimization method to maximize the monitoring success probability (MSP) based on the Bayesian optimization framework. Numerical results show that (a) our CF-mMIMO proactive monitoring system relying on the proposed CSI acquisition and optimization approach significantly outperforms the considered benchmarks; (b) the MSP performance of our CF-mMIMO proactive monitoring system is greater than 0.8, regardless of the number of antennas at the untrusted nodes or the precoding scheme for the untrusted transmission link.

Paper number 64:
Title: Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation
Authors: Nathan Hollet, Oumeymah Cherkaoui, Philippe C. Cattin, Sidaty El hadramy
Abstract: Liver segmentation is essential for preoperative planning in interventions like tumor resection or transplantation, but implementation in clinical workflows faces challenges due to modality-specific tools and data scarcity. We propose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentation that generalizes to out-of-distribution (OOD) data. Our method integrates classical edge detection with foundation models. Modality-agnostic edge maps are first extracted from input images, then processed by a U-Net to generate logit-based prompts. These prompts condition the Segment Anything Model 2 (SAM-2) to generate 2D liver segmentations, which can then be reconstructed into 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Prompt achieves competitive results compared to classical segmentation methods when trained and tested in-distribution (ID), and outperforms them in data-scarce scenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Score of 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and other self-prompting methods by 9.1%, demonstrating its effectiveness. This work bridges classical and foundation models for clinically adaptable, data-efficient segmentation.

Paper number 65:
Title: REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with Diffusion Transformers
Authors: Yuepeng Jiang, Ziqian Ning, Shuai Wang, Chengjia Wang, Mengxiao Bi, Pengcheng Zhu, Zhonghua Fu, Lei Xie
Abstract: In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges. Traditional ASR-based methods ensure noise robustness but suppress prosody richness, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive voice conversion system. Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL features, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps. Experimental results demonstrate that REF-VC outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible with singing voice conversion within one model.

Paper number 66:
Title: Reinforcement Learning Based Sensor Optimization for Bio-markers
Authors: Sajal Khandelwal, Pawan Kumar, Syed Azeemuddin
Abstract: Radio frequency (RF) biosensors, in particular those based on inter-digitated capacitors (IDCs), are pivotal in areas like biomedical diagnosis, remote sensing, and wireless communication. Despite their advantages of low cost and easy fabrication, their sensitivity can be hindered by design imperfections, environmental factors, and circuit noise. This paper investigates enhancing the sensitivity of IDC-based RF sensors using novel reinforcement learning based Binary Particle Swarm Optimization (RLBPSO), and it is compared to Ant Colony Optimization (ACO), and other state-of-the-art methods. By focusing on optimizing design parameters like electrode design and finger width, the proposed study found notable improvements in sensor sensitivity. The proposed RLBPSO method shows best optimized design for various frequency ranges when compared to current state-of-the-art methods.

Paper number 67:
Title: A Markov Random Field model for Hypergraph-based Machine Learning
Authors: Bohan Tang, Keyue Jiang, Laura Toni, Siheng Chen, Xiaowen Dong
Abstract: Understanding the data-generating process is essential for building machine learning models that generalise well while ensuring robustness and interpretability. This paper addresses the fundamental challenge of modelling the data generation processes on hypergraphs and explores how such models can inform the design of machine learning algorithms for hypergraph data. The key to our approach is the development of a hypergraph Markov random field that models the joint distribution of the node features and hyperedge features in a hypergraph through a multivariate Gaussian distribution whose covariance matrix is uniquely determined by the hypergraph structure. The proposed data-generating process provides a valuable inductive bias for various hypergraph machine learning tasks, thus enhancing the algorithm design. In this paper, we focus on two representative downstream tasks: structure inference and node classification. Accordingly, we introduce two novel frameworks: 1) an original hypergraph structure inference framework named HGSI, and 2) a novel learning framework entitled Hypergraph-MLP for node classification on hypergraphs. Empirical evaluation of the proposed frameworks demonstrates that: 1) HGSI outperforms existing hypergraph structure inference methods on both synthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in six hypergraph node classification benchmarks, at the same time promoting runtime efficiency and robustness against structural perturbations during inference.

Paper number 68:
Title: Off-Policy Evaluation for Sequential Persuasion Process with Unobserved Confounding
Authors: Nishanth Venkatesh S., Heeseung Bang, Andreas A. Malikopoulos
Abstract: In this paper, we expand the Bayesian persuasion framework to account for unobserved confounding variables in sender-receiver interactions. While traditional models assume that belief updates follow Bayesian principles, real-world scenarios often involve hidden variables that impact the receiver's belief formation and decision-making. We conceptualize this as a sequential decision-making problem, where the sender and receiver interact over multiple rounds. In each round, the sender communicates with the receiver, who also interacts with the environment. Crucially, the receiver's belief update is affected by an unobserved confounding variable. By reformulating this scenario as a Partially Observable Markov Decision Process (POMDP), we capture the sender's incomplete information regarding both the dynamics of the receiver's beliefs and the unobserved confounder. We prove that finding an optimal observation-based policy in this POMDP is equivalent to solving for an optimal signaling strategy in the original persuasion framework. Furthermore, we demonstrate how this reformulation facilitates the application of proximal learning for off-policy evaluation in the persuasion process. This advancement enables the sender to evaluate alternative signaling strategies using only observational data from a behavioral policy, thus eliminating the necessity for costly new experiments.

Paper number 69:
Title: Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot
Authors: Davide Gorbani, Giuseppe L'Erario, Hosameldin Awadalla Omer Mohamed, Daniele Pucci
Abstract: We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot's joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humanoid robot iRonCub, performing simulations in Mujoco; the simulation results demonstrate the robot's ability to recover from external disturbances and perform stable, non-abrupt flight manoeuvres, validating the effectiveness of the proposed approach.

Paper number 70:
Title: ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models
Authors: Kaizhi Qian, Xulin Fan, Junrui Ni, Slava Shechtman, Mark Hasegawa-Johnson, Chuang Gan, Yang Zhang
Abstract: Speech language models refer to language models with speech processing and understanding capabilities. One key desirable capability for speech language models is the ability to capture the intricate interdependency between content and prosody. The existing mainstream paradigm of training speech language models, which converts speech into discrete tokens before feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone. To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed tokenization scheme retains more complete prosody information, and is more understandable to text-based LLMs. We find that ProsodyLM can learn surprisingly diverse emerging prosody processing capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining prosody consistency in long contexts.
    