
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A Multiscale Approach for Enhancing Weak Signal Detection
Authors: Dixon Vimalajeewa, Ursula U. Muller, Brani Vidakovic
Abstract: Stochastic resonance (SR), a phenomenon originally introduced in climate modeling, enhances signal detection by leveraging optimal noise levels within non-linear systems. Traditional SR techniques, mainly based on single-threshold detectors, are limited to signals whose behavior does not depend on time. Often large amounts of noise are needed to detect weak signals, which can distort complex signal characteristics. To address these limitations, this study explores multi-threshold systems and the application of SR in multiscale applications using wavelet transforms. In the multiscale domain signals can be analyzed at different levels of resolution to better understand the underlying dynamics. We propose a double-threshold detection system that integrates two single-threshold detectors to enhance weak signal detection. We evaluate it both in the original data domain and in the multiscale domain using simulated and real-world signals and compare its performance with existing methods. Experimental results demonstrate that, in the original data domain, the proposed double-threshold detector significantly improves weak signal detection compared to conventional single-threshold approaches. Its performance is further improved in the frequency domain, requiring lower noise levels while outperforming existing detection systems. This study advances SR-based detection methodologies by introducing a robust approach to weak signal identification, with potential applications in various disciplines.

Paper number 2:
Title: Can large audio language models understand child stuttering speech? speech summarization, and source separation
Authors: Chibuzor Okocha, Maya Bakri, Christan Grant
Abstract: Child speech differs from adult speech in acoustics, prosody, and language development, and disfluencies (repetitions, prolongations, blocks) further challenge Automatic Speech Recognition (ASR) and downstream Natural Language Processing (NLP). Recent large audio-language models (LALMs) demonstrate strong cross-modal audio understanding; however, their behavior in disfluent child speech remains underexplored. We evaluate several state-of-the-art LALMs in two settings: an interview (mixed speakers) and a reading task (single child). The tasks are (i) single-channel source separation to isolate the child and (ii) child-only summarization that preserves clinically relevant disfluencies and avoids adult-speech leakage. Evaluation combines Large Language Model (LLM) as a judge, human expert ratings, and BERTScore (F1), and we report agreement between models and between models and humans to assess reliability. Our findings delineate the conditions under which LALMs produce faithful child-only summaries from mixed audio and where they fail, offering practical guidance for clinical and educational deployments. We provide prompts and evaluation scripts to support replication.

Paper number 3:
Title: Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization
Authors: Hyungjun Yoon, Seungjoo Lee, Yu Yvonne Wu, Xiaomeng Chen, Taiting Lu, Freddy Yifei Liu, Taeckyung Lee, Hyeongheon Cha, Haochen Zhao, Gaoteng Zhao, Sung-Ju Lee, Cecilia Mascolo, Dongyao Chen, Lili Qiu
Abstract: Electrophysiological (ExG) signals offer valuable insights into human physiology, yet building foundation models that generalize across everyday tasks remains challenging due to two key limitations: (i) insufficient data diversity, as most ExG recordings are collected in controlled labs with bulky, expensive devices; and (ii) task-specific model designs that require tailored processing (i.e., targeted frequency filters) and architectures, which limit generalization across tasks. To address these challenges, we introduce an approach for scalable, task-agnostic ExG monitoring in the wild. We collected 50 hours of unobtrusive free-living ExG data with an earphone-based hardware prototype to narrow the data diversity gap. At the core of our approach is Physiology-informed Multi-band Tokenization (PiMT), which decomposes ExG signals into 12 physiology-informed tokens, followed by a reconstruction task to learn robust representations. This enables adaptive feature recognition across the full frequency spectrum while capturing task-relevant information. Experiments on our new DailySense dataset-the first to enable ExG-based analysis across five human senses-together with four public ExG benchmarks, demonstrate that PiMT consistently outperforms state-of-the-art methods across diverse tasks.

Paper number 4:
Title: Lightweight Classifier for Detecting Intracranial Hemorrhage in Ultrasound Data
Authors: Phat Tran, Enbai Kuang, Fred Xu
Abstract: Intracranial hemorrhage (ICH) secondary to Traumatic Brain Injury (TBI) represents a critical diagnostic challenge, with approximately 64,000 TBI-related deaths annually in the United States. Current diagnostic modalities including Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) have significant limitations: high cost, limited availability, and infrastructure dependence, particularly in resource-constrained environments. This study investigates machine learning approaches for automated ICH detection using Ultrasound Tissue Pulsatility Imaging (TPI), a portable technique measuring tissue displacement from hemodynamic forces during cardiac cycles. We analyze ultrasound TPI signals comprising 30 temporal frames per cardiac cycle with recording angle information, collected from TBI patients with CT-confirmed ground truth labels. Our preprocessing pipeline employs z-score normalization and Principal Component Analysis (PCA) for dimensionality reduction, retaining components explaining 95% of cumulative variance. We systematically evaluate multiple classification algorithms spanning probabilistic, kernel-based, neural network, and ensemble learning approaches across three feature representations: original 31-dimensional space, reduced subset, and PCA-transformed space. Results demonstrate that PCA transformation substantially improves classifier performance, with ensemble methods achieving 98.0% accuracy and F1-score of 0.890, effectively balancing precision and recall despite class imbalance. These findings establish the feasibility of machine learning-based ICH detection in TBI patients using portable ultrasound devices, with applications in emergency medicine, rural healthcare, and military settings where traditional imaging is unavailable.

Paper number 5:
Title: Data-Centric Lessons To Improve Speech-Language Pretraining
Authors: Vishaal Udandarao, Zhiyun Lu, Xuankai Chang, Yongqiang Wang, Violet Z. Yao, Albin Madapally Jose, Fartash Faghri, Josh Gardner, Chung-Cheng Chiu
Abstract: Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs. We focus on three research questions fundamental to speech-language pretraining data: (1) how to process raw web-crawled audio content for speech-text pretraining, (2) how to construct synthetic pretraining datasets to augment web-crawled data and (3) how to interleave (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a 3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up to 3x larger by 10.2% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.

Paper number 6:
Title: Eye-Tracking as a Tool to Quantify the Effects of CAD Display on Radiologists' Interpretation of Chest Radiographs
Authors: Daisuke Matsumoto, Tomohiro Kikuchi, Yusuke Takagi, Soichiro Kojima, Ryoma Kobayashi, Daiju Ueda, Kohei Yamamoto, Sho Kawabe, Harushi Mori
Abstract: Rationale and Objectives: Computer-aided detection systems for chest radiographs are widely used, and concurrent reader displays, such as bounding-box (BB) highlights, may influence the reading process. This pilot study used eye tracking to conduct a preliminary experiment to quantify which aspects of visual search were affected. Materials and Methods: We sampled 180 chest radiographs from the VinDR-CXR dataset: 120 with solitary pulmonary nodules or masses and 60 without. The BBs were configured to yield an overall display sensitivity and specificity of 80%. Three radiologists (with 11, 5, and 1 years of experience, respectively) interpreted each case twice - once with BBs visible and once without - after a washout of >= 2 weeks. Eye movements were recorded using an EyeTech VT3 Mini. Metrics included interpretation time, time to first fixation on the lesion, lesion dwell time, total gaze-path length, and lung-field coverage ratio. Outcomes were modeled using a linear mixed model, with reading condition as a fixed effect and case and reader as random intercepts. The primary analysis was restricted to true positives (n=96). Results: Concurrent BB display prolonged interpretation time by 4.9 s (p<0.001) and increased lesion dwell time by 1.3 s (p<0.001). Total gaze-path length increased by 2,076 pixels (p<0.001), and lung-field coverage ratio increased by 10.5% (p<0.001). Time to first fixation on the lesion was reduced by 1.3 s (p<0.001). Conclusion: Eye tracking captured measurable alterations in search behavior associated with concurrent BB displays during chest radiograph interpretation. These findings support the feasibility of this approach and highlight the need for larger studies to confirm effects and explore implications across modalities and clinical contexts.

Paper number 7:
Title: Is Repeater-Assisted Massive MIMO Compatible with Dynamic TDD?
Authors: Martin Andersson, Anubhab Chowdhury, Erik G. Larsson
Abstract: We present a framework for joint amplification and phase shift optimization of the repeater gain in dynamic time-division duplex (TDD) repeater-assisted massive MIMO networks. Repeaters, being active scatterers with amplification and phase shift, enhance the received signal strengths for users. However, they inevitably also amplify undesired noise and interference signals, which become particularly prominent in dynamic TDD systems due to the concurrent downlink (DL) and uplink (UL) transmissions, introducing cross-link interference among access points and users operating in opposite transmit directions. This causes a non-trivial trade-off between amplification of desired and undesired signals. To underpin the conditions under which such a trade-off can improve performance, we first derive DL and UL spectral efficiencies (SEs), and then develop a repeater gain optimization algorithm for SE maximization. Numerically, we show that our proposed algorithm successfully calibrates the repeater gain to amplify the desired signal while limiting the interference.

Paper number 8:
Title: Safety Monitor for Off-Road Planning with Uncertainty Bounded Bekker Costs
Authors: Akshay Naik, Ramavarapu S. Sreenivas, William R. Norris, Albert E. Patterson, Ahmet Soylemezoglu, Dustin Nottage
Abstract: Reliable off-road autonomy requires operational constraints so that behavior stays predictable and safe when soil strength is uncertain. This paper presents a runtime assurance safety monitor that collaborates with any planner and uses a Bekker-based cost model with bounded uncertainty. The monitor builds an upper confidence traversal cost from a lightweight pressure sinkage model identified in field tests and checks each planned motion against two limits: maximum sinkage and rollover margin. If the risk of crossing either limit is too high, the monitor switches to a certified fallback that reduces vehicle speed, increases standoff from soft ground, or stops on firmer soil. This separation lets the planner focus on efficiency while the monitor keeps the vehicle within clear safety limits on board. Wheel geometry, wheel load estimate, and a soil raster serve as inputs, which tie safety directly to vehicle design and let the monitor set clear limits on speed, curvature, and stopping at run time. The method carries uncertainty analytically into the upper confidence cost and applies simple intervention rules. Tuning of the sinkage limit, rollover margin, and risk window trades efficiency for caution while keeping the monitor light enough for embedded processors. Results from a simulation environment spanning loam to sand include intervention rates, violation probability, and path efficiency relative to the nominal plan, and a benchtop static loading check provides initial empirical validation.

Paper number 9:
Title: refess-qi: reference-free evaluation for speech separation with joint quality and intelligibility scoring
Authors: Ari Frummer, Helin Wang, Tianyu Cao, Adi Arbel, Yuval Sieradzki, Oren Gal, Jesús Villalba, Thomas Thebaud, Najim Dehak
Abstract: Source separation is a crucial pre-processing step for various speech processing tasks, such as automatic speech recognition (ASR). Traditionally, the evaluation metrics for speech separation rely on the matched reference audios and corresponding transcriptions to assess audio quality and intelligibility. However, they cannot be used to evaluate real-world mixtures for which no reference exists. This paper introduces a text-free reference-free evaluation framework based on self-supervised learning (SSL) representations. The proposed framework utilize the mixture and separated tracks to predict jointly audio quality, through the Scale Invariant Signal to Noise Ratio (SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER) metric. We conducted experiments on the WHAMR! dataset, which shows a WER estimation with a mean absolute error (MAE) of 17\% and a Pearson correlation coefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of 0.95. We further demonstrate the robustness of our estimator by using various SSL representations.

Paper number 10:
Title: A Connectively Stable and Robust DAPI Control Scheme for Islanded Networks of Microgrids
Authors: Ahmed Saad Al-Karsani, Maryam Khanbaghi, Aleksandar Zečević
Abstract: The transition towards clean energy and the introduction of Distributed Energy Resources (DERs) are giving rise to the emergence of Microgrids (MGs) and Networks of MGs (NMGs). MGs and NMGs can operate autonomously in islanded mode. However, they face challenges in terms of secondary level frequency and voltage regulation, due to the variable nature of Renewable Energy Sources (RES) and loads. Distributed-Averaging Proportional-Integral (DAPI) control has been proposed in the literature for distributed frequency and voltage control of droop-controlled DERs, but it is not robust to operational or structural perturbations. To address this, we propose a robust DAPI frequency and voltage control scheme that ensures robustness using the concept of connective stability, along with the invariant ellipsoid technique for disturbance rejection. Simulation of an NMG model in MATLAB\textsuperscript{\textregistered}/Simulink\textsuperscript{\textregistered} consisting of 3 MGs and 5 DERs validates the effectiveness of the proposed method, and demonstrates that it can successfully mitigate the effects of major disturbances such as cyberattacks.

Paper number 11:
Title: Efficient Meningioma Tumor Segmentation Using Ensemble Learning
Authors: Mohammad Mahdi Danesh Pajouh, Sara Saeedi
Abstract: Meningiomas represent the most prevalent form of primary brain tumors, comprising nearly one-third of all diagnosed cases. Accurate delineation of these tumors from MRI scans is crucial for guiding treatment strategies, yet remains a challenging and time-consuming task in clinical practice. Recent developments in deep learning have accelerated progress in automated tumor segmentation; however, many advanced techniques are hindered by heavy computational demands and long training schedules, making them less accessible for researchers and clinicians working with limited hardware. In this work, we propose a novel ensemble-based segmentation approach that combines three distinct architectures: (1) a baseline SegResNet model, (2) an attention-augmented SegResNet with concatenative skip connections, and (3) a dual-decoder U-Net enhanced with attention-gated skip connections (DDUNet). The ensemble aims to leverage architectural diversity to improve robustness and accuracy while significantly reducing training demands. Each baseline model was trained for only 20 epochs and Evaluated on the BraTS-MEN 2025 dataset. The proposed ensemble model achieved competitive performance, with average Lesion-Wise Dice scores of 77.30%, 76.37% and 73.9% on test dataset for Enhancing Tumor (ET), Tumor Core (TC) and Whole Tumor (WT) respectively. These results highlight the effectiveness of ensemble learning for brain tumor segmentation, even under limited hardware constraints. Our proposed method provides a practical and accessible tool for aiding the diagnosis of meningioma, with potential impact in both clinical and research settings.

Paper number 12:
Title: House Thermal Model Estimation: Robustness Across Seasons and Setpoints
Authors: Kunal Shankar, Ninad Gaikwad, Anamika Dubey
Abstract: Achieving the flexibility from house heating, cooling, and ventilation systems (HVAC) has the potential to enable large-scale demand response by aggregating HVAC load adjustments across many homes. This demand response strategy helps distribution grid to flexibly ramp-up or ramp-down local load demand so that it can optimally match the bulk power system generation profile. However, achieving this capability requires house thermal models that are both computationally efficient and robust to operating conditions. In this work, parameters of the Resistance-Capacitance (RC) network thermal model for houses are estimated using three optimization algorithms: Nonlinear Least Squares (NLS), Batch Estimation (BE), and Maximum Likelihood Estimation (MLE). The resulting models are evaluated through a Forward-Simulation across four different seasons and three setpoints. The results illustrate a principled way of selecting reduced order models and estimation methods with respect to the robustness offered to seasonal and setpoint variations in training-testing datasets

Paper number 13:
Title: Lyapunov-Based Physics-Informed Deep Neural Networks with Skew Symmetry Considerations
Authors: Rebecca G. Hart, Wanjiku A. Makumi, Rushikesh Kamalapurkar, Warren E. Dixon
Abstract: Deep neural networks (DNNs) are powerful black-box function approximators which have been shown to yield improved performance compared to traditional neural network (NN) architectures. However, black-box algorithms do not incorporate known physics of the system and can yield results which are physically implausible. Physics-informed neural networks (PINNs) have grown in popularity due to their ability to leverage known physical principles in the learning process which has been empirically shown to improve performance compared to traditional black-box methods. This paper introduces the first physics-informed DNN controller for an Euler-Lagrange dynamic system where the adaptation laws are designed using a Lyapunov-based stability analysis to account for the skew-symmetry property of the inertia matrix and centripetal-Coriolis matrix. A Lyapunov-based stability analysis is provided to guarantee asymptotic convergence of the tracking error and the skew-symmetric prediction error. Simulations indicate that the developed update law demonstrates improvement in individual and overall function approximation capabilities when compared to a physics-informed adaptation law which does not incorporate knowledge of system symmetries.

Paper number 14:
Title: Environment-Dependent Components Identification of Behind-the-Meter Resources via Inverse Optimization
Authors: Chengming Lyu, Zhenfei Tan, Xiaoyuan Xu, Chen Fu, Zheng Yan, Mohammad Shahidehpour
Abstract: With the increasing penetration of behind-the-meter (BTM) resources, it is vital to monitor the components of these resources and deduce their response behavior to external environment. Owing to data privacy, however, the appliance-wise measurement is invisible to the power system operator, which hinders the accurate modeling of load identification. To this end, this paper proposes a hybrid physics-inspired and data-driven framework for decomposing BTM components based on external measurement of total load and environmental factors. The total load is decomposed into different environment-dependent components, namely storage-like component, PV generation component, thermostatically-controlled load component, and periodic component. The overall load identification adopts a double-layer iterative solution framework. A data-driven inverse optimization algorithm is developed to identify parameters of the energy storage-like component. The physics-inspired model is proposed to identify the capacity and response of the rest components. The modeling accuracy and robustness of the proposed method are validated by numerical tests. The application significance of the proposed BTM identification method is also validated in electricity market clearing for reducing system operation costs.

Paper number 15:
Title: 6D Movable Holographic Surface Assisted Integrated Data and Energy Transfer: A Sensing Enhanced Approach
Authors: Zhonglun Wang, Yizhe Zhao, Gangming Hu, Yali Zheng, Kun Yang
Abstract: Reconfigurable holographic surface (RHS) enables cost-effective large-scale arrays with high spatial gain. However, its amplitude-controlled holographic beamforming suffers from directional fluctuations, making it difficult to fully exploit the spatial gain of RHS. Fortunately, the promising 6D movable antenna (6DMA) provides a potential solution to this problem. In this paper, we study a 6D movable holographic surface (6DMHS) integrated data and energy transfer (IDET) system, where a three-stage protocol is proposed, consisting of an uplink sensing stage, an orientation adjustment stage and a downlink transmission stage, to coordinate the 6DMHS and effectively serve the IDET receivers. Firstly, the holographic-based sensing technology is proposed and the sensing information of the IDET receivers is exploited. Secondly, by fixing the rotations with the sensing information, the orientation optimization problem is formulated for designing the holographic beamforming of the RHS and adjusting the translations of the 6DMHS. As a result, the directions with maximum beamforming gain are aligned with each IDET receiver. Thirdly, by fixing the orientation of the 6DMHS and the holographic beamforming, the equivalent wireless channel is obtained. The IDET performance optimization problem is formulated for obtaining the optimal digital beamforming, power splitting factor and energy harvesting (EH) power. Simulation results demonstrate that the proposed scheme is capable of improving the IDET performance compared to the benchmarks.

Paper number 16:
Title: Green Hydrogen under Uncertainty: Evaluating Power-to-X Strategies Using Agent-Based Simulation and Multi-Criteria Decision Framework
Authors: Frederik Wagner Madsen, Joy Dalmacio Billanes, Bo Nørregaard Jørgensen, Zheng Ma
Abstract: The transition toward net-zero energy systems requires scalable and cost-effective deployment of Power-to-X technologies, particularly green hydrogen production. Despite increasing investments, a critical research gap remains in dynamically assessing how different operational strategies affect the feasibility of hydrogen production under real-world energy market conditions. Most existing studies rely on static, techno-economic models and overlook actor interactions, infrastructure limitations, and regulatory complexity. This paper presents a novel modeling framework that integrates agent-based simulation with multi-criteria decision-making to evaluate green hydrogen production strategies using co-located wind and solar generation. Three operational strategies - grid-only, on-site-only, and hybrid - are applied across three electrolyzer capacity levels (10 MW, 50 MW, and 100 MW) within a Danish case study. Real electricity tariffs, emissions factors, and market data are used to simulate technical, economic, and environmental performance indicators. The results show that hybrid strategies consistently outperform grid-only configurations in terms of cost and emissions while maintaining stable hydrogen output. Although on-site-only strategies minimize emissions and costs, they fail to meet fixed production demands. This framework offers novel scientific contributions by modeling dynamic actor interactions and integrating system performance evaluation into strategic planning. Practically, it provides actionable insights for energy planners and policymakers designing resilient and efficient Power-to-X systems in renewable-rich contexts.

Paper number 17:
Title: PhoenixCodec: Taming Neural Speech Coding for Extreme Low-Resource Scenarios
Authors: Zixiang Wan, Haoran Zhao, Guochang Zhang, Runqiang Han, Jianqiang Wei, Yuexian Zou
Abstract: This paper presents PhoenixCodec, a comprehensive neural speech coding and decoding framework designed for extremely low-resource conditions. The proposed system integrates an optimized asymmetric frequency-time architecture, a Cyclical Calibration and Refinement (CCR) training strategy, and a noise-invariant fine-tuning procedure. Under stringent constraints - computation below 700 MFLOPs, latency less than 30 ms, and dual-rate support at 1 kbps and 6 kbps - existing methods face a trade-off between efficiency and quality. PhoenixCodec addresses these challenges by alleviating the resource scattering of conventional decoders, employing CCR to escape local optima, and enhancing robustness through noisy-sample fine-tuning. In the LRAC 2025 Challenge Track 1, the proposed system ranked third overall and demonstrated the best performance at 1 kbps in both real-world noise and reverberation and intelligibility in clean tests, confirming its effectiveness.

Paper number 18:
Title: SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain
Authors: Zixiang Wan, Guochang Zhang, Yifeng He, Jianqiang Wei
Abstract: Neural Audio Codecs (NACs) have gained growing attention in recent years as technologies for audio compression and audio representation in speech language models. While mainstream NACs typically require G-level computation and M-level parameters, the performance of lightweight and streaming NACs remains underexplored. This paper proposes SpecTokenizer, a lightweight streaming codec that operates in the compressed spectral domain. Composed solely of alternating CNN and RNN layers, SpecTokenizer achieves greater efficiency and better representational capability through multi-scale modeling in the compressed spectrum domain. At 4 kbps, the proposed SpecTokenizer achieves comparable or superior performance compared to the codec with state-of-the-art lightweight architecture while requiring only 20% of the computation and 10% of the parameters. Furthermore, it significantly outperforms the codec when using similar computational and storage resources.

Paper number 19:
Title: The Role of Information Incompleteness in Defending Against Stealth Attacks
Authors: Ke Sun, Jingyi Yan, Zhenglin Li, Shaorong Xie
Abstract: The effectiveness of Data Injections Attacks (DIAs) critically depends on the completeness of the system information accessible to adversaries. This relationship positions information incompleteness enhancement as a vital defense strategy for degrading DIA performance. In this paper, we focus on the information-theoretic stealth attacks, where the attacker encounters a fundamental tradeoff between the attack stealthiness and destructiveness. Specifically, we systematically characterize how incomplete admittance information impacts the dual objectives. In particular, we establish sufficient conditions for two distinct operational regimes: (i) stealthiness intensifies while destructive potential diminishes and (ii) destructiveness increases while stealth capability weakens. For scenarios beyond these regimes, we propose a maximal incompleteness strategy to optimally degrade stealth capability. To solve the associated optimization problem, the feasible region is reduced without excluding the optimal solution, and a heuristic algorithm is then introduced to effectively identify the near-optimal solutions within the reduced region. Numerical simulations are conducted on IEEE test systems to validate the findings.

Paper number 20:
Title: Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction
Authors: Wangqian Chen, Junting Chen, Shuguang Cui
Abstract: As communication networks evolve towards greater complexity (e.g., 6G and beyond), a deep understanding of the wireless environment becomes increasingly crucial. When explicit knowledge of the environment is unavailable, geometry-aware feature extraction from channel state information (CSI) emerges as a pivotal methodology to bridge physical-layer measurements with network intelligence. This paper proposes to explore the received signal strength (RSS) data, without explicit 3D environment knowledge, to jointly construct the radio beam map and environmental geometry for a multiple-input multiple-output (MIMO) system. Unlike existing methods that only learn blockage structures, we propose an oriented virtual obstacle model that captures the geometric features of both blockage and reflection. Reflective zones are formulated to identify relevant reflected paths according to the geometry relation of the environment. We derive an analytical expression for the reflective zone and further analyze its geometric characteristics to develop a reformulation that is more compatible with deep learning representations. A physics-informed deep learning framework that incorporates the reflective-zone-based geometry model is proposed to learn the blockage, reflection, and scattering components, along with the beam pattern, which leverages physics prior knowledge to enhance network transferability. Numerical experiments demonstrate that, in addition to reconstructing the blockage and reflection geometry, the proposed model can construct a more accurate MIMO beam map with a 32%-48% accuracy improvement.

Paper number 21:
Title: Track-to-Track Association for Collective Perception based on Stochastic Optimization
Authors: Laura M. Wolf, Vincent Albert Wolff, Simon Steuernagel, Kolja Thormann, Marcus Baum
Abstract: Collective perception is a key aspect for autonomous driving in smart cities as it aims to combine the local environment models of multiple intelligent vehicles in order to overcome sensor limitations. A crucial part of multi-sensor fusion is track-to-track association. Previous works often suffer from high computational complexity or are based on heuristics. We propose an association algorithms based on stochastic optimization, which leverages a multidimensional likelihood incorporating the number of tracks and their spatial distribution and furthermore computes several association hypotheses. We demonstrate the effectiveness of our approach in Monte Carlo simulations and a realistic collective perception scenario computing high-likelihood associations in ambiguous settings.

Paper number 22:
Title: WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation
Authors: Christiaan M. Geldenhuys, Günther Tonitz, Thomas R. Niesler
Abstract: While recent sound event detection (SED) systems can identify baleen whale calls in marine audio, challenges related to false positive and minority-class detection persist. We propose the boundary proposal network (BPN), which extends an existing lightweight SED system. The BPN is inspired by work in image object detection and aims to reduce the number of false positive detections. It achieves this by using intermediate latent representations computed within the backbone classification model to gate the final output. When added to an existing SED system, the BPN achieves a 16.8 % absolute increase in precision, as well as 21.3 % and 9.4 % improvements in the F1-score for minority-class d-calls and bp-calls, respectively. We further consider two approaches to the selection of post-processing hyperparameters: a forward-search and a backward-search. By separately optimising event-level and frame-level hyperparameters, these two approaches lead to considerable performance improvements over parameters selected using empirical methods. The complete WhaleVAD-BPN system achieves a cross-validated development F1-score of 0.475, which is a 9.8 % absolute improvement over the baseline.

Paper number 23:
Title: The PhasorArray Toolbox for Harmonic Analysis and Control Design
Authors: Maxime Grosso (CRAN), Pierre Riedinger (CRAN), Jamal Daafouz (CRAN)
Abstract: We present a MATLAB package called the Pha-sorArray Toolbox that has been developed to make harmonic analysis and control methods both practical and user-friendly. The toolbox adopts an object-oriented architecture that enables intuitive manipulation of periodic matrices through overloaded operators for addition, multiplication, convolution, and automatic Toeplitz construction. Its advanced features include harmonic Sylvester, Lyapunov and Riccati equations solvers, and seamless integration with YALMIP, thereby facilitating advanced control and analysis techniques based on Linear Matrix Inequalities (LMIs) in the harmonic framework.

Paper number 24:
Title: Data-driven Koopman MPC using Mixed Stochastic-Deterministic Tubes
Authors: Zhengang Zhong, Ehecatl Antonio del Rio-Chanona, Panagiotis Petsagkourakis
Abstract: This paper presents a novel data-driven stochastic MPC design for discrete-time nonlinear systems with additive disturbances by leveraging the Koopman operator and a distributionally robust optimization (DRO) framework. By lifting the dynamical system into a linear space, we achieve a finite-dimensional approximation of the Koopman operator. We explicitly account for the modeling approximation and additive disturbance error by a mixed stochastic-deterministic tube for the lifted linear model. This ensures the regulation of the original nonlinear system while complying with the prespecified constraints. Stochastic and deterministic tubes are constructed using a DRO and a hyper-cube hull, respectively. We provide finite sample error bounds for both types of tubes. The effectiveness of the proposed approach is demonstrated through numerical simulations.

Paper number 25:
Title: Are These Even Words? Quantifying the Gibberishness of Generative Speech Models
Authors: Danilo de Oliveira, Tal Peer, Jonas Rochdi, Timo Gerkmann
Abstract: Significant research efforts are currently being dedicated to non-intrusive quality and intelligibility assessment, especially given how it enables curation of large scale datasets of in-the-wild speech data. However, with the increasing capabilities of generative models to synthesize high quality speech, new types of artifacts become relevant, such as generative hallucinations. While intrusive metrics are able to spot such sort of discrepancies from a reference signal, it is not clear how current non-intrusive methods react to high-quality phoneme confusions or, more extremely, gibberish speech. In this paper we explore how to factor in this aspect under a fully unsupervised setting by leveraging language models. Additionally, we publish a dataset of high-quality synthesized gibberish speech for further development of measures to assess implausible sentences in spoken language, alongside code for calculating scores from a variety of speech language models.

Paper number 26:
Title: Predictive control barrier functions for piecewise affine systems with non-smooth constraints
Authors: Kanghui He, Anil Alan, Shengling Shi, Ton van den Boom, Bart De Schutter
Abstract: Obtaining control barrier functions (CBFs) with large safe sets for complex nonlinear systems and constraints is a challenging task. Predictive CBFs address this issue by using an online finite-horizon optimal control problem that implicitly defines a large safe set. The optimal control problem, also known as the predictive safety filter (PSF), involves predicting the system's flow under a given backup control policy. However, for non-smooth systems and constraints, some key elements, such as CBF gradients and the sensitivity of the flow, are not well-defined, making the current methods inadequate for ensuring safety. Additionally, for control-non-affine systems, the PSF is generally nonlinear and non-convex, posing challenges for real-time computation. This paper considers piecewise affine systems, which are usually control-non-affine, under nonlinear state and polyhedral input constraints. We solve the safety issue by incorporating set-valued generalized Clarke derivatives in the PSF design. We show that enforcing CBF constraints across all elements of the generalized Clarke derivatives suffices to guarantee safety. Moreover, to lighten the computational overhead, we propose an explicit approximation of the PSF. The resulting control methods are demonstrated through numerical examples.

Paper number 27:
Title: Optimized Power Control for Multi-User Integrated Sensing and Edge AI
Authors: Biao Dong, Bin Cao
Abstract: This work investigates an integrated sensing and edge artificial intelligence (ISEA) system, where multiple devices first transmit probing signals for target sensing and then offload locally extracted features to the access point (AP) via analog over-the-air computation (AirComp) for collaborative inference. To characterize the relationship between AirComp error and inference performance, two proxies are established: the \emph{computation-optimal} proxy that minimizes the aggregation distortion, and the \emph{decision-optimal} proxy that maximizes the inter-class separability, respectively. Optimal transceiver designs in terms of closed-form power allocation are derived for both time-division multiplexing (TDM) and frequency-division multiplexing (FDM) settings, revealing threshold-based and dual-decomposition structures, respectively. Experimental results validate the theoretical findings.

Paper number 28:
Title: Compressing Quaternion Convolutional Neural Networks for Audio Classification
Authors: Arshdeep Singh, Vinayak Abrol, Mark D. Plumbley
Abstract: Conventional Convolutional Neural Networks (CNNs) in the real domain have been widely used for audio classification. However, their convolution operations process multi-channel inputs independently, limiting the ability to capture correlations among channels. This can lead to suboptimal feature learning, particularly for complex audio patterns such as multi-channel spectrogram representations. Quaternion Convolutional Neural Networks (QCNNs) address this limitation by employing quaternion algebra to jointly capture inter-channel dependencies, enabling more compact models with fewer learnable parameters while better exploiting the multi-dimensional nature of audio signals. However, QCNNs exhibit higher computational complexity due to the overhead of quaternion operations, resulting in increased inference latency and reduced efficiency compared to conventional CNNs, posing challenges for deployment on resource-constrained platforms. To address this challenge, this study explores knowledge distillation (KD) and pruning, to reduce the computational complexity of QCNNs while maintaining performance. Our experiments on audio classification reveal that pruning QCNNs achieves similar or superior performance compared to KD while requiring less computational effort. Compared to conventional CNNs and Transformer-based architectures, pruned QCNNs achieve competitive performance with a reduced learnable parameter count and computational complexity. On the AudioSet dataset, pruned QCNNs reduce computational cost by 50\% and parameter count by 80\%, while maintaining performance comparable to the conventional CNNs. Furthermore, pruned QCNNs generalize well across multiple audio classification benchmarks, including GTZAN for music genre recognition, ESC-50 for environmental sound classification and RAVDESS for speech emotion recognition.

Paper number 29:
Title: On Irradiance Distributions for Weakly Turbulent FSO Links: Log-Normal vs. Gamma-Gamma
Authors: Carmen Álvarez Roa, Yunus Can Gültekin, Vincent van Vliet, Menno van den Hout, Chigo Okonkwo, Alex Alvarado
Abstract: Weak turbulence is commonly modeled using the log-normal distribution. Our experimental results show that this distribution fails to capture irradiance fluctuations in this regime. The Gamma-Gamma model is shown to be more accurate.

Paper number 30:
Title: Auction-Based Responsibility Allocation for Scalable Decentralized Safety Filters in Cooperative Multi-Agent Collision Avoidance
Authors: Johannes Autenrieb, Mark Spiller
Abstract: This paper proposes a scalable decentralized safety filter for multi-agent systems based on high-order control barrier functions (HOCBFs) and auction-based responsibility allocation. While decentralized HOCBF formulations ensure pairwise safety under input bounds, they face feasibility and scalability challenges as the number of agents grows. Each agent must evaluate an increasing number of pairwise constraints, raising the risk of infeasibility and making it difficult to meet real-time requirements. To address this, we introduce an auction-based allocation scheme that distributes constraint enforcement asymmetrically among neighbors based on local control effort estimates. The resulting directed responsibility graph guarantees full safety coverage while reducing redundant constraints and per-agent computational load. Simulation results confirm safe and efficient coordination across a range of network sizes and interaction densities.

Paper number 31:
Title: System-Theoretic Analysis of Dynamic Generalized Nash Equilibrium Problems -- Turnpikes and Dissipativity
Authors: Sophie Hall, Florian Dörfler, Timm Faulwasser
Abstract: Generalized Nash equilibria are used in multi-agent control applications to model strategic interactions between agents that are coupled in the cost, dynamics, and constraints. We study the properties of open-loop GNE trajectories from a system-theoretic perspective. We show how strict dissipativity generates the turnpike phenomenon in GNE solutions. Moreover, we establish a converse turnpike result, i.e., the implication from turnpike to strict dissipativity. We derive conditions under which the steady-state GNE is the optimal operating point and, using a game value function, we give a local characterization of the geometry of storage functions. Finally, we design linear terminal penalties that ensure GNE open-loop trajectories converge to and remain at the steady-state GNE. These connections provide the foundation for future system-theoretic analysis of GNEs similar to those existing in optimal control.

Paper number 32:
Title: Rate-cost tradeoffs in continuous-time control with a biomolecular application
Authors: Yorie Nakahira, Fangzhou Xiao, Victoria Kostina, John C. Doyle
Abstract: This paper focuses on rate-limited control of the generalized Ornstein-Uhlenbeck process where the control action can be either multiplicative or additive, and the noise variance can depend on the control action. We derive a lower bound on the data rate necessary to achieve the desired control cost. The lower bound is attained with equality if the control is performed via an additive white Gaussian channel. The system model approximates the dynamics of a discrete-state molecular birth-death process, and the result has direct implications on the control of a biomolecular system via chemical reactions, where the multiplicative control corresponds to the degradation rate, the additive control corresponds to the production rate, and the control objective is to decrease the fluctuations of the controlled molecular species around their desired concentration levels.

Paper number 33:
Title: Opto-Electronic Clock Regeneration -- A Tutorial
Authors: Palle Jeppesen, Bjarne Tromborg
Abstract: A tutorial on opto-electronic clock regeneration at very high bit rates beyond reach with purely electronic solutions is given. Emphasis is placed on sum frequency generation in a nonlinear material such as LiNbO3. We first provide a basic introduction to CR (clock recovery) and a PLL (phase-locked loop); two examples are considered, an input signal frequency step and a slow input signal frequency. Next we discuss opto-electronic clock recovery based on an OPLL (opto-electronic PLL). The OPLL contains a phase comparator consisting of a planar LiNbO3 waveguide, a lowpass filter, a VCO (voltage controlled oscillator) and a local oscillator laser. The error signal from the comparator determined by the difference in electrical phase between the signal and the VCO controls the VCO. The VCO has two outputs; one that modulates the local oscillator laser and another that triggers a decision circuit that samples the output from the OPLL. The VCO is continuously adjusted by the OPLL so that it will ensure sampling of the signal in the optimal moments.

Paper number 34:
Title: Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance
Authors: Sydney M. Katz, Robert J. Moss, Dylan M. Asmar, Wesley A. Olson, James K. Kuchar, Mykel J. Kochenderfer
Abstract: Aircraft collision avoidance systems is critical to modern aviation. These systems are designed to predict potential collisions between aircraft and recommend appropriate avoidance actions. Creating effective collision avoidance systems requires solutions to a variety of technical challenges related to surveillance, decision making, and validation. These challenges have sparked significant research and development efforts over the past several decades that have resulted in a variety of proposed solutions. This article provides an overview of these challenges and solutions with an emphasis on those that have been put through a rigorous validation process and accepted by regulatory bodies. The challenges posed by the collision avoidance problem are often present in other domains, and aircraft collision avoidance systems can serve as case studies that provide valuable insights for a wide range of safety-critical systems.

Paper number 35:
Title: A Short Note on Upper Bounds for Graph Neural Operator Convergence Rate
Authors: Roxanne Holden, Luana Ruiz
Abstract: Graphons, as limits of graph sequences, provide a framework for analyzing the asymptotic behavior of graph neural operators. Spectral convergence of sampled graphs to graphons yields operator-level convergence rates, enabling transferability analyses of GNNs. This note summarizes known bounds under no assumptions, global Lipschitz continuity, and piecewise-Lipschitz continuity, highlighting tradeoffs between assumptions and rates, and illustrating their empirical tightness on synthetic and real data.

Paper number 36:
Title: NeuroPilot: A Realtime Brain-Computer Interface system to enhance concentration of students in online learning
Authors: Asif Islam, Farhan Ishtiaque, Md. Muhyminul Haque, Kaled Masukur Rahman, Ravi Vaidyanathan, Khondaker A. Mamun
Abstract: Prevalence of online learning poses a vital challenge in real-time monitoring of students' concentration. Traditional methods such as questionnaire assessments require manual interventions and webcam-based monitoring fails to provide accurate insights into learners' mental focus as they are deceived by mere screen fixation without cognitive engagement. Existing BCI-based approaches lack real-time validation and evaluation procedures. To address these limitations, a Brain-Computer Interface (BCI) system is developed using a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record brainwave activity under attentive and non-attentive states. 20 minutes of data were collected from each of 20 participants watching a pre-recorded educational video. The data validation employed a novel intra-video questionnaire assessment. Subsequently, collected signals were segmented (sliding window), filtered (butterworth bandpass), and cleaned (removal of high-amplitude and EOG artifacts such as eye blinks). Time, frequency, wavelet and statistical features have been extracted, followed by recursive feature elimination (RFE) with Support vector machines (SVMs) to classify attention and non-attention states. The leave-one-subject-out (LOSO) cross-validation accuracy has been tested to be 88.77%. The system provides feedback alerts upon non-attention state detection and keeps focus profile logs. A pilot study was conducted to evaluate the effectiveness of real-time feedback. Five participants completed a 10-minute session consisting of a 5-minute baseline phase without feedback followed by a 5-minute feedback phase, during which alerts were issued if participants remained non-attentive for approximately 8 consecutive seconds. A paired t-test (t = 5.73, p = 0.007) indicated a statistically significant improvement in concentration during the feedback phase.

Paper number 37:
Title: AL-CoLe: Augmented Lagrangian for Constrained Learning
Authors: Ignacio Boero, Ignacio Hounie, Alejandro Ribeiro
Abstract: Despite the non-convexity of most modern machine learning parameterizations, Lagrangian duality has become a popular tool for addressing constrained learning problems. We revisit Augmented Lagrangian methods, which aim to mitigate the duality gap in non-convex settings while requiring only minimal modifications, and have remained comparably unexplored in constrained learning settings. We establish strong duality results under mild conditions, prove convergence of dual ascent algorithms to feasible and optimal primal solutions, and provide PAC-style generalization guarantees. Finally, we demonstrate its effectiveness on fairness constrained classification tasks.

Paper number 38:
Title: The local Gaussian correlation networks among return tails in the Chinese stock market
Authors: Peng Liu
Abstract: Financial networks based on Pearson correlations have been intensively studied. However, previous studies may have led to misleading and catastrophic results because of several critical shortcomings of the Pearson correlation. The local Gaussian correlation coefficient, a new measurement of statistical dependence between variables, has unique advantages including capturing local nonlinear dependence and handling heavy-tailed distributions. This study constructs financial networks using the local Gaussian correlation coefficients between tail regions of stock returns in the Shanghai Stock Exchange. The work systematically analyzes fundamental network metrics including node centrality, average shortest path length, and entropy. Compared with the local Gaussian correlation network among positive tails and the conventional Pearson correlation network, the properties of the local Gaussian correlation network among negative tails are more sensitive to the stock market risks. This finding suggests researchers should prioritize the local Gaussian correlation network among negative tails. Future work should reevaluate existing findings using the local Gaussian correlation method.

Paper number 39:
Title: Robust Regret Control with Uncertainty-Dependent Baseline
Authors: Jietian Liu, Peter Seiler
Abstract: This paper proposes a robust regret control framework in which the performance baseline adapts to the realization of system uncertainty. The plant is modeled as a discrete-time, uncertain linear time-invariant system with real-parametric uncertainty. The performance baseline is the optimal non-causal controller constructed with full knowledge of the disturbance and the specific realization of the uncertain plant. We show that a controller achieves robust additive regret relative to this baseline if and only if it satisfies a related, robust $H_\infty$ performance condition on a modified plant. One technical issue is that the modified plant can, in general, have a complicated nonlinear dependence on the uncertainty. We use a linear approximation step so that the robust additive regret condition can be recast as a standard $\mu$-synthesis problem. A numerical example is used to demonstrate the proposed approach.

Paper number 40:
Title: Anisotropic Pooling for LUT-realizable CNN Image Restoration
Authors: Xi Zhang, Xiaolin Wu
Abstract: Table look-up realization of image restoration CNNs has the potential of achieving competitive image quality while being much faster and resource frugal than the straightforward CNN implementation. The main technical challenge facing the LUT-based CNN algorithm designers is to manage the table size without overly restricting the receptive field. The prevailing strategy is to reuse the table for small pixel patches of different orientations (apparently assuming a degree of isotropy) and then fuse the look-up results. The fusion is currently done by average pooling, which we find being ill suited to anisotropic signal structures. To alleviate the problem, we investigate and discuss anisotropic pooling methods to replace naive averaging for improving the performance of the current LUT-realizable CNN restoration methods. First, we introduce the method of generalized median pooling which leads to measurable gains over average pooling. We then extend this idea by learning data-dependent pooling coefficients for each orientation, so that they can adaptively weigh the contributions of differently oriented pixel patches. Experimental results on various restoration benchmarks show that our anisotropic pooling strategy yields both perceptually and numerically superior results compared to existing LUT-realizable CNN methods.

Paper number 41:
Title: FlexIO: Flexible Single- and Multi-Channel Speech Separation and Enhancement
Authors: Yoshiki Masuyama, Kohei Saijo, Francesco Paissan, Jiangyu Han, Marc Delcroix, Ryo Aihara, François G. Germain, Gordon Wichern, Jonathan Le Roux
Abstract: Speech separation and enhancement (SSE) has advanced remarkably and achieved promising results in controlled settings, such as a fixed number of speakers and a fixed array configuration. Towards a universal SSE system, single-channel systems have been extended to deal with a variable number of speakers (i.e., outputs). Meanwhile, multi-channel systems accommodating various array configurations (i.e., inputs) have been developed. However, these attempts have been pursued separately. In this paper, we propose a flexible input and output SSE system, named FlexIO. It performs conditional separation using prompt vectors, one per speaker as a condition, allowing separation of an arbitrary number of speakers. Multi-channel mixtures are processed together with the prompt vectors via an array-agnostic channel communication mechanism. Our experiments demonstrate that FlexIO successfully covers diverse conditions with one to five microphones and one to three speakers. We also confirm the robustness of FlexIO on CHiME-4 real data.

Paper number 42:
Title: Analysis and Synthesis of Switched Optimization Algorithms
Authors: Jared Miller, Fabian Jakob, Carsten Scherer, Andrea Iannelli
Abstract: Deployment of optimization algorithms on networked systems face challenges associated with time delays and corruptions. One particular instance is the presence of time-varying delays arising from factors such as packet drops and irregular sampling. Fixed time delays can destabilize gradient descent algorithms, and this degradation is exacerbated by time-varying delays. This work concentrates on the analysis and creation of discrete-time optimization algorithms with certified exponential convergence rates that are robust against switched uncertainties between the optimizer and the gradient oracle. These optimization algorithms are implemented by a switch-scheduled output feedback controllers. Rate variation and sawtooth behavior (packet drops) in time-varying delays can be imposed through constraining switching sequences. Analysis is accomplished by bisection in the convergence rate to find Zames-Falb filter coefficents. Synthesis is performed by alternating between a filter coefficient search for a fixed controller, and a controller search for fixed multipliers.

Paper number 43:
Title: Size and Smoothness Aware Adaptive Focal Loss for Small Tumor Segmentation
Authors: Md Rakibul Islam, Riad Hassan, Abdullah Nazib, Kien Nguyen, Clinton Fookes, Md Zahidul Islam
Abstract: Deep learning has achieved remarkable accuracy in medical image segmentation, particularly for larger structures with well-defined boundaries. However, its effectiveness can be challenged by factors such as irregular object shapes and edges, non-smooth surfaces, small target areas, etc. which complicate the ability of networks to grasp the intricate and diverse nature of anatomical regions. In response to these challenges, we propose an Adaptive Focal Loss (A-FL) that takes both object boundary smoothness and size into account, with the goal to improve segmentation performance in intricate anatomical regions. The proposed A-FL dynamically adjusts itself based on an object's surface smoothness, size, and the class balancing parameter based on the ratio of targeted area and background. We evaluated the performance of the A-FL on the PICAI 2022 and BraTS 2018 datasets. In the PICAI 2022 dataset, the A-FL achieved an Intersection over Union (IoU) score of 0.696 and a Dice Similarity Coefficient (DSC) of 0.769, outperforming the regular Focal Loss (FL) by 5.5% and 5.4% respectively. It also surpassed the best baseline by 2.0% and 1.2%. In the BraTS 2018 dataset, A-FL achieved an IoU score of 0.883 and a DSC score of 0.931. Our ablation experiments also show that the proposed A-FL surpasses conventional losses (this includes Dice Loss, Focal Loss, and their hybrid variants) by large margin in IoU, DSC, and other metrics. The code is available at this https URL.

Paper number 44:
Title: Multi-Atlas Brain Network Classification through Consistency Distillation and Complementary Information Fusion
Authors: Jiaxing Xu, Mengcheng Lan, Xia Dong, Kai He, Wei Zhang, Qingtian Bian, Yiping Ke
Abstract: In the realm of neuroscience, identifying distinctive patterns associated with neurological disorders via brain networks is crucial. Resting-state functional magnetic resonance imaging (fMRI) serves as a primary tool for mapping these networks by correlating blood-oxygen-level-dependent (BOLD) signals across different brain regions, defined as regions of interest (ROIs). Constructing these brain networks involves using atlases to parcellate the brain into ROIs based on various hypotheses of brain division. However, there is no standard atlas for brain network classification, leading to limitations in detecting abnormalities in disorders. Some recent methods have proposed utilizing multiple atlases, but they neglect consistency across atlases and lack ROI-level information exchange. To tackle these limitations, we propose an Atlas-Integrated Distillation and Fusion network (AIDFusion) to improve brain network classification using fMRI data. AIDFusion addresses the challenge of utilizing multiple atlases by employing a disentangle Transformer to filter out inconsistent atlas-specific information and distill distinguishable connections across atlases. It also incorporates subject- and population-level consistency constraints to enhance cross-atlas consistency. Additionally, AIDFusion employs an inter-atlas message-passing mechanism to fuse complementary information across brain regions. Experimental results on four datasets of different diseases demonstrate the effectiveness and efficiency of AIDFusion compared to state-of-the-art methods. A case study illustrates AIDFusion extract patterns that are both interpretable and consistent with established neuroscience findings.

Paper number 45:
Title: A Traffic Prediction-Based Individualized Driver Warning System to Reduce Red Light Violations
Authors: Suiyi He, Maziar Zamanpour, Jianshe Guo, Michael W. Levin, Zongxuan Sun
Abstract: Red light violation is a major cause of traffic collisions and resulting injuries and fatalities. Despite extensive prior work to reduce red light violations, they continue to be a major problem in practice, partly because existing systems suffer from the flaw of providing the same guidance to all drivers. As a result, some violations are avoided, but other drivers ignore or respond inappropriately to red light running systems, resulting in safety issues overall. We show a method of providing accurate warnings to individual drivers to avoid the broad guidance approach of most existing systems. Recognizing if a driver will run red lights is highly dependent on signal phase and timing, traffic conditions along the road, and individual driver behaviour, the proposed warning system contains three parts: a traffic prediction algorithm, an individual warning signal optimizer, and a driver warning display. The traffic prediction algorithm predicts future traffic states along the road towards the signalized intersections using the latest traffic conditions obtained through vehicle-to-vehicle and vehicle-to-infrastructure communications. Then, an optimization problem is formulated to compute the optimal warning signal based on predicted traffic states and driver reaction model. Finally, the optimal warning signal is shown on the display screen to advise driver on how much braking is needed to avoid running the red light. The system continuously updates the latest warning signal as the vehicle is approaching the intersection. Both numerical simulated driving scenarios and real-world road tests are used to demonstrate the proposed algorithm's performance under different conditions by comparing with previous work on red light running warning system. The results show that the system provides more effective and accurate warning signals to drivers, helping them avoid running red lights.

Paper number 46:
Title: Guided MRI Reconstruction via Schrödinger Bridge
Authors: Yue Wang, Yuanbiao Yang, Zhuo-xu Cui, Tian Zhou, Bingsheng Huang, Hairong Zheng, Dong Liang, Yanjie Zhu
Abstract: Magnetic Resonance Imaging (MRI) is an inherently multi-contrast modality, where cross-contrast priors can be exploited to improve image reconstruction from undersampled data. Recently, diffusion models have shown remarkable performance in MRI reconstruction. However, they still struggle to effectively utilize such priors, mainly because existing methods rely on feature-level fusion in image or latent spaces, which lacks explicit structural correspondence and thus leads to suboptimal performance. To address this issue, we propose $\mathbf{I}^2$SB-Inversion, a multi-contrast guided reconstruction framework based on the Schrödinger Bridge (SB). The proposed method performs pixel-wise translation between paired contrasts, providing explicit structural constraints between the guidance and target images. Furthermore, an Inversion strategy is introduced to correct inter-modality misalignment, which often occurs in guided reconstruction, thereby mitigating artifacts and improving reconstruction accuracy. Experiments on paired T1- and T2-weighted datasets demonstrate that $\mathbf{I}^2$SB-Inversion achieves a high acceleration factor of up to 14.4 and consistently outperforms existing methods in both quantitative and qualitative evaluations.

Paper number 47:
Title: LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models
Authors: Julius Richter, Danilo de Oliveira, Tal Peer, Timo Gerkmann
Abstract: We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.

Paper number 48:
Title: Grids Often Outperform Implicit Neural Representation at Compressing Dense Signals
Authors: Namhoon Kim, Sara Fridovich-Keil
Abstract: Implicit Neural Representations (INRs) have recently shown impressive results, but their fundamental capacity, implicit biases, and scaling behavior remain poorly understood. We investigate the performance of diverse INRs across a suite of 2D and 3D real and synthetic signals with varying effective bandwidth, as well as both overfitting and generalization tasks including tomography, super-resolution, and denoising. By stratifying performance according to model size as well as signal type and bandwidth, our results shed light on how different INR and grid representations allocate their capacity. We find that, for most tasks and signals, a simple regularized grid with interpolation trains faster and to higher quality than any INR with the same number of parameters. We also find limited settings--namely fitting binary signals such as shape contours--where INRs outperform grids, to guide future development and use of INRs towards the most advantageous applications.

Paper number 49:
Title: InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding
Authors: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang
Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time-quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy-even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

Paper number 50:
Title: A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\left[^{18}\text{F}\right]$FDG PET imaging
Authors: Christian Salomonsen, Luigi T Luppino, Fredrik Aspheim, Kristoffer K. Wickstrøm, Elisabeth Wetzer, Michael C. Kampffmeyer, Rodrigo Berzaghi, Rune Sundset, Robert Jenssen, Samuel Kuttner
Abstract: Dynamic positron emission tomography (PET) and kinetic modeling are pivotal in advancing tracer development research in small animal studies. Accurate kinetic modeling requires precise input function estimation, traditionally achieved via arterial blood sampling. However, arterial cannulation in small animals like mice, involves intricate, time-consuming, and terminal procedures, precluding longitudinal studies. This work proposes a non-invasive, fully convolutional deep learning-based approach (FC-DLIF) to predict input functions directly from PET imaging, potentially eliminating the need for blood sampling in dynamic small-animal PET. The proposed FC-DLIF model includes a spatial feature extractor acting on the volumetric time frames of the PET sequence, extracting spatial features. These are subsequently further processed in a temporal feature extractor that predicts the arterial input function. The proposed approach is trained and evaluated using images and arterial blood curves from [$^{18}$F]FDG data using cross validation. Further, the model applicability is evaluated on imaging data and arterial blood curves collected using two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The model was further evaluated on data truncated and shifted in time, to simulate shorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts the arterial input function with respect to mean squared error and correlation. Furthermore, the FC-DLIF model is able to predict the arterial input function even from truncated and shifted samples. The model fails to predict the AIF from samples collected using different radiotracers, as these are not represented in the training data. Our deep learning-based input function offers a non-invasive and reliable alternative to arterial blood sampling, proving robust and flexible to temporal shifts and different scan durations.

Paper number 51:
Title: Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation
Authors: Szymon Płotka, Gizem Mert, Maciej Chrabaszcz, Ewa Szczurek, Arkadiusz Sitek
Abstract: In recent years, artificial intelligence has significantly advanced medical image segmentation. Nonetheless, challenges remain, including efficient 3D medical image processing across diverse modalities and handling data variability. In this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a two-level token-routing layer for efficient long-context modeling, specifically designed for 3D medical image segmentation. Built on the Mamba Selective State Space Model (SSM) backbone, HoME enhances sequential modeling through adaptive expert routing. In the first level, a Soft Mixture-of-Experts (SMoE) layer partitions input sequences into local groups, routing tokens to specialized per-group experts for localized feature extraction. The second level aggregates these outputs through a global SMoE layer, enabling cross-group information fusion and global context refinement. This hierarchical design, combining local expert routing with global expert refinement, enhances generalizability and segmentation performance, surpassing state-of-the-art results across datasets from the three most widely used 3D medical imaging modalities and varying data qualities. The code is publicly available at this https URL.

Paper number 52:
Title: Robust Residual Finite Scalar Quantization for Neural Compression
Authors: Xiaoxu Zhu, Jiakui Li, Ken Zheng, Guiping Zhong, Huimeng Wang, Shiyin Kang, Dahua Lin
Abstract: Finite Scalar Quantization (FSQ) offers simplified training but suffers from residual magnitude decay in multi-stage settings, where subsequent stages receive exponentially weaker signals. We propose Robust Residual Finite Scalar Quantization (RFSQ), addressing this fundamental limitation through two novel conditioning strategies: learnable scaling factors and invertible layer normalization. Our experiments across audio and image modalities demonstrate RFSQ's effectiveness and generalizability. In audio reconstruction at 24 bits/frame, RFSQ-LayerNorm achieves 3.646 DNSMOS, a 3.6% improvement over state-of-the-art RVQ (3.518). On ImageNet, RFSQ achieves 0.102 L1 loss and 0.100 perceptual loss, with LayerNorm providing 9.7% L1 improvement and 17.4% perceptual improvement over unconditioned variants. The LayerNorm strategy consistently outperforms alternatives by maintaining normalized input statistics across stages, effectively preventing exponential magnitude decay that limits naive residual approaches. RFSQ combines FSQ's simplicity with multi-stage quantization's representational power, establishing a new standard for neural compression across diverse modalities.

Paper number 53:
Title: COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques
Authors: Boyang Chen, Mohd Tasleem Khan, George Goussetis, Mathini Sellathurai, Yuan Ding, João F. C. Mota, Jongeun Lee
Abstract: Convolutional Neural Networks (CNNs) are highly effective for computer vision and pattern recognition tasks; however, their computational intensity and reliance on hardware such as FPGAs pose challenges for deployment on low-power edge devices. In this work, we present COMET, a framework of CNN designs that employ efficient hardware offset-binary coding (OBC) techniques to enable co-optimization of performance and resource utilization. The approach formulates CNN inference with OBC representations of inputs (Scheme A) and weights (Scheme B) separately, enabling exploitation of bit-width asymmetry. The shift-accumulate operation is modified by incorporating the offset term with the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we introduce four novel look-up table (LUT) techniques -- parallel, shared, split, and hybrid -- and analyze them to identify the most efficient options. Building on this foundation, we develop an OBC-based general matrix multiplication core using the im2col transformation, enabling efficient acceleration of a fixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the proposed co-optimization approach significantly reduces resource utilization compared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on accuracy.

Paper number 54:
Title: Operational Risks in Grid Integration of Large Data Center Loads: Characteristics, Stability Assessments, and Sensitivity Studies
Authors: Kyung-Bin Kwon, Sayak Mukherjee, Veronica Adetola
Abstract: This paper investigates the dynamic interactions between large-scale data centers and the power grid, focusing on reliability challenges arising from sudden fluctuations in demand. With the rapid growth of AI-driven workloads, such fluctuations, along with fast ramp patterns, are expected to exacerbate stressed grid conditions and system instabilities. We consider a few open-source AI data center consumption profiles from the MIT supercloud datasets, along with generating a few experimental HPC job-distribution-based inference profiles. Subsequently, we develop analytical methodologies for real-time assessment of grid stability, focusing on both transient and small-signal stability assessments. Energy-flow-like metrics for nonlinear transient stability, formulated by computing localized data center bus kinetic-like flows and coupling interactions with neighboring buses over varying time windows, help provide operators with real-time assessments of the regional grid stress in the data center hubs. On the other hand, small-signal stability metrics, constructed from analytical state matrices under variable operating conditions during a fast ramping period, enable snapshot-based assessments of data center load fluctuations and provide enhanced observability into evolving grid conditions. By quantifying the stability impacts of large data center clusters, studies conducted in the modified IEEE benchmark $68-$bus model support improved operator situational awareness to capture risks in reliable integration of large data center loads.

Paper number 55:
Title: Time-causal and time-recursive wavelets
Authors: Tony Lindeberg
Abstract: When to apply wavelet analysis to real-time temporal signals, where the future cannot be accessed, it is essential to base all the steps in the signal processing pipeline on computational mechanisms that are truly time-causal. This paper describes how a time-causal wavelet analysis can be performed based on concepts developed in the area of temporal scale-space theory, originating from a complete classification of temporal smoothing kernels that guarantee non-creation of new structures from finer to coarser temporal scale levels. By necessity, convolution with truncated exponential kernels in cascade constitutes the only permissable class of kernels, as well as their temporal derivatives as a natural complement to fulfil the admissibility conditions of wavelet representations. For a particular way of choosing the time constants in the resulting infinite convolution of truncated exponential kernels, to ensure temporal scale covariance and thus self-similarity over temporal scales, we describe how mother wavelets can be chosen as temporal derivatives of the resulting time-causal limit kernel. By developing connections between wavelet theory and scale-space theory, we characterize and quantify how the continuous scaling properties transfer to the discrete implementation, demonstrating how the proposed time-causal wavelet representation can reflect the duration of locally dominant temporal structures in the input signals. We propose that this notion of time-causal wavelet analysis could be a valuable tool for signal processing tasks, where streams of signals are to be processed in real time, specifically for signals that may contain local variations over a rich span of temporal scales, or more generally for analysing physical or biophysical temporal phenomena, where a fully time-causal analysis is called for to be physically realistic.

Paper number 56:
Title: Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs
Authors: Xinlu He, Swayambhu Nath Ray, Harish Mallidi, Jia-Hong Huang, Ashwin Bellur, Chander Chandak, M. Maruf, Venkatesh Ravichandran
Abstract: Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information. In this work, we investigate the TTS within the MLLM paradigm using continuous speech representations. We design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis. (3) Masked training is employed to address exposure bias in autoregressive decoding. (4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.

Paper number 57:
Title: Proceedings of the second edition of the International Symposium on Computational Sensing (ISCS25)
Authors: Thomas Feuillen, Amirafshar Moshtaghpour
Abstract: The International Symposium on Computational Sensing (ISCS) brings together researchers from optical microscopy, electron microscopy, RADAR, astronomical imaging, biomedical imaging, remote sensing, and signal processing. With a particular focus on applications and demonstrators, the purpose of this symposium is to be a forum where researchers in computational sensing working in seemingly unrelated applications can learn, discover, and exchange on their new findings and challenges. This 3-day symposium in the heart of Europe features 6 keynotes speakers and is open to extended abstracts for scientific presentations and show-and-tell demonstrations.

Paper number 58:
Title: Rydberg Atomic Quantum Satellites for Enhanced Ground-to-Space Direct Uplink Access
Authors: Qihao Peng, Tierui Gong, Zihang Song, Qu Luo, Cunhua Pan, Pei Xiao, Chau Yuen
Abstract: This paper investigates the performance advantages of Rydberg atomic quantum (RAQ)-based multiple-input multiple-output (MIMO) satellites for enhancing direct ground-to-space uplink this http URL analytically evaluate the impact of Rydberg atoms on channel estimation by deriving closed-form expressions for the mean-square error (MSE) and normalized mean-square error (NMSE). Based on the estimated channels, we further derive lower bounds on the achievable data rates for maximum ratio combining (MRC) and zero-forcing (ZF) detection schemes. Rigorous analysis demonstrates that RAQ-MIMO outperforms conventional radio-frequency (RF) MIMO under both Rayleigh and satellite channel conditions. Specifically, compared with conventional MIMO, RAQR achieves a ``squaring" gain under Rayleigh fading, especially in long-distance transmission scenarios with stringent power constraints. In contrast, under line-of-sight (LoS)-dominated satellite channels, this gain saturates as channel-estimation benefits diminish, with the remaining improvement primarily arising from the normalized noise background. Monte Carlo simulations validate the analytical results and show that the performance gains of RAQ-MIMO satellites translate into smaller antenna apertures, lower transmit power, and longer communication ranges, thereby paving the way for next-generation satellite networks.

Paper number 59:
Title: Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks
Authors: Alexandra E. Ballentine, Raghvendra V. Cowlagi
Abstract: We apply a physics-informed neural network (PINN) to solve the two-point boundary value problem (BVP) arising from the necessary conditions postulated by Pontryagin's Minimum Principle for optimal control. Such BVPs are known to be numerically difficult to solve by traditional shooting methods due to extremely high sensitivity to initial guesses. In the light of recent successes in applying PINNs for solving high-dimensional differential equations, we develop a PINN to solve the problem of finding trajectories with minimum exposure to a spatiotemporal threat for a vehicle kinematic model. First, we implement PINNs that are trained to solve the BVP for a given pair of initial and final states for a given threat field. Next, we implement a PINN conditioned on the initial state for a given threat field, which eliminates the need for retraining for each initial state. We demonstrate that the PINN outputs satisfy the necessary conditions with low numerical error.

Paper number 60:
Title: A Novel State-Centric Necessary Condition for Time-Optimal Control of Controllable Linear Systems Based on Augmented Switching Laws (Extended Version)
Authors: Yunan Wang, Chuxiong Hu, Yujie Lin, Zeyang Li, Shize Lin, Suqin He
Abstract: Most existing necessary conditions for optimal control based on adjoining methods require both state and costate information, yet the unobservability of costates for a given feasible trajectory impedes the determination of optimality in practice. This paper establishes a novel theoretical framework for time-optimal control of controllable linear systems with a single input, proposing the augmented switching law (ASL) that represents the input control and the feasibility in a compact form. Given a feasible trajectory, the perturbed trajectory under the constraints of ASL is guaranteed to be feasible, resulting in a novel state-centric necessary condition without dependence on costate information. A first-order necessary condition is proposed that the Jacobian matrix of the ASL is not of full row rank, which also results in a potential approach to optimizing a given feasible trajectory with the preservation of arc structures. The proposed necessary condition is applied to high-order chain-of-integrator systems with full box constraints, contributing to some theoretical results challenging to reason by costate-based conditions.

Paper number 61:
Title: Optimal Rates of Convergence for Entropy Regularization in Discounted Markov Decision Processes
Authors: Johannes Müller, Semih Cayci
Abstract: We study the error introduced by entropy regularization in infinite-horizon, discrete, discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent. This is in contrast to previously known estimates, of the order $O(\tau)$, where $\tau$ is the regularization strength. We provide a lower bound matching our upper bound up to a polynomial term, thereby characterizing the exponential convergence rate for entropy regularization. Our proof relies on the observation that the solutions of entropy-regularized Markov decision processes solve a gradient flow of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. This correspondence allows us to identify the limit of this gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of this gradient flow, which corresponds to a time-continuous version of the natural policy gradient method. We use our improved error estimates to show that for entropy-regularized natural policy gradient methods, the overall error decays exponentially in the square root of the number of iterations, improving over existing sublinear guarantees. Finally, we extend our analysis to settings beyond the entropy. In particular, we characterize the implicit bias regarding general convex potentials and their resulting generalized natural policy gradients.

Paper number 62:
Title: Feed-forward active magnetic shielding
Authors: Alain de Cheveigné
Abstract: Magnetic fields from the brain are tiny relative to ambient fields which therefore need to be suppressed. The common solution of passive shielding is expensive, bulky and insufficiently effective, thus motivating research into the alternative of active shielding which comes in two flavours: feed-back and feed-forward. In feed-back designs (the most common), corrective fields are created by coils driven from sensors within the area that they correct, for example from the main sensors of an MEG device. In feed-forward designs (less common), corrective fields are driven from dedicated reference sensors outside the area they correct. Feed-forward can achieve better performance than feed-back, in principle, however its implementation is hobbled by an unavoidable coupling between coils and reference sensors, which reduces the effectiveness of the shielding and may affect stability, complicating the design. This paper suggests a solution that relies on a ``decoupling matrix," inserted in the signal pathway between sensors and corrective coils, to counteract the spurious coupling. This allows feed-forward shielding do reduce the ambient field to zero across the full frequency range, in principle, although performance may be limited by other factors such as current noise. The solution, which is fully data-driven and does not require geometric calculations, high-tolerance fabrication, or physical calibration, has been evaluated by simulation, but not implemented in hardware. It might contribute to the deployment of a new generation of measurement systems based on optically-pumped magnetometers (OPM). The lower cost and reduced constraints of those systems are a strong incentive to likewise reduce the cost and constraints of the shielding required to operate them, hence the appeal of active shielding.

Paper number 63:
Title: Variational autoencoders stabilise TCN performance when classifying weakly labelled bioacoustics data: an interdisciplinary approach
Authors: Laia Garrobé Fonollosa, Douglas Gillespie, Lina Stankovic, Vladimir Stankovic, Luke Rendell
Abstract: Passive acoustic monitoring (PAM) data is often weakly labelled, audited at the scale of detection presence or absence on timescales of minutes to hours. Moreover, this data exhibits great variability from one deployment to the next, due to differences in ambient noise and the signals across sources and geographies. This study proposes a two-step solution to leverage weakly annotated data for training Deep Learning (DL) detection models. Our case study involves binary classification of the presence/absence of sperm whale (\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from a dataset comprising diverse sources and deployment conditions to maximise generalisability. We tested methods for extracting acoustic features from lengthy audio segments and integrated Temporal Convolutional Networks (TCNs) trained on the extracted features for sequence classification. For feature extraction, we introduced a new approach using Variational AutoEncoders (VAEs) to extract information from both waveforms and spectrograms, which eliminates the necessity for manual threshold setting or time-consuming strong labelling. For classification, TCNs were trained separately on sequences of either VAE embeddings or handpicked acoustic features extracted from the waveform and spectrogram representations using classical methods, to compare the efficacy of the two approaches. The TCN demonstrated robust classification capabilities on a validation set, achieving accuracies exceeding 85\% when applied to 4-minute acoustic recordings. Notably, TCNs trained on handpicked acoustic features exhibited greater variability in performance across recordings from diverse deployment conditions, whereas those trained on VAEs showed a more consistent performance, highlighting the robust transferability of VAEs for feature extraction across different deployment conditions.

Paper number 64:
Title: Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning
Authors: Emile Anand, Ishani Karmarkar, Guannan Qu
Abstract: Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.

Paper number 65:
Title: VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction
Authors: Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
Abstract: Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. Code has been released at this https URL.

Paper number 66:
Title: Robust time series generation via Schrödinger Bridge: a comprehensive evaluation
Authors: Alexandre Alouadi, Baptiste Barreau, Laurent Carlier, Huyên Pham
Abstract: We investigate the generative capabilities of the Schrödinger Bridge (SB) approach for time series. The SB framework formulates time series synthesis as an entropic optimal interpolation transport problem between a reference probability measure on path space and a target joint distribution. This results in a stochastic differential equation over a finite horizon that accurately captures the temporal dynamics of the target time series. While the SB approach has been largely explored in fields like image generation, there is a scarcity of studies for its application to time series. In this work, we bridge this gap by conducting a comprehensive evaluation of the SB method's robustness and generative performance. We benchmark it against state-of-the-art (SOTA) time series generation methods across diverse datasets, assessing its strengths, limitations, and capacity to model complex temporal dependencies. Our results offer valuable insights into the SB framework's potential as a versatile and robust tool for time series generation.

Paper number 67:
Title: Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids
Authors: Yuting Cai, Shaohuai Liu, Chao Tian, Le Xie
Abstract: Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fréchet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.

Paper number 68:
Title: Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization
Authors: Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, Mengmi Zhang
Abstract: Imagine hearing a dog bark and turning toward the sound only to see a parked car, while the real, silent dog sits elsewhere. Such sensory conflicts test perception, yet humans reliably resolve them by prioritizing sound over misleading visuals. Despite advances in multimodal AI integrating vision and audio, little is known about how these systems handle cross-modal conflicts or whether they favor one modality. In this study, we systematically examine modality bias and conflict resolution in AI sound localization. We assess leading multimodal models and benchmark them against human performance in psychophysics experiments across six audiovisual conditions, including congruent, conflicting, and absent cues. Humans consistently outperform AI, demonstrating superior resilience to conflicting or missing visuals by relying on auditory information. In contrast, AI models often default to visual input, degrading performance to near chance levels. To address this, we propose a neuroscience-inspired model, EchoPin, which uses a stereo audio-image dataset generated via 3D simulations. Even with limited training data, EchoPin surpasses existing benchmarks. Notably, it also mirrors human-like horizontal localization bias favoring left-right precision-likely due to the stereo audio structure reflecting human ear placement. These findings underscore how sensory input quality and system architecture shape multimodal representation accuracy.

Paper number 69:
Title: Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space
Authors: Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, Min Zhang
Abstract: We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.

Paper number 70:
Title: Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry
Authors: Antoine Collas, Ce Ju, Nicolas Salvy, Bertrand Thirion
Abstract: Generating realistic brain connectivity matrices is key to analyzing population heterogeneity in brain organization, understanding disease, and augmenting data in challenging classification problems. Functional connectivity matrices lie in constrained spaces, such as the set of symmetric positive definite or correlation matrices, that can be modeled as Riemannian manifolds. However, using Riemannian tools typically requires redefining core operations (geodesics, norms, integration), making generative modeling computationally inefficient. In this work, we propose DiffeoCFM, an approach that enables conditional flow matching (CFM) on matrix manifolds by exploiting pullback metrics induced by global diffeomorphisms on Euclidean spaces. We show that Riemannian CFM with such metrics is equivalent to applying standard CFM after data transformation. This equivalence allows efficient vector field learning, and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two different settings: the matrix logarithm for covariance matrices and the normalized Cholesky decomposition for correlation matrices. We evaluate DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from 2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables fast training and achieves state-of-the-art performance, all while preserving manifold constraints. Code: this https URL

Paper number 71:
Title: MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees
Authors: Herbert Woisetschläger, Ryan Zhang, Shiqiang Wang, Hans-Arno Jacobsen
Abstract: Open-weight large language model (LLM) zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. We introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction. Across a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of $2\times$ cost savings compared to existing LLM routing techniques.

Paper number 72:
Title: Visual Cues Support Robust Turn-taking Prediction in Noise
Authors: Sam O'Connor Russell, Naomi Harte
Abstract: Accurate predictive turn-taking models (PTTMs) are essential for naturalistic human-robot interaction. However, little is known about their performance in noise. This study therefore explores PTTM performance in types of noise likely to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10 dB music noise. Training with noisy data enables a multimodal PTTM, which includes visual features to better exploit visual cues, with 72% accuracy in 10 dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all noise types and SNRs, highlighting its ability to exploit visual cues; however, this does not always generalise to new types of noise. Analysis also reveals that successful training relies on accurate transcription, limiting the use of ASR-derived transcriptions to clean conditions. We make code publicly available for future research.

Paper number 73:
Title: Online Learning for Dynamic Vickrey-Clarke-Groves Mechanism in Unknown Environments
Authors: Vincent Leon, S. Rasoul Etesami
Abstract: We consider the problem of online dynamic mechanism design for sequential auctions in unknown environments, where the underlying market and, thus, the bidders' values vary over time as interactions between the seller and the bidders progress. We model the sequential auctions as an infinite-horizon average-reward Markov decision process (MDP). In each round, the seller determines an allocation and sets a payment for each bidder, while each bidder receives a private reward and submits a sealed bid to the seller. The state, which represents the underlying market, evolves according to an unknown transition kernel and the seller's allocation policy without episodic resets. We first extend the Vickrey-Clarke-Groves (VCG) mechanism to sequential auctions, thereby obtaining a dynamic counterpart that preserves the desired properties: efficiency, truthfulness, and individual rationality. We then focus on the online setting and develop a reinforcement learning algorithm for the seller to learn the underlying MDP and implement a mechanism that closely resembles the dynamic VCG mechanism. We show that the learned mechanism approximately satisfies efficiency, truthfulness, and individual rationality and achieves guaranteed performance in terms of various notions of regret.

Paper number 74:
Title: Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability
Authors: Xiaoxu Zhu, Junhua Li, Aaron J. Li, Yiming Ren, Baoxiang Li
Abstract: Self-supervised speech models learn representations that capture both content and speaker information. Yet this entanglement creates problems: content tasks suffer from speaker bias, and privacy concerns arise when speaker identity leaks through supposedly anonymized representations. We present two contributions to address these challenges. First, we develop InterpTRQE-SptME (Timbre Residual Quantitative Evaluation Benchmark of Speech pre-training Models Encoding via Interpretability), a benchmark that directly measures residual speaker information in content embeddings using SHAP-based interpretability analysis. Unlike existing indirect metrics, our approach quantifies the exact proportion of speaker information remaining after disentanglement. Second, we propose InterpTF-SptME, which uses these interpretability insights to filter speaker information from embeddings. Testing on VCTK with seven models including HuBERT, WavLM, and ContentVec, we find that SHAP Noise filtering reduces speaker residuals from 18.05% to nearly zero while maintaining recognition accuracy (CTC loss increase under 1%). The method is model-agnostic and requires no retraining.

Paper number 75:
Title: Predictability Enables Parallelization of Nonlinear State Space Models
Authors: Xavier Gonzalez, Leo Kozachkov, David M. Zoltowski, Kenneth L. Clarkson, Scott W. Linderman
Abstract: The rise of parallel computing hardware has made it increasingly important to understand which nonlinear state space models can be efficiently parallelized. Recent advances like DEER (arXiv:2309.12252) or DeepPCR (arXiv:2309.16318) have shown that evaluating a state space model can be recast as solving a parallelizable optimization problem, and sometimes this approach can yield dramatic speed-ups in evaluation time. However, the factors that govern the difficulty of these optimization problems remain unclear, limiting the larger adoption of the technique. In this work, we establish a precise relationship between the dynamics of a nonlinear system and the conditioning of its corresponding optimization formulation. We show that the predictability of a system, defined as the degree to which small perturbations in state influence future behavior, impacts the number of optimization steps required for evaluation. In predictable systems, the state trajectory can be computed in $O((\log T)^2)$ time, where $T$ is the sequence length, a major improvement over the conventional sequential approach. In contrast, chaotic or unpredictable systems exhibit poor conditioning, with the consequence that parallel evaluation converges too slowly to be useful. Importantly, our theoretical analysis demonstrates that for predictable systems, the optimization problem is always well-conditioned, whereas for unpredictable systems, the conditioning degrades exponentially as a function of the sequence length. We validate our claims through extensive experiments, providing practical guidance on when nonlinear dynamical systems can be efficiently parallelized, and highlighting predictability as a key design principle for parallelizable models.

Paper number 76:
Title: NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems
Authors: Roman Jacome, Romario Gualdrón-Hurtado, Leon Suarez, Henry Arguello
Abstract: Imaging inverse problems aim to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose Non-Linear Projections of the Null-Space (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.

Paper number 77:
Title: Hybrid MAC Protocol with Integrated Multi-Layered Security for Resource-Constrained UAV Swarm Communications
Authors: Dhrumil Bhatt, Siddharth Penumatsa, Vidushi Kumar
Abstract: Flying Ad Hoc Networks (FANETs) present unique challenges due to high node mobility, dynamic topologies, and strict resource constraints. Existing routing protocols often optimize for a single metric, such as path length or energy, while neglecting the complex dependencies between network performance, security, and MAC layer efficiency. This paper introduces a novel hardware software co design framework for secure and adaptive UAV swarm communications, featuring an energy aware protocol stack. The architecture employs a multicast, clustered organization where routing decisions integrate dynamic trust scores, historical link quality, and internodal distance. A hybrid MAC protocol combines contention based and scheduled channel access for optimized throughput. Security is ensured through a zero trust model that fuses cryptographic authentication with a behavioral reputation system, alongside hardware accelerated AES GCM encryption. Comparative analysis in an NS 3 simulation environment demonstrates the framework's superiority in packet delivery ratio, latency, resilience, and overhead, providing a scalable foundation for high performance swarm operations.

Paper number 78:
Title: Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment
Authors: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone
Abstract: Edge intelligence enables low-latency inference via compact on-device models, but assuring reliability remains challenging. We study edge-cloud cascades that must preserve conditional coverage: whenever the edge returns a prediction set, it should contain the true label with a user-specified probability, as if produced by the cloud model. We formalize conditional coverage with respect to the cloud predictive distribution, and introduce a conformal alignment-based (CAb) cascading mechanism that certifies this property with user control over the risk level. Our method casts escalation from edge to cloud models as a multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA) to select which inputs can be safely handled at the edge. The proposed CAb model cascading method yields statistical guarantees on the average fraction of edge decisions that satisfy cloud-level conditional coverage. The procedure applies to arbitrary edge prediction sets, including variants of conformal prediction (CP), and exposes a tunable trade-off among coverage, deferral rate, and set size. Experiments on CIFAR-100 image classification and the TeleQnA question-answering (QA) benchmark show that the proposed CAb cascade maintains the target conditional coverage for edge predictions while substantially reducing offloading to the cloud and incurring modest increases in prediction-set size.
    