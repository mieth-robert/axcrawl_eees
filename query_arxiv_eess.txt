
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: A Digital Twin-Based Simulation Framework for Safe Curve Speed Estimation Using Unity
Authors: Araf Rahman (1), M. Sabbir Salek (1), Mashrur Chowdhury (1), Wayne A. Sarasua (1) ((1) Clemson University)
Abstract: Horizontal curves are often associated with roadway crashes due to speed misjudgment and loss of control. With the growing adoption of autonomous and connected vehicles, the accurate estimation of safe speed at curves is becoming increasingly important. The widely used AASHTO design method for safe curve speed estimation relies on an analytical equation based on a simplified point mass model, which often uses conservative parameters to account for vehicular and environmental variations. This paper presents a digital twin-based framework for estimating safe speed at curves using a physics-driven virtual environment developed in the Unity engine. In this framework, a real-world horizontal road curve is selected, and vehicle speed data are collected using a radar gun under various weather conditions. A 3D model of the road curve is constructed in a Unity environment using roadway geometric and elevation data. A parameterized vehicle model is integrated, allowing for variations in mass, acceleration, and center of gravity to reflect different vehicle types and loading scenarios. This simulation identifies the maximum safe speed at which a vehicle can traverse the given curve, providing a more vehicle and environment-specific estimate of the safe operating speed. The study validated that the safe curve speed estimates generated by the simulation were consistent with the real-world speed values observed at a curve. This study demonstrates how a physics-based digital twin can estimate a safer and more adaptive operating speed for vehicles traversing horizontal curves.

Paper number 2:
Title: RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition
Authors: Pengcheng Wang, Sheng Li, Takahiro Shinozaki
Abstract: In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I) with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR hypothesis queries a vector store of audio-text pairs and domain terms, and the retrieved results are fused with the live ASR hypotheses to fix recognition errors. The fused hypotheses are passed to the LLM, yielding improved responses.

Paper number 3:
Title: MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis
Authors: Jaskaran Singh, Amartya Roy Chowdhury, Raghav Prabhakar, Varshul C. W
Abstract: Current Text-to-Speech models pose a multilingual challenge, where most of the models traditionally focus on English and European languages, thereby hurting the potential to provide access to information to many more people. To address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker Text-To-Speech (TTS) system that has excellent multilingual expressive capabilities in Indic languages. The model has been trained on around 20K hours of data specifically focused on Indian languages. Our approach leverages Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for text-to-semantic modeling. Additionally, we have used a Conditional Flow Model (CFM) for semantics to melspectogram generation. The experimental results indicate the effectiveness of the proposed approach over other frameworks. Our code is available at this https URL

Paper number 4:
Title: Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings
Authors: Taous Iatariene, Alexandre Guérin, Romain Serizel (MULTISPEECH)
Abstract: Speaker embeddings are promising identity-related features that can enhance the identity assignment performance of a tracking system by leveraging its spatial predictions, i.e, by performing identity reassignment. Common speaker embedding extractors usually struggle with short temporal contexts and overlapping speech, which imposes long-term identity reassignment to exploit longer temporal contexts. However, this increases the probability of tracking system errors, which in turn impacts negatively on identity reassignment. To address this, we propose a Knowledge Distillation (KD) based training approach for short context speaker embedding extraction from two speaker mixtures. We leverage the spatial information of the speaker of interest using beamforming to reduce overlap. We study the feasibility of performing identity reassignment over blocks of fixed size, i.e., blockwise identity reassignment, to go towards a low-latency speaker embedding based tracking system. Results demonstrate that our distilled models are effective at short-context embedding extraction and more robust to overlap. Although, blockwise reassignment results indicate that further work is needed to handle simultaneous speech more effectively.

Paper number 5:
Title: Hallucinations in medical devices
Authors: Jason Granstedt, Prabhat Kc, Rucha Deshpande, Victor Garcia, Aldo Badano
Abstract: Computer methods in medical devices are frequently imperfect and are known to produce errors in clinical or diagnostic tasks. However, when deep learning and data-based approaches yield output that exhibit errors, the devices are frequently said to hallucinate. Drawing from theoretical developments and empirical studies in multiple medical device areas, we introduce a practical and universal definition that denotes hallucinations as a type of error that is plausible and can be either impactful or benign to the task at hand. The definition aims at facilitating the evaluation of medical devices that suffer from hallucinations across product areas. Using examples from imaging and non-imaging applications, we explore how the proposed definition relates to evaluation methodologies and discuss existing approaches for minimizing the prevalence of hallucinations.

Paper number 6:
Title: 3D Cardiac Anatomy Generation Using Mesh Latent Diffusion Models
Authors: Jolanta Mozyrska, Marcel Beetz, Luke Melas-Kyriazi, Vicente Grau, Abhirup Banerjee, Alfonso Bueno-Orovio
Abstract: Diffusion models have recently gained immense interest for their generative capabilities, specifically the high quality and diversity of the synthesized data. However, examples of their applications in 3D medical imaging are still scarce, especially in cardiology. Generating diverse realistic cardiac anatomies is crucial for applications such as in silico trials, electromechanical computer simulations, or data augmentations for machine learning models. In this work, we investigate the application of Latent Diffusion Models (LDMs) for generating 3D meshes of human cardiac anatomies. To this end, we propose a novel LDM architecture -- MeshLDM. We apply the proposed model on a dataset of 3D meshes of left ventricular cardiac anatomies from patients with acute myocardial infarction and evaluate its performance in terms of both qualitative and quantitative clinical and 3D mesh reconstruction metrics. The proposed MeshLDM successfully captures characteristics of the cardiac shapes at end-diastolic (relaxation) and end-systolic (contraction) cardiac phases, generating meshes with a 2.4% difference in population mean compared to the gold standard.

Paper number 7:
Title: Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants
Authors: Aditya Bagri, Vasanthakumar Venugopal, Anandakumar D, Revathi Ezhumalai, Kalyan Sivasailam, Bargava Subramanian, VarshiniPriya, Meenakumari K S, Abi M, Renita S
Abstract: Background: Accurate diagnosis of wrist and hand fractures using radiographs is essential in emergency care, but manual interpretation is slow and prone to errors. Transformer-based models show promise in improving medical image analysis, but their application to extremity fractures is limited. This study addresses this gap by applying object detection transformers to wrist and hand X-rays. Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO, using over 26,000 annotated X-rays from a proprietary clinical dataset. Each image was labeled for fracture presence with bounding boxes. A ResNet-50 classifier was trained on cropped regions to refine abnormality classification. Supervised contrastive learning was used to enhance embedding quality. Performance was evaluated using AP@50, precision, and recall metrics, with additional testing on real-world X-rays. Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR outperformed it with an AP@50 of 0.615 and faster convergence. The integrated pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on real-world X-rays, demonstrating strong generalization across 13 fracture types. Visual inspection confirmed accurate localization. Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and clinical relevance in wrist and hand fracture detection, offering reliable localization and differentiation of fracture types. It is scalable, efficient, and suitable for real-time deployment in hospital workflows, improving diagnostic speed and reliability in musculoskeletal radiology.

Paper number 8:
Title: EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition
Authors: Hugo Thimonier, Antony Perzo, Renaud Seguier
Abstract: Emotion recognition from speech is a challenging task that requires capturing both linguistic and paralinguistic cues, with critical applications in human-computer interaction and mental health monitoring. Recent works have highlighted the ability of Large Language Models (LLMs) to perform tasks outside of the sole natural language area. In particular, recent approaches have investigated coupling LLMs with other data modalities by using pre-trained backbones and different fusion mechanisms. This work proposes a novel approach that fine-tunes an LLM with audio and text representations for emotion prediction. Our method first extracts audio features using an audio feature extractor, which are then mapped into the LLM's representation space via a learnable interfacing module. The LLM takes as input (1) the transformed audio features, (2) additional features in the form of natural language (e.g., the transcript), and (3) a textual prompt describing the emotion prediction task. To efficiently adapt the LLM to this multimodal task, we employ Low-Rank Adaptation (LoRA), enabling parameter-efficient fine-tuning. Experimental results on standard emotion recognition benchmarks demonstrate that our model outperforms all but one existing Speech-Text LLMs in the literature, while requiring less than half the parameters of competing approaches. This highlights our approach's effectiveness in integrating multi-modal inputs for speech-based emotion understanding while maintaining significant computational efficiency.

Paper number 9:
Title: Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI
Authors: Karin A. Olthof, Matteo Fusagli, Bianca Güttner, Tiziano Natali, Bram Westerink, Stefanie Speidel, Theo J.M. Ruers, Koert F.D. Kuhlmann, Andrey Zhylka
Abstract: Background: The aim of this study was to develop and evaluate a deep learning-based automated segmentation method for hepatic anatomy (i.e., parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the clinical workflow of preoperative planning. Methods: Manual segmentation was performed on hepatobiliary phase MRI scans from 90 consecutive patients who underwent liver surgery between January 2020 and October 2023. A deep learning network (nnU-Net v1) was trained on 72 patients with an extra focus on thin structures and topography preservation. Performance was evaluated on an 18-patient test set by comparing automated and manual segmentations using Dice similarity coefficient (DSC). Following clinical integration, 10 segmentations (assessment dataset) were generated using the network and manually refined for clinical use to quantify required adjustments using DSC. Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma, 0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was 76.6+/-24.1%, with a median of one false-positive per patient. The assessment dataset showed minor adjustments were required for clinical use of the 3D models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01) and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater variability (DSC 0.80+/-0.27). During prospective clinical use, the model detected three additional tumors initially missed by radiologists. Conclusions: The proposed nnU-Net-based segmentation method enables accurate and automated delineation of hepatic anatomy. This enables 3D planning to be applied efficiently as a standard-of-care for every patient undergoing liver surgery.

Paper number 10:
Title: A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans
Authors: Justin Yiu, Kushank Arora, Daniel Steinberg, Rohit Ghiya
Abstract: Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for assessing knee injuries. However, manual interpretation of MRI slices remains time-consuming and prone to inter-observer variability. This study presents a systematic evaluation of various deep learning architectures combined with explainable AI (xAI) techniques for automated region of interest (ROI) detection in knee MRI scans. We investigate both supervised and self-supervised approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and multiple U-Net variants augmented with multi-layer perceptron (MLP) classifiers. To enhance interpretability and clinical relevance, we integrate xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed using AUC for classification and PSNR/SSIM for reconstruction quality, along with qualitative ROI visualizations. Our results demonstrate that ResNet50 consistently excels in classification and ROI identification, outperforming transformer-based models under the constraints of the MRNet dataset. While hybrid U-Net + MLP approaches show potential for leveraging spatial features in reconstruction and interpretability, their classification performance remains lower. Grad-CAM consistently provided the most clinically meaningful explanations across architectures. Overall, CNN-based transfer learning emerges as the most effective approach for this dataset, while future work with larger-scale pretraining may better unlock the potential of transformer models.

Paper number 11:
Title: Towards Unified Probabilistic Verification and Validation of Vision-Based Autonomy
Authors: Jordan Peper, Yan Miao, Sayan Mitra, Ivan Ruchkin
Abstract: Precise and comprehensive situational awareness is a critical capability of modern autonomous systems. Deep neural networks that perceive task-critical details from rich sensory signals have become ubiquitous; however, their black-box behavior and sensitivity to environmental uncertainty and distribution shifts make them challenging to verify formally. Abstraction-based verification techniques for vision-based autonomy produce safety guarantees contingent on rigid assumptions, such as bounded errors or known unique distributions. Such overly restrictive and inflexible assumptions limit the validity of the guarantees, especially in diverse and uncertain test-time environments. We propose a methodology that unifies the verification models of perception with their offline validation. Our methodology leverages interval MDPs and provides a flexible end-to-end guarantee that adapts directly to the out-of-distribution test-time conditions. We evaluate our methodology on a synthetic perception Markov chain with well-defined state estimation distributions and a mountain car benchmark. Our findings reveal that we can guarantee tight yet rigorous bounds on overall system safety.

Paper number 12:
Title: InverTwin: Solving Inverse Problems via Differentiable Radio Frequency Digital Twin
Authors: Xingyu Chen, Jianrong Ding, Kai Zheng, Xinmin Fang, Xinyu Zhang, Chris Xiaoxuan Lu, Zhengxiong Li
Abstract: Digital twins (DTs), virtual simulated replicas of physical scenes, are transforming various industries. However, their potential in radio frequency (RF) sensing applications has been limited by the unidirectional nature of conventional RF simulators. In this paper, we present InverTwin, an optimization-driven framework that creates RF digital twins by enabling bidirectional interaction between virtual and physical realms. InverTwin overcomes the fundamental differentiability challenges of RF optimization problems through novel design components, including path-space differentiation to address discontinuity in complex simulation functions, and a radar surrogate model to mitigate local non-convexity caused by RF signal periodicity. These techniques enable smooth gradient propagation and robust optimization of the DT model. Our implementation and experiments demonstrate InverTwin's versatility and effectiveness in augmenting both data-driven and model-driven RF sensing systems for DT reconstruction.

Paper number 13:
Title: A Data-Based Review of Battery Electric Vehicle and Traction Inverter Trends
Authors: Christoph Sachs, Martin Neuburger
Abstract: Battery electric vehicles (BEVs) have advanced significantly during the past decade, yet drivetrain energy losses continue to restrict practical range and elevate cost. A dataset comprising more than 1000 European-market BEVs (model years 2010-2025) is combined with detailed inverter-motor co-simulation to chart technology progress for and quantify the efficiency and cost-saving potential of partial-load optimised multi-level inverter (MLI) for 2030. Average drive-cycle range has climbed from 135 km to 455 km, while fleet-average energy consumption has remained virtually constant. Three inverter topologies are assessed to evaluate future efficiency and cost enhancements: a conventional two-level (2L) six halfbridge (B6) inverter with silicon (Si) and silicon carbide (SiC) devices, and two three-level (3L) T-type neutral point clamped (TNPC) and active neutral point clamped (ANPC) inverters tailored for partial-load operation. The 3L-TNPC inverter, realised with only 30% additional SiC chip area, lowers drive-cycle drivetrain losses by 0.67 kWh/100 km relative to a SiC 2L-B6 baseline. These results identify partial-load optimised MLIs as a cost-effective route to further reduce BEV energy consumption and total system cost.

Paper number 14:
Title: Design and Optimization of a Hybrid VLC/THz Infrastructure-to-Vehicle Communication System for Intelligent Transportation
Authors: Yusef Modami, Hamzeh Beiranvand, Mohammad Taghi Dabiri
Abstract: This paper proposes a hybrid infrastructure-to-vehicle (I2V) communication framework to support future 6G-enabled intelligent transportation systems (ITS) in smart cities. Leveraging existing LED streetlighting infrastructure, the system simultaneously delivers energy-efficient illumination and high-speed wireless connectivity. The proposed scheme integrates visible light communication (VLC) with a complementary ter-ahertz (THz) antenna array to overcome VLC limitations under high ambient light and adverse weather conditions. Key con-tributions include the design of a VLC/THz access network, seamless integration with lighting infrastructure, a proposed switching-combination (PSC) mechanism, and a physical layout optimization strategy. Using a grid search method, thousands of configurations were evaluated to maximize lighting coverage, re-ceived power, signal-to-noise ratio (SNR), signal-to-interference-and-noise ratio (SINR), and minimize outage probability. Results show that optimized lighting coverage improves from 35% to 97%, while hybrid communication coverage increases from 49%to 99.9% at the same power level. Under extreme environmental conditions, the hybrid system maintains up to 99% coverage, compared to 69% with VLC alone. These results demonstrate the scalability, cost-efficiency, and practicality of the proposed system for next-generation ITS deployment.

Paper number 15:
Title: Autonomy at Levels for Spacecraft
Authors: Daniel Baker (1), Jeremy Wojcik (1), Sean Phillips (2) ((1) Defense Sector, BlueHalo, LLC, (2) Space Control Branch (RVSW), Air Force Research Laboratory)
Abstract: Autonomy at Levels is the idea that autonomy should be embedded within and throughout a spacecraft. Using Systems Engineering methods a spacecraft is typically decomposed into systems, subsystems, assemblies, components, and so on. All these decomposition levels within all the spacecraft's systems, could and should have autonomy elements built in. As a result, the "autonomy system" is made of autonomy elements or units that are integrated, distributed and embedded within the whole spacecraft. This is like how the power system would be designed and implemented. Linking control loops and autonomy loops illustrates how to achieve Autonomy at Levels.

Paper number 16:
Title: Robust tracking MPC for perturbed nonlinear systems -- Extended version
Authors: Marco Polver, Daniel Limon, Fabio Previdi, Antonio Ferramosca
Abstract: This paper presents a novel robust predictive controller for constrained nonlinear systems that is able to track piece-wise constant setpoint signals. The tracking model predictive controller presented in this paper extends the nonlinear MPC for tracking to the more complex case of nonlinear systems subject to bounded and not necessarily additive perturbations. The optimal control problem that is solved at each step penalizes the deviation of the predicted nominal system trajectory from an artificial reference, which is added as a decision variable, as well as the distance between the artificial reference and the setpoint. Robust feasibility is ensured by imposing conservative constraints that take into account the effect of uncertainties and convergence to a neighborhood of any feasible setpoint is guaranteed by means of an appropriate terminal cost and an extended stabilizing terminal constraint. In the case of unreachable setpoints, convergence to a neighborhood of the optimal reachable steady output is also proved.

Paper number 17:
Title: Grid-Edge Energy-Flexible Technologies: A Comparative Analysis Across Generators, Loads, and Energy Storage Systems
Authors: Jesus Silva-Rodriguez, Tianxia Zhao, Ran Mo, Xingpeng Li
Abstract: This review analysis presents a comprehensive exploration of energy flexibility in modern power systems. It examines the roles and mechanisms of flexible technologies across three main categories: generators, energy storage systems (ESS), and loads. Energy flexibility is defined as the ability to dynamically adjust supply and/or demand in response to grid conditions to maintain balance and stability. This is of particular importance to facilitate the integration of the growing variable renewable energy sources (RES) into modern power grids. Additionally, traditional supply-side mechanisms to maintain balance and stability are complemented by advancements in demand-side management and demand response strategies, which enable loads to adjust consumption patterns and schedules in response to grid requirements. ESS are also explored to further enhance flexibility by absorbing excess generation and/or supplying large load increases that are not able to be met by the less flexible resources. This paper also explores specific flexibility technologies, examining their characteristics, control strategies, advantages, and limitations. Energy flexibility services are also categorized into intermittency mitigation, peak shaving, and energy reserve provisioning. Each service is supported by case studies and examples demonstrating how different resources respond to varying conditions. Ultimately, the findings and reviews of the various flexible resources in this paper provide a roadmap for optimizing energy flexibility across diverse resource types, paving the way for a more sustainable and resilient energy future.

Paper number 18:
Title: Iterative Youla-Kucera Loop Shaping For Precision Motion Control
Authors: Xiaohai Hu, Jason Laks, Guoxiao Guo, Xu Chen
Abstract: This paper presents a numerically robust approach to multi-band disturbance rejection using an iterative Youla-Kucera parameterization technique. The proposed method offers precise control over shaping the frequency response of a feedback loop while maintaining numerical stability through a systematic design process. By implementing an iterative approach, we overcome a critical numerical issue in rejecting vibrations with multiple frequency bands. Meanwhile, our proposed modification of the all-stabilizing Youla-Kucera architecture enables intuitive design while respecting fundamental performance trade-offs and minimizing undesired waterbed amplifications. Numerical validation on a hard disk drive servo system demonstrates significant performance improvements, enabling enhanced positioning precision for increased storage density. The design methodology extends beyond storage systems to various high-precision control applications where multi-band disturbance rejection is critical.

Paper number 19:
Title: Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control with Almost Global Exponential Convergence on SO(3)
Authors: Tianhua Gao, Masashi Izumita, Kohji Tomita, Akiya Kamimura
Abstract: This paper introduces a lightweight and interpretable online learning approach called Dimension-Decomposed Learning (DiD-L) for disturbance identification in quadrotor geometric attitude control. As a module instance of DiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to address underlying underfitting problems, the high-dimensional mapping for online identification is axially ``sliced" into multiple low-dimensional submappings (slices). In this way, the complex high-dimensional problem is decomposed into a set of simple low-dimensional subtasks addressed by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation without the persistent excitation (PE) condition. To enhance the interpretability of the proposed approach, we prove that the state solution of the rotational error dynamics exponentially converges into an arbitrarily small ball within an almost global attraction domain, despite time-varying disturbances and inertia uncertainties. This result is novel as it demonstrates exponential convergence without requiring pre-training for unseen disturbances and specific knowledge of the model. To our knowledge in the quadrotor control field, DiD-L is the first online learning approach that is lightweight enough to run in real-time at 400 Hz on microcontroller units (MCUs) such as STM32, and has been validated through real-world experiments.

Paper number 20:
Title: Weakly-Convex Regularization for Magnetic Resonance Image Denoising
Authors: Akash Prabakar, Abhishek Shreekant Bhandiwad, Abijith Jagannath Kamath, Chandra Sekhar Seelamantula
Abstract: Regularization for denoising in magnetic resonance imaging (MRI) is typically achieved using convex regularization functions. Recently, deep learning techniques have been shown to provide superior denoising performance. However, this comes at the price of lack of explainability, interpretability and stability, which are all crucial to MRI. In this work, we present a constructive approach for designing weakly-convex regularization functions for MR image denoising. We show that our technique performs on par with state-of-the-art denoisers for diffusion-weighted MR image denoising. Our technique can be applied to design weakly-convex convolutional neural networks with prototype activation functions that impart interpretability and are provably convergent. We also show that our technique exhibits fewer denoising artifacts by demonstrating its effect on brain microstructure modelling.

Paper number 21:
Title: Reformulating Parallel-Connected Lithium-Ion Battery Pack Dynamics with Interconnection Resistances as Ordinary Differential Equations
Authors: Jaffar Ali Lone, Nilsu Atlan, Simone Fasolato, Davide M Raimondo, Ross Drummond
Abstract: This work presents analytical solutions for the current distribution in lithium-ion battery packs composed of cells connected in parallel, explicitly accounting for the presence of interconnection resistances. These solutions enable the reformulation of the differential-algebraic equations describing the pack dynamics into a set of ordinary differential equations, thereby simplifying simulation and analysis. Conditions under which uniform current sharing across all cells occurs are also derived. The proposed formulation is validated against experimental data and confirms its ability to capture the key behaviours induced by interconnection resistances. These results can support the improved design and control of parallel-connected battery packs.

Paper number 22:
Title: Pinching-Antenna Systems-Enabled Multi-User Communications: Transmission Structures and Beamforming Optimization
Authors: Jingjing Zhao, Haowen Song, Xidong Mu, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu
Abstract: Pinching-antenna systems (PASS) represent an innovative advancement in flexible-antenna technologies, aimed at significantly improving wireless communications by ensuring reliable line-of-sight connections and dynamic antenna array reconfigurations. To employ multi-waveguide PASS in multi-user communications, three practical transmission structures are proposed, namely waveguide multiplexing (WM), waveguide division (WD), and waveguide switching (WS). Based on the proposed structures, the joint baseband signal processing and pinching beamforming design is studied for a general multi-group multicast communication system, with the unicast communication encompassed as a special case. A max-min fairness problem is formulated for each proposed transmission structure, subject to the maximum transmit power constraint. For WM, to solve the highly-coupled and non-convex MMF problem with complex exponential and fractional expressions, a penalty dual decomposition (PDD)-based algorithm is invoked for obtaining locally optimal solutions. Specifically, the augmented Lagrangian relaxation is first applied to alleviate the stringent coupling constraints, which is followed by the block decomposition over the resulting augmented Lagrangian function. Then, the proposed PDD-based algorithm is extended to solve the MMF problem for both WD and WS. Furthermore, a low-complexity algorithm is proposed for the unicast case employing the WS structure, by simultaneously aligning the signal phases and minimizing the large-scale path loss at each user. Finally, numerical results reveal that: 1) the MMF performance is significantly improved by employing the PASS compared to conventional fixed-position antenna systems; 2) WS and WM are suitable for unicast and multicast communications, respectively; 3) the performance gap between WD and WM can be significantly alleviated when the users are geographically isolated.

Paper number 23:
Title: Fine-grained Image Quality Assessment for Perceptual Image Restoration
Authors: Xiangfei Sheng, Xiaofeng Pan, Zhichao Yang, Pengfei Chen, Leida Li
Abstract: Recent years have witnessed remarkable achievements in perceptual image restoration (IR), creating an urgent demand for accurate image quality assessment (IQA), which is essential for both performance comparison and algorithm optimization. Unfortunately, the existing IQA metrics exhibit inherent weakness for IR task, particularly when distinguishing fine-grained quality differences among restored images. To address this dilemma, we contribute the first-of-its-kind fine-grained image quality assessment dataset for image restoration, termed FGRestore, comprising 18,408 restored images across six common IR tasks. Beyond conventional scalar quality scores, FGRestore was also annotated with 30,886 fine-grained pairwise preferences. Based on FGRestore, a comprehensive benchmark was conducted on the existing IQA metrics, which reveal significant inconsistencies between score-based IQA evaluations and the fine-grained restoration quality. Motivated by these findings, we further propose FGResQ, a new IQA model specifically designed for image restoration, which features both coarse-grained score regression and fine-grained quality ranking. Extensive experiments and comparisons demonstrate that FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and model weights have been released in this https URL

Paper number 24:
Title: Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis
Authors: Xin Wang, Xiaopei Zhang, Xingang Wang
Abstract: This paper proposes a high-precision semantic segmentation method based on an improved TransUNet architecture to address the challenges of complex lesion structures, blurred boundaries, and significant scale variations in skin lesion images. The method integrates a transformer module into the traditional encoder-decoder framework to model global semantic information, while retaining a convolutional branch to preserve local texture and edge features. This enhances the model's ability to perceive fine-grained structures. A boundary-guided attention mechanism and multi-scale upsampling path are also designed to improve lesion boundary localization and segmentation consistency. To verify the effectiveness of the approach, a series of experiments were conducted, including comparative studies, hyperparameter sensitivity analysis, data augmentation effects, input resolution variation, and training data split ratio tests. Experimental results show that the proposed model outperforms existing representative methods in mIoU, mDice, and mAcc, demonstrating stronger lesion recognition accuracy and robustness. In particular, the model achieves better boundary reconstruction and structural recovery in complex scenarios, making it well-suited for the key demands of automated segmentation tasks in skin lesion analysis.

Paper number 25:
Title: Markov Chain-based Model of Blockchain Radio Access Networks
Authors: Vasileios Kouvakis, Stylianos E. Trevlakis, Alexandros-Apostolos A. Boulogeorgos, Hongwu Liu, Theodoros A. Tsiftsis, Octavia A. Dobre
Abstract: Security has always been a priority, for researchers, service providers and network operators when it comes to radio access networks (RAN). One wireless access approach that has captured attention is blockchain enabled RAN (B-RAN) due to its secure nature. This research introduces a framework that integrates blockchain technology into RAN while also addressing the limitations of state-of-the-art models. The proposed framework utilizes queuing and Markov chain theory to model the aspects of B-RAN. An extensive evaluation of the models performance is provided, including an analysis of timing factors and a focused assessment of its security aspects. The results demonstrate reduced latency and comparable security making the presented framework suitable for diverse application scenarios.

Paper number 26:
Title: From Slices to Structures: Unsupervised 3D Reconstruction of Female Pelvic Anatomy from Freehand Transvaginal Ultrasound
Authors: Max Krähenmann, Sergio Tascon-Morales, Fabian Laumer, Julia E. Vogt, Ece Ozkan
Abstract: Volumetric ultrasound has the potential to significantly improve diagnostic accuracy and clinical decision-making, yet its widespread adoption remains limited by dependence on specialized hardware and restrictive acquisition protocols. In this work, we present a novel unsupervised framework for reconstructing 3D anatomical structures from freehand 2D transvaginal ultrasound (TVS) sweeps, without requiring external tracking or learned pose estimators. Our method adapts the principles of Gaussian Splatting to the domain of ultrasound, introducing a slice-aware, differentiable rasterizer tailored to the unique physics and geometry of ultrasound imaging. We model anatomy as a collection of anisotropic 3D Gaussians and optimize their parameters directly from image-level supervision, leveraging sensorless probe motion estimation and domain-specific geometric priors. The result is a compact, flexible, and memory-efficient volumetric representation that captures anatomical detail with high spatial fidelity. This work demonstrates that accurate 3D reconstruction from 2D ultrasound images can be achieved through purely computational means, offering a scalable alternative to conventional 3D systems and enabling new opportunities for AI-assisted analysis and diagnosis.

Paper number 27:
Title: Broadband Near-Infrared Compressive Spectral Imaging System with Reflective Structure
Authors: Yutong Li, Zhenming Yu, Liming Cheng, Jiayu Di, Liang Lin, Jingyue Ma, Tongshuo Zhang, Yue Zhou, Haiying Zhao, Kun Xu
Abstract: Near-infrared (NIR) hyperspectral imaging has become a critical tool in modern analytical science. However, conventional NIR hyperspectral imaging systems face challenges including high cost, bulky instrumentation, and inefficient data collection. In this work, we demonstrate a broadband NIR compressive spectral imaging system that is capable of capturing hyperspectral data covering a broad spectral bandwidth ranging from 700 to 1600 nm. By segmenting wavelengths and designing specialized optical components, our design overcomes hardware spectral limitations to capture broadband data, while the reflective optical structure makes the system compact. This approach provides a novel technical solution for NIR hyperspectral imaging.

Paper number 28:
Title: Integrated Snapshot Near-infrared Hypersepctral Imaging Framework with Diffractive Optics
Authors: Jingyue Ma, Zhenming Yu, Zhengyang Li, Liang Lin, Liming Cheng, Kun Xu
Abstract: We propose an integrated snapshot near-infrared hyperspectral imaging framework that combines designed DOE with NIRSA-Net. The results demonstrate near-infrared spectral imaging at 700-1000nm with 10nm resolution while achieving improvement of PSNR 1.47dB and SSIM 0.006.

Paper number 29:
Title: FPGA Design and Implementation of Fixed-Point Fast Divider Using Goldschmidt Division Algorithm and Mitchell Multiplication Algorithm
Authors: Jinkun Yang
Abstract: This paper presents a variable bit-width fixed-point fast divider using Goldschmidt division algorithm and Mitchell multiplication algorithm. Described using Verilog HDL and implemented on a Xilinx XC7Z020-2CLG400I FPGA, the proposed divider achieves over 99% computational accuracy with a minimum latency of 99.1 ns, which is 31.7 ns faster than existing single-precision dividers. Compared with a Goldschmidt divider using a Vedic multiplier, the proposed design reduces Slice Registers by 46.68%, Slice LUTs by 4.93%, and Slices by 11.85%, with less than 1% accuracy loss and only 24.1 ns additional delay. These results demonstrate an improved balance between computational speed and resource utilization, making the divider well-suited for high-performance FPGA-based systems with strict resource constraints.

Paper number 30:
Title: A Study of the Scale Invariant Signal to Distortion Ratio in Speech Separation with Noisy References
Authors: Simon Dahl Jepsen, Mads Græsbøll Christensen, Jesper Rindom Jensen
Abstract: This paper examines the implications of using the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) as both evaluation and training objective in supervised speech separation, when the training references contain noise, as is the case with the de facto benchmark WSJ0-2Mix. A derivation of the SI-SDR with noisy references reveals that noise limits the achievable SI-SDR, or leads to undesired noise in the separated outputs. To address this, a method is proposed to enhance references and augment the mixtures with WHAM!, aiming to train models that avoid learning noisy references. Two models trained on these enhanced datasets are evaluated with the non-intrusive NISQA.v2 metric. Results show reduced noise in separated speech but suggest that processing references may introduce artefacts, limiting overall quality gains. Negative correlation is found between SI-SDR and perceived noisiness across models on the WSJ0-2Mix and Libri2Mix test sets, underlining the conclusion from the derivation.

Paper number 31:
Title: Design of a Gm-C Dynamic Amplifier with High Linearity and High Temperature and Power Supply Voltage Stability
Authors: Jinkun Yang, Pengbin Xu
Abstract: This paper presents a Gm-C dynamic amplifier with high linearity and high temperature and power supply voltage stability. The main part of the amplifier employs two asymmetric differential pairs to enhance transconductance linearity. The amplifier maintains a nearly constant gain within a differential input range of -40 mV to 40 mV, and achieves a total harmonic distortion (THD) of 70.5 dB. The bias part of the amplifier adopts a constant-gm bias circuit, which improves the temperature and supply voltage stability of the amplifier's transconductance and gain. When the differential input is 1 mV, the power supply voltage fluctuates by $\pm$10%, and the temperature varies between -40$\mathrm{^\circ C}$ and 120$\mathrm{^\circ C}$, the standard deviation of the gain distribution is 262m, and the distribution range is from 15.1 to 16.3.

Paper number 32:
Title: Distributed Multiple Fault Detection and Estimation in DC Microgrids with Unknown Power Loads
Authors: Jingwei Dong, Mahdieh S. Sadabadi, Per Mattsson, André Teixeira
Abstract: This paper proposes a distributed diagnosis scheme to detect and estimate actuator and power line faults in DC microgrids subject to unknown power loads and stochastic noise. To address actuator faults, we design a fault estimation filter whose parameters are determined through a tractable optimization problem to achieve fault estimation, decoupling from power line faults, and robustness against noise. In contrast, the estimation of power line faults poses greater challenges due to the inherent coupling between fault currents and unknown power loads, which becomes ill-posed when the underlying system is insufficiently excited. To the best of our knowledge, this is the first study to address this critical yet underexplored issue. Our solution introduces a novel differentiate-before-estimate strategy. A set of diagnostic rules based on the temporal characteristics of a constructed residual is developed to distinguish load changes from line faults. Once a power line fault is detected, a regularized least-squares method is activated to estimate the fault currents, for which we further derive an upper bound on the estimation error. Finally, comprehensive simulation results validate the effectiveness of the proposed methods.

Paper number 33:
Title: Assessment of Power System Stability Considering Multiple Time-Scale Dynamics: Insights into Hopf Bifurcations in Presence of GFL and GFM IBRs
Authors: Luis David Pabon Ospina
Abstract: Real power systems exhibit dynamics that evolve across a wide range of time scales, from very fast to very slow phenomena. Historically, incorporating these wide-ranging dynamics into a single model has been impractical. As a result, power engineers rely on time-scale decomposition to simplify models. When fast phenomena are evaluated, slow dynamics are neglected (assumed stable), and vice versa. This paper challenges this paradigm by showing the importance of assessing power system stability while considering multiple time scales simultaneously. Using the concept of Hopf bifurcations, it exemplifies instability issues that would be missed if multi-time-scale dynamics are not considered. Although this work employs both grid-following and grid-forming inverter-based resource models, it is not a direct comparison. Instead, it presents a case study demonstrating how one technology can complement the other from a multi time-scale dynamics perspective.

Paper number 34:
Title: Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model
Authors: Hyun-Jic Oh, Junsik Kim, Zhiyi Shi, Yichen Wu, Yu-An Chen, Peter K. Sorger, Hanspeter Pfister, Won-Ki Jeong
Abstract: Multiplex imaging is revolutionizing pathology by enabling the simultaneous visualization of multiple biomarkers within tissue samples, providing molecular-level insights that traditional hematoxylin and eosin (H&E) staining cannot provide. However, the complexity and cost of multiplex data acquisition have hindered its widespread adoption. Additionally, most existing large repositories of H&E images lack corresponding multiplex images, limiting opportunities for multimodal analysis. To address these challenges, we leverage recent advances in latent diffusion models (LDMs), which excel at modeling complex data distributions utilizing their powerful priors for fine-tuning to a target domain. In this paper, we introduce a novel framework for virtual multiplex staining that utilizes pretrained LDM parameters to generate multiplex images from H&E images using a conditional diffusion model. Our approach enables marker-by-marker generation by conditioning the diffusion model on each marker, while sharing the same architecture across all markers. To tackle the challenge of varying pixel value distributions across different marker stains and to improve inference speed, we fine-tune the model for single-step sampling, enhancing both color contrast fidelity and inference efficiency through pixel-level loss functions. We validate our framework on two publicly available datasets, notably demonstrating its effectiveness in generating up to 18 different marker types with improved accuracy, a substantial increase over the 2-3 marker types achieved in previous approaches. This validation highlights the potential of our framework, pioneering virtual multiplex staining. Finally, this paper bridges the gap between H&E and multiplex imaging, potentially enabling retrospective studies and large-scale analyses of existing H&E image repositories.

Paper number 35:
Title: Optimal Unpredictable Control for Linear Systems
Authors: Chendi Qu, Jianping He, Jialun Li, Xiaoming Duan
Abstract: In this paper, we investigate how to achieve the unpredictability against malicious inferences for linear systems. The key idea is to add stochastic control inputs, named as unpredictable control, to make the outputs irregular. The future outputs thus become unpredictable and the performance of inferences is degraded. The major challenges lie in: i) how to formulate optimization problems to obtain an optimal distribution of stochastic input, under unknown prediction accuracy of the adversary; and ii) how to achieve the trade-off between the unpredictability and control performance. We first utilize both variance and confidence probability of prediction error to quantify unpredictability, then formulate two two-stage stochastic optimization problems, respectively. Under variance metric, the analytic optimal distribution of control input is provided. With probability metric, it is a non-convex optimization problem, thus we present a novel numerical method and convert the problem into a solvable linear optimization problem. Last, we quantify the control performance under unpredictable control, and accordingly design the unpredictable LQR and cooperative control. Simulations demonstrate the unpredictability of our control algorithm. The obtained optimal distribution outperforms Gaussian and Laplace distributions commonly used in differential privacy under proposed metrics.

Paper number 36:
Title: Rule-based Key-Point Extraction for MR-Guided Biomechanical Digital Twins of the Spine
Authors: Robert Graf, Tanja Lerchl, Kati Nispel, Hendrik Möller, Matan Atad, Julian McGinnis, Julius Maria Watrinet, Johannes Paetzold, Daniel Rueckert, Jan S. Kirschke
Abstract: Digital twins offer a powerful framework for subject-specific simulation and clinical decision support, yet their development often hinges on accurate, individualized anatomical modeling. In this work, we present a rule-based approach for subpixel-accurate key-point extraction from MRI, adapted from prior CT-based methods. Our approach incorporates robust image alignment and vertebra-specific orientation estimation to generate anatomically meaningful landmarks that serve as boundary conditions and force application points, like muscle and ligament insertions in biomechanical models. These models enable the simulation of spinal mechanics considering the subject's individual anatomy, and thus support the development of tailored approaches in clinical diagnostics and treatment planning. By leveraging MR imaging, our method is radiation-free and well-suited for large-scale studies and use in underrepresented populations. This work contributes to the digital twin ecosystem by bridging the gap between precise medical image analysis with biomechanical simulation, and aligns with key themes in personalized modeling for healthcare.

Paper number 37:
Title: Improving Resource-Efficient Speech Enhancement via Neural Differentiable DSP Vocoder Refinement
Authors: Heitor R. Guimarães, Ke Tan, Juan Azcarreta, Jesus Alvarez, Prabhav Agrawal, Ashutosh Pandey, Buye Xu
Abstract: Deploying speech enhancement (SE) systems in wearable devices, such as smart glasses, is challenging due to the limited computational resources on the device. Although deep learning methods have achieved high-quality results, their computational cost limits their feasibility on embedded platforms. This work presents an efficient end-to-end SE framework that leverages a Differentiable Digital Signal Processing (DDSP) vocoder for high-quality speech synthesis. First, a compact neural network predicts enhanced acoustic features from noisy speech: spectral envelope, fundamental frequency (F0), and periodicity. These features are fed into the DDSP vocoder to synthesize the enhanced waveform. The system is trained end-to-end with STFT and adversarial losses, enabling direct optimization at the feature and waveform levels. Experimental results show that our method improves intelligibility and quality by 4% (STOI) and 19% (DNSMOS) over strong baselines without significantly increasing computation, making it well-suited for real-time applications.

Paper number 38:
Title: Long-Context Speech Synthesis with Context-Aware Memory
Authors: Zhipeng Li, Xiaofen Xing, Jingyuan Xing, Hangrui Hu, Heng Lu, Xiangmin Xu
Abstract: In long-text speech synthesis, current approaches typically convert text to speech at the sentence-level and concatenate the results to form pseudo-paragraph-level speech. These methods overlook the contextual coherence of paragraphs, leading to reduced naturalness and inconsistencies in style and timbre across the long-form speech. To address these issues, we propose a Context-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The CAM block integrates and retrieves both long-term memory and local context details, enabling dynamic memory updates and transfers within long paragraphs to guide sentence-level speech synthesis. Furthermore, the prefix mask enhances the in-context learning ability by enabling bidirectional attention on prefix tokens while maintaining unidirectional generation. Experimental results demonstrate that the proposed method outperforms baseline and state-of-the-art long-context methods in terms of prosody expressiveness, coherence and context inference cost across paragraph-level speech.

Paper number 39:
Title: Recursive Gaussian Process Regression with Integrated Monotonicity Assumptions for Control Applications
Authors: Ricus Husmann, Sven Weishaupt, Harald Aschemann
Abstract: In this paper, we present an extension to the recursive Gaussian Process (RGP) regression that enables the satisfaction of inequality constraints and is well suited for a real-time execution in control applications. The soft inequality constraints are integrated by introducing an additional extended Kalman Filter (EKF) update step using pseudo-measurements. The sequential formulation of the algorithm and several developed heuristics ensure both the performance and a low computational effort of the algorithm. A special focus lies on an efficient consideration of monotonicity assumptions for GPs in the form of inequality constraints. The algorithm is statistically validated in simulations, where the possible advantages in comparison with the standard RGP algorithm become obvious. The paper is concluded with a successful experimental validation of the developed algorithm for the monotonicity-preserving learning of heat transfer values for the control of a vapor compression cycle evaporator, leveraging a previously published partial input output linearization (IOL).

Paper number 40:
Title: PadAug: Robust Speaker Verification with Simple Waveform-Level Silence Padding
Authors: Zijun Huang, Chengdong Liang, Jiadi Yao, Xiao-Lei Zhang
Abstract: The presence of non-speech segments in utterances often leads to the performance degradation of speaker verification. Existing systems usually use voice activation detection as a preprocessing step to cut off long silence segments. However, short silence segments, particularly those between speech segments, still remain a problem for speaker verification. To address this issue, in this paper, we propose a simple wave-level data augmentation method, \textit{PadAug}, which aims to enhance the system's robustness to silence segments. The core idea of \textit{PadAug} is to concatenate silence segments with speech segments at the waveform level for model training. Due to its simplicity, it can be directly applied to the current state-of-the art architectures. Experimental results demonstrate the effectiveness of the proposed \textit{PadAug}. For example, applying \textit{PadAug} to ResNet34 achieves a relative equal error rate reduction of 5.0\% on the voxceleb dataset. Moreover, the \textit{PadAug} based systems are robust to different lengths and proportions of silence segments in the test data.

Paper number 41:
Title: Failure Tolerant Phase-Only Indoor Positioning via Deep Learning
Authors: Fatih Ayten, Mehmet C. Ilter, Akshay Jain, Ossi Kaltiokallio, Jukka Talvitie, Elena Simona Lohan, Henk Wymeersch, Mikko Valkama
Abstract: High-precision localization turns into a crucial added value and asset for next-generation wireless systems. Carrier phase positioning (CPP) enables sub-meter to centimeter-level accuracy and is gaining interest in 5G-Advanced standardization. While CPP typically complements time-of-arrival (ToA) measurements, recent literature has introduced a phase-only positioning approach in a distributed antenna/MIMO system context with minimal bandwidth requirements, using deep learning (DL) when operating under ideal hardware assumptions. In more practical scenarios, however, antenna failures can largely degrade the performance. In this paper, we address the challenging phase-only positioning task, and propose a new DL-based localization approach harnessing the so-called hyperbola intersection principle, clearly outperforming the previous methods. Additionally, we consider and propose a processing and learning mechanism that is robust to antenna element failures. Our results show that the proposed DL model achieves robust and accurate positioning despite antenna impairments, demonstrating the viability of data-driven, impairment-tolerant phase-only positioning mechanisms. Comprehensive set of numerical results demonstrates large improvements in localization accuracy against the prior art methods.

Paper number 42:
Title: Full-Duplex Beamforming Optimization for Near-Field ISAC
Authors: Ahsan Nazar, Zhambyl Shaikhanov, Sennur Ulukus
Abstract: Integrated Sensing and Communications (ISAC) is a promising technology for future wireless networks, enabling simultaneous communication and sensing using shared resources. This paper investigates the performance of full-duplex (FD) communication in near-field ISAC systems, where spherical-wave propagation introduces unique beam-focusing capabilities. We propose a joint optimization framework for transmit and receive beamforming at the base station to minimize transmit power while satisfying rate constraints for multi-user downlink transmission, multi-user uplink reception, and multi-target sensing. Our approach employs alternating optimization combined with semidefinite relaxation and Rayleigh quotient techniques to address the non-convexity of the problem. Simulation results demonstrate that FD-enabled near-field ISAC achieves superior power efficiency compared to half-duplex and far-field benchmarks, effectively detecting targets at identical angles while meeting communication requirements.

Paper number 43:
Title: Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks
Authors: Brian Kim, Justin H. Kong, Terrence J. Moore, Fikadu T. Dagefu
Abstract: Routing in multi-hop wireless networks is a complex problem, especially in heterogeneous networks where multiple wireless communication technologies coexist. Reinforcement learning (RL) methods, such as Q-learning, have been introduced for decentralized routing by allowing nodes to make decisions based on local observations. However, Q-learning suffers from scalability issues and poor generalization due to the difficulty in managing the Q-table in large or dynamic network topologies, especially in heterogeneous networks (HetNets) with diverse channel characteristics. Thus, in this paper, we propose a novel deep Q-network (DQN)-based routing framework for heterogeneous multi-hop wireless networks to maximize the end-to-end rate of the route by improving scalability and adaptability, where each node uses a deep neural network (DNN) to estimate the Q-values and jointly select the next-hop relay and a communication technology for transmission. To achieve better performance with the DNN, selecting which nodes to exchange information is critical, as it not only defines the state and action spaces but also determines the input to the DNN. To this end, we propose neighbor node selection strategies based on channel gain and rate between nodes rather than a simple distance-based approach for an improved set of states and actions for DQN-based routing. During training, the model experiences diverse network topologies to ensure generalization and robustness, and simulation results show that the proposed neighbor node selection outperforms simple distance-based selection. Further, we observe that the DQN-based approach outperforms various benchmark schemes and performs comparably to the optimal approach.

Paper number 44:
Title: Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach
Authors: Kartik Pandey, Arun Balasubramanian, Debasis Samanta
Abstract: Electroencephalogram (EEG) signals have gained widespread adoption in brain-computer interface (BCI) applications due to their non-invasive, low-cost, and relatively simple acquisition process. The demand for higher spatial resolution, particularly in clinical settings, has led to the development of high-density electrode arrays. However, increasing the number of channels introduces challenges such as cross-channel interference and computational overhead. To address these issues, modern BCI systems often employ channel selection algorithms. Existing methods, however, are typically task-specific and require re-optimization for each new application. This work proposes a task-agnostic channel selection method, Activity Coefficient-based Channel Selection (ACCS), which uses a novel metric called the Channel Activity Coefficient (CAC) to quantify channel utility based on activity levels. By selecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97% improvement in multi-class classification accuracy. Unlike traditional approaches, ACCS identifies a reusable set of informative channels independent of the downstream task or model, making it highly adaptable for diverse EEG-based applications.

Paper number 45:
Title: Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation
Authors: Chinmoy Biswas, Nafis Faisal, Vivek Chowdhury, Abrar Al-Shadid Abir, Sabir Mahmud, Mithon Rahman, Shaikh Anowarul Fattah, Hafiz Imtiaz
Abstract: Sparsity, defined as the presence of missing or zero values in a dataset, often poses a major challenge while operating on real-life datasets. Sparsity in features or target data of the training dataset can be handled using various interpolation methods, such as linear or polynomial interpolation, spline, moving average, or can be simply imputed. Interpolation methods usually perform well with Strict Sense Stationary (SSS) data. In this study, we show that an approximately 62\% sparse dataset with hourly load data of a power plant can be utilized for load forecasting assuming the data is Wide Sense Stationary (WSS), if augmented with Gaussian interpolation. More specifically, we perform statistical analysis on the data, and train multiple machine learning and deep learning models on the dataset. By comparing the performance of these models, we empirically demonstrate that Gaussian interpolation is a suitable option for dealing with load forecasting problems. Additionally, we demonstrate that Long Short-term Memory (LSTM)-based neural network model offers the best performance among a diverse set of classical and neural network-based models.

Paper number 46:
Title: Systematic FAIRness Assessment of Open Voice Biomarker Datasets for Mental Health and Neurodegenerative Diseases
Authors: Ishaan Mahapatra, Nihar R. Mahapatra
Abstract: Voice biomarkers--human-generated acoustic signals such as speech, coughing, and breathing--are promising tools for scalable, non-invasive detection and monitoring of mental health and neurodegenerative diseases. Yet, their clinical adoption remains constrained by inconsistent quality and limited usability of publicly available datasets. To address this gap, we present the first systematic FAIR (Findable, Accessible, Interoperable, Reusable) evaluation of 27 publicly available voice biomarker datasets focused on these disease areas. Using the FAIR Data Maturity Model and a structured, priority-weighted scoring method, we assessed FAIRness at subprinciple, principle, and composite levels. Our analysis revealed consistently high Findability but substantial variability and weaknesses in Accessibility, Interoperability, and Reusability. Mental health datasets exhibited greater variability in FAIR scores, while neurodegenerative datasets were slightly more consistent. Repository choice also significantly influenced FAIRness scores. To enhance dataset quality and clinical utility, we recommend adopting structured, domain-specific metadata standards, prioritizing FAIR-compliant repositories, and routinely applying structured FAIR evaluation frameworks. These findings provide actionable guidance to improve dataset interoperability and reuse, thereby accelerating the clinical translation of voice biomarker technologies.

Paper number 47:
Title: Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces
Authors: Thomas Gallien
Abstract: Trust region-based optimization methods have become foundational reinforcement learning algorithms that offer stability and strong empirical performance in continuous control tasks. Growing interest in scalable and reusable control policies translate also in a demand for morphological generalization, the ability of control policies to cope with different kinematic structures. Graph-based policy architectures provide a natural and effective mechanism to encode such structural differences. However, while these architectures accommodate variable morphologies, the behavior of trust region methods under varying action space dimensionality remains poorly understood. To this end, we conduct a theoretical analysis of trust region-based policy optimization methods, focusing on both Trust Region Policy Optimization (TRPO) and its widely used first-order approximation, Proximal Policy Optimization (PPO). The goal is to demonstrate how varying action space dimensionality influence the optimization landscape, particularly under the constraints imposed by KL-divergence or policy clipping penalties. Complementing the theoretical insights, an empirical evaluation under morphological variation is carried out using the Gymnasium Swimmer environment. This benchmark offers a systematically controlled setting for varying the kinematic structure without altering the underlying task, making it particularly well-suited to study morphological generalization.

Paper number 48:
Title: High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images
Authors: Surajit Das, Gourav Roy, Pavel Zun
Abstract: Live cell culture is crucial in biomedical studies for analyzing cell properties and dynamics in vitro. This study focuses on segmenting unstained live cells imaged with bright-field microscopy. While many segmentation approaches exist for microscopic images, none consistently address the challenges of bright-field live-cell imaging with high throughput, where temporal phenotype changes, low contrast, noise, and motion-induced blur from cellular movement remain major obstacles. We developed a low-cost CNN-based pipeline incorporating comparative analysis of frozen encoders within a unified U-Net architecture enhanced with attention mechanisms, instance-aware systems, adaptive loss functions, hard instance retraining, dynamic learning rates, progressive mechanisms to mitigate overfitting, and an ensemble technique. The model was validated on a public dataset featuring diverse live cell variants, showing consistent competitiveness with state-of-the-art methods, achieving 93% test accuracy and an average F1-score of 89% (std. 0.07) on low-contrast, noisy, and blurry images. Notably, the model was trained primarily on bright-field images with limited exposure to phase-contrast microscopy (<10%), yet it generalized effectively to the phase-contrast LIVECell dataset, demonstrating modality, robustness and strong performance. This highlights its potential for real-world laboratory deployment across imaging conditions. The model requires minimal compute power and is adaptable using basic deep learning setups such as Google Colab, making it practical for training on other cell variants. Our pipeline outperforms existing methods in robustness and precision for bright-field microscopy segmentation. The code and dataset are available for reproducibility

Paper number 49:
Title: Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method
Authors: Evanns Morales-Cuadrado, Luke Baird, Yorai Wardi, Samuel Coogan
Abstract: We investigate the performance of a lightweight tracking controller, based on a flow version of the Newton-Raphson method, applied to a miniature blimp and a mid-size quadrotor. This tracking technique has been shown to enjoy theoretical guarantees of performance and has been applied with success in simulation studies and on mobile robots with simple motion models. This paper investigates the technique through real-world flight experiments on aerial hardware platforms subject to realistic deployment and onboard computational constraints. The technique's performance is assessed in comparison with the established control frameworks of feedback linearization for the blimp, and nonlinear model predictive control for both quadrotor and blimp. The performance metrics under consideration are (i) root mean square error of flight trajectories with respect to target trajectories, (ii) algorithms' computation times, and (iii) CPU energy consumption associated with the control algorithms. The experimental findings show that the Newton-Raphson flow-based tracking controller achieves comparable or superior tracking performance to the baseline methods with substantially reduced computation time and energy expenditure.

Paper number 50:
Title: Reliability comparison of vessel trajectory prediction models via Probability of Detection
Authors: Zahra Rastin, Kathrin Donandt, Dirk Söffker
Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.

Paper number 51:
Title: OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos
Authors: Miao Zhang, Yifei Zhu, Linfeng Shen, Fangxin Wang, Jiangchuan Liu
Abstract: With the reduced hardware costs of omnidirectional cameras and the proliferation of various extended reality applications, more and more $360^\circ$ videos are being captured. To fully unleash their potential, advanced video analytics is expected to extract actionable insights and situational knowledge without blind spots from the videos. In this paper, we present OmniSense, a novel edge-assisted framework for online immersive video analytics. OmniSense achieves both low latency and high accuracy, combating the significant computation and network resource challenges of analyzing $360^\circ$ videos. Motivated by our measurement insights into $360^\circ$ videos, OmniSense introduces a lightweight spherical region of interest (SRoI) prediction algorithm to prune redundant information in $360^\circ$ frames. Incorporating the video content and network dynamics, it then smartly scales vision models to analyze the predicted SRoIs with optimized resource utilization. We implement a prototype of OmniSense with commodity devices and evaluate it on diverse real-world collected $360^\circ$ videos. Extensive evaluation results show that compared to resource-agnostic baselines, it improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping the accuracy on par with the highest accuracy of baselines.

Paper number 52:
Title: Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization
Authors: Kim Hammar, Tao Li
Abstract: Effective responses to cyberattacks require fast decisions, even when information about the attack is incomplete or inaccurate. However, most decision-support frameworks for incident response rely on a detailed system model that describes the incident, which restricts their practical utility. In this paper, we address this limitation and present an online method for incident response planning under model misspecification, which we call MOBAL: Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, which facilitates model adaptation as the incident unfolds. To determine effective responses online, we quantize the conjectured model into a finite Markov model, which enables efficient response planning through dynamic programming. We prove that Bayesian learning is asymptotically consistent with respect to the information feedback. Additionally, we establish bounds on misspecification and quantization errors. Experiments on the CAGE-2 benchmark show that MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.

Paper number 53:
Title: EmoTale: An Enacted Speech-emotion Dataset in Danish
Authors: Maja J. Hjuler, Harald V. Skat-Rørdam, Line H. Clemmensen, Sneha Das
Abstract: While multiple emotional speech corpora exist for commonly spoken languages, there is a lack of functional datasets for smaller (spoken) languages, such as Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is the only other database of Danish emotional speech. We present EmoTale; a corpus comprising Danish and English speech recordings with their associated enacted emotion annotations. We demonstrate the validity of the dataset by investigating and presenting its predictive power using speech emotion recognition (SER) models. We develop SER models for EmoTale and the reference datasets using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. We find the embeddings superior to the hand-crafted features. The best model achieves an unweighted average recall (UAR) of 64.1% on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable to the performance on DES.

Paper number 54:
Title: Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions
Authors: Euiyeon Kim, Yong-Hoon Choi
Abstract: We introduce a new music source separation model tailored for accurate vocal isolation. Unlike Transformer-based approaches, which often fail to capture intermittently occurring vocals, our model leverages Mamba2, a recent state space model, to better capture long-range temporal dependencies. To handle long input sequences efficiently, we combine a band-splitting strategy with a dual-path architecture. Experiments show that our approach outperforms recent state-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to date-and delivering substantial gains in uSDR. Moreover, the model exhibits stable and consistent performance across varying input lengths and vocal occurrence patterns. These results demonstrate the effectiveness of Mamba-based models for high-resolution audio processing and open up new directions for broader applications in audio research.

Paper number 55:
Title: Improving OCR using internal document redundancy
Authors: Diego Belzarena, Seginus Mowlavi, Aitor Artola, Camilo Mariño, Marina Gardella, Ignacio Ramírez, Antoine Tadros, Roy He, Natalia Bottaioli, Boshra Rajaei, Gregory Randall, Jean-Michel Morel
Abstract: Current OCR systems are based on deep learning models trained on large amounts of data. Although they have shown some ability to generalize to unseen data, especially in detection tasks, they can struggle with recognizing low-quality data. This is particularly evident for printed documents, where intra-domain data variability is typically low, but inter-domain data variability is high. In that context, current OCR methods do not fully exploit each document's redundancy. We propose an unsupervised method by leveraging the redundancy of character shapes within a document to correct imperfect outputs of a given OCR system and suggest better clustering. To this aim, we introduce an extended Gaussian Mixture Model (GMM) by alternating an Expectation-Maximization (EM) algorithm with an intra-cluster realignment process and normality statistical testing. We demonstrate improvements in documents with various levels of degradation, including recovered Uruguayan military archives and 17th to mid-20th century European newspapers.

Paper number 56:
Title: A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives
Authors: Juepeng Zheng, Zi Ye, Yibin Wen, Jianxi Huang, Zhiwei Zhang, Qingmei Li, Qiong Hu, Baodong Xu, Lingyuan Zhao, Haohuan Fu
Abstract: Powered by advances in multiple remote sensing sensors, the production of high spatial resolution images provides great potential to achieve cost-efficient and high-accuracy agricultural inventory and analysis in an automated way. Lots of studies that aim at providing an inventory of the level of each agricultural parcel have generated many methods for Agricultural Parcel and Boundary Delineation (APBD). This review covers APBD methods for detecting and delineating agricultural parcels and systematically reviews the past and present of APBD-related research applied to remote sensing images. With the goal to provide a clear knowledge map of existing APBD efforts, we conduct a comprehensive review of recent APBD papers to build a meta-data analysis, including the algorithm, the study site, the crop type, the sensor type, the evaluation method, etc. We categorize the methods into three classes: (1) traditional image processing methods (including pixel-based, edge-based and region-based); (2) traditional machine learning methods (such as random forest, decision tree); and (3) deep learning-based methods. With deep learning-oriented approaches contributing to a majority, we further discuss deep learning-based methods like semantic segmentation-based, object detection-based and Transformer-based methods. In addition, we discuss five APBD-related issues to further comprehend the APBD domain using remote sensing data, such as multi-sensor data in APBD task, comparisons between single-task learning and multi-task learning in the APBD domain, comparisons among different algorithms and different APBD tasks, etc. Finally, this review proposes some APBD-related applications and a few exciting prospects and potential hot topics in future APBD research. We hope this review help researchers who involved in APBD domain to keep track of its development and tendency.

Paper number 57:
Title: FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics
Authors: Chen Chen, Runze Li, Zejun Zhang, Pukun Zhao, Fanqing Zhou, Longxiang Wang, Haojian Huang
Abstract: FakeHunter is a multimodal deepfake detection framework that combines memory-guided retrieval, chain-of-thought (Observation-Thought-Action) reasoning, and tool-augmented verification to provide accurate and interpretable video forensics. FakeHunter encodes visual content using CLIP and audio using CLAP, generating joint audio-visual embeddings that retrieve semantically similar real exemplars from a FAISS-indexed memory bank for contextual grounding. Guided by the retrieved context, the system iteratively reasons over evidence to localize manipulations and explain them. When confidence is low, it automatically invokes specialized tools-such as zoom-in image forensics or mel-spectrogram inspection-for fine-grained verification. Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that specify what was modified, where it occurs, and why it is judged fake. We also introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos (950+ min) annotated with manipulation type, region/entity, violated reasoning category, and free-form justification. On X-AVFake, FakeHunter achieves an accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87 percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies reveal that memory retrieval contributes a 7.75 percentage point gain, and tool-based inspection improves low-confidence cases to 46.50%. Despite its multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x), demonstrating practical deployability.

Paper number 58:
Title: DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning
Authors: Xudong Wang, Guoming Tang, Junyu Xue, Srinivasan Keshav, Tongxin Li, Chris Ding
Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain fine-grained appliance-level energy consumption in smart homes and building applications. However, the increasing adoption of behind-the-meter energy sources, such as solar panels and battery storage, poses new challenges for conventional NILM methods that rely solely on at-the-meter data. The injected energy from the behind-the-meter sources can obscure the power signatures of individual appliances, leading to a significant decline in NILM performance. To address this challenge, we present DualNILM, a deep multi-task learning framework designed for the dual tasks of appliance state recognition and injected energy identification in NILM. By integrating sequence-to-point and sequence-to-sequence strategies within a Transformer-based architecture, DualNILM can effectively capture multi-scale temporal dependencies in the aggregate power consumption patterns, allowing for accurate appliance state recognition and energy injection identification. We conduct validation of DualNILM using both self-collected and synthesized open NILM datasets that include both appliance-level energy consumption and energy injection. Extensive experimental results demonstrate that DualNILM maintains an excellent performance for the dual tasks in NILM, much outperforming conventional methods.

Paper number 59:
Title: BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?
Authors: Veronica Ruozzi, Sasan Matinfar, Laura Schütz, Benedikt Wiestler, Alberto Redaelli, Emiliano Votta, Nassir Navab
Abstract: Perceptualizing tool interactions with deformable structures in surgical procedures remains challenging, as unimodal visualization techniques often fail to capture the complexity of these interactions due to constraints such as occlusion and limited depth perception. This paper presents a novel approach to augment tool navigation in mixed reality environments by providing auditory representations of tool-tissue dynamics, particularly for interactions with soft tissue. BioSonix, a physics-informed design framework, utilizes tissue displacements in 3D space to compute excitation forces for a sound model encoding tissue properties such as stiffness and density. Biomechanical simulations were employed to model particle displacements resulting from tool-tissue interactions, establishing a robust foundation for the method. An optimization approach was used to define configurations for capturing diverse interaction scenarios with varying tool trajectories. Experiments were conducted to validate the accuracy of the sound-displacement mappings. Additionally, two user studies were performed: the first involved two clinical professionals (a neuroradiologist and a cardiologist), who confirmed the method's impact and achieved high task accuracy; the second included 22 biomedical experts, who demonstrated high discrimination accuracy in tissue differentiation and targeting tasks. The results revealed a strong correlation between tool-tissue dynamics and their corresponding auditory profiles, highlighting the potential of these sound representations to enhance the intuitive understanding of complex interactions.

Paper number 60:
Title: Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification
Authors: Mengliang Zhang, Jacob M. Luber
Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in whole-slide image (WSI) diagnosis. However, pathology images from different hospitals often vary due to differences in scanning hardware and preprocessing styles, which may lead PFMs to inadvertently learn hospital-specific features, posing risks for clinical deployment. In this work, we present the first systematic study of domain bias in PFMs arising from hospital source characteristics. Specifically, we (1) construct a pipeline for quantifying domain bias in PFMs, (2) evaluate and compare the performance of multiple models, and (3) propose a lightweight adversarial framework that removes latent hospital-specific features from frozen representations without modifying the encoder itself. By introducing a trainable adapter and a domain classifier connected through a gradient reversal layer (GRL), our method learns task-discriminative yet domain-invariant representations. Experiments on multi-center histopathology datasets demonstrate that our approach substantially reduces domain predictability while maintaining or even improving disease classification performance, particularly in out-of-domain (unseen hospital) scenarios. Further analyses, including hospital detection and feature space visualization, confirm the effectiveness of our method in mitigating hospital bias. We will provide our code based on acceptance.

Paper number 61:
Title: ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems
Authors: Soumyo Bhattacharjee, Federico Villani, Christian Vogt, Andrea Cossettini, Luca Benini
Abstract: The demand for hundreds of tightly synchronized channels operating at tens of MSPS in ultrasound systems exceeds conventional low-voltage differential signaling links' bandwidth, pin count, and latency. Although the JESD204B serial interface mitigates these limitations, commercial FPGA IP cores are proprietary, costly, and resource-intensive. We present ListenToJESD204B, an open-source receiver IP core released under a permissive Solderpad 0.51 license for AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable SystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides cycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It occupies only 107 configurable logic blocks (approximately 437 LUTs), representing a 79\% reduction compared to comparable commercially available IP. A modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC generation, and optional LFSR descrambling facilitates scaling to high lane counts. We verified protocol compliance through simulation against the Xilinx JESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block stability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s links for 30 minutes with no errors.

Paper number 62:
Title: A State-Space Representation of Coupled Linear Multivariate PDEs and Stability Analysis using SDP
Authors: Declan S. Jagt, Matthew M. Peet
Abstract: Physical processes evolving in both time and space are often modeled using Partial Differential Equations (PDEs). Recently, it has been shown how stability analysis and control of coupled PDEs in a single spatial variable can be more conveniently performed using an equivalent Partial Integral Equation (PIE) representation. The construction of this PIE representation is based on an analytic expression for the inverse of the spatial differential operator, $\partial_s^{d}$, on the domain defined by boundary conditions. In this paper, we show how this univariate representation may be extended inductively to multiple spatial variables by representing the domain as the intersection of lifted univariate domains. Specifically, we show that if each univariate domain is well-posed, then there exists a readily verified consistency condition which is necessary and sufficient for existence of an inverse to the multivariate spatial differential operator, $D^\alpha=\partial_{s_1}^{\alpha_1}\cdots\partial_{s_N}^{\alpha_N}$, on the PDE domain. Furthermore, we show that this inverse is an element of a $*$-algebra of Partial Integral (PI) operators defined by polynomial semi-separable kernels. Based on this operator algebra, we show that the evolution of any suitably well-posed linear multivariate PDE may be described by a PIE, parameterized by elements of the PI algebra. A convex computational test for PDE stability is then proposed using a positive matrix parameterization of positive PI operators, and software (PIETOOLS) is provided which automates the process of representation and stability analysis of such PDEs. This software is used to analyze stability of 2D heat, wave, and plate equations, obtaining accurate bounds on the rate of decay.

Paper number 63:
Title: Improving Infrared Thermography after Solar Loading
Authors: Ellin Q. Zhao, Alexander Vilesov, Pradyumna Chari, Laleh Jalilian, Achuta Kadambi
Abstract: Widely deployed for fever screening, infrared thermometers (IRTs) enable rapid non-contact detection of body temperature, but they are inaccurate in unconstrained environments. Previous works have studied the impact of transient skin temperature on IRTs, but no studies have quantified the effect of skin temperature elevation due to absorbed solar radiation, which we call solar loading. Solar loading leads to poor specificity in fever detection and is a skin tone-dependent effect, introducing inequity in IRTs. The current solution to solar loading is to have a subject reacclimate for up to 30 minutes before IRT measurement. We propose a machine learning method to improve IR thermography by removing the solar loading effect from thermal images of the face. This correction only uses a single frame of thermal data, allowing sub-second correction of skin temperature. On average, forehead skin temperature increases by 2°C after solar loading, and our machine learning model, SL-Net, not only reduces this error by 68% to 0.64°C, but also removes the positive correlation between solar loading error and melanin concentration. We open source a diverse dataset of 100 subjects with co-registered RGB-thermal images, and IRT and skin tone measurements. Our work shows that it is possible to use machine learning to correct complex thermal perturbations and enable robust and equitable human thermography.

Paper number 64:
Title: Diffusion MRI with Machine Learning
Authors: Davood Karimi, Simon K. Warfield
Abstract: \hspace{2mm} Diffusion-weighted magnetic resonance imaging (dMRI) of the brain offers unique capabilities including noninvasive probing of tissue microstructure and structural connectivity. It is widely used for clinical assessment of disease and injury, and for neuroscience research. Analyzing the dMRI data to extract useful information for medical and scientific purposes can be challenging. The dMRI measurements may suffer from strong noise and artifacts, and may exhibit high inter-session and inter-scanner variability in the data, as well as inter-subject heterogeneity in brain structure. Moreover, the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed data preprocessing and harmonization, microstructure mapping, tractography, and white matter tract analysis. We study the main findings, strengths, and weaknesses of the existing methods and suggest topics for future research. We find that machine learning may be exceptionally suited to tackle some of the difficult tasks in dMRI analysis. However, for this to happen, several shortcomings of existing methods and critical unresolved issues need to be addressed. There is a pressing need to improve evaluation practices, to increase the availability of rich training datasets and validation benchmarks, as well as model generalizability, reliability, and explainability concerns.

Paper number 65:
Title: Moving horizon estimation for nonlinear systems with time-varying parameters
Authors: Julian D. Schiller, Matthias A. Müller
Abstract: We propose a moving horizon estimation scheme for estimating the states and time-varying parameters of nonlinear systems. We consider the case where observability of the parameters depends on the excitation of the system and may be absent during operation, with the parameter dynamics fulfilling a weak incremental bounded-energy bounded-state property to ensure boundedness of the estimation error (with respect to the disturbance energy). The proposed estimation scheme involves a standard quadratic cost function with an adaptive regularization term depending on the current parameter observability. We develop robustness guarantees for the overall estimation error that are valid for all times, and that improve the more often the parameters are detected to be observable during operation. The theoretical results are illustrated by a simulation example.

Paper number 66:
Title: Game-theoretic Energy Management Strategies With Interacting Agents in Formula 1
Authors: Giona Fieni, Marc-Philippe Neumann, Alessandro Zanardi, Alberto Cerofolini, Christopher H. Onder
Abstract: This paper presents an interaction-aware energy management optimization framework for Formula 1 racing. The considered scenario involves two agents and a drag reduction model. Strategic interactions between the agents are captured by a Stackelberg game formulated as a bilevel program. To address the computational challenges associated with bilevel optimization, the problem is reformulated as a single-level nonlinear program employing the Karush-Kuhn-Tucker conditions. The proposed framework contributes towards the development of new energy management and allocation strategies, caused by the presence of another agent. For instance, it provides valuable insights on how to redistribute the energy in order to optimally exploit the wake effect, showcasing a notable difference with the behavior studied in previous works. Robust energy allocations can be identified to reduce the lap time loss associated with unexpected choices of the other agent. It allows to recognize the boundary conditions for the interaction to become relevant, impacting the system's behavior, and to assess if overtaking is possible and beneficial. Overall, the framework provides a comprehensive approach for a two-agent Formula 1 racing problem with strategic interactions, offering physically intuitive and practical results.

Paper number 67:
Title: Towards pedestrian head tracking: A benchmark dataset and a multi-source data fusion network
Authors: Kailai Sun, Xinwei Wang, Shaobo Liu, Qianchuan Zhao, Gao Huang, Chang Liu
Abstract: Pedestrian detection and tracking in crowded video sequences have many applications, including autonomous driving, robot navigation and pedestrian flow analysis. However, detecting and tracking pedestrians in high-density crowds face many challenges, including intra-class occlusions, complex motions, and diverse poses. Although artificial intelligence (AI) models have achieved great progress in head detection, head tracking datasets and methods are extremely lacking. Existing head datasets have limited coverage of complex pedestrian flows and scenes (e.g., pedestrian interactions, occlusions, and object interference). It is of great importance to develop new head tracking datasets and methods. To address these challenges, we present a Chinese Large-scale Cross-scene Pedestrian Head Tracking dataset (Cchead) and a Multi-source Data Fusion Network (MDFN). The dataset has features that are of considerable interest, including 10 diverse scenes of 50,528 frames with about 2,366,249 heads and 2,358 tracks. Our dataset contains diverse pedestrian moving speeds, directions, and complex crowd pedestrian flows with collision avoidance behaviors. Existing state-of-the-art (SOTA) algorithms are tested and compared on the Cchead dataset. MDFN is the first end-to-end convolutional neural network (CNN)-based head detection and tracking network that jointly trains Red, Green, Blue (RGB) frames, pixel-level motion information, depth maps, and density maps in videos. Ablation experiments confirm the significance of multi-source data fusion. Compared with SOTA pedestrian detection and tracking methods, MDFN achieves superior performance across three datasets: Cchead, Restaurant and Crowd of Heads Dataset (CroHD). To promote further development, we share our source code and trained models for global researchers: this https URL.

Paper number 68:
Title: Collision Avoidance for Convex Primitives via Differentiable Optimization Based High-Order Control Barrier Functions
Authors: Shiqing Wei, Rooholla Khorrambakht, Prashanth Krishnamurthy, Vinicius Mariano Gonçalves, Farshad Khorrami
Abstract: Ensuring the safety of dynamical systems is crucial, where collision avoidance is a primary concern. Recently, control barrier functions (CBFs) have emerged as an effective method to integrate safety constraints into control synthesis through optimization techniques. However, challenges persist when dealing with convex primitives and tasks requiring torque control, as well as the occurrence of unintended equilibria. This work addresses these challenges by introducing a high-order CBF (HOCBF) framework for collision avoidance among convex primitives. We transform nonconvex safety constraints into linear constraints by differentiable optimization and prove the high-order continuous differentiability. Then, we employ HOCBFs to accommodate torque control, enabling tasks involving forces or high dynamics. Additionally, we analyze the issue of spurious equilibria in high-order cases and propose a circulation mechanism to prevent the undesired equilibria on the boundary of the safe set. Finally, we validate our framework with three experiments on the Franka Research 3 robotic manipulator, demonstrating successful collision avoidance and the efficacy of the circulation mechanism.

Paper number 69:
Title: Data-Efficient System Identification via Lipschitz Neural Networks
Authors: Shiqing Wei, Prashanth Krishnamurthy, Farshad Khorrami
Abstract: Extracting dynamic models from data is of enormous importance in understanding the properties of unknown systems. In this work, we employ Lipschitz neural networks, a class of neural networks with a prescribed upper bound on their Lipschitz constant, to address the problem of data-efficient nonlinear system identification. Under the (fairly weak) assumption that the unknown system is Lipschitz continuous, we propose a method to estimate the approximation error bound of the trained network and the bound on the difference between the simulated trajectories by the trained models and the true system. Empirical results show that our method outperforms classic fully connected neural networks and Lipschitz regularized networks through simulation studies on three dynamical systems, and the advantage of our method is more noticeable when less data is used for training.

Paper number 70:
Title: Domain-Factored Untrained Deep Prior for Spectrum Cartography
Authors: Subash Timilsina, Sagar Shrestha, Lei Cheng, Xiao Fu
Abstract: Spectrum cartography (SC) focuses on estimating the radio power propagation map of multiple emitters across space and frequency using limited sensor measurements. Recent advances in SC have shown that leveraging learned deep generative models (DGMs) as structural constraints yields state-of-the-art performance. By harnessing the expressive power of neural networks, these structural "priors" capture intricate patterns in radio maps. However, training DGMs requires substantial data, which is not always available, and distribution shifts between training and testing data can further degrade performance. To address these challenges, this work proposes using untrained neural networks (UNNs) for SC. UNNs, commonly applied in vision tasks to represent complex data without training, encode structural information of data in neural architectures. In our approach, a custom-designed UNN represents radio maps under a spatio-spectral domain factorization model, leveraging physical characteristics to reduce sample complexity of SC. Experiments show that the method achieves performance comparable to learned DGM-based SC, without requiring training data.

Paper number 71:
Title: GenVC: Self-Supervised Zero-Shot Voice Conversion
Authors: Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews
Abstract: Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.

Paper number 72:
Title: Beyond Quadratic Costs: A Bregman Divergence Approach to H$_\infty$ Control
Authors: Joudi Hajar, Reza Ghane, Babak Hassibi
Abstract: In the past couple of decades, non-quadratic convex penalties have reshaped signal processing and machine learning; in robust control, however, general convex costs break the Riccati and storage function structure that make the design tractable. Practitioners thus default to approximations, heuristics or robust model predictive control that are solved online for short horizons. We close this gap by extending $H_\infty$ control of discrete-time linear systems to strictly convex penalties on state, input, and disturbance, recasting the objective with Bregman divergences that admit a completion-of-squares decomposition. The result is a closed-form, time-invariant, full-information stabilizing controller that minimizes a worst-case performance ratio over the infinite horizon. Necessary and sufficient existence/optimality conditions are given by a Riccati-like identity together with a concavity requirement; with quadratic costs, these collapse to the classical $H_\infty$ algebraic Riccati equation and the associated negative-semidefinite condition, recovering the linear central controller. Otherwise, the optimal controller is nonlinear and can enable safety envelopes, sparse actuation, and bang-bang policies with rigorous $H_\infty$ guarantees.

Paper number 73:
Title: Multi-agent Auditory Scene Analysis
Authors: Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón
Abstract: Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a publicly available framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.

Paper number 74:
Title: Cellular, Cell-less, and Everything in Between: A Unified Framework for Utility Region Analysis in Wireless Networks
Authors: Renato Luis Garrido Cavalcante, Tomasz Piotrowski, Slawomir Stanczak
Abstract: We introduce a unified framework for analyzing utility regions of wireless networks, with a focus on the signal-to-interference-noise-ratio (SINR) and achievable rate regions. The framework provides valuable insights into interference patterns of modern network architectures, such as cell-less and extremely large MIMO networks, and it generalizes existing characterizations of the weak Pareto boundary. A central contribution is the derivation of sufficient conditions that guarantee convexity of the utility regions. Convexity is an important property because it ensures that time sharing (or user grouping) cannot simultaneously increase the utility of all users when the network operates on the weak Pareto boundary. These sufficient conditions also have two key implications. First, they identify a family of (weighted) sum-rate maximization problems that are inherently convex without any variable transformations, thus paving the way for the development of efficient, provably optimal solvers for this family. Second, they provide a rigorous justification for formulating sum-rate maximization problems directly in terms of achievable rates, rather than SINR levels. Our theoretical insights also motivate an alternative to the concept of favorable propagation in the massive MIMO literature -- one that explicitly accounts for self-interference and the beamforming strategy.

Paper number 75:
Title: A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions
Authors: Dhruv Singh Kushwaha, Zoleikha Abdollahi Biron
Abstract: Reinforcement learning (RL) has proven to be particularly effective in solving complex decision-making problems for a wide range of applications. From a control theory perspective, RL can be considered as an adaptive optimal control scheme. Lyapunov and barrier functions are the most commonly used certificates to guarantee system stability for a proposed/derived controller and constraint satisfaction guarantees, respectively, in control theoretic approaches. However, compared to theoretical guarantees available in control theoretic methods, RL lacks closed-loop stability of a computed policy and constraint satisfaction guarantees. Safe reinforcement learning refers to a class of constrained problems where the constraint violations lead to partial or complete system failure. The goal of this review is to provide an overview of safe RL techniques using Lyapunov and barrier functions to guarantee this notion of safety discussed (stability of the system in terms of a computed policy and constraint satisfaction during training and deployment). The different approaches employed are discussed in detail along with their shortcomings and benefits to provide critique and possible future research directions. Key motivation for this review is to discuss current theoretical approaches for safety and stability guarantees in RL similar to control theoretic approaches using Lyapunov and barrier functions. The review provides proven potential and promising scope of providing safety guarantees for complex dynamical systems with operational constraints using model-based and model-free RL.

Paper number 76:
Title: ASAudio: A Survey of Advanced Spatial Audio Research
Authors: Zhiyuan Zhu, Yu Zhang, Wenxiang Guo, Changhao Pan, Zhou Zhao
Abstract: With the rapid development of spatial audio technologies today, applications in AR, VR, and other scenarios have garnered extensive attention. Unlike traditional mono sound, spatial audio offers a more realistic and immersive auditory experience. Despite notable progress in the field, there remains a lack of comprehensive surveys that systematically organize and analyze these methods and their underlying technologies. In this paper, we provide a comprehensive overview of spatial audio and systematically review recent literature in the area. To address this, we chronologically outlining existing work related to spatial audio and categorize these studies based on input-output representations, as well as generation and understanding tasks, thereby summarizing various research aspects of spatial audio. In addition, we review related datasets, evaluation metrics, and benchmarks, offering insights from both training and evaluation perspectives. Related materials are available at this https URL.

Paper number 77:
Title: FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with advanced prosodic modeling based on Mixture of Experts
Authors: Qingliang Meng, Yuqing Deng, Wei Liang, Limei Yu, Huizhi Liang, Tian Li
Abstract: Achieving natural and human-like speech synthesis with low inference costs remains a major challenge in speech synthesis research. This study focuses on human prosodic patterns and synthesized spectrum harmony, addressing the challenges of prosody modeling and artifact issues in non-autoregressive models. To enhance prosody modeling and synthesis quality, we introduce a new Duration Predictor based on the Mixture of Experts alongside a new Vocoder with two advanced multi-scale discriminators. We integrated the these new modules into the VITS system, forming our FNH-TTS system. Our experiments on LJSpeech, VCTK, and LibriTTS demonstrate the system's superiority in synthesis quality, phoneme duration prediction, Vocoder results, and synthesis speed. Our prosody visualization results show that FNH-TTS produces duration predictions that more closely align with natural human beings than other systems.

Paper number 78:
Title: A Novel Vascular Risk Scoring Framework for Quantifying Sex-Specific Cerebral Perfusion from 3D pCASL MRI
Authors: Sneha Noble, Neelam Sinha, Vaanathi Sundareshan, Thomas Gregor Issac
Abstract: We present a novel framework that leverages 3D pseudo-continuous arterial spin labeling (pCASL) MRI to investigate sex- and age-dependent heterogeneity in cerebral perfusion and to establish a biologically informed vascular risk quantification metric. A custom convolutional neural network was trained on ASL-derived cerebral blood flow (CBF) maps from 186 cognitively healthy individuals (89 males and 97 females, ages 8-92 years), achieving 95% accuracy in sex classification and revealing robust sex-specific perfusion signatures. Regional analyses identified significantly elevated CBF in females across medial Brodmann areas 6 and 10, the visual area of the cortex, the polar occipital cortex, and both ventral and dorsal dysgranular insula, highlighting sex-specific neurovascular specialization in motor, cognitive, sensory, and affective domains. In addition, we observed a consistent global age-related decline in CBF across both sexes, reflecting progressive cerebrovascular aging. To integrate these findings, we propose a biologically informed Vascular Risk Score (VRS) derived from age- and sex-stratified normative CBF distributions. The VRS enables individualized assessment of cerebral perfusion integrity by quantifying deviations from expected normative patterns. This metric offers a sensitive, personalized biomarker for detecting early hypoperfusion and stratifying vascular contributions to neurodegenerative diseases, including Alzheimer's disease, thereby advancing the goals of precision neurology.

Paper number 79:
Title: Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction
Authors: Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan
Abstract: Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent \textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging.

Paper number 80:
Title: Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users
Authors: Ana Lucic, Sheeraz Ahmad, Amanda Furtado Brinhosa, Vera Liao, Himani Agrawal, Umang Bhatt, Krishnaram Kenthapadi, Alice Xiang, Maarten de Rijke, Nicholas Drabowski
Abstract: When using medical images for diagnosis, either by clinicians or artificial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only flagged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially difficult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for flagging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at OurCompany, and, (iii) a longitudinal user study design to examine the effect of including explanations on the workflow of the technicians in our clinics. To the best of our knowledge, this would be the first longitudinal study on evaluating the effects of XAI methods on end-users -- stakeholders that use AI systems but do not have AI-specific expertise. We welcome feedback and suggestions on our experimental setup.

Paper number 81:
Title: Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces
Authors: Thien Duc Hua, Mohammadali Mohammadi, Hien Quoc Ngo, Michail Matthaiou
Abstract: This paper investigates the integration of beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) into cell-free massive multiple-input multiple-output (CF-mMIMO) systems, focusing on applications involving simultaneous wireless information and power transfer (SWIPT). The system supports concurrently two user groups: information users (IUs) and energy users (EUs). A BD-RIS is employed to enhance the wireless power transfer (WPT) directed towards the EUs. To comprehensively evaluate the system's performance, we present an analytical framework for the spectral efficiency (SE) of IUs and the average harvested energy (HE) of EUs in the presence of spatial correlation among the BD-RIS elements and for a non-linear energy harvesting circuit. Our findings offer important insights into the transformative potential of BD-RIS, setting the stage for the development of more efficient and effective SWIPT networks. Finally, incorporating a heuristic scattering matrix design at the BD-RIS results in a substantial improvement compared to the scenario with random scattering matrix

Paper number 82:
Title: Chain of Correction for Full-text Speech Recognition with Large Language Models
Authors: Zhiyuan Tang, Dong Wang, Zhikai Zhou, Yong Liu, Shen Huang, Shidong Shang
Abstract: Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) is attracting increased attention for its ability to address a wide range of error types, such as punctuation restoration and inverse text normalization, across long context. However, challenges remain regarding stability, controllability, completeness, and fluency. To mitigate these issues, this paper proposes the Chain of Correction (CoC), which uses a multi-turn chat format to correct errors segment by segment, guided by pre-recognized text and full-text context for better semantic understanding. Utilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to evaluate CoC's performance. Experiments show that CoC significantly outperforms baseline and benchmark systems in correcting full-text ASR outputs. We also analyze correction thresholds to balance under-correction and over-rephrasing, extrapolate CoC on extra-long ASR outputs, and explore using other types of information to guide error correction.

Paper number 83:
Title: FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation
Authors: Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-Ü-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.

Paper number 84:
Title: Customizing Speech Recognition Model with Large Language Model Feedback
Authors: Shaoshi Ling, Guoli Ye
Abstract: Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.

Paper number 85:
Title: Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments
Authors: Jelmer van der Saag, Elia Trevisan, Wouter Falkena, Javier Alonso-Mora
Abstract: Unmanned Surface Vessels (USVs) face significant control challenges due to uncertain environmental disturbances like waves and currents. This paper proposes a trajectory tracking controller based on Active Disturbance Rejection Control (ADRC) implemented on the DUS V2500. A custom simulation incorporating realistic waves and current disturbances is developed to validate the controller's performance, supported by further validation through field tests in the harbour of Scheveningen, the Netherlands, and at sea. Simulation results demonstrate that ADRC significantly reduces cross-track error across all tested conditions compared to a baseline PID controller but increases control effort and energy consumption. Field trials confirm these findings while revealing a further increase in energy consumption during sea trials compared to the baseline.

Paper number 86:
Title: Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress
Authors: Zain ul Abdeen, Ming Jin
Abstract: This paper explores Reinforcement learning (RL) policy robustness by systematically analyzing network parameters under internal and external stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering introduces internal stress by selectively perturbing parameters, while adversarial attacks apply external stress through modified agent observations. This dual approach enables the classification of parameters as fragile, robust, or antifragile, based on their influence on policy performance in clean and adversarial settings. Parameter scores are defined to quantify these characteristics, and the framework is validated on PPO-trained agents in Mujoco continuous control environments. The results highlight the presence of antifragile parameters that enhance policy performance under stress, demonstrating the potential of targeted filtering techniques to improve RL policy adaptability. These insights provide a foundation for future advancements in the design of robust and antifragile RL systems.

Paper number 87:
Title: TolerantECG: A Foundation Model for Imperfect Electrocardiogram
Authors: Huynh Dang Nguyen, Trong-Thang Pham, Ngan Le, Van Nguyen
Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.

Paper number 88:
Title: The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents
Authors: Lixu Wang, Kaixiang Yao, Xinfeng Li, Dong Yang, Haoyang Li, Xiaofeng Wang, Wei Dong
Abstract: Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.

Paper number 89:
Title: When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs
Authors: Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin
Abstract: As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.

Paper number 90:
Title: Binary Decision Process in Pre-Evacuation Behavior
Authors: Peng Wang, Peter Luh, Xuesong Lu
Abstract: In crowd evacuation the time interval before decisive movement towards a safe place is defined as the pre-evacuation phase, and it has crucial impact on the total time required for safe egress. This process mainly refers to situation awareness and response to an external stressors, e.g., fire alarm. Due to the complexity of human cognitive process, simulation is used to study this important time interval. In this paper a binary decision process is formulated to simulate pre-evacuation time of many evacuees in a given social context. The model combines classic opinion dynamics with binary phase transition to describe how group pre-evacuation time emerges from individual interaction. The model parameters are quantitatively meaningful to human factors research within socio-psychological background, e.g., whether an individual is stubborn or open-minded, or what kind of the social topology exists among the individuals and how it matters in aggregating individuals into social groups. The modeling framework also describes collective motion of many evacuees in a planar space, and the resulting multi-agent system is partly similar to Vicsek model, and it is meaningful to explore complex crowd behavior in social context.

Paper number 91:
Title: Is Transfer Learning Necessary for Violin Transcription?
Authors: Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K.M. Cheung
Abstract: Automatic music transcription (AMT) has achieved remarkable progress for instruments such as the piano, largely due to the availability of large-scale, high-quality datasets. In contrast, violin AMT remains underexplored due to limited annotated data. A common approach is to fine-tune pretrained models for other downstream tasks, but the effectiveness of such transfer remains unclear in the presence of timbral and articulatory differences. In this work, we investigate whether training from scratch on a medium-scale violin dataset can match the performance of fine-tuned piano-pretrained models. We adopt a piano transcription architecture without modification and train it on the MOSA dataset, which contains about 30 hours of aligned violin recordings. Our experiments on URMP and Bach10 show that models trained from scratch achieved competitive or even superior performance compared to fine-tuned counterparts. These findings suggest that strong violin AMT is possible without relying on pretrained piano representations, highlighting the importance of instrument-specific data collection and augmentation strategies.

Paper number 92:
Title: Amplitude maximization in stable systems, Schur positivity, and some conjectures on polynomial interpolation
Authors: Dmitrii M. Ostrovskii, Pavel S. Shcherbakov
Abstract: For $r > 0$ and integers $t \ge n > 0$, we consider the following problem: maximize the amplitude $|x_t|$ at time $t$, over all complex solutions $x = (x_0, x_1, \dots)$ of arbitrary homogeneous linear difference equations of order $n$ with the characteristic roots in the disc $\{z \in \mathbb{C}: |z| \le r\}$, and with initial values $x_0, \dots, x_{n-1}$ in the unit disc. We find that for any triple $t,n,r$, the maximum is attained with coinciding roots on the boundary circle; in particular, this implies that the peak amplitude $\sup_{t \ge n} |x_t|$ can be maximized explicitly, by studying a unique equation with the characteristic polynomial $(z-r)^n$. Moreover, the optimality of the cophase root configuration holds for origin-centered polydiscs. To prove this result, we first reduce the problem to a certain interpolation problem over monomials, then solve the latter by leveraging the theory of symmetric functions and identifying the associated Schur positivity structure. We also discuss the implications for more general Reinhardt domains. Finally, we study the problem of estimating the derivatives of a real entire function from its values at $n/2$ pairs of complex conjugate points in the unit disc. We propose conjectures on the extremality of the monomial $z^n$, and restate them in terms of Schur polynomials.
    