
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Sex-Specific Vascular Score: A Novel Perfusion Biomarker from Supervoxel Analysis of 3D pCASL MRI
Authors: Sneha Noble, Neelam Sinha, Vaanathi Sundareshan, Thomas Gregor Issac
Abstract: We propose a novel framework that leverages 3D pseudo-continuous arterial spin labeling (3D pCASL) MRI to compute sex-specific vascular scores that quantify cerebrovascular health and potential disease susceptibility. The brain is parcellated into spatially contiguous regions of homogeneous perfusion using supervoxel clustering, capturing both microvascular and macrovascular contributions. Mean cerebral blood flow (CBF) values are extracted from 186 cognitively healthy participants and used to train a custom convolutional neural network, achieving 95 percent accuracy in sex classification. This highlights robust, sex-specific perfusion patterns across the brain. Additionally, regional CBF variations and age-related effects are systematically evaluated within male and female cohorts. The proposed vascular risk-scoring framework enhances understanding of normative brain perfusion and aging, and may facilitate early detection and personalized interventions for neurodegenerative diseases such as Alzheimer's.

Paper number 2:
Title: Colon Polyps Detection from Colonoscopy Images Using Deep Learning
Authors: Md Al Amin, Bikash Kumar Paul
Abstract: Colon polyps are precursors to colorectal cancer, a leading cause of cancer-related mortality worldwide. Early detection is critical for improving patient outcomes. This study investigates the application of deep learning-based object detection for early polyp identification using colonoscopy images. We utilize the Kvasir-SEG dataset, applying extensive data augmentation and splitting the data into training (80\%), validation (20\% of training), and testing (20\%) sets. Three variants of the YOLOv5 architecture (YOLOv5s, YOLOv5m, YOLOv5l) are evaluated. Experimental results show that YOLOv5l outperforms the other variants, achieving a mean average precision (mAP) of 85.1\%, with the highest average Intersection over Union (IoU) of 0.86. These findings demonstrate that YOLOv5l provides superior detection performance for colon polyp localization, offering a promising tool for enhancing colorectal cancer screening accuracy.

Paper number 3:
Title: Benchmarking GPT-5 for Zero-Shot Multimodal Medical Reasoning in Radiology and Radiation Oncology
Authors: Mingzhe Hu, Zach Eidex, Shansong Wang, Mojtaba Safari, Qiang Li, Xiaofeng Yang
Abstract: Radiology, radiation oncology, and medical physics require decision-making that integrates medical images, textual reports, and quantitative data under high-stakes conditions. With the introduction of GPT-5, it is critical to assess whether recent advances in large multimodal models translate into measurable gains in these safety-critical domains. We present a targeted zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano) against GPT-4o across three representative tasks. We present a targeted zero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano) against GPT-4o across three representative tasks: (1) VQA-RAD, a benchmark for visual question answering in radiology; (2) SLAKE, a semantically annotated, multilingual VQA dataset testing cross-modal grounding; and (3) a curated Medical Physics Board Examination-style dataset of 150 multiple-choice questions spanning treatment planning, dosimetry, imaging, and quality assurance. Across all datasets, GPT-5 achieved the highest accuracy, with substantial gains over GPT-4o up to +20.00% in challenging anatomical regions such as the chest-mediastinal, +13.60% in lung-focused questions, and +11.44% in brain-tissue interpretation. On the board-style physics questions, GPT-5 attained 90.7% accuracy (136/150), exceeding the estimated human passing threshold, while GPT-4o trailed at 78.0%. These results demonstrate that GPT-5 delivers consistent and often pronounced performance improvements over GPT-4o in both image-grounded reasoning and domain-specific numerical problem-solving, highlighting its potential to augment expert workflows in medical imaging and therapeutic physics.

Paper number 4:
Title: PediDemi -- A Pediatric Demyelinating Lesion Segmentation Dataset
Authors: Maria Popa, Gabriela Adriana Visa
Abstract: Demyelinating disorders of the central nervous system may have multiple causes, the most common are infections, autoimmune responses, genetic or vascular etiology. Demyelination lesions are characterized by areas were the myelin sheath of the nerve fibers are broken or destroyed. Among autoimmune disorders, Multiple Sclerosis (MS) is the most well-known Among these disorders, Multiple Sclerosis (MS) is the most well-known and aggressive form. Acute Disseminated Encephalomyelitis (ADEM) is another type of demyelinating disease, typically with a better prognosis. Magnetic Resonance Imaging (MRI) is widely used for diagnosing and monitoring disease progression by detecting lesions. While both adults and children can be affected, there is a significant lack of publicly available datasets for pediatric cases and demyelinating disorders beyond MS. This study introduces, for the first time, a publicly available pediatric dataset for demyelinating lesion segmentation. The dataset comprises MRI scans from 13 pediatric patients diagnosed with demyelinating disorders, including 3 with ADEM. In addition to lesion segmentation masks, the dataset includes extensive patient metadata, such as diagnosis, treatment, personal medical background, and laboratory results. To assess the quality of the dataset and demonstrate its relevance, we evaluate a state-of-the-art lesion segmentation model trained on an existing MS dataset. The results underscore the importance of diverse datasets

Paper number 5:
Title: Automated Cervical Cancer Detection through Visual Inspection with Acetic Acid in Resource-Poor Settings with Lightweight Deep Learning Models Deployed on an Android Device
Authors: Leander Melroy Maben, Keerthana Prasad, Shyamala Guruvare, Vidya Kudva, P C Siddalingaswamy
Abstract: Cervical cancer is among the most commonly occurring cancer among women and claims a huge number of lives in low and middle-income countries despite being relatively easy to treat. Several studies have shown that public screening programs can bring down cervical cancer incidence and mortality rates significantly. While several screening tests are available, visual inspection with acetic acid (VIA) presents itself as the most viable option for low-resource settings due to the affordability and simplicity of performing the test. VIA requires a trained medical professional to interpret the test and is subjective in nature. Automating VIA using AI eliminates subjectivity and would allow shifting of the task to less trained health workers. Task shifting with AI would help further expedite screening programs in low-resource settings. In our work, we propose a lightweight deep learning algorithm that includes EfficientDet-Lite3 as the Region of Interest (ROI) detector and a MobileNet- V2 based model for classification. These models would be deployed on an android-based device that can operate remotely and provide almost instant results without the requirement of highly-trained medical professionals, labs, sophisticated infrastructure, or internet connectivity. The classification model gives an accuracy of 92.31%, a sensitivity of 98.24%, and a specificity of 88.37% on the test dataset and presents itself as a promising automated low-resource screening approach.

Paper number 6:
Title: InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting
Authors: Shuxin Liang, Yihan Xiao, Wenlu Tang
Abstract: 3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: this https URL.

Paper number 7:
Title: Stochastic Black Start Resource Allocation to Enable Dynamic Formation of Networked Microgrids and DER-aided Restoration
Authors: Cong Bai, Salish Maharjan, Han Wang, Zhaoyu Wang
Abstract: Extended outages in distributed systems (DSs) dominated by distributed energy resources (DERs) require innovative strategies to efficiently and securely deploy black start (BS) resources. To address the need, this paper proposes a two-stage stochastic resource allocation method within synchronizing dynamic microgrids (MGs) for black start (SDMG-BS), enabling risk-averse and adaptive restoration across various scenarios while ensuring frequency security. Virtual synchronous generator (VSG)-controlled grid-forming inverters (GFMIs) equipped with primary frequency governors (PFGs) are modeled as BS resources. Their frequency response is characterized by three transient indices, which are deployed as frequency dynamic constraints on load pick-up events to ensure frequency stability during the BS process. SDMG-BS framework facilitates location-independent synchronization among restored MGs and with the transmission grid (TG) with the help of smart switches (SSWs). The model incorporates scenario-based stochastic programming to address multi-source uncertainties, including season-dependent operational conditions and unpredictable TG outage durations, ensuring a resilient allocation plan. The proposed approach is validated on a modified IEEE 123-node feeder with three study cases designed across sixteen uncertainty scenarios.

Paper number 8:
Title: Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of Synthesized Speech Under Distribution Shifts
Authors: Ashi Garg, Zexin Cai, Henry Li Xinyuan, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews
Abstract: We address the challenge of detecting synthesized speech under distribution shifts -- arising from unseen synthesis methods, speakers, languages, or audio conditions -- relative to the training data. Few-shot learning methods are a promising way to tackle distribution shifts by rapidly adapting on the basis of a few in-distribution samples. We propose a self-attentive prototypical network to enable more robust few-shot adaptation. To evaluate our approach, we systematically compare the performance of traditional zero-shot detectors and the proposed few-shot detectors, carefully controlling training conditions to introduce distribution shifts at evaluation time. In conditions where distribution shifts hamper the zero-shot performance, our proposed few-shot adaptation technique can quickly adapt using as few as 10 in-distribution samples -- achieving upto 32% relative EER reduction on deepfakes in Japanese language and 20% relative reduction on ASVspoof 2021 Deepfake dataset.

Paper number 9:
Title: Susceptibility Distortion Correction of Diffusion MRI with a single Phase-Encoding Direction
Authors: Sedigheh Dargahi, Sylvain Bouix, Christian Desrosier
Abstract: Diffusion MRI (dMRI) is a valuable tool to map brain microstructure and connectivity by analyzing water molecule diffusion in tissue. However, acquiring dMRI data requires to capture multiple 3D brain volumes in a short time, often leading to trade-offs in image quality. One challenging artifact is susceptibility-induced distortion, which introduces significant geometric and intensity deformations. Traditional correction methods, such as topup, rely on having access to blip-up and blip-down image pairs, limiting their applicability to retrospective data acquired with a single phase encoding direction. In this work, we propose a deep learning-based approach to correct susceptibility distortions using only a single acquisition (either blip-up or blip-down), eliminating the need for paired acquisitions. Experimental results show that our method achieves performance comparable to topup, demonstrating its potential as an efficient and practical alternative for susceptibility distortion correction in dMRI.

Paper number 10:
Title: Low-Cost Sensing and Classification for Early Stress and Disease Detection in Avocado Plants
Authors: Abdulrahman Bukhari, Bullo Mamo, Mst Shamima Hossain, Ziliang Zhang, Mohsen Karimi, Daniel Enright, Patricia Manosalva, Hyoseung Kim
Abstract: With rising demands for efficient disease and salinity management in agriculture, early detection of plant stressors is crucial, particularly for high-value crops like avocados. This paper presents a comprehensive evaluation of low-cost sensors deployed in the field for early stress and disease detection in avocado plants. Our monitoring system was deployed across 72 plants divided into four treatment categories within a greenhouse environment, with data collected over six months. While leaf temperature and conductivity measurements, widely used metrics for controlled settings, were found unreliable in field conditions due to environmental interference and positioning challenges, leaf spectral measurements produced statistically significant results when combined with our machine learning approach. For soil data analysis, we developed a two-level hierarchical classifier that leverages domain knowledge about treatment characteristics, achieving 75-86\% accuracy across different avocado genotypes and outperforming conventional machine learning approaches by over 20\%. In addition, performance evaluation on an embedded edge device demonstrated the viability of our approach for resource-constrained environments, with reasonable computational efficiency while maintaining high classification accuracy. Our work bridges the gap between theoretical potential and practical application of low-cost sensors in agriculture and offers insights for developing affordable, scalable monitoring systems.

Paper number 11:
Title: System-Level Performance and Communication Tradeoff in Networked Control with Predictions
Authors: Yifei Wu, Jing Yu, Tongxin Li
Abstract: Distributed control of large-scale systems is challenging due to the need for scalable and localized communication and computation. In this work, we introduce a Predictive System-Level Synthesis PredSLS framework that designs controllers by jointly integrating communication constraints and local disturbance predictions into an affine feedback structure. Rather than focusing on the worst-case uncertainty, PredSLS leverages both current state feedback and future system disturbance predictions to achieve distributed control of networked systems. In particular, PredSLS enables a unified system synthesis of the optimal $\kappa$-localized controller, therefore outperforms approaches with post hoc communication truncation, as was commonly seen in the literature. The PredSLS framework can be naturally decomposed into spatial and temporal components for efficient and parallelizable computation across the network, yielding a regret upper bound that explicitly depends on the prediction error and communication range. Our regret analysis not only reveals a non-monotonic trade-off between control performance and communication range when prediction errors are present, but also guides the identification of an optimal size for local communication neighborhoods, thereby enabling the co-design of controller and its underlying communication topology.

Paper number 12:
Title: Towards Understanding and Harnessing the Transferability of Prognostic Knowledge in Computational Pathology
Authors: Pei Liu, Luping Ji, Jiaxiang Gou, Xiangxiang Zeng
Abstract: Whole-Slide Image (WSI) is an important tool for evaluating the prognosis of cancer patients. Present WSI-based prognosis studies generally follow a conventional paradigm -- cancer-specific model development -- where one cancer disease corresponds to one model and this model cannot make use of the prognostic knowledge from others. Despite its notable success in recent years, this paradigm has inherent limitations and has always been struggling with practical requirements: (i) scaling to the rare tumor diseases with very limited samples and (ii) benefiting from the generalizable prognostic knowledge in other cancers. To this end, this paper presents the first systematic study on Prognostic Knowledge Transfer in Pathology, called Path-PKT. It comprises three main parts. (1) We curate a large dataset (UNI2-h-DSS) with 13 cancers and use it to evaluate the transferability of prognostic knowledge between different cancers computationally. (2) We design experiments to understand what factors affect knowledge transfer and what causes positive transfers. (3) Motivated by empirical findings, we propose a new baseline approach (MoE-PKT) with a routing mechanism to utilize the generalizable prognostic knowledge in other cancers. Finally, we show the transferability of source models to rare tumor diseases. This study could lay solid foundations for the study of knowledge transfer in WSI-based cancer prognosis. Source code is available at this https URL.

Paper number 13:
Title: End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments
Authors: Meng-Ping Lin, Enoch Hsin-Ho Huang, Shao-Yi Chien, Yu Tsao
Abstract: The cochlear implant (CI) is a remarkable biomedical device that successfully enables individuals with severe-to-profound hearing loss to perceive sound by converting speech into electrical stimulation signals. Despite advancements in the performance of recent CI systems, speech comprehension in noisy or reverberant conditions remains a challenge. Recent and ongoing developments in deep learning reveal promising opportunities for enhancing CI sound coding capabilities, not only through replicating traditional signal processing methods with neural networks, but also through integrating visual cues as auxiliary data for multimodal speech processing. Therefore, this paper introduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an audio-visual speech enhancement (AVSE) model as a pre-processing module for the deep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically, a joint training approach is applied to model AVSE-ECS, an end-to-end CI system. Experimental results indicate that the proposed method outperforms the previous ECS strategy in noisy conditions, with improved objective speech intelligibility scores. The methods and findings in this study demonstrate the feasibility and potential of using deep learning to integrate the AVSE module into an end-to-end CI system

Paper number 14:
Title: Power-Series Approach to Moment-Matching-Based Model Reduction of MIMO Polynomial Nonlinear Systems
Authors: Chao Huang, Alessandro Astolfi
Abstract: The model reduction problem for high-order multi-input, multi-output (MIMO) polynomial nonlinear systems based on moment matching is addressed. The technique of power-series decomposition is exploited: this decomposes the solution of the nonlinear PDE characterizing the center manifold into the solutions of a series of recursively defined Sylvester equations. This approach allows yielding nonlinear reduced-order models in very much the same way as in the linear case (e.g. analytically). Algorithms are proposed for obtaining the order and the parameters of the reduced-order models with precision of degree $\kappa$. The approach also provides new insights into the nonlinear moment matching problem: first, a lower bound for the order of the reduced-order model is obtained, which, in the MIMO case, can be strictly less than the number of matched moments; second, it is revealed that the lower bound is affected by the ratio of the number of the input and output channels; third, it is shown that under mild conditions, a nonlinear reduced-order model can always be constructed with either a linear state equation or a linear output equation.

Paper number 15:
Title: Scalable Sensor Placement for Cyclic Networks with Observability Guarantees: Application to Water Distribution Networks
Authors: J.J.H. van Gemert, V. Breschi, D.R. Yntema, K.J. Keesman, M. Lazar
Abstract: Optimal sensor placement is essential for state estimation and effective network monitoring. As known in the literature, this problem becomes particularly challenging in large-scale undirected or bidirected cyclic networks with parametric uncertainties, such as water distribution networks (WDNs), where pipe resistance and demand patterns are often unknown. Motivated by the challenges of cycles, parametric uncertainties, and scalability, this paper proposes a sensor placement algorithm that guarantees structural observability for cyclic and acyclic networks with parametric uncertainties. By leveraging a graph-based strategy, the proposed method efficiently addresses the computational complexities of large-scale networks. To demonstrate the algorithm's effectiveness, we apply it to several EPANET benchmark WDNs. Most notably, the developed algorithm solves the sensor placement problem with guaranteed structured observability for the L-town WDN with 1694 nodes and 124 cycles in under 0.1 seconds.

Paper number 16:
Title: Towards safe control parameter tuning in distributed multi-agent systems
Authors: Abdullah Tokmak, Thomas B. Schön, Dominik Baumann
Abstract: Many safety-critical real-world problems, such as autonomous driving and collaborative robots, are of a distributed multi-agent nature. To optimize the performance of these systems while ensuring safety, we can cast them as distributed optimization problems, where each agent aims to optimize their parameters to maximize a coupled reward function subject to coupled constraints. Prior work either studies a centralized setting, does not consider safety, or struggles with sample efficiency. Since we require sample efficiency and work with unknown and nonconvex rewards and constraints, we solve this optimization problem using safe Bayesian optimization with Gaussian process regression. Moreover, we consider nearest-neighbor communication between the agents. To capture the behavior of non-neighboring agents, we reformulate the static global optimization problem as a time-varying local optimization problem for each agent, essentially introducing time as a latent variable. To this end, we propose a custom spatio-temporal kernel to integrate prior knowledge. We show the successful deployment of our algorithm in simulations.

Paper number 17:
Title: State of Abdominal CT Datasets: A Critical Review of Bias, Clinical Relevance, and Real-world Applicability
Authors: Saeide Danaei, Zahra Dehghanian, Elahe Meftah, Nariman Naderi, Seyed Amir Ahmad Safavi-Naini, Faeze Khorasanizade, Hamid R. Rabiee
Abstract: This systematic review critically evaluates publicly available abdominal CT datasets and their suitability for artificial intelligence (AI) applications in clinical settings. We examined 46 publicly available abdominal CT datasets (50,256 studies). Across all 46 datasets, we found substantial redundancy (59.1\% case reuse) and a Western/geographic skew (75.3\% from North America and Europe). A bias assessment was performed on the 19 datasets with >=100 cases; within this subset, the most prevalent high-risk categories were domain shift (63\%) and selection bias (57\%), both of which may undermine model generalizability across diverse healthcare environments -- particularly in resource-limited settings. To address these challenges, we propose targeted strategies for dataset improvement, including multi-institutional collaboration, adoption of standardized protocols, and deliberate inclusion of diverse patient populations and imaging technologies. These efforts are crucial in supporting the development of more equitable and clinically robust AI models for abdominal imaging.

Paper number 18:
Title: Transient Stability Analysis for Grid Following Converters in Low-Inertia Power Systems by Direct Method
Authors: Fangyuan Sun, Ruisheng Diao, Ruiyuan Zeng, Zhanning Liu, Baorong Zhou, Junjie Li, Wangqianyun Tang
Abstract: With the increased penetration of renewable energy and reduced proportion of synchronous generators, the low-inertia characteristics of todays power system become prominent, and the transient stability issue of grid following converter (GFLC) under low inertia system (LIS) condition becomes critical. There are two prominent problems in the transient stability analysis of GFLC-LIS. The angular dynamic of LIS increases the complexity of transient stability analysis, and the nonlinear, possibly negative damping of GFLC makes it difficult to guarantee the conservative of the traditional methods. These problems make the traditional methods inapplicable. In this paper, the transient stability analysis of GFLC LIS is investigated to provide an accurate estimation of the attraction boundary and critical clearance time (CCT). Firstly, a dynamic model of GFLC-LIS is constructed, considering the phase-locked loop (PLL)-based GFLC dynamics and swing equation-based LIS dynamics. The frequency mutation of PLL at fault occurrence and clearing time is also considered. Secondly, a Zubov based transient stability analysis method is proposed, which can construct the energy function in a way that is different from the traditional conservation of energy perspective and can address the negative damping issue. Moreover, the accuracy of the CCT estimation is analyzed, and the influences of LIS parameters on transient stability are illustrated. Finally, simulation experiments are carried out to verify the effectiveness of the proposed method

Paper number 19:
Title: Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations
Authors: Jan Krejčí, Oliver Kost, Yuxuan Xia, Lennart Svensson, Ondřej Straka
Abstract: This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.

Paper number 20:
Title: AutoMPC: A Code Generator for MPC-based Automated Driving
Authors: Georg Schildbach, Jasper Pflughaupt
Abstract: Model Predictive Control (MPC) is a powerful technique to control nonlinear, multi-input multi-output systems subject to input and state constraints. It is now a standard tool for trajectory tracking control of automated vehicles. As such it has been used in many research and development projects. However, MPC faces several challenges to be integrated into industrial production vehicles. The most important ones are its high computational demands and the complexity of implementation. The software packages AutoMPC aims to address both of these challenges. It builds on a robustified version of an active set algorithm for Nonlinear MPC. The algorithm is embedded into a framework for vehicle trajectory tracking, which makes it easy to used, yet highly customizable. Automatic code generation transforms the selections into a standalone, computationally efficient C-code file with static memory allocation. As such it can be readily deployed on a wide range of embedded platforms, e.g., based on Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version of the code, the vehicle model and the numerical integration method can be manually specified, besides basic algorithm parameters. All of this information and all specifications are directly baked into the generated C-code. The algorithm is suitable driving scenarios at low or high speeds, even drifting, and supports direction changes. Multiple simulation scenarios show the versatility and effectiveness of the AutoMPC code, with the guarantee of a feasible solution, a high degree of robustness, and computational efficiency.

Paper number 21:
Title: CKM-Assisted Physical-Layer Security for Resilience Against Unknown Eavesdropping Location
Authors: Ladan Khaloopour, Matthias Hollick, Vahid Jamali
Abstract: Channel Knowledge Map (CKM) is an emerging data-driven toolbox that captures our awareness of the wireless channel and enables efficient communication and resource allocation beyond the state of the art. In this work, we consider CKM for improving physical-layer security (PLS) in the presence of a passive eavesdropper (Eve), without making any assumptions about Eve's location or channel state information (CSI). We employ highly directional mmWave transmissions, with the confidential message jointly encoded across multiple beams. By exploiting CKM, we derive an algorithm for time and power allocation among the beams that maximizes the absolute secrecy rate under the worst-case scenario for Eve's location.

Paper number 22:
Title: subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery
Authors: Jacob Hanimann, Daniel Siegismund, Mario Wieser, Stephan Steigele
Abstract: High-throughput screening using automated microscopes is a key driver in biopharma drug discovery, enabling the parallel evaluation of thousands of drug candidates for diseases such as cancer. Traditional image analysis and deep learning approaches have been employed to analyze these complex, large-scale datasets, with cell segmentation serving as a critical step for extracting relevant structures. However, both strategies typically require extensive manual parameter tuning or domain-specific model fine-tuning. We present a novel method that applies a segmentation foundation model in a zero-shot setting (i.e., without fine-tuning), guided by an in-context learning strategy. Our approach employs a three-step process for nuclei, cell, and subcellular segmentation, introducing a self-prompting mechanism that encodes morphological and topological priors using growing masks and strategically placed foreground/background points. We validate our method on both standard cell segmentation benchmarks and industry-relevant hit validation assays, demonstrating that it accurately segments biologically relevant structures without the need for dataset-specific tuning.

Paper number 23:
Title: Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms
Authors: Nizheen A. Ali, Ramadhan J. Mstafa
Abstract: With the widespread use of the internet, there is an increasing need to ensure the security and privacy of transmitted data. This has led to an intensified focus on the study of video steganography, which is a technique that hides data within a video cover to avoid detection. The effectiveness of any steganography method depends on its ability to embed data without altering the original video quality while maintaining high efficiency. This paper proposes a new method to video steganography, which involves utilizing a Genetic Algorithm (GA) for identifying the Region of Interest (ROI) in the cover video. The ROI is the area in the video that is the most suitable for data embedding. The secret data is encrypted using the Advanced Encryption Standard (AES), which is a widely accepted encryption standard, before being embedded into the cover video, utilizing up to 10% of the cover video. This process ensures the security and confidentiality of the embedded data. The performance metrics for assessing the proposed method are the Peak Signal to Noise Ratio (PSNR) and the encoding and decoding time. The results show that the proposed method has a high embedding capacity and efficiency, with a PSNR ranging between 64 and 75 dBs, which indicates that the embedded data is almost indistinguishable from the original video. Additionally, the method can encode and decode data quickly, making it efficient for real time applications.

Paper number 24:
Title: Airy beams for near-field communications: Fundamentals, potentials, and limitations
Authors: Donatella Darsena, Francesco Verde, Marco Di Renzo, Vincenzo Galdi
Abstract: In next-generation wireless networks, the combination of electrically large radiating apertures and high-frequency transmission extends the radiating near-field region around the transmitter. In this region, unlike in the far field, the wavefront is nonplanar, which provides additional degrees of freedom to shape and steer the transmitted beam in a desired manner. In this paper, we focus on Airy beams, which may exhibit several highly desirable properties in the near-field region. Ideally, these beams follow self-accelerating (curved) trajectories, demonstrate resilience to perturbations through self-healing, and maintain a consistent intensity profile across all planes perpendicular to the propagation direction, making them effectively diffraction-free. Specifically, we first present the underlying principles of self-accelerating beams radiated by continuous aperture field distributions. We then address several challenges regarding the generation of Airy beams, including their exponential decay due to finite energy constraints and spatial truncation of the aperture. Moreover, we examine their free-space propagation characteristics. The second part of the paper focuses on the propagation behavior of Airy beams in non-line-of-sight (NLoS) scenarios. A comparison is also presented between Airy beams and Gaussian beams. Our theoretical and numerical results show that Airy beams may offer a performance advantage over Gaussian beams in certain NLoS channels, provided that their key properties are largely preserved, specifically, self-acceleration along a parabolic trajectory and diffraction-free propagation. In the presence of an obstacle, this requires that the portion of the transmit aperture with a clear line-of-sight to the receiver is sufficiently large.

Paper number 25:
Title: Singularity-free prescribed performance guaranteed control for perturbed system
Authors: Yiwei Liu
Abstract: This paper addresses the prescribed performance control (PPC) challenge for high-order nonlinear systems affected by mismatched disturbances. The research aims to prevent singularity issues arising from error boundary violations during abrupt changes in reference trajectories. We introduce a novel transformation function with infinite-order differentiability at connection points, advancing beyond mere continuous differentiability. Utilizing this transformation function, we develop a comprehensive transformation strategy that ensures: (1) errors remain within prescribed boundaries when reference trajectories are smooth, and (2) errors return to prescribed boundaries within a specified timeframe following abrupt changes in reference trajectories. Additionally, the complexity explosion issue inherent in backstepping design is effectively resolved. Simulation results corroborate the validity of the proposed theoretical advancements.

Paper number 26:
Title: BioGAP-Ultra: A Modular Edge-AI Platform for Wearable Multimodal Biosignal Acquisition and Processing
Authors: Sebastian Frey, Giusy Spacone, Andrea Cossettini, Marco Guermandi, Philipp Schilk, Luca Benini, Victor Kartsch
Abstract: The growing demand for continuous physiological monitoring and human-machine interaction in real-world settings calls for wearable platforms that are flexible, low-power, and capable of on-device intelligence. This work presents BioGAP-Ultra, an advanced multimodal biosensing platform that supports synchronized acquisition of diverse electrophysiological and hemodynamic signals such as EEG, EMG, ECG, and PPG while enabling embedded AI processing at state-of-the-art energy efficiency. BioGAP-Ultra is a major extension of our previous design, BioGAP [1], aimed at meeting the rapidly growing requirements of wearable biosensing applications. It features (i) increased on-device storage (x2 SRAM, x4 FLASH), (ii) improved wireless connectivity (1.4 Mbit/s bandwidth, x4 higher than BioGAP), (iii) enhanced number of signal modalities (from 3 to 5) and analog input channels (x2). Further, it is complemented by a complete real-time visualization and analysis software suite, providing access to raw data and real-time configurability on a mobile phone. Electrical characterization and multiple case studies confirm the platform's robustness, configurability, and suitability for real-world multimodal biosignal acquisition and edge intelligence. Finally, we demonstrate the system's versatility through integration into various wearable form factors: an EEG-PPG headband consuming 32.8 mW, an EMG sleeve at 26.7 mW, and an ECG-PPG chest band requiring only 9.3 mW, tailored for diverse biosignal applications. All hardware and software design files are also released open-source with a permissive license.

Paper number 27:
Title: Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift Registration
Authors: Tiago Assis, Ines P. Machado, Benjamin Zwick, Nuno C. Garcia, Reuben Dorent
Abstract: Accurate compensation of brain shift is critical for maintaining the reliability of neuronavigation during neurosurgery. While keypoint-based registration methods offer robustness to large deformations and topological changes, they typically rely on simple geometric interpolators that ignore tissue biomechanics to create dense displacement fields. In this work, we propose a novel deep learning framework that estimates dense, physically plausible brain deformations from sparse matched keypoints. We first generate a large dataset of synthetic brain deformations using biomechanical simulations. Then, a residual 3D U-Net is trained to refine standard interpolation estimates into biomechanically guided deformations. Experiments on a large set of simulated displacement fields demonstrate that our method significantly outperforms classical interpolators, reducing by half the mean square error while introducing negligible computational overhead at inference time. Code available at: \href{this https URL}{this https URL}.

Paper number 28:
Title: Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free Massive MIMO
Authors: Mustafa S. Abbas, Zahra Mobini, Hien Quoc Ngo, Hyundong Shin, Michail Matthaiou
Abstract: Joint unicast and multicast transmissions are becoming increasingly important in practical wireless systems, such as Internet of Things networks. This paper investigates a cell-free massive multiple-input multiple-output system that simultaneously supports both transmission types, with multicast serving multiple groups. Exact closed-form expressions for the achievable downlink spectral efficiency (SE) of both unicast and multicast users are derived for zero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum SE (SSE) maximization problem is formulated to jointly optimize the access point (AP) selection and power allocation. The optimization framework accounts for practical constraints, including the maximum transmit power per AP, fronthaul capacity limitations between APs and the central processing unit, and quality-of-service requirements for all users. The resulting non-convex optimization problem is reformulated into a tractable structure, and an accelerated projected gradient (APG)-based algorithm is developed to efficiently obtain near-optimal solutions. As a performance benchmark, a successive convex approximation (SCA)-based algorithm is also implemented. Simulation results demonstrate that the proposed joint optimization approach significantly enhances the SSE across various system setups and precoding strategies. In particular, the APG-based algorithm achieves substantial complexity reduction while maintaining competitive performance, making it well-suited for large-scale practical deployments.

Paper number 29:
Title: Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images
Authors: Sebastian Ibarra, Javier del Riego, Alessandro Catanese, Julian Cuba, Julian Cardona, Nataly Leon, Jonathan Infante, Karim Lekadir, Oliver Diaz, Richard Osuala
Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at this https URL.

Paper number 30:
Title: Robust Optimization for Movable Antenna-aided Cell-Free ISAC with Time Synchronization Errors
Authors: Yue Xiu, Yang Zhao, Ran Yang, Wanting Lyu, Dusit Niyato, Dong In Kim, Guangyi Liu, Ning Wei
Abstract: The cell-free integrated sensing and communication (CF-ISAC) system, which effectively mitigates intra-cell interference and provides precise sensing accuracy, is a promising technology for future 6G networks. However, to fully capitalize on the potential of CF-ISAC, accurate time synchronization (TS) between access points (APs) is critical. Due to the limitations of current synchronization technologies, TS errors have become a significant challenge in the development of the CF-ISAC system. In this paper, we propose a novel CF-ISAC architecture based on movable antennas (MAs), which exploits spatial diversity to enhance communication rates, maintain sensing accuracy, and reduce the impact of TS errors. We formulate a worst-case sensing accuracy optimization problem for TS errors to address this challenge, deriving the worst-case Cramér-Rao lower bound (CRLB). Subsequently, we develop a joint optimization framework for AP beamforming and MA positions to satisfy communication rate constraints while improving sensing accuracy. A robust optimization framework is designed for the highly complex and non-convex problem. Specifically, we employ manifold optimization (MO) to solve the worst-case sensing accuracy optimization problem. Then, we propose an MA-enabled meta-reinforcement learning (MA-MetaRL) to design optimization variables while satisfying constraints on MA positions, communication rate, and transmit power, thereby improving sensing accuracy. The simulation results demonstrate that the proposed robust optimization algorithm significantly improves the accuracy of the detection and is strong against TS errors. Moreover, compared to conventional fixed position antenna (FPA) technologies, the proposed MA-aided CF-ISAC architecture achieves higher system capacity, thus validating its effectiveness.

Paper number 31:
Title: Direct vascular territory segmentation on cerebral digital subtraction angiography
Authors: P. Matthijs van der Sluijs, Lotte Strong, Frank G. te Nijenhuis, Sandra Cornelissen, Pieter Jan van Doormaal, Geert Lycklama a Nijeholt, Wim van Zwam, Ad van Es, Diederik Dippel, Aad van der Lugt, Danny Ruijters, Ruisheng Su, Theo van Walsum
Abstract: X-ray digital subtraction angiography (DSA) is frequently used when evaluating minimally invasive medical interventions. DSA predominantly visualizes vessels, and soft tissue anatomy is less visible or invisible in DSA. Visualization of cerebral anatomy could aid physicians during treatment. This study aimed to develop and evaluate a deep learning model to predict vascular territories that are not explicitly visible in DSA imaging acquired during ischemic stroke treatment. We trained an nnUNet model with manually segmented intracranial carotid artery and middle cerebral artery vessel territories on minimal intensity projection DSA acquired during ischemic stroke treatment. We compared the model to a traditional atlas registration model using the Dice similarity coefficient (DSC) and average surface distance (ASD). Additionally, we qualitatively assessed the success rate in both models using an external test. The segmentation model was trained on 1224 acquisitions from 361 patients with ischemic stroke. The segmentation model had a significantly higher DSC (0.96 vs 0.82, p<0.001) and lower ASD compared to the atlas model (13.8 vs 47.3, p<0.001). The success rate of the segmentation model (85%) was higher compared to the atlas registration model (66%) in the external test set. A deep learning method for the segmentation of vascular territories without explicit borders on cerebral DSA demonstrated superior accuracy and quality compared to the traditional atlas-based method. This approach has the potential to be applied to other anatomical structures for enhanced visualization during X-ray guided medical procedures. The code is publicly available at this https URL.

Paper number 32:
Title: Improving Deep Learning for Accelerated MRI With Data Filtering
Authors: Kang Lin, Anselm Krainovic, Kun Wang, Reinhard Heckel
Abstract: Deep neural networks achieve state-of-the-art results for accelerated MRI reconstruction. Most research on deep learning based imaging focuses on improving neural network architectures trained and evaluated on fixed and homogeneous training and evaluation data. In this work, we investigate data curation strategies for improving MRI reconstruction. We assemble a large dataset of raw k-space data from 18 public sources consisting of 1.1M images and construct a diverse evaluation set comprising 48 test sets, capturing variations in anatomy, contrast, number of coils, and other key factors. We propose and study different data filtering strategies to enhance performance of current state-of-the-art neural networks for accelerated MRI reconstruction. Our experiments show that filtering the training data leads to consistent, albeit modest, performance gains. These performance gains are robust across different training set sizes and accelerations, and we find that filtering is particularly beneficial when the proportion of in-distribution data in the unfiltered training set is low.

Paper number 33:
Title: Energy Management and Wake-up for IoT Networks Powered by Energy Harvesting
Authors: David Ernesto Ruiz-Guirola, Samuel Montejo-Sanchez, Israel Leyva-Mayorga, Zhu Han, Petar Popovski, Onel L. A. Lopez
Abstract: The rapid growth of the Internet of Things (IoT) presents sustainability challenges such as increased maintenance requirements and overall higher energy consumption. This motivates self-sustainable IoT ecosystems based on Energy Harvesting (EH). This paper treats IoT deployments in which IoT devices (IoTDs) rely solely on EH to sense and transmit information about events/alarms to a base station (BS). The objective is to effectively manage the duty cycling of the IoTDs to prolong battery life and maximize the relevant data sent to the BS. The BS can also wake up specific IoTDs if extra information about an event is needed upon initial detection. We propose a K-nearest neighbors (KNN)-based duty cycling management to optimize energy efficiency and detection accuracy by considering spatial correlations among IoTDs' activity and their EH process. We evaluate machine learning approaches, including reinforcement learning (RL) and decision transformers (DT), to maximize information captured from events while managing energy consumption. Significant improvements over the state-ofthe-art approaches are obtained in terms of energy saving by all three proposals, KNN, RL, and DT. Moreover, the RL-based solution approaches the performance of a genie-aided benchmark as the number of IoTDs increases.

Paper number 34:
Title: Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction
Authors: Niklas Bubeck, Suprosanna Shit, Chen Chen, Can Zhao, Pengfei Guo, Dong Yang, Georg Zitzlsberger, Daguang Xu, Bernhard Kainz, Daniel Rueckert, Jiazhen Pan
Abstract: Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent \textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging.

Paper number 35:
Title: Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems
Authors: Yue Xiu, Yang Zhao, Ran Yang, Zheng Dong, Wanting Lyu, Zeyuan Zhang, Dusit Niyato, Guangyi Liu, Ning Wei
Abstract: The cell-free integrated sensing and communication (CF-ISAC) architecture is a promising enabler for 6G, offering spectrum efficiency and ubiquitous coverage. However, real deployments suffer from hardware impairments, especially nonlinear distortion from power amplifiers (PAs), which degrades both communication and sensing. To address this, we propose a movable antenna (MA)-aided CF-ISAC system that mitigates distortion and enhances robustness. The PAs nonlinearities are modeled by a third-order memoryless polynomial, where the third-order distortion coefficients (3RDCs) vary across access points (APs) due to hardware differences, aging, and environmental conditions. We design a distributed distortion-aware worst-case robust optimization framework that explicitly incorporates uncertainty in 3RDCs. First, we analyze the worst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB) and communication rate. Then, to address the resulting non-convexity, we apply successive convex approximation (SCA) for estimating the 3RDCs. With these, we jointly optimize beamforming and MA positions under transmit power and sensing constraints. To efficiently solve this highly non-convex problem, we develop an MA-enabled self-attention convolutional graph neural network (SACGNN) algorithm. Simulations demonstrate that our method substantially enhances the communication-sensing trade-off under distortion and outperforms fixed-position antenna baselines in terms of robustness and capacity, thereby highlighting the advantages of MA-aided CF-ISAC systems.

Paper number 36:
Title: A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler
Authors: Wenxuan Zhang (1), Shuai Li (1), Xinyi Wang (1), Yu Sun (1), Hongyu Kang (1), Pui Yuk Chryste Wan (1), Yong-Ping Zheng (1 and 2), Sai-Kit Lam (1 and 2) ((1), Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China, (2), the Research Institute of Smart Ageing, The Hong Kong Polytechnic University, Hong Kong SAR, China)
Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.

Paper number 37:
Title: Learning to See Through Flare
Authors: Xiaopeng Peng, Heath Gemar, Erin Fleet, Kyle Novak, Abbie Watnik, Grover Swartzlander
Abstract: Machine vision systems are susceptible to laser flare, where unwanted intense laser illumination blinds and distorts its perception of the environment through oversaturation or permanent damage to sensor pixels. We introduce NeuSee, the first computational imaging framework for high-fidelity sensor protection across the full visible spectrum. It jointly learns a neural representation of a diffractive optical element (DOE) and a frequency-space Mamba-GAN network for image restoration. NeuSee system is adversarially trained end-to-end on 100K unique images to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold $I_{\textrm{sat}}$, the point at which camera sensors may experience damage without the DOE. Our system leverages heterogeneous data and model parallelism for distributed computing, integrating hyperspectral information and multiple neural networks for realistic simulation and image restoration. NeuSee takes into account open-world scenes with dynamically varying laser wavelengths, intensities, and positions, as well as lens flare effects, unknown ambient lighting conditions, and sensor noises. It outperforms other learned DOEs, achieving full-spectrum imaging and laser suppression for the first time, with a 10.1\% improvement in restored image quality.

Paper number 38:
Title: LLMind 2.0: Distributed IoT Automation with Natural Language M2M Communication and Lightweight LLM Agents
Authors: Yuyang Du, Qun Yang, Liujianfu Wang, Jingqi Lin, Hongwei Cui, Soung Chang Liew
Abstract: Recent advances in large language models (LLMs) have sparked interest in their application to IoT and automation systems, particularly for facilitating device management through natural language instructions. However, existing centralized approaches face significant scalability challenges when managing and coordinating the collaboration between IoT devices of diverse capabilities in large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a distributed IoT automation framework that addresses the scalability challenges through lightweight LLM-empowered device agents via natural language-based machine-to-machine (M2M) communication. Unlike previous LLM-controlled automation systems that rely on a centralized coordinator to generate device-specific code to be executed on individual devices, LLMind 2.0 distributes intelligence across individual devices through lightweight LLMs embedded in IoT devices. The central coordinator translates human instructions into simple subtasks described in natural human language, which are then processed by device-specific agents to generate device-specific code locally at the associated devices. This approach transcends device heterogeneity barriers by using natural language as a unified communication medium, enabling seamless collaboration between devices from different manufacturers. The system incorporates several key innovations: a Retrieval-Augmented Generation (RAG) mechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for reliable code generation, and a finite state machine-based task execution framework. Experimental validation in multi-robot warehouse scenarios and real-world WiFi network deployments demonstrates significant improvements in scalability, reliability, and privacy protection compared to the centralized approach.

Paper number 39:
Title: MMIS-Net for Retinal Fluid Segmentation and Detection
Authors: Nchongmaje Ndipenocha, Alina Mirona, Kezhi Wanga, Yongmin Li
Abstract: Purpose: Deep learning methods have shown promising results in the segmentation, and detection of diseases in medical images. However, most methods are trained and tested on data from a single source, modality, organ, or disease type, overlooking the combined potential of other available annotated data. Numerous small annotated medical image datasets from various modalities, organs, and diseases are publicly available. In this work, we aim to leverage the synergistic potential of these datasets to improve performance on unseen data. Approach: To this end, we propose a novel algorithm called MMIS-Net (MultiModal Medical Image Segmentation Network), which features Similarity Fusion blocks that utilize supervision and pixel-wise similarity knowledge selection for feature map fusion. Additionally, to address inconsistent class definitions and label contradictions, we created a one-hot label space to handle classes absent in one dataset but annotated in another. MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities to build a single model. Results: The algorithm was evaluated on the RETOUCH grand challenge hidden test set, outperforming large foundation models for medical image segmentation and other state-of-the-art algorithms. We achieved the best mean Dice score of 0.83 and an absolute volume difference of 0.035 for the fluids segmentation task, as well as a perfect Area Under the Curve of 1 for the fluid detection task. Conclusion: The quantitative results highlight the effectiveness of our proposed model due to the incorporation of Similarity Fusion blocks into the network's backbone for supervision and similarity knowledge selection, and the use of a one-hot label space to address label class inconsistencies and contradictions.

Paper number 40:
Title: Evaluating Particle Filtering for RSS-Based Target Localization under Varying Noise Levels and Sensor Geometries
Authors: Halim Lee, Jongmin Park, Kwansik Park
Abstract: Target localization is a critical task in various applications, such as search and rescue, surveillance, and wireless sensor networks. When a target emits a radio frequency (RF) signal, spatially distributed sensors can collect signal measurements to estimate the target's location. Among various measurement modalities, received signal strength (RSS) is particularly attractive due to its low cost, low power consumption, and ease of deployment. While particle filtering has previously been applied to RSS-based target localization, few studies have systematically analyzed its performance under varying sensor geometries and RSS noise levels. This paper addresses this gap by designing and evaluating a particle filtering algorithm for localizing a stationary target. The proposed method is compared with a conventional RSS-based trilateration approach across different sensor configurations and noise conditions. Simulation results indicate that particle filtering provides more accurate target localization than trilateration, particularly in scenarios with unfavorable sensor geometries and high RSS noise.

Paper number 41:
Title: Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols
Authors: Yiqun Lin, Haoran Sun, Yongqing Li, Rabia Aslam, Lung Fung Tse, Tiange Cheng, Chun Sing Chui, Wing Fung Yau, Victorine R. Le Meur, Meruyert Amangeldy, Kiho Cho, Yinyu Ye, James Zou, Wei Zhao, Xiaomeng Li
Abstract: Patient-specific bone models are essential for designing surgical guides and preoperative planning, as they enable the visualization of intricate anatomical structures. However, traditional CT-based approaches for creating bone models are limited to preoperative use due to the low flexibility and high radiation exposure of CT and time-consuming manual delineation. Here, we introduce Semi-Supervised Reconstruction with Knowledge Distillation (SSR-KD), a fast and accurate AI framework to reconstruct high-quality bone models from biplanar X-rays in 30 seconds, with an average error under 1.0 mm, eliminating the dependence on CT and manual work. Additionally, high tibial osteotomy simulation was performed by experts on reconstructed bone models, demonstrating that bone models reconstructed from biplanar X-rays have comparable clinical applicability to those annotated from CT. Overall, our approach accelerates the process, reduces radiation exposure, enables intraoperative guidance, and significantly improves the practicality of bone models, offering transformative applications in orthopedics.

Paper number 42:
Title: MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence
Authors: Sonal Kumar, Šimon Sedláček, Vaibhavi Lokegaonkar, Fernando López, Wenyi Yu, Nishit Anand, Hyeonggon Ryu, Lichang Chen, Maxim Plička, Miroslav Hlaváček, William Fineas Ellingwood, Sathvik Udupa, Siyuan Hou, Allison Ferner, Sara Barahona, Cecilia Bolaños, Satish Rahi, Laura Herrera-Alarcón, Satvik Dixit, Siddhi Patil, Soham Deshmukh, Lasha Koroshinadze, Yao Liu, Leibny Paola Garcia Perera, Eleni Zanou, Themos Stafylakis, Joon Son Chung, David Harwath, Chao Zhang, Dinesh Manocha, Alicia Lozano-Diez, Santosh Kesiraju, Sreyan Ghosh, Ramani Duraiswami
Abstract: Audio comprehension-including speech, non-speech sounds, and music-is essential for achieving human-level intelligence. Consequently, AI agents must demonstrate holistic audio understanding to qualify as generally intelligent. However, evaluating auditory intelligence comprehensively remains challenging. To address this gap, we introduce MMAU-Pro, the most comprehensive and rigorously curated benchmark for assessing audio intelligence in AI systems. MMAU-Pro contains 5,305 instances, where each instance has one or more audios paired with human expert-generated question-answer pairs, spanning speech, sound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro evaluates auditory intelligence across 49 unique skills and multiple complex dimensions, including long-form audio comprehension, spatial audio reasoning, multi-audio understanding, among others. All questions are meticulously designed to require deliberate multi-hop reasoning, including both multiple-choice and open-ended response formats. Importantly, audio data is sourced directly ``from the wild" rather than from existing datasets with known distributions. We evaluate 22 leading open-source and proprietary multimodal AI models, revealing significant limitations: even state-of-the-art models such as Gemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy, respectively, approaching random performance in multiple categories. Our extensive analysis highlights specific shortcomings and provides novel insights, offering actionable perspectives for the community to enhance future AI systems' progression toward audio general intelligence. The benchmark and code is available at this https URL.

Paper number 43:
Title: UNICON: UNIfied CONtinual Learning for Medical Foundational Models
Authors: Mohammad Areeb Qazi, Munachiso S Nwadike, Ibrahim Almakky, Mohammad Yaqub, Numan Saeed
Abstract: Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Continual learning offers a solution by fine-tuning a model sequentially on different domains or tasks, enabling it to integrate new knowledge without requiring large datasets for each training phase. In this paper, we propose UNIfied CONtinual Learning for Medical Foundational Models (UNICON), a framework that enables the seamless adaptation of foundation models to diverse domains, tasks, and modalities. Unlike conventional adaptation methods that treat these changes in isolation, UNICON provides a unified, perpetually expandable framework. Through careful integration, we show that foundation models can dynamically expand across imaging modalities, anatomical regions, and clinical objectives without catastrophic forgetting or task interference. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification to a prognosis and segmentation task. Our results show improved performance across both additional tasks. Furthermore, we continually incorporated PET scans and achieved a 5\% improvement in Dice score compared to respective baselines. These findings establish that foundation models are not inherently constrained to their initial training scope but can evolve, paving the way toward generalist AI models for medical imaging.

Paper number 44:
Title: Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory
Authors: Johann Licher, Max Bartholdt, Henrik Krauss, Tim-Lukas Habich, Thomas Seel, Moritz Schappler
Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for expanding their applications, but remains a challenging problem due to the high computational demands of accurate dynamic models. While data-driven approaches like Koopman-operator-based methods have been proposed, they typically lack adaptability and cannot capture the full robot shape, limiting their applicability. This work introduces a real-time-capable nonlinear model-predictive control (MPC) framework for SCRs based on a domain-decoupled physics-informed neural network (DD-PINN) with adaptable bending stiffness. The DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a speed-up factor of 44000. It is also used within an unscented Kalman filter for estimating the model states and bending compliance from end-effector position measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories and setpoint control with end-effector position errors below 3 mm (2.3% of the actuator's length). In real-world experiments, the controller achieves similar accuracy and accelerations up to 3.55 m/s2.

Paper number 45:
Title: Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists
Authors: Haohang Xu, Chengjie Liu, Qihang Wang, Wenhao Huang, Yongjian Xu, Weiyu Chen, Anlan Peng, Zhijun Li, Bo Li, Lei Qi, Jun Yang, Yuan Du, Li Du
Abstract: Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\% successful rate, which is 34.62\%-45.19\% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1\%-69.6\% lower than state-of-the-arts.

Paper number 46:
Title: Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices
Authors: Dominik Loroch, Johannes Feldmann, Vladimir Rybalkin, Norbert Wehn
Abstract: Atrial fibrillation (AF) is a common arrhythmia and major risk factor for cardiovascular complications. While commercially available devices and supporting Artificial Intelligence (AI) algorithms exist for reliable detection of AF, the scaling of this technology to the amount of people who need this diagnosis is still a major challenge. This paper presents a novel wearable device, designed specifically for the early and reliable detection of AF. We present an FPGA-based patch-style wearable monitor with embedded deep learning-based AF detection. Operating with 3.8mW system power, which is 1-3 orders of magnitude lower than the state-of-the-art, the device enables continuous AF detection for over three weeks while achieving 95% accuracy, surpassing cardiologist-level performance. A key innovation is the combination of energy-efficient hardware-software co-design and optimized power management through the application of hardware-aware neural architecture search. This advancement represents a significant step toward scalable, reliable, and sustainable AF monitoring.

Paper number 47:
Title: YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection
Authors: Zhebin Jin, Ligang Dong
Abstract: Driver fatigue detection is of paramount importance for intelligent transportation systems due to its critical role in mitigating road traffic accidents. While physiological and vehicle dynamics-based methods offer accuracy, they are often intrusive, hardware-dependent, and lack robustness in real-world environments. Vision-based techniques provide a non-intrusive and scalable alternative, but still face challenges such as poor detection of small or occluded objects and limited multi-scale feature modeling. To address these issues, this paper proposes YOLO11-CR, a lightweight and efficient object detection model tailored for real-time fatigue detection. YOLO11-CR introduces two key modules: the Convolution-and-Attention Fusion Module (CAFM), which integrates local CNN features with global Transformer-based context to enhance feature expressiveness; and the Rectangular Calibration Module (RCM), which captures horizontal and vertical contextual information to improve spatial localization, particularly for profile faces and small objects like mobile phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of 55.93%, outperforming baseline models significantly. Ablation studies further validate the effectiveness of the CAFM and RCM modules in improving both sensitivity and localization accuracy. These results demonstrate that YOLO11-CR offers a practical and high-performing solution for in-vehicle fatigue monitoring, with strong potential for real-world deployment and future enhancements involving temporal modeling, multi-modal data integration, and embedded optimization.

Paper number 48:
Title: PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism
Authors: Yuyan Ye, Hang Xu, Yanghang Huang, Jiali Huang, Qian Weng
Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural Radiance Field (NeRF) framework, capable of reconstructing high-quality scene surfaces from RGB-D sequences in a short time. The method integrates RGB, depth, and semantic information to improve reconstruction performance. Specifically, a novel SG-MLP sampling structure combined with PR-MLP (Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering, allowing the model to capture scene-related information earlier and better distinguish noise from local details. Furthermore, progressive semantic modeling is adopted to extract semantic information at increasing levels of precision, reducing training time while enhancing scene understanding. Experiments on seven synthetic scenes with six evaluation metrics show that PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while maintaining competitive results in NC, Accuracy, and Completeness, demonstrating its effectiveness and practical applicability.

Paper number 49:
Title: Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller
Authors: Marco Giordano, Pietro Bonazzi, Luca Benini, Michele Magno
Abstract: This paper presents a novel event-based eye-tracking system deployed on a resource-constrained microcontroller, addressing the challenges of real-time, low-latency, and low-power performance in embedded systems. The system leverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with an average temporal resolution of 200 {\mu}s, to capture rapid eye movements with extremely low latency. The system is implemented on a novel low-power and high-performance microcontroller from STMicroelectronics, the STM32N6. The microcontroller features an 800 MHz Arm Cortex-M55 core and AI hardware accelerator, the Neural-ART Accelerator, enabling real-time inference with milliwatt power consumption. The paper propose a hardware-aware and sensor-aware compact Convolutional Neuron Network (CNN) optimized for event-based data, deployed at the edge, achieving a mean pupil prediction error of 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The system achieves an end-to-end inference latency of just 385 {\mu}s and a neural network throughput of 52 Multiply and Accumulate (MAC) operations per cycle while consuming just 155 {\mu}J of energy. This approach allows for the development of a fully embedded, energy-efficient eye-tracking solution suitable for applications such as smart glasses and wearable devices.

Paper number 50:
Title: Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction
Authors: Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Murat Yildirim, Gozde Tutuncuoglu
Abstract: Exponential growth in global computing demand is exacerbated due to the higher-energy requirements of conventional architectures, primarily due to energy-intensive data movement. In-memory computing with Resistive Random Access Memory (RRAM) addresses this by co-integrating memory and processing, but faces significant hurdles related to device-level non-idealities and poor scalability for large computing tasks. Here, we introduce \textbf{MELISO+} (In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed framework for energy-efficient in-memory computing. MELISO+ proposes a novel two-tier error correction mechanism to mitigate device non-idealities and develops a distributed RRAM computing framework to enable matrix computations exceeding dimensions of $65,000 \times 65,000$. This approach reduces first- and second-order arithmetic errors due to device non-idealities by over 90\%, enhances energy efficiency by three to five orders of magnitude, and decreases latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to outperform high-precision device alternatives in accuracy, energy and latency metrics. By unifying algorithm-hardware co-design with scalable architecture, MELISO+ significantly advances sustainable, high-dimensional computing suitable for applications like large language models and generative AI.

Paper number 51:
Title: Differentiable Forward and Back-Projector for Rigid Motion Estimation in X-ray Imaging
Authors: Xiao Jiang, Xin Wang, Ali Uneri, Wojciech B. Zbijewski, J. Webster Stayman
Abstract: Objective: In this work, we propose a framework for differentiable forward and back-projector that enables scalable, accurate, and memory-efficient gradient computation for rigid motion estimation tasks. Methods: Unlike existing approaches that rely on auto-differentiation or that are restricted to specific projector types, our method is based on a general analytical gradient formulation for forward/backprojection in the continuous domain. A key insight is that the gradients of both forward and back-projection can be expressed directly in terms of the forward and back-projection operations themselves, providing a unified gradient computation scheme across different projector types. Leveraging this analytical formulation, we develop a discretized implementation with an acceleration strategy that balances computational speed and memory usage. Results: Simulation studies illustrate the numerical accuracy and computational efficiency of the proposed algorithm. Experiments demonstrates the effectiveness of this approach for multiple X-ray imaging tasks we conducted. In 2D/3D registration, the proposed method achieves ~8x speedup over an existing differentiable forward projector while maintaining comparable accuracy. In motion-compensated analytical reconstruction and cone-beam CT geometry calibration, the proposed method enhances image sharpness and structural fidelity on real phantom data while showing significant efficiency advantages over existing gradient-free and gradient-based solutions. Conclusion: The proposed differentiable projectors enable effective and efficient gradient-based solutions for X-ray imaging tasks requiring rigid motion estimation.

Paper number 52:
Title: Observed Control -- Linearly Scalable Nonlinear Model Predictive Control with Adaptive Horizons
Authors: Eugene T. Hamzezadeh, Andrew J. Petruska
Abstract: This work highlights the duality between state estimation methods and model predictive control. A predictive controller, observed control, is presented that uses this duality to efficiently compute control actions with linear time-horizon length scalability. The proposed algorithms provide exceptional computational efficiency, adaptive time horizon lengths, and early optimization termination criteria. The use of Kalman smoothers as the backend optimization framework provides for a straightforward implementation supported by strong theoretical guarantees. Additionally, a formulation is presented that separates linear model predictive control into purely reactive and anticipatory components, enabling any-time any-horizon observed control while ensuring controller stability for short time horizons. Finally, numerical case studies confirm that nonlinear filter extensions, i.e., the extended Kalman filter and unscented Kalman filter, effectively extend observed control to nonlinear systems and objectives.

Paper number 53:
Title: Robust Live Streaming over LEO Satellite Constellations: Measurement, Analysis, and Handover-Aware Adaptation
Authors: Hao Fang, Haoyuan Zhao, Jianxin Shi, Miao Zhang, Guanzhen Wu, Yi Ching Chou, Feng Wang, Jiangchuan Liu
Abstract: Live streaming has experienced significant growth recently. Yet this rise in popularity contrasts with the reality that a substantial segment of the global population still lacks Internet access. The emergence of Low Earth orbit Satellite Networks (LSNs), such as SpaceX's Starlink and Amazon's Project Kuiper, presents a promising solution to fill this gap. Nevertheless, our measurement study reveals that existing live streaming platforms may not be able to deliver a smooth viewing experience on LSNs due to frequent satellite handovers, which lead to frequent video rebuffering events. Current state-of-the-art learning-based Adaptive Bitrate (ABR) algorithms, even when trained on LSNs' network traces, fail to manage the abrupt network variations associated with satellite handovers effectively. To address these challenges, for the first time, we introduce Satellite-Aware Rate Adaptation (SARA), a versatile and lightweight middleware that can seamlessly integrate with various ABR algorithms to enhance the performance of live streaming over LSNs. SARA intelligently modulates video playback speed and furnishes ABR algorithms with insights derived from the distinctive network characteristics of LSNs, thereby aiding ABR algorithms in making informed bitrate selections and effectively minimizing rebuffering events that occur during satellite handovers. Our extensive evaluation shows that SARA can effectively reduce the rebuffering time by an average of $39.41\%$ and slightly improve latency by $0.65\%$ while only introducing an overall loss in bitrate by $0.13\%$.

Paper number 54:
Title: Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference
Authors: Yunxiang Yang, Ningning Xu, Jidong J. Yang
Abstract: Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.

Paper number 55:
Title: Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle
Authors: Xu Yang, Jun Ni, Hengyang Feng, Feiyu Wang, Tiezhen Wang
Abstract: An all-wheel omni-directional independent steering vehicle (AWOISV) is a specialized all-wheel independent steering vehicle with each wheel capable of steering up to 90°, enabling unique maneuvers like yaw and diagonal movement. This paper introduces a theoretical steering radius angle and sideslip angle (\( \theta_R \)-\(\beta_R \)) representation, based on the position of the instantaneous center of rotation relative to the wheel rotation center, defining the motion modes and switching criteria for AWOISVs. A generalized \( v\)-\(\beta\)-\(r \) dynamic model is developed with forward velocity \(v\), sideslip angle \(\beta\), and yaw rate \(r\) as states, and \(\theta_R\) and \(\beta_R\) as control inputs. This model decouples longitudinal and lateral motions into forward and rotational motions, allowing seamless transitions across all motion modes under specific conditions. A filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed, achieving simultaneous tracking of lateral position and arbitrary heading angles, with robustness to model inaccuracies and parameter uncertainties. Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC enables high-precision control of both position and heading while ensuring excellent real-time performance.

Paper number 56:
Title: AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results
Authors: Chao Wang, Francesco Banterle, Bin Ren, Radu Timofte, Xin Lu, Yufeng Peng, Chengjie Ge, Zhijing Sun, Ziang Zhou, Zihao Li, Zishun Liao, Qiyu Kang, Xueyang Fu, Zheng-Jun Zha, Zhijing Sun, Xingbo Wang, Kean Liu, Senyan Xu, Yang Qiu, Yifan Ding, Gabriel Eilertsen, Jonas Unger, Zihao Wang, Ke Wu, Jinshan Pan, Zhen Liu, Zhongyang Li, Shuaicheng Liu, S.M Nadim Uddin
Abstract: This paper presents a comprehensive review of the AIM 2025 Challenge on Inverse Tone Mapping (ITM). The challenge aimed to push forward the development of effective ITM algorithms for HDR image reconstruction from single LDR inputs, focusing on perceptual fidelity and numerical consistency. A total of \textbf{67} participants submitted \textbf{319} valid results, from which the best five teams were selected for detailed analysis. This report consolidates their methodologies and performance, with the lowest PU21-PSNR among the top entries reaching 29.22 dB. The analysis highlights innovative strategies for enhancing HDR reconstruction quality and establishes strong benchmarks to guide future research in inverse tone mapping.

Paper number 57:
Title: AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes
Authors: Tianyi Xu, Fan Zhang, Boxin Shi, Tianfan Xue, Yujin Wang
Abstract: Mainstream high dynamic range imaging techniques typically rely on fusing multiple images captured with different exposure setups (shutter speed and ISO). A good balance between shutter speed and ISO is crucial for achieving high-quality HDR, as high ISO values introduce significant noise, while long shutter speeds can lead to noticeable motion blur. However, existing methods often overlook the complex interaction between shutter speed and ISO and fail to account for motion blur effects in dynamic scenes. In this work, we propose AdaptiveAE, a reinforcement learning-based method that optimizes the selection of shutter speed and ISO combinations to maximize HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an image synthesis pipeline that incorporates motion blur and noise simulation into our training procedure, leveraging semantic information and exposure histograms. It can adaptively select optimal ISO and shutter speed sequences based on a user-defined exposure time budget, and find a better exposure schedule than traditional solutions. Experimental results across multiple datasets demonstrate that it achieves the state-of-the-art performance.

Paper number 58:
Title: Is Transfer Learning Necessary for Violin Transcription?
Authors: Yueh-Po Peng, Ting-Kang Wang, Li Su, Vincent K.M. Cheung
Abstract: Automatic music transcription (AMT) has achieved remarkable progress for instruments such as the piano, largely due to the availability of large-scale, high-quality datasets. In contrast, violin AMT remains underexplored due to limited annotated data. A common approach is to fine-tune pretrained models for other downstream tasks, but the effectiveness of such transfer remains unclear in the presence of timbral and articulatory differences. In this work, we investigate whether training from scratch on a medium-scale violin dataset can match the performance of fine-tuned piano-pretrained models. We adopt a piano transcription architecture without modification and train it on the MOSA dataset, which contains about 30 hours of aligned violin recordings. Our experiments on URMP and Bach10 show that models trained from scratch achieved competitive or even superior performance compared to fine-tuned counterparts. These findings suggest that strong violin AMT is possible without relying on pretrained piano representations, highlighting the importance of instrument-specific data collection and augmentation strategies.

Paper number 59:
Title: AI-Augmented Photon-Trapping Spectrometer-on-a-Chip on Silicon Platform with Extended Near-Infrared Sensitivity
Authors: Ahasan Ahamed, Htet Myat, Amita Rawat, Lisa N McPhillips, M Saif Islam
Abstract: We present a compact, noise-resilient reconstructive spectrometer-on-a-chip that achieves high-resolution hyperspectral imaging across an extended near-infrared (NIR) range up to 1100nm. The device integrates monolithically fabricated silicon photodiodes enhanced with photon-trapping surface textures (PTST), enabling improved responsivity in the low-absorption NIR regime. Leveraging a fully connected neural network, we demonstrate accurate spectral reconstruction from only 16 uniquely engineered detectors, achieving <0.05 RMSE and 8nm resolution over a wide spectral range of 640nm to 1100nm. Our system outperforms conventional spectrometers, maintaining signal-to-noise ratio above 30dB even with 40dB of added detector noise; extending functionality to longer wavelengths up to 1100nm, while the traditional spectrometers fail to perform beyond 950nm due to poor detector efficiency and noise performance. With a footprint of 0.4mm2, dynamic range of 50dB, ultrafast time response (57ps), and high photodiode gain (>7000), this AI-augmented silicon spectrometer is well-suited for portable, real-time, and low-light applications in biomedical imaging, environmental monitoring, and remote sensing. The results establish a pathway toward fully integrated, high-performance hyperspectral sensing in a CMOS-compatible platform.

Paper number 60:
Title: MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination
Authors: Ziyan Wu, Ivan Korolija, Rui Tang
Abstract: With the increasing penetration of renewable generation on the power grid, maintaining system balance requires coordinated demand flexibility from aggregations of buildings. Reinforcement learning (RL) has been widely explored for building controls because of its model-free nature. Open-source simulation testbeds are essential not only for training RL agents but also for fairly benchmarking control strategies. However, most building-sector testbeds target single buildings; multi-building platforms are relatively limited and typically rely on simplified models (e.g., Resistance-Capacitance) or data-driven approaches, which lack the ability to fully capture the physical intricacies and intermediate variables necessary for interpreting control performance. Moreover, these platforms often impose fixed inputs, outputs, and model formats, restricting their applicability as benchmarking tools across diverse control scenarios. To address these gaps, MuFlex, a scalable, open-source platform for benchmarking and testing control strategies for multi-building flexibility coordination, was developed in this study. MuFlex enables synchronous information exchange across EnergyPlus building models and adheres to the latest OpenAI Gym interface, providing a modular, standardized RL implementation. The platform capabilities were demonstrated in a case study coordinating demand flexibility across four office buildings using the Soft Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results show that aggregating the four buildings flexibility reduced total peak demand below a specified threshold while maintaining indoor environmental quality.

Paper number 61:
Title: A Lightweight Dual-Mode Optimization for Generative Face Video Coding
Authors: Zihan Zhang, Shanzhi Yin, Bolin Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye
Abstract: Generative Face Video Coding (GFVC) achieves superior rate-distortion performance by leveraging the strong inference capabilities of deep generative models. However, its practical deployment is hindered by large model parameters and high computational costs. To address this, we propose a lightweight GFVC framework that introduces dual-mode optimization - combining architectural redesign and operational refinement - to reduce complexity whilst preserving reconstruction quality. Architecturally, we replace traditional 3 x 3 convolutions with slimmer and more efficient layers, reducing complexity without compromising feature expressiveness. Operationally, we develop a two-stage adaptive channel pruning strategy: (1) soft pruning during training identifies redundant channels via learnable thresholds, and (2) hard pruning permanently eliminates these channels post-training using a derived mask. This dual-phase approach ensures both training stability and inference efficiency. Experimental results demonstrate that the proposed lightweight dual-mode optimization for GFVC can achieve 90.4% parameter reduction and 88.9% computation saving compared to the baseline, whilst achieving superior performance compared to state-of-the-art video coding standard Versatile Video Coding (VVC) in terms of perceptual-level quality metrics. As such, the proposed method is expected to enable efficient GFVC deployment in resource-constrained environments such as mobile edge devices.

Paper number 62:
Title: Amplitude maximization in stable systems, Schur positivity, and some conjectures on polynomial interpolation
Authors: Dmitrii M. Ostrovskii, Pavel S. Shcherbakov
Abstract: For $r > 0$ and integers $t \ge n > 0$, we consider the following ``amplitude maximization'' problem: maximize the quantity $|x_t|$ over the set of complex solutions $x = (x_0, x_1, \dots)$ of all homogeneous linear difference equations of order $n$ with the roots of characteristic polynomial in the disc $\{z \in \mathbb{C}: |z| \le r\}$, and with initial values $x_0, \dots, x_{n-1}$ in the unit disc. We find that for any $t,n,r$, the maximum is attained in the case of coinciding roots on the boundary circle; this implies that for all $r < 1$, the maximum amplitude can be computed explicitly, by studying a single equation whose characteristic polynomial is $(z-r)^n$. Moreover, the optimality of the co-phase root configuration holds for origin-centered polydiscs. To prove this result, we first reduce the problem to a certain interpolation problem over monomials, then solve the latter by leveraging the theory of symmetric functions and identifying the associated Schur positivity structure. We also discuss the implications for more general Reinhardt domains. Finally, we study the problem of estimating the derivatives of a real entire function from its values at $n/2$ pairs of complex conjugate points in the unit disc. Here, we propose conjectures on the extremality of the monomial $z^n$, and restate them in terms of Schur polynomials.

Paper number 63:
Title: Power and Rate Allocations for Positive-rate Covert Communications in Block-Fading Channels
Authors: Yubo Zhang, Hassan ZivariFard, Xiaodong Wang
Abstract: We aim to achieve keyless covert communication with a positive-rate in Rayleigh block-fading channels. Specifically, the transmitter and the legitimate receiver are assumed to have either causal or non-causal knowledge of the \ac{CSI} for both the legitimate and the warden channels, while the warden only knows the statistical distribution of the \ac{CSI}. Two problem formulations are considered in this work: (a) Power allocation: maximizing the sum covert rate subject to a maximum power constraint, and (b) Rate allocation: minimizing the power consumption subject to a minimum covert rate constraint. Both problems are formulated based on recent information theoretical results on covert communication over state-dependent channels. When the \ac{CSI} of each fading block is known non-causally, we propose a novel three-step method to solve both the power and rate allocation problems. In the case where the \ac{CSI} is known causally, the power allocation problem can be formulated as \ac{MDP} and be solved using a \ac{DDQN} approach. Although the rate allocation problem under causal \ac{CSI} does not directly conform to an \ac{MDP} structure, it can be approximately solved using the \ac{DDQN} trained for power allocation. Simulation results demonstrate the effectiveness of the proposed power and rate allocation methods and provide comprehensive performance comparisons across different allocation schemes.

Paper number 64:
Title: Repeater Swarm-Assisted Cellular Systems: Interaction Stability and Performance Analysis
Authors: Jianan Bai, Anubhab Chowdhury, Anders Hansson, Erik G. Larsson
Abstract: We consider a cellular massive MIMO system where swarms of wireless repeaters are deployed to improve coverage. These repeaters are full-duplex relays with small form factors that receive and instantaneously retransmit signals. They can be deployed in a plug-and-play manner at low cost, while being transparent to the network--conceptually they are active channel scatterers with amplification capabilities. Two fundamental questions need to be addressed in repeater deployments: (I) How can we prevent destructive effects of positive feedback caused by inter-repeater interaction (i.e., each repeater receives and amplifies signals from others)? (ii) How much performance improvement can be achieved given that repeaters also inject noise and may introduce more interference? To answer these questions, we first derive a generalized Nyquist stability criterion for the repeater swarm system, and provide an easy-to-check stability condition. Then, we study the uplink performance and develop an efficient iterative algorithm that jointly optimizes the repeater gains, user transmit powers, and receive combining weights to maximize the weighted sum rate while ensuring system stability. Numerical results corroborate our theoretical findings and show that the repeaters can significantly improve the system performance, both in sub-6 GHz and millimeter-wave bands. The results also warrant careful deployment to fully realize the benefits of repeaters, for example, by ensuring a high probability of line-of-sight links between repeaters and the base station.

Paper number 65:
Title: Leveraging Mamba with Full-Face Vision for Audio-Visual Speech Enhancement
Authors: Rong Chao, Wenze Ren, You-Jin Li, Kuo-Hsuan Hung, Sung-Feng Huang, Szu-Wei Fu, Wen-Huang Cheng, Yu Tsao
Abstract: Recent Mamba-based models have shown promise in speech enhancement by efficiently modeling long-range temporal dependencies. However, models like Speech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios and struggle in complex multi-speaker environments such as the cocktail party problem. To overcome this, we introduce AVSEMamba, an audio-visual speech enhancement model that integrates full-face visual cues with a Mamba-based temporal backbone. By leveraging spatiotemporal visual information, AVSEMamba enables more accurate extraction of target speech in challenging conditions. Evaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba outperforms other monaural baselines in speech intelligibility (STOI), perceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves \textbf{1st place} on the monaural leaderboard.

Paper number 66:
Title: Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems
Authors: Eike-Manuel Edelmann, Alexander von Bank, Laurent Schmalen
Abstract: Neural encoding parameters for spiking neural networks (SNNs) are typically set heuristically. We propose a reinforcement learning-based algorithm to optimize them. Applied to an SNN-based equalizer and demapper in an IM/DD system, the method improves performance while reducing computational load and network size.

Paper number 67:
Title: Joint Beamforming Design for RIS-Empowered NOMA-ISAC Systems
Authors: Chunjie Wang, Xuhui Zhang, Jinke Ren, Wenchao Liu, Shuqiang Wang, Yanyan Shen, Kejiang Ye, Chengzhong Xu, Dusit Niyato
Abstract: This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing and communication (ISAC) system and proposes a joint communication and sensing beamforming design based on non-orthogonal multiple access (NOMA) technology. The system employs a dual-functional base station (DFBS) to simultaneously serve multiple users and sense multiple targets with the aid of RIS. To maximize the sum-rate of users, we jointly optimize the DFBS's active beamforming, the RIS's reflection coefficients, and the radar receive filters. The optimization is performed under constraints including the radar signal-to-noise ratio thresholds, the user signal-to-interference-plus-noise ratio requirements, the phase shifts of the RIS, the total transmit power, the receive filters, and the successive interference cancellation decoding order. To tackle the complex interdependencies and non-convex nature of the optimization problem, we introduce an effective iterative algorithm based on the alternating optimization framework. Simulation results demonstrate that the proposed algorithm outperforms baseline algorithms, highlighting its distinct advantages in the considered RIS-empowered NOMA-ISAC systems.

Paper number 68:
Title: Optical communication-based identification for multi-UAV systems: theory and practice
Authors: Daniel Bonilla Licea, Viktor Walter, Mounir Ghogho, Martin Saska
Abstract: Mutual relative localization and identification is an important feature for the stabilization and navigation of multi-Unmanned Aerial Vehicle (UAV) systems. Camera-based communications technology, also referred to as Optical Camera Communications (OCC) in the literature, is a novel approach that could bring a valuable solution to such a complex task. In such system, the UAVs are equipped with LEDs that act as beacons and with cameras allowing them to locate the LEDs of other UAVs. Specific blinking sequences are assigned to the LEDs of each of the UAVs in order to uniquely identify them. This camera-based relative localization and identification system is immune to Radio Frequency (RF) electromagnetic interference and operates in Global Navigation satellite (GNSS) denied environments. In addition, since many UAVs are already equipped with cameras, the implementation of this system is inexpensive. In this article, we study in detail the capacity of this system and its limitations. Furthermore, we show how to construct blinking sequences for UAV LEDs in order to improve system performance. Finally, experimental results are presented to corroborate the analytical derivations.

Paper number 69:
Title: Radio Map Estimation: Empirical Validation and Analysis
Authors: Raju Shrestha, Tien Ngoc Ha, Pham Q. Viet, Daniel Romero
Abstract: Radio maps provide metrics such as the received signal strength at every location in a geographical region of interest. Extensive research has been carried out in this context, but it relies almost exclusively on synthetic-data experiments. Thus, the practical aspects of the radio map estimation (RME) problem as well as the performance of existing estimators in the real world remain unknown. To fill this gap end, this paper puts forth the first comprehensive, rigorous, and reproducible study of RME with real data. The main contributions include (C1) an assessment of the viability of RME based on the estimation error that can be achieved, (C2) the analysis of the main phenomena and trade-offs involved in RME, including the experimental verification of theoretical findings in the literature, and (C3) a thorough evaluation of a wide range of estimators on realworld data. Remarkably, this reveals that the performance gain of existing deep estimators in their pure form may not compensate for their complexity. A simple enhancement (C4) is proposed to alleviate this issue. The vast amount of data collected for this study is published along with the developed simulator to enable research on new schemes, hopefully bringing RME one step closer to practical deployment.

Paper number 70:
Title: A Closed-Form Solution for Kernel Adaptive Filtering
Authors: Benjamin Colburn, Luis G. Sanchez Giraldo, Kan Li, Jose C. Principe
Abstract: Conventional kernel adaptive filtering (KAF) uses a prescribed, positive-definite, nonlinear function to define the Reproducing Kernel Hilbert Space (RKHS), where the minimum mean square error solution is found using search techniques. This paper proposes embedding the statistics of the input data in the kernel definition, obtaining a closed-form solution for nonlinear adaptive filtering applications. We call this solution the Functional Wiener Filter (FWF), which extends Parzen's work on the autocorrelation RKHS to nonlinear functional spaces. We first derive a space-time covariance operator on a RKHS that generalizes Parzen's autocovariance RKHS and includes nonlinear functions of a random process. We then present a method for approximating the FWF in an explicit, finite-dimensional RKHS to model time series. The FWF's computational complexity at test time is an improvement over other KAFs. We demonstrate how the difference equation learned by the FWF can be extracted, leading to system identification applications, beyond optimal nonlinear filtering.

Paper number 71:
Title: Input-Output Extension of Underactuated Nonlinear Systems
Authors: Mirko Mizzoni, Amr Afifi, Antonio Franchi
Abstract: This letter proposes a method to integrate auxiliary actuators that enhance the task space capabilities of commercial underactuated systems, leaving the internal certified low level controller untouched. The additional actuators are combined with a feedback linearizing outer loop controller, enabling full pose tracking. We provide the conditions under which legacy high level commands and new actuator inputs can be cohesively coordinated to achieve decoupled control of all degrees of freedom. A comparative study with a standard quadrotor originally not designed for physical interaction demonstrates that the proposed modified platform remains stable under contact, while the baseline system diverges. Additionally, simulation results under parameter uncertainty illustrate the robustness of the approach.

Paper number 72:
Title: Joint Channel and Data Estimation for Multiuser Extremely Large-Scale MIMO Systems
Authors: Kabuto Arai, Koji Ishibashi, Hiroki Iimori, Paulo Valente Klaine, Szabolcs Malomsoky
Abstract: This paper proposes a joint channel and data estimation (JCDE) algorithm for uplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO) systems. The initial channel estimation is formulated as a sparse reconstruction problem based on the angle and distance sparsity under the near-field propagation condition. This problem is solved using non-orthogonal pilots through an efficient low complexity two-stage compressed sensing algorithm. Furthermore, the initial channel estimates are refined by employing a JCDE framework driven by both non-orthogonal pilots and estimated data. The JCDE problem is solved by sequential expectation propagation (EP) algorithms, where the channel and data are alternately updated in an iterative manner. In the channel estimation phase, integrating Bayesian inference with a model-based deterministic approach provides precise estimations to effectively exploit the near-field characteristics in the beam-domain. In the data estimation phase, a linear minimum mean square error (LMMSE)-based filter is designed at each sub-array to address the correlation due to energy leakage in the beam-domain arising from the near-field effects. Numerical simulations reveal that the proposed initial channel estimation and JCDE algorithm outperforms the state-of-the-art approaches in terms of channel estimation, data detection, and computational complexity.

Paper number 73:
Title: Regional quality estimation for echocardiography using deep learning
Authors: Gilles Van De Vyver, Svein-Erik Måsøy, Håvard Dalen, Bjørnar Leangen Grenne, Espen Holte, Sindre Hellum Olaisen, John Nyberg, Andreas Østvik, Lasse Løvstakken, Erik Smistad
Abstract: Automatic estimation of cardiac ultrasound image quality can be beneficial for guiding operators and ensuring the accuracy of clinical measurements. Previous work often fails to distinguish the view correctness of the echocardiogram from the image quality. Additionally, previous studies only provide a global image quality value, which limits their practical utility. In this work, we developed and compared three methods to estimate image quality: 1) classic pixel-based metrics like the generalized contrast-to-noise ratio (gCNR) on myocardial segments as region of interest and left ventricle lumen as background, obtained using a U-Net segmentation 2) local image coherence derived from a U-Net model that predicts coherence from B-Mode images 3) a deep convolutional network that predicts the quality of each region directly in an end-to-end fashion. We evaluate each method against manual regional image quality annotations by three experienced cardiologists. The results indicate poor performance of the gCNR metric, with Spearman correlation to the annotations of rho = 0.24. The end-to-end learning model obtains the best result, rho = 0.69, comparable to the inter-observer correlation, rho = 0.63. Finally, the coherence-based method, with rho = 0.58, outperformed the classical metrics and is more generic than the end-to-end approach. The image quality prediction tool is available as an open source Python library at this https URL.

Paper number 74:
Title: Exposing Barriers to Flexibility Aggregation in Unbalanced Distribution Networks
Authors: Andrey Churkin, Wangwei Kong, Pierluigi Mancarella, Eduardo A. Martínez Ceseña
Abstract: The increasing integration of distributed energy resources (DER) offers new opportunities for distribution system operators (DSO) to improve network operation through flexibility services. To utilise flexible resources, various DER flexibility aggregation methods have been proposed, such as the concept of aggregated P-Q flexibility areas. Yet, many existing studies assume perfect coordination among DER and rely on single-phase power flow analysis, thus overlooking barriers to flexibility aggregation in real unbalanced systems. To quantify the impact of these barriers, this paper proposes a three-phase optimal power flow (OPF) framework for P-Q flexibility assessment, implemented as an open-source Julia tool this http URL. The framework explicitly accounts for voltage unbalance and imperfect coordination among DER in low voltage (LV) distribution networks. Simulations on an illustrative 5-bus system and a real 221-bus LV network in the UK reveal that over 30% of the theoretical aggregated flexibility potential can be lost due to phase unbalance and lack of coordination across phases. These findings highlight the need for improved flexibility aggregation tools applicable to real unbalanced distribution networks.

Paper number 75:
Title: MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation
Authors: Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel
Abstract: Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: this https URL

Paper number 76:
Title: Exploiting structural observability and graph colorability for optimal sensor placement in water distribution networks
Authors: J.J.H. van Gemert, V. Breschi, D.R. Yntema, K.J. Keesman, M. Lazar
Abstract: Water distribution networks (WDNs) are critical systems for our society and detecting leakages is important for minimizing losses and water waste. This makes optimal sensor placement for leakage detection very relevant. Existing sensor placement methods rely on simulation-based scenarios, often lacking structure and generalizability, or depend on the knowledge of specific parameters of the WDN as well as on initial sensor data for linearization and demand estimation. Motivated by this, this paper investigates the observability of an entire WDN, based on structural observability theory. This allows us to establish the conditions for the observability of the WDN model, independently of parameter uncertainties. Additionally, a sensor placement algorithm is proposed that leverages such observability conditions and graph theory and accounts for the industrial and material costs. To demonstrate the effectiveness of our approach, we apply it to a hydraulic-transient WDN model.

Paper number 77:
Title: Achieving Dispatchability in Data Centers: Carbon and Cost-Aware Sizing of Energy Storage and Local Photovoltaic Generation
Authors: Enea Figini, Mario Paolone
Abstract: Data centers are large electricity consumers due to the high consumption needs of servers and their cooling systems. Given the current crypto-currency and artificial intelligence trends, the data center electricity demand is bound to grow significantly. With the electricity sector being responsible for a large share of global greenhouse gas (GHG) emissions, it is important to lower the carbon footprint of data centers to meet GHG emissions targets set by international agreements. Moreover, uncontrolled integration of data centers in power distribution grids contributes to increasing the stochasticity of the power system demand, thus increasing the need for capacity reserves, which leads to economic and environmental inefficiencies in the power grid operation. This work provides a method to size a PhotoVoltaic (PV) system and an Energy Storage System (ESS) for an existing data center looking to reduce both its carbon footprint and demand stochasticity via dispatching. The proposed scenario-based optimization framework allows to size the ESS and the PV system to minimize the expected operational and capital costs, along with the carbon footprint of the data center complex. The life cycle assessment of the resources, as well as the dynamic carbon emissions of the upstream power distribution grid, are accounted for while computing the day-ahead planning of the data center aggregated demand and PV generation. Case studies in different Swiss cantons and regions of Germany emphasize the need for location-aware sizing processes since the obtained optimal solutions strongly depend on the local electricity carbon footprint, cost and on the local irradiance conditions. Some regions show potential in carbon footprint reduction, while other regions do not.

Paper number 78:
Title: RadGPT: Constructing 3D Image-Text Tumor Datasets
Authors: Pedro R. A. S. Bassi, Mehmet Can Yavuz, Kang Wang, Xiaoxi Chen, Wenxuan Li, Sergio Decherchi, Andrea Cavalli, Yang Yang, Alan Yuille, Zongwei Zhou
Abstract: Cancers identified in CT scans are usually accompanied by detailed radiology reports, but publicly available CT datasets often lack these essential reports. This absence limits their usefulness for developing accurate report generation AI. To address this gap, we present AbdomenAtlas 3.0, the first public, high-quality abdominal CT dataset with detailed, expert-reviewed radiology reports. All reports are paired with per-voxel masks and they describe liver, kidney and pancreatic tumors. AbdomenAtlas 3.0 has 9,262 triplets of CT, mask and report--3,955 with tumors. These CT scans come from 17 public datasets. Besides creating the reports for these datasets, we expanded their number of tumor masks by 4.2x, identifying 3,011 new tumor cases. Notably, the reports in AbdomenAtlas 3.0 are more standardized, and generated faster than traditional human-made reports. They provide details like tumor size, location, attenuation and surgical resectability. These reports were created by 12 board-certified radiologists using our proposed RadGPT, a novel framework that converted radiologist-revised tumor segmentation masks into structured and narrative reports. Besides being a dataset creation tool, RadGPT can also become a fully-automatic, segmentation-assisted report generation method. We benchmarked this method and 5 state-of-the-art report generation vision-language models. Our results show that segmentation strongly improves tumor detection in AI-made reports.

Paper number 79:
Title: Finite Sample Analysis of System Poles for Ho-Kalman Algorithm
Authors: Shuai Sun, Xu Wang
Abstract: The Ho-Kalman algorithm has been widely employed for the identification of discrete-time linear time-invariant (LTI) systems. In this paper, we investigate the pole estimation error for the Ho-Kalman algorithm based on finite input/output sample data. Building upon prior works, we derive finite sample error bounds for system pole estimation in both single-trajectory and multiple-trajectory scenarios. Specifically, we prove that, with high probability, the estimation error for an $n$-dimensional system decreases at a rate of at least $\mathcal{O}(T^{-1/2n})$ in the single-trajectory setting with trajectory length $T$, and at a rate of at least $\mathcal{O}(N^{-1/2n})$ in the multiple-trajectory setting with $N$ independent trajectories. Furthermore, we reveal that in both settings, achieving a constant estimation error requires a super-polynomial sample size in $ \max\{n/m, n/p\} $, where $n/m$ and $n/p$ denote the state-to-output and state-to-input dimension ratios, respectively. Finally, numerical experiments are conducted to validate the non-asymptotic results of system pole estimation.

Paper number 80:
Title: Reinforcement learning for robust dynamic metabolic control
Authors: Sebastián Espinel-Ríos, River Walser, Dongda Zhang
Abstract: Dynamic metabolic control allows key metabolic fluxes to be modulated in real time, enhancing bioprocess flexibility and expanding available optimization degrees of freedom. This is achieved, e.g., via targeted modulation of metabolic enzyme expression. However, identifying optimal dynamic control policies is challenging due to the generally high-dimensional solution space and the need to manage metabolic burden and cytotoxic effects arising from inducible enzyme expression. The task is further complicated by stochastic dynamics, which reduce bioprocess reproducibility. We propose a reinforcement learning framework} to derive optimal policies by allowing an agent (the controller) to interact with a surrogate dynamic model. To promote robustness, we apply domain randomization, enabling the controller to generalize across uncertainties. When transferred to an experimental system, the agent can in principle continue fine-tuning the policy. Our framework provides an alternative to conventional model-based control such as model predictive control, which requires model differentiation with respect to decision variables; often impractical for complex stochastic, nonlinear, stiff, and piecewise-defined dynamics. In contrast, our approach relies on forward integration of the model, thereby simplifying the task. We demonstrate the framework in two $\textit{Escherichia coli}$ bioprocesses: dynamic control of acetyl-CoA carboxylase for fatty-acid synthesis and of adenosine triphosphatase for lactate synthesis.

Paper number 81:
Title: Rapid Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing
Authors: Artur Grigorev, Adriana-Simona Mihaita
Abstract: Effective placement of Out-of-Home advertising and street furniture requires accurate identification of locations offering maximum visual exposure to target audiences, particularly vehicular traffic. Traditional site selection methods often rely on static traffic counts or subjective assessments. This research introduces a data-driven methodology to objectively quantify location visibility by analyzing large-scale connected vehicle trajectory data (sourced from Compass IoT) within urban environments. We model the dynamic driver field-of-view using a forward-projected visibility area for each vehicle position derived from interpolated trajectories. By integrating this with building vertex locations extracted from OpenStreetMap, we quantify the cumulative visual exposure, or ``visibility count'', for thousands of potential points of interest near roadways. The analysis reveals that visibility is highly concentrated, identifying specific ``visual hotspots'' that receive disproportionately high exposure compared to average locations. The core technical contribution involves the construction of a BallTree spatial index over building vertices. This enables highly efficient (O(logN) complexity) radius queries to determine which vertices fall within the viewing circles of millions of trajectory points across numerous trips, significantly outperforming brute-force geometric checks. Analysis reveals two key findings: 1) Visibility is highly concentrated, identifying distinct 'visual hotspots' receiving disproportionately high exposure compared to average locations. 2) The aggregated visibility counts across vertices conform to a Log-Normal distribution.

Paper number 82:
Title: BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet
Authors: Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh
Abstract: Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis. This is primarily due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a newly developed MRI dataset named BRISC designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians. It includes three major tumor types, namely glioma, meningioma, and pituitary, as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. In this work, we propose a transformer-based model designed for both segmentation and classification of brain tumors, leveraging multi-scale feature representations from a Swin Transformer backbone. The model is benchmarked against established baselines to demonstrate the utility of the dataset, enabling accurate segmentation and robust classification across four diagnostic categories: glioma, meningioma, pituitary, and non-tumorous cases. In this work, our proposed transformer-based model demonstrates superior performance in both segmentation and classification tasks for brain tumor analysis. For the segmentation task, the method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3\%, with improvements observed across all tumor categories. For the classification task, the model attains an accuracy of 99.63\%, effectively distinguishing between glioma, meningioma, pituitary, and non-tumorous cases. this https URL

Paper number 83:
Title: Less is More: Data Curation Matters in Scaling Speech Enhancement
Authors: Chenda Li, Wangyou Zhang, Wei Wang, Robin Scheibler, Kohei Saijo, Samuele Cornell, Yihui Fu, Marvin Sach, Zhaoheng Ni, Anurag Kumar, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian
Abstract: The vast majority of modern speech enhancement systems rely on data-driven neural network models. Conventionally, larger datasets are presumed to yield superior model performance, an observation empirically validated across numerous tasks in other domains. However, recent studies reveal diminishing returns when scaling speech enhancement data. We focus on a critical factor: prevalent quality issues in ``clean'' training labels within large-scale datasets. This work re-examines this phenomenon and demonstrates that, within large-scale training sets, prioritizing high-quality training data is more important than merely expanding the data volume. Experimental findings suggest that models trained on a carefully curated subset of 700 hours can outperform models trained on the 2,500-hour full dataset. This outcome highlights the crucial role of data curation in scaling speech enhancement systems effectively.

Paper number 84:
Title: UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis
Authors: Zhuo Li, Xuhang Chen, Shuqiang Wang, Bin Yuan, Nou Sotheany, Ngeth Rithea
Abstract: Functional ultrasound (fUS) is a neuroimaging technique known for its high spatiotemporal resolution, enabling non-invasive observation of brain activity through neurovascular coupling. Despite its potential in clinical applications such as neonatal monitoring and intraoperative guidance, the development of fUS faces challenges related to data scarcity and limitations in generating realistic fUS images. This paper explores the use of a generative adversarial network (GAN) framework tailored for fUS image synthesis. The proposed method incorporates architectural enhancements, including feature enhancement modules and normalization techniques, aiming to improve the fidelity and physiological plausibility of generated images. The study evaluates the performance of the framework against existing generative models, demonstrating its capability to produce high-quality fUS images under various experimental conditions. Additionally, the synthesized images are assessed for their utility in downstream tasks, showing improvements in classification accuracy when used for data augmentation. Experimental results are based on publicly available fUS datasets, highlighting the framework's effectiveness in addressing data limitations.

Paper number 85:
Title: A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model
Authors: Zhe Xu, Ziyi Liu, Junlin Hou, Jiabo Ma, Cheng Jin, Yihui Wang, Zhixuan Chen, Zhengyu Zhang, Fuxiang Huang, Zhengrui Guo, Fengtao Zhou, Yingxue Xu, Xi Wang, Ronald Cheong Kin Chan, Li Liang, Hao Chen
Abstract: Multimodal large language models (MLLMs) have emerged as powerful tools for computational pathology, offering unprecedented opportunities to integrate pathological images with language context for comprehensive diagnostic analysis. These models hold particular promise for automating complex tasks that traditionally require expert interpretation of pathologists. However, current MLLM approaches in pathology demonstrate significantly constrained reasoning capabilities, primarily due to their reliance on expensive chain-of-thought annotations. Additionally, existing methods remain limited to simplex application of visual question answering (VQA) at the region-of-interest (ROI) level, failing to address the full spectrum of diagnostic needs such as ROI classification, detection, segmentation, whole-slide-image (WSI) classification and VQA in clinical practice. In this study, we present SmartPath-R1, a versatile MLLM capable of simultaneously addressing both ROI-level and WSI-level tasks while demonstrating robust pathological reasoning capability. Our framework combines scale-dependent supervised fine-tuning and task-aware reinforcement fine-tuning, which circumvents the requirement for chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM. Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks. We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI samples for training and evaluation. Extensive experiments across 72 tasks validate the effectiveness and superiority of the proposed approach. This work represents a significant step toward developing versatile, reasoning-enhanced AI systems for precision pathology.

Paper number 86:
Title: Dual-Head Physics-Informed Graph Decision Transformer for Distribution System Restoration
Authors: Hong Zhao, Jin Wei-Kocsis, Adel Heidari Akhijahani, Karen L Butler-Purry
Abstract: Driven by recent advances in sensing and computing, deep reinforcement learning (DRL) technologies have shown great potential for addressing distribution system restoration (DSR) under uncertainty. However, their data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit their ability to handle scenarios that require long-term temporal dependencies or few-shot and zero-shot decision making. Emerging Decision Transformers (DTs), which leverage causal transformers for sequence modeling in DRL tasks, offer a promising alternative. However, their reliance on return-to-go (RTG) cloning and limited generalization capacity restricts their effectiveness in dynamic power system environments. To address these challenges, we introduce an innovative Dual-Head Physics-informed Graph Decision Transformer (DH-PGDT) that integrates physical modeling, structural reasoning, and subgoal-based guidance to enable scalable and robust DSR even in zero-shot or few-shot scenarios. DH-PGDT features a dual-head physics-informed causal transformer architecture comprising Guidance Head, which generates subgoal representations, and Action Head, which uses these subgoals to generate actions independently of RTG. It also incorporates an operational constraint-aware graph reasoning module that encodes power system topology and operational constraints to generate a confidence-weighted action vector for refining DT trajectories. This design effectively improves generalization and enables robust adaptation to unseen scenarios. While this work focuses on DSR, the underlying computing model of the proposed PGDT is broadly applicable to sequential decision making across various power system operations and other complex engineering domains.

Paper number 87:
Title: Speech Enhancement based on cascaded two flows
Authors: Seonggyu Lee, Sein Cheong, Sangwook Han, Kihyuk Kim, Jong Won Shin
Abstract: Speech enhancement (SE) based on diffusion probabilistic models has exhibited impressive performance, while requiring a relatively high number of function evaluations (NFE). Recently, SE based on flow matching has been proposed, which showed competitive performance with a small NFE. Early approaches adopted the noisy speech as the only conditioning variable. There have been other approaches which utilize speech enhanced with a predictive model as another conditioning variable and to sample an initial value, but they require a separate predictive model on top of the generative SE model. In this work, we propose to employ an identical model based on flow matching for both SE and generating enhanced speech used as an initial starting point and a conditioning variable. Experimental results showed that the proposed method required the same or fewer NFEs even with two cascaded generative methods while achieving equivalent or better performances to the previous baselines.

Paper number 88:
Title: Nonlinear Systems in Wireless Power Transfer Applications
Authors: H Chan
Abstract: As a novel pattern of energization, the wireless power transfer (WPT) offers a brand-new way to the energy acquisition for electric-driven devices, thus alleviating the over-dependence on the battery. This report presents three types of WPT systems that use nonlinear control methods, in order to acquire an in-depth understanding of the course of Nonlinear Systems.

Paper number 89:
Title: Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study
Authors: Md Basit Azam, Sarangthem Ibotombi Singh
Abstract: Non-invasive glucose monitoring remains a critical challenge in the management of diabetes. HRV during sleep shows promise for glucose prediction however, age-related autonomic changes significantly confound traditional HRV analyses. We analyzed 43 subjects with multi-modal data including sleep-stage specific ECG, HRV features, and clinical measurements. A novel age-normalization technique was applied to the HRV features by, dividing the raw values by age-scaled factors. BayesianRidge regression with 5-fold cross-validation was employed for log-glucose prediction. Age-normalized HRV features achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction, representing a 25.6% improvement over non-normalized features (R2 = 0.132). The top predictive features were hrv rem mean rr age normalized (r = 0.443, p = 0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic blood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed age-normalization as the critical component, with sleep-stage specific features providing additional predictive value. Age-normalized HRV features significantly enhance glucose prediction accuracy compared with traditional approaches. This sleep-aware methodology addresses fundamental limitations in autonomic function assessment and suggests a preliminary feasibility for non-invasive glucose monitoring applications. However, these results require validation in larger cohorts before clinical consideration.

Paper number 90:
Title: A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Control
Authors: Sebastian Hirt, Lukas Theiner, Maik Pfefferkorn, Rolf Findeisen
Abstract: Many control problems require repeated tuning and adaptation of controllers across distinct closed-loop tasks, where data efficiency and adaptability are critical. We propose a hierarchical Bayesian optimization (BO) framework that is tailored to efficient controller parameter learning in sequential decision-making and control scenarios for distinct tasks. Instead of treating the closed-loop cost as a black-box, our method exploits structural knowledge of the underlying problem, consisting of a dynamical system, a control law, and an associated closed-loop cost function. We construct a hierarchical surrogate model using Gaussian processes that capture the closed-loop state evolution under different parameterizations, while the task-specific weighting and accumulation into the closed-loop cost are computed exactly via known closed-form expressions. This allows knowledge transfer and enhanced data efficiency between different closed-loop tasks. The proposed framework retains sublinear regret guarantees on par with standard black-box BO, while enabling multi-task or transfer learning. Simulation experiments with model predictive control demonstrate substantial benefits in both sample efficiency and adaptability when compared to purely black-box BO approaches.

Paper number 91:
Title: Framework for Modeling and Optimization of On-Orbit Servicing Operations under Demand Uncertainties
Authors: Tristan Sarton du Jonchay, Hao Chen, Onalli Gunasekara, Koki Ho
Abstract: This paper develops a framework that models and optimizes the operations of complex on-orbit servicing infrastructures involving one or more servicers and orbital depots to provide multiple types of services to a fleet of geostationary satellites. The proposed method extends the state-of-the-art space logistics technique by addressing the unique challenges in on-orbit servicing applications, and integrate it with the Rolling Horizon decision making approach. The space logistics technique enables modeling of the on-orbit servicing logistical operations as a Mixed-Integer Linear Program whose optimal solutions can efficiently be found. The Rolling Horizon approach enables the assessment of the long-term value of an on-orbit servicing infrastructure by accounting for the uncertain service needs that arise over time among the geostationary satellites. Two case studies successfully demonstrate the effectiveness of the framework for (1) short-term operational scheduling and (2) long-term strategic decision making for on-orbit servicing architectures under diverse market conditions.

Paper number 92:
Title: On-Orbit Servicing Optimization Framework with High- and Low-Thrust Propulsion Tradeoff
Authors: Tristan Sarton du Jonchay, Hao Chen, Masafumi Isaji, Yuri Shimane, Koki Ho
Abstract: This paper proposes an on-orbit servicing logistics optimization framework that is capable of performing the short-term operational scheduling and long-term strategic planning of sustainable servicing infrastructures that involve high-thrust, low-thrust, and/or multimodal servicers supported by orbital depots. The proposed framework generalizes the state-of-the-art on-orbit servicing logistics optimization method by incorporating user-defined trajectory models and optimizing the logistics operations with the propulsion technology and trajectory tradeoff in consideration. Mixed-Integer Linear Programming is leveraged to find the optimal operations of the servicers over a given period, while the Rolling Horizon approach is used to consider a long time horizon accounting for the uncertainties in service demand. Several analyses are carried out to demonstrate the value of the proposed framework in automatically trading off the high- and low-thrust propulsion systems for both short-term operational scheduling and long-term strategic planning of on-orbit servicing infrastructures.

Paper number 93:
Title: Adaptation and Optimization of Automatic Speech Recognition (ASR) for the Maritime Domain in the Field of VHF Communication
Authors: Emin Cagatay Nakilcioglu, Maximilian Reimann, Ole John
Abstract: This paper introduces a multilingual automatic speech recognizer (ASR) for maritime radio communi-cation that automatically converts received VHF radio signals into text. The challenges of maritime radio communication are described at first, and the deep learning architecture of marFM consisting of audio processing techniques and machine learning algorithms is presented. Subsequently, maritime radio data of interest is analyzed and then used to evaluate the transcription performance of our ASR model for various maritime radio data.

Paper number 94:
Title: Rethinking Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising
Authors: Junyi Li, Zhilu Zhang, Wangmeng Zuo
Abstract: Blind-spot networks (BSN) have been prevalent neural architectures in self-supervised image denoising (SSID). However, most existing BSNs are conducted with convolution layers. Although transformers have shown the potential to overcome the limitations of convolutions in many image restoration tasks, the attention mechanisms may violate the blind-spot requirement, thereby restricting their applicability in BSN. To this end, we propose to analyze and redesign the channel and spatial attentions to meet the blind-spot requirement. Specifically, channel self-attention may leak the blind-spot information in multi-scale architectures, since the downsampling shuffles the spatial feature into channel dimensions. To alleviate this problem, we divide the channel into several groups and perform channel attention separately. For spatial selfattention, we apply an elaborate mask to the attention matrix to restrict and mimic the receptive field of dilated convolution. Based on the redesigned channel and window attentions, we build a Transformer-based Blind-Spot Network (TBSN), which shows strong local fitting and global perspective abilities. Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-theart SSID methods.

Paper number 95:
Title: AxLSTMs: learning self-supervised audio representations with xLSTMs
Authors: Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan
Abstract: While the transformer has emerged as the eminent neural architecture, several independent lines of research have emerged to address its limitations. Recurrent neural approaches have observed a lot of renewed interest, including the extended long short-term memory (xLSTM) architecture, which reinvigorates the original LSTM. However, while xLSTMs have shown competitive performance compared to the transformer, their viability for learning self-supervised general-purpose audio representations has not been evaluated. This work proposes Audio xLSTM (AxLSTM), an approach for learning audio representations from masked spectrogram patches in a self-supervised setting. Pretrained on the AudioSet dataset, the proposed AxLSTM models outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by up to 25% in relative performance across a set of ten diverse downstream tasks while having up to 45% fewer parameters.

Paper number 96:
Title: Algebraic Methods and Computational Strategies for Pseudoinverse-Based MR Image Reconstruction (Pinv-Recon)
Authors: Kylie Yeung, Christine Tobler, Rolf F Schulte, Benjamin White, Anthony McIntyre, Sebastien Serres, Peter Morris, Dorothee Auer, Fergus V Gleeson, Damian J Tyler, James T Grist, Florian Wiesinger
Abstract: Image reconstruction in Magnetic Resonance Imaging (MRI) is fundamentally a linear inverse problem, such that the image can be recovered via explicit pseudoinversion of the encoding matrix by solving $\textbf{data} = \textbf{Encode} \times \textbf{image}$ - a method referred to here as Pinv-Recon. While the benefits of this approach were acknowledged in early studies, the field has historically favored fast Fourier transforms (FFT) and iterative techniques due to perceived computational limitations of the pseudoinversion approach. This work revisits Pinv-Recon in the context of modern hardware, software, and optimized linear algebra routines. We compare various matrix inversion strategies, assess regularization effects, and demonstrate incorporation of advanced encoding physics into a unified reconstruction framework. While hardware advances have already significantly reduced computation time compared to earlier studies, our work further demonstrates that leveraging Cholesky decomposition leads to a two-order-of-magnitude improvement in computational efficiency over previous Singular Value Decomposition-based implementations. Moreover, we demonstrate the versatility of Pinv-Recon on diverse $\textit{in vivo}$ datasets encompassing a range of encoding schemes, starting with low- to medium-resolution functional and metabolic imaging and extending to high-resolution cases. Our findings establish Pinv-Recon as a versatile and robust reconstruction framework that aligns with the increasing emphasis on open-source and reproducible MRI research.

Paper number 97:
Title: Functional Ultrasound Imaging Combined with Machine Learning for Whole-Brain Analysis of Drug-Induced Hemodynamic Changes
Authors: Jared Deighton, Shan Zhong, Kofi Agyeman, Wooseong Choi, Charles Liu, Darrin Lee, Vasileios Maroulas, Vasileios Christopoulos
Abstract: Functional ultrasound imaging (fUSI) is a cutting-edge technology that measures changes in cerebral blood volume (CBV) by detecting backscattered echoes from red blood cells moving within its field of view (FOV). It offers high spatiotemporal resolution and sensitivity, allowing for detailed visualization of cerebral blood flow dynamics. While fUSI has been utilized in preclinical drug development studies to explore the mechanisms of action of various drugs targeting the central nervous system, many of these studies rely on predetermined regions of interest (ROIs). This focus may overlook relevant brain activity outside these specific areas, which could influence the results. To address this limitation, we compared three machine learning approaches-convolutional neural network (CNN), support vector machine (SVM), and vision transformer (ViT)-combined with fUSI to analyze the pharmacodynamics of Dizocilpine (MK-801), a potent non-competitive NMDA receptor antagonist commonly used in preclinical models for memory and learning impairments. While all three machine learning techniques could distinguish between drug and control conditions, CNN proved particularly effective due to their ability to capture hierarchical spatial features while maintaining anatomical specificity. Class activation mapping revealed brain regions, including the prefrontal cortex and hippocampus, that are significantly affected by drug administration, consistent with the literature reporting a high density of NMDA receptors in these areas. Overall, the combination of fUSI and CNN creates a novel analytical framework for examining pharmacological mechanisms, allowing for data-driven identification and regional mapping of drug effects while preserving anatomical context and physiological relevance.

Paper number 98:
Title: DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework
Authors: Yu-Zheng Lin, Qinxuan Shi, Zhanglong Yang, Banafsheh Saber Latibari, Shalaka Satam, Sicong Shao, Soheil Salehi, Pratik Satam
Abstract: Digital twin (DT) technology enables real-time simulation, prediction, and optimization of physical systems, but practical deployment faces challenges from high data requirements, proprietary data constraints, and limited adaptability to evolving conditions. This work introduces DDD-GenDT, a dynamic data-driven generative digital twin framework grounded in the Dynamic Data-Driven Application Systems (DDDAS) paradigm. The architecture comprises the Physical Twin Observation Graph (PTOG) to represent operational states, an Observation Window Extraction process to capture temporal sequences, a Data Preprocessing Pipeline for sensor structuring and filtering, and an LLM ensemble for zero-shot predictive inference. By leveraging generative AI, DDD-GenDT reduces reliance on extensive historical datasets, enabling DT construction in data-scarce settings while maintaining industrial data privacy. The DDDAS feedback mechanism allows the DT to autonomically adapt predictions to physical twin (PT) wear and degradation, supporting DT-aging, which ensures progressive synchronization of DT with PT evolution. The framework is validated using the NASA CNC milling dataset, with spindle current as the monitored variable. In a zero-shot setting, the GPT-4-based DT achieves an average RMSE of 0.479 A (4.79% of the 10 A spindle current), accurately modeling nonlinear process dynamics and PT aging without retraining. These results show that DDD-GenDT provides a generalizable, data-efficient, and adaptive DT modeling approach, bridging generative AI with the performance and reliability requirements of industrial DT applications.

Paper number 99:
Title: Environmental Feature Engineering and Statistical Validation for ML-Based Path Loss Prediction
Authors: Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose
Abstract: Wireless communications rely on path loss modeling, which is most effective when it includes the physical details of the propagation environment. Acquiring this data has historically been challenging, but geographic information systems data is becoming increasingly available with higher resolution and accuracy. Access to such details enables propagation models to more accurately predict coverage and account for interference in wireless deployments. Machine learning-based modeling can significantly support this effort, with feature based approaches allowing for accurate, efficient, and scalable propagation modeling. Building on previous work, we introduce an extended set of features that improves prediction accuracy while, most importantly, proving model generalization through rigorous statistical assessment and the use of test set holdouts.

Paper number 100:
Title: Can Masked Autoencoders Also Listen to Birds?
Authors: Lukas Rauch, René Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz
Abstract: Masked Autoencoders (MAEs) learn rich semantic representations in audio classification through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, revealing the performance limitations of general-domain Audio-MAEs. This work demonstrates that bridging this domain gap domain gap requires full-pipeline adaptation, not just domain-specific pretraining data. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSet's multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAE's prototypical probes outperform linear probing by up to 37 percentage points in mean average precision and narrow the gap to fine-tuning across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains.

Paper number 101:
Title: VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning
Authors: Qianyue Hu, Junyan Wu, Wei Lu, Xiangyang Luo
Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at this https URL.

Paper number 102:
Title: Hyperspectral Image Generation with Unmixing Guided Diffusion Model
Authors: Shiyu Shen, Bin Pan, Ziye Zhang, Zhenwei Shi
Abstract: We address hyperspectral image (HSI) synthesis, a problem that has garnered growing interest yet remains constrained by the conditional generative paradigms that limit sample diversity. While diffusion models have emerged as a state-of-the-art solution for high-fidelity image generation, their direct extension from RGB to hyperspectral domains is challenged by the high spectral dimensionality and strict physical constraints inherent to HSIs. To overcome the challenges, we introduce a diffusion framework explicitly guided by hyperspectral unmixing. The approach integrates two collaborative components: (i) an unmixing autoencoder that projects generation from the image domain into a low-dimensional abundance manifold, thereby reducing computational burden while maintaining spectral fidelity; and (ii) an abundance diffusion process that enforces non-negativity and sum-to-one constraints, ensuring physical consistency of the synthesized data. We further propose two evaluation metrics tailored to hyperspectral characteristics. Comprehensive experiments, assessed with both conventional measures and the proposed metrics, demonstrate that our method produces HSIs with both high quality and diversity, advancing the state of the art in hyperspectral data generation.

Paper number 103:
Title: Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Weighted Intermediate Feature Divergence
Authors: Chun Liu, Bingqian Zhu, Tao Xu, Zheng Zheng, Zheng Li, Wei Yang, Zhigang Han, Jiayao Wang
Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose security challenges to hyperspectral image (HSI) classification based on DNNs. Numerous adversarial attack methods have been designed in the domain of natural images. However, different from natural images, HSIs contains high-dimensional rich spectral information, which presents new challenges for generating adversarial examples. Based on the specific characteristics of HSIs, this paper proposes a novel method to enhance the transferability of the adversarial examples for HSI classification using 3D structure-invariant transformation and weighted intermediate feature divergence. While keeping the HSIs structure invariant, the proposed method divides the image into blocks in both spatial and spectral dimensions. Then, various transformations are applied on each block to increase input diversity and mitigate the overfitting to substitute models. Moreover, a weighted intermediate feature divergence loss is also designed by leveraging the differences between the intermediate features of original and adversarial examples. It constrains the perturbation direction by enlarging the feature maps of the original examples, and assigns different weights to different feature channels to destroy the features that have a greater impact on HSI classification. Extensive experiments demonstrate that the adversarial examples generated by the proposed method achieve more effective adversarial transferability on three public HSI datasets. Furthermore, the method maintains robust attack performance even under defense strategies.

Paper number 104:
Title: Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers
Authors: Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun
Abstract: Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.

Paper number 105:
Title: RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening
Authors: Tao Tang, Chengxu Yang
Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.

Paper number 106:
Title: Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer
Authors: Go Nishikawa, Wataru Nakata, Yuki Saito, Kanami Imamura, Hiroshi Saruwatari, Tomohiko Nakamura
Abstract: We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3: mean opinion score (MOS) prediction for speech with multiple sampling frequencies (SFs). Our submitted model integrates an SF-independent (SFI) convolutional layer into a self-supervised learning (SSL) model to achieve SFI speech feature extraction for MOS prediction. We present some strategies to improve the MOS prediction performance of our model: distilling knowledge from a pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset. Our submission to the AMC 2025 Track 3 ranked the first in one evaluation metric and the fourth in the final ranking. We also report the results of our ablation study to investigate essential factors of our model.

Paper number 107:
Title: Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition
Authors: Xuetao Lin (1 and 2), Tianhao Peng (1 and 2), Peihong Dai (1 and 2), Yu Liang (3), Wenjun Wu (1 and 2) ((1) Beihang University, Beijing, China, (2) SKLCCSE, Beijing, China, (3) Beijing University of Technology, Beijing, China)
Abstract: EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.

Paper number 108:
Title: Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces
Authors: Duc Thien Hua, Mohammadali Mohammadi, Hien Quoc Ngo, Michail Matthaiou
Abstract: We investigate the integration of beyond diagonal reconfigurable intelligent surfaces (BDRISs) into cell free massive multiple input multiple output (CFmMIMO) systems to enhance simultaneous wireless information and power transfer (SWIPT). To simultaneously support two groups of users energy receivers (ERs) and information receivers (IRs) without sacrificing time frequency resources, a subset of access points (APs) is dedicated to serving ERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A protective partial zero forcing precoding technique is implemented at the APs to manage the non coherent interference between the ERs and IRs. Subsequently, closed form expressions for the spectral efficiency of the IRs and the average sum of harvested energy at the ERs are leveraged to formulate a comprehensive optimization problem. This problem jointly optimizes the AP selection, AP power control, and scattering matrix design at the BDRIS, all based on long term statistical channel state information. This challenging problem is then effectively transformed into more tractable forms. To solve these sub problems, efficient algorithms are proposed, including a heuristic search for the scattering matrix design, as well as successive convex approximation and deep reinforcement learning methods for the joint AP mode selection and power control design. Numerical results show that a BDRIS with a group or fully connected architecture achieves significant energy harvesting gains over the conventional diagonal RIS, especially delivering up to a seven fold increase in the average sum of harvested energy when a heuristic based scattering matrix design is employed.

Paper number 109:
Title: Adaptive Lattice-based Motion Planning
Authors: Abhishek Dhar, Sarthak Mishra, Spandan Roy, Daniel Axehill
Abstract: This paper proposes an adaptive lattice-based motion planning solution to address the problem of generating feasible trajectories for systems, represented by a linearly parameterizable non-linear model operating within a cluttered environment. The system model is considered to have uncertain model parameters. The key idea here is to utilize input/output data online to update the model set containing the uncertain system parameter, as well as a dynamic estimated parameter of the model, so that the associated model estimation error reduces over time. This in turn improves the quality of the motion primitives generated by the lattice-based motion planner using a nominal estimated model selected on the basis of suitable criteria. The motion primitives are also equipped with tubes to account for the model mismatch between the nominal estimated model and the true system model, to guarantee collision-free overall motion. The tubes are of uniform size, which is directly proportional to the size of the model set containing the uncertain system parameter. The adaptive learning module guarantees a reduction in the diameter of the model set as well as in the parameter estimation error between the dynamic estimated parameter and the true system parameter. This directly implies a reduction in the size of the implemented tubes and guarantees that the utilized motion primitives go arbitrarily close to the resolution-optimal motion primitives associated with the true model of the system, thus significantly improving the overall motion planning performance over time. The efficiency of the motion planner is demonstrated by a suitable simulation example that considers a drone model represented by Euler-Lagrange dynamics containing uncertain parameters and operating within a cluttered environment.

Paper number 110:
Title: Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks
Authors: Changyuan Qiu, Hangrui Cao, Qihan Ren, Ruiyu Li, Yuqing Qiu
Abstract: Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20]. Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.
    