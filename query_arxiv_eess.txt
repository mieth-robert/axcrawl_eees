
    Selection criteria:
    Papers that are related to power and energy systems or electricity markets.

    Below is a list of papers. For each paper, indicate if it matches the criteria. 
    Respond with a list of the numbers of the matching papers.
    Only write the numbers separated by commas. 
    You should not respond with numbers that are not in the paper list. 

    Paper number 1:
Title: Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays
Authors: Gaurav Singh
Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a critical global health challenge requiring rapid and accurate diagnosis. This study presents a comprehensive comparison of traditional machine learning and state-of-the-art deep learning approaches for automated pneumonia detection using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from conventional machine learning techniques (PCA-based clustering, Logistic Regression, and Support Vector Classification) to advanced deep learning architectures including Convolutional Neural Networks (Modified LeNet, DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT, Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856 pediatric CXR images, we demonstrate that Vision Transformers, particularly the Cross-ViT architecture, achieve superior performance with 88.25% accuracy and 99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that architectural choices impact performance more significantly than model size, with Cross-ViT's 75M parameters outperforming larger models. The study also addresses practical considerations including computational efficiency, training requirements, and the critical balance between precision and recall in medical diagnostics. Our findings suggest that Vision Transformers offer a promising direction for automated pneumonia detection, potentially enabling more rapid and accurate diagnosis during health crises.

Paper number 2:
Title: A Survey on Medical Image Compression: From Traditional to Learning-Based
Authors: Guofeng Tong, Sixuan Liu, Yang Lv, Hanyu Pei, Feng-Lei Fan
Abstract: The exponential growth of medical imaging has created significant challenges in data storage, transmission, and management for healthcare systems. In this vein, efficient compression becomes increasingly important. Unlike natural image compression, medical image compression prioritizes preserving diagnostic details and structural integrity, imposing stricter quality requirements and demanding fast, memory-efficient algorithms that balance computational complexity with clinically acceptable reconstruction quality. Meanwhile, the medical imaging family includes a plethora of modalities, each possessing different requirements. For example, 2D medical image (e.g., X-rays, histopathological images) compression focuses on exploiting intra-slice spatial redundancy, while volumetric medical image faces require handling intra-slice and inter-slice spatial correlations, and 4D dynamic imaging (e.g., time-series CT/MRI, 4D ultrasound) additionally demands processing temporal correlations between consecutive time frames. Traditional compression methods, grounded in mathematical transforms and information theory principles, provide solid theoretical foundations, predictable performance, and high standardization levels, with extensive validation in clinical environments. In contrast, deep learning-based approaches demonstrate remarkable adaptive learning capabilities and can capture complex statistical characteristics and semantic information within medical images. This comprehensive survey establishes a two-facet taxonomy based on data structure (2D vs 3D/4D) and technical approaches (traditional vs learning-based), thereby systematically presenting the complete technological evolution, analyzing the unique technical challenges, and prospecting future directions in medical image compression.

Paper number 3:
Title: Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach
Authors: Thomas Feys, Liesbet Van der Perre, François Rottenberg
Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency chains, higher carrier frequencies and larger bandwidths. As such, digital-to-analog converters (DACs) are becoming a bottleneck in terms of hardware complexity and power consumption. In this work, non-linear precoding for coarsely quantized downlink massive MIMO is studied. Given the NP-hard nature of this problem, a graph neural network (GNN) is proposed that directly outputs the precoded quantized vector based on the channel matrix and the intended transmit symbols. The model is trained in a self-supervised manner, by directly maximizing the achievable rate. To overcome the non-differentiability of the objective function, introduced due to the non-differentiable DAC functions, a straight-through Gumbel-softmax estimation of the gradient is proposed. The proposed method achieves a significant increase in achievable sum rate under coarse quantization. For instance, in the single-user case, the proposed method can achieve the same sum rate as maximum ratio transmission (MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs respectively. This, however, comes at the cost of increased digital signal processing power consumption. When accounting for this, the reduction in overall power consumption holds for a system bandwidth up to 3.5 MHz for baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for higher bandwidths. Notably, indirect effects, which further reduce the power consumption, such as a reduced fronthaul consumption and reduction in other components, are not considered in this analysis.

Paper number 4:
Title: A Leap-on-Success Exhaustive Search Method to Find Optimal Robust Minimum Redundancy Arrays (RMRAs): New Array Configurations for Sensor Counts 11 to 20
Authors: Pradyumna Kunchala, Ashish Patwari
Abstract: Two-fold redundant sparse arrays (TFRAs) are designed to maintain accurate direction estimation even in the event of a single sensor failure, leveraging the deliberate coarray redundancy infused into their design. Robust Minimum Redundancy Arrays (RMRAs), a specialized class of TFRAs, optimize this redundancy to achieve the maximum possible aperture for a given number of sensors. However, finding optimal RMRA configurations is an NP-hard problem, with prior research reporting optimal solutions only for arrays of up to ten sensors. This paper presents newly discovered optimal RMRA configurations for array sizes 11 to 15, identified using a novel Leap-on-Success exhaustive search algorithm that efficiently reduces computational effort by terminating the search upon locating optimal solutions. The robustness of these arrays was validated under all single-element failure scenarios using MATLAB simulations, confirming their superior resilience compared to some existing TFRAs vulnerable to failures at specific sensor positions. Furthermore, near-optimal configurations for array sizes 16 to 20 are also reported, highlighting the potential applicability of the proposed method for larger array designs given sufficient computational resources. This work not only advances the state-of-the-art in RMRA design but also introduces an effective search methodology that can be leveraged for future explorations in array configuration optimization.

Paper number 5:
Title: Standardized Evaluation of Fetal Phonocardiography Processing Methods
Authors: Kristóf Müller, Janka Hatvani, Márton Áron Goda, Miklós Koller
Abstract: Motivation. Phonocardiography can give access to the fetal heart rate as well as direct heart sound data, and is entirely passive, using no radiation of any kind. Approach. We discuss the currently available methods for fetal heart sound detection and heart rate estimation and compare them using a common benchmarking platform and a pre-selected testing dataset. Compared to previous reviews, we evaluated the discussed methods in a standardized manner for a fair comparison. Our tests included tolerance-based detection accuracy, error rates for label insertions, deletions, and substitutions, and statistical measures for heart rate mean square error. Results. Based on our results, there is no definite best method that can achieve the highest scores in all of the tests, and simpler methods could perform comparably to more complex ones. The best model for first heart sound detection achieved 97.6% F1-score, 97.4% positive predictive value, and 12.2+-8.0 ms mean absolute error. In terms of second heart sound detection the best model had 91.4% F1-score, 91.3% positive predictive value, and 17.3+-12.2 ms mean absolute error. For fetal heart rate a 0.644 mean square error was achieved by the best method. Significance. Our main conclusion is that further standardization is required in fetal heart rate and heart sound detection method evaluation. The tests and algorithm implementations are openly available at: this https URL.

Paper number 6:
Title: Waterfilling at the Edge: Optimal Percentile Resource Allocation via Risk-Averse Reduction
Authors: Gokberk Yaylali, Ahmad Ali Khan, Dionysios S. Kalogerias
Abstract: We address deterministic resource allocation in point-to-point multi-terminal AWGN channels without inter-terminal interference, with particular focus on optimizing quantile transmission rates for cell-edge terminal service. Classical utility-based approaches -- such as minimum rate, sumrate, and proportional fairness -- are either overconservative, or inappropriate, or do not provide a rigorous and/or interpretable foundation for fair rate optimization at the edge. To overcome these challenges, we employ Conditional Value-at-Risk (CVaR), a popular coherent risk measure, and establish its equivalence with the sum-least-$\alpha$th-quantile (SL$\alpha$Q) utility. This connection enables an exact convex reformulation of the SL$\alpha$Q maximization problem, facilitating analytical tractability and precise and interpretable control over cell-edge terminal performance. Utilizing Lagrangian duality, we provide (for the first time) parameterized closed-form solutions for the optimal resource policy -- which is of waterfilling-type -- as well as the associated (auxiliary) Value-at-Risk variable. We further develop a novel inexact dual subgradient descent algorithm of minimal complexity to determine globally optimal resource policies, and we rigorously establish its convergence. The resulting edge waterfilling algorithm iteratively and efficiently allocates resources while explicitly ensuring transmission rate fairness across (cell-edge) terminals. Several (even large-scale) numerical experiments validate the effectiveness of the proposed method for enabling robust quantile rate optimization at the edge.

Paper number 7:
Title: Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification
Authors: Chetan Madan, Aarjav Satia, Soumen Basu, Pankaj Gupta, Usha Dutta, Chetan Arora
Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for self-supervised representation learning in natural images, where models are pre-trained to reconstruct masked patches with a pixel-wise mean squared error (MSE) between original and reconstructed RGB values as the loss. We observe that MSE encourages blurred image re-construction, but still works for natural images as it preserves dominant edges. However, in medical imaging, when the texture cues are more important for classification of a visual abnormality, the strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM) feature in Radiomics studies, we propose a novel MAE based pre-training framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM captures intensity and spatial relationships in an image, hence proposed loss helps preserve morphological features. Further, we propose a novel formulation to convert matching GLCM matrices into a differentiable loss function. We demonstrate that unsupervised pre-training on medical images with the proposed GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms the current state-of-the-art across four tasks - gallbladder cancer detection from ultrasound images by 2.1%, breast cancer detection from ultrasound by 3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by 0.6%. Source code and pre-trained models are available at: this https URL.

Paper number 8:
Title: Data-Driven Safety Certificates of Infinite Networks with Unknown Models and Interconnection Topologies
Authors: Mahdieh Zaker, Amy Nejati, Abolfazl Lavaei
Abstract: Infinite networks are complex interconnected systems comprising a countably infinite number of subsystems, where counting them precisely poses a significant challenge due to the seemingly endless interconnected nature of the network (e.g., counting vehicles on the road). In such scenarios, the presence of infinitely many subsystems within the network renders the existing analysis frameworks tailored for finite networks inapplicable to infinite ones. This paper is concerned with offering a data-driven approach, within a compositional framework, for the safety certification of infinite networks with both unknown mathematical models and interconnection topologies. Given the immense computational complexity stemming from the extensive dimension of infinite networks, our approach capitalizes on the joint dissipativity-type properties of subsystems, characterized by storage certificates. We introduce innovative compositional data-driven conditions to construct a barrier certificate for the infinite network leveraging storage certificates of its unknown subsystems derived from data, while offering correctness guarantees across the network safety. We demonstrate that our compositional data-driven reasoning eliminates the requirement for checking the traditional dissipativity condition, which typically mandates precise knowledge of the interconnection topology. In addition, while existing data-driven literature demonstrates an exponential trend in sample complexity with respect to network size, we showcase that our compositional strategy notably reduces it to a linear scale in terms of the number of subsystems. We illustrate our data-driven results on two physical infinite networks with unknown models and interconnection topologies.

Paper number 9:
Title: Approximate solutions to games of ordered preference
Authors: Pau de las Heras Molins, Eric Roy-Almonacid, Dong Ho Lee, Lasse Peters, David Fridovich-Keil, Georgios Bakirtzis
Abstract: Autonomous vehicles must balance ranked objectives, such as minimizing travel time, ensuring safety, and coordinating with traffic. Games of ordered preference effectively model these interactions but become computationally intractable as the time horizon, number of players, or number of preference levels increase. While receding horizon frameworks mitigate long-horizon intractability by solving sequential shorter games, often warm-started, they do not resolve the complexity growth inherent in existing methods for solving games of ordered preference. This paper introduces a solution strategy that avoids excessive complexity growth by approximating solutions using lexicographic iterated best response (IBR) in receding horizon, termed "lexicographic IBR over time." Lexicographic IBR over time uses past information to accelerate convergence. We demonstrate through simulated traffic scenarios that lexicographic IBR over time efficiently computes approximate-optimal solutions for receding horizon games of ordered preference, converging towards generalized Nash equilibria.

Paper number 10:
Title: Dual RIS-Assisted Monostatic L-Band Radar Target Detection in NLoS Scenarios
Authors: Salman Liaquat, Ijaz Haider Naqvi, Nor Muzlifah Mahyuddin
Abstract: The use of a single Reconfigurable Intelligent Surface (RIS) to boost the signal-to-noise ratio (SNR) at the radar offers significant improvement in detecting targets, especially in non-line-of-sight (NLoS) scenarios. However, there are scenarios where no path exists between the radar and the target, even with a single RIS-assisted radar, due to other present obstacles. This paper derives an expression for SNR in target detection scenarios where dual RISs assist a monostatic radar in NLoS situations. We calculate the power received at the radar through a dual RIS configuration. We show that the SNR performance of RIS-assisted radars can improve with known locations of the radar and RISs. Our results demonstrate that the required accuracy in target localization can be achieved by controlling the number of RISs, the number of unit cells in each RIS, and properly selecting the locations of RISs to cover the desired region. The performance of dual RIS-assisted radar systems can surpass that of single RIS-assisted radar systems under favourable alignment and sufficiently large RIS sizes.

Paper number 11:
Title: Real-Time Foreign Object Recognition Based on Improved Wavelet Scattering Deep Network and Edge Computing
Authors: He Zhichao, Shen Xiangyu, Zhang Yong, Xie Nan
Abstract: The increasing penetration rate of new energy in the power system has put forward higher requirements for the operation and maintenance of substations and transmission lines. Using the Unmanned Aerial Vehicles (UAV) to identify foreign object in real time can quickly and effectively eliminate potential safety hazards. However, due to the limited computation power, the captured image cannot be real-time processed on edge devices in UAV locally. To overcome this problem, a lightweight model based on an improved wavelet scatter deep network is proposed. This model contains improved wavelet scattering network for extracting the scatter coefficients and modulus coefficients of image single channel, replacing the role of convolutional layer and pooling layer in convolutional neural network. The following 3 fully connected layers, also constituted a simplified Multilayer Perceptron (MLP), are used to classify the extracted features. Experiments prove that the model constructed with biorthogonal wavelets basis is able to recognize and classify the foreign object in edge devices such as Raspberry Pi and Jetson Nano, with accuracy higher than 90% and inference time less than 7ms for 720P (1280*720) images. Further experiments demonstrate that the recognition accuracy of our model is 1.1% higher than YOLOv5s and 0.3% higher than YOLOv8s.

Paper number 12:
Title: Using Continual Learning for Real-Time Detection of Vulnerable Road Users in Complex Traffic Scenarios
Authors: Faryal Aurooj Nasir, Salman Liaquat, Nor Muzlifah Mahyuddin
Abstract: Pedestrians and bicyclists are among the vulnerable road users (VRUs) that are inherently exposed to intricate traffic scenarios, which puts them at increased risk of sustaining injuries or facing fatal outcomes. This study presents an intelligent adaptive system that uses the YOLOv8-Dynamic (YOLOv8-D) algorithm that detects vulnerable road users and adapts in real time to prevent accidents before they occur. We select YOLOv8x as the detector by comparing it with other state-of-the-art object detection models, including Faster-RCNN, YOLOv5, YOLOv7, and variants. Compared to YOLOv5x, YOLOv8x shows improvements of 12.14% in F1 score and 45.61% in mean Average Precision (mAP). Against YOLOv7x, the improvements are 21.26% in F1 score and 128.44% in mAP. Our algorithm integrates continual learning ability in the architecture of the YOLOv8 detector to adjust to evolving road conditions flexibly, ensuring adaptability across multiple dataset domains and facilitating continuous enhancement of detection and tracking accuracy for VRUs, embracing the dynamic nature of real-world environments. In our proposed framework, we optimized the gradient descent mechanism of YOLOv8 model and train our optimized algorithm on two statistically different datasets in terms of image viewpoint and number of classes to achieve a 21.08% improvement in F1 score and a 31.86% improvement in mAP as compared to a custom YOLOv8 framework trained on a new dataset, thus overcoming the issue of catastrophic forgetting, which occurs when deep models are trained on statistically different types of datasets.

Paper number 13:
Title: Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems
Authors: Sehyun Ryu, Hyun Jong Yang
Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as the growing number of antennas in modern massive MIMO systems substantially increases the channel state information (CSI) feedback demand in frequency division duplex (FDD) systems. To address this, extensive research has focused on CSI compression and prediction, with neural network-based approaches gaining momentum and being considered for integration into the 3GPP 5G-Advanced standards. While deep learning has been effectively applied to CSI-limited beamforming and handover optimization, reference signal allocation under such constraints remains surprisingly underexplored. To fill this gap, we introduce the concept of channel prediction-based reference signal allocation (CPRS), which jointly optimizes channel prediction and DM-RS allocation to improve data throughput without requiring CSI feedback. We further propose a standards-compliant ViViT/CNN-based architecture that implements CPRS by treating evolving CSI matrices as sequential image-like data, enabling efficient and adaptive transmission in dynamic environments. Simulation results using ray-tracing channel data generated in NVIDIA Sionna validate the proposed method, showing up to 36.60% throughput improvement over benchmark strategies.

Paper number 14:
Title: Physics-Informed Transfer Learning for Data-Driven Sound Source Reconstruction in Near-Field Acoustic Holography
Authors: Xinmeng Luan, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
Abstract: We propose a transfer learning framework for sound source reconstruction in Near-field Acoustic Holography (NAH), which adapts a well-trained data-driven model from one type of sound source to another using a physics-informed procedure. The framework comprises two stages: (1) supervised pre-training of a complex-valued convolutional neural network (CV-CNN) on a large dataset, and (2) purely physics-informed fine-tuning on a single data sample based on the Kirchhoff-Helmholtz integral. This method follows the principles of transfer learning by enabling generalization across different datasets through physics-informed adaptation. The effectiveness of the approach is validated by transferring a pre-trained model from a rectangular plate dataset to a violin top plate dataset, where it shows improved reconstruction accuracy compared to the pre-trained model and delivers performance comparable to that of Compressive-Equivalent Source Method (C-ESM). Furthermore, for successful modes, the fine-tuned model outperforms both the pre-trained model and C-ESM in accuracy.

Paper number 15:
Title: Array-Aware Ambisonics and HRTF Encoding for Binaural Reproduction With Wearable Arrays
Authors: Yhonatan Gayer, Vladimir Tourbabin, Zamir Ben Hur, David Lou Alon, Boaz Rafaely
Abstract: This work introduces a novel method for binaural reproduction from arbitrary microphone arrays, based on array-aware optimization of Ambisonics encoding through Head-Related Transfer Function (HRTF) pre-processing. The proposed approach integrates array-specific information into the HRTF processing pipeline, leading to improved spatial accuracy in binaural rendering. Objective evaluations demonstrate superior performance under simulated wearable-array and head rotations compared to conventional Ambisonics encoding method. A listening experiment further confirms that the method achieves significantly higher perceptual ratings in both timbre and spatial quality. Fully compatible with standard Ambisonics, the proposed method offers a practical solution for spatial audio rendering in applications such as virtual reality, augmented reality, and wearable audio capture.

Paper number 16:
Title: Optimizing Fluid Antenna Configurations for Constructive Interference Precoding
Authors: Wenxuan Sun, Mingjie Shao, Luteng Zhu, Yao Ge, Tong Zhang, Zhi Liu
Abstract: The fluid antenna system (FAS) has emerged as a new physical-layer concept to provide enhanced propagation conditions for multiuser multiple-input multiple-output (MIMO) communications over conventional fixed arrays. This work focuses on minimizing the maximum symbol error probability (SEP) under $M$-ary phase shift keying (MPSK) signaling in a multiuser downlink equipped with FAS, where each antenna moves within nonoverlapping intervals. This specific problem of joint SEP minimization with FAS and constructive interference (CI) precoding has not been previously addressed. The resulting problem turns out to be a nonconvex and nonsmooth optimization challenge. We transform the SEP minimization problem into a safety margin maximization problem in constructive interference precoding. Then, we customize a smoothing technique and a block coordinate descent (BCD) algorithm, with emphasis on low computational complexity. Simulation results show that our approach can reduce bit error rate (BER) compared to both the fixed arrays and FAS designed by existing particle swarm optimization (PSO). Also, our approach shows attractively low computational complexity compared to PSO benchmarks.

Paper number 17:
Title: Optimal Honeypot Ratio and Convergent Fictitious-Play Learning in Signaling Games for CPS Defense
Authors: Yueyue Xu, Yuewei Chen, Lin Wang, Zhaoyang Cheng, Xiaoming Hu
Abstract: Cyber-Physical Systems (CPSs) are facing a fast-growing wave of attacks. To achieve effective proactive defense, this paper models honeypot deployment as a gamma-fixed signaling game in which node liveness serves as the only signal and normal-node signal gamma is exogenously fixed. We define the gamma-perfect Bayesian-Nash equilibrium (gamma-PBNE). Analytical expressions are obtained for all gamma-PBNEs, revealing three distinct equilibrium regimes that depend on the priori honeypot ratio. Furthermore, the optimal honeypot ratio and signaling strategy that jointly maximize the network average utility are obtained. To capture strategic interaction over time, we develop a discrete-time fictitious-play algorithm that couples Bayesian belief updates with empirical best responses. We prove that, as long as the honeypot ratio is perturbed within a non-degenerate neighbourhood of the optimum, every fictitious-play path converges to the defender-optimal gamma-PBNE. Numerical results confirm the effectiveness of the proposed method and demonstrate its applicability to CPS defense.

Paper number 18:
Title: Fairness-Aware Secure Integrated Sensing and Communications with Fractional Programming
Authors: Ali Khandan Boroujeni, Kuranage Roche Rayan Ranasinghe, Giuseppe Thadeu Freitas de Abreu, Stefan Köpsell, Ghazal Bagheri, Rafael F. Schaefer
Abstract: We propose a novel secure integrated sensing and communications (ISAC) system designed to serve multiple communication users (CUs) and targets. To that end, we formulate an optimization problem that maximizes the secrecy rate under constraints balancing both communication and sensing requirements. To enhance fairness among users, an entropy-regularized fairness metric is introduced within the problem framework. We then propose a solution employing an accelerated quadratic transform (QT) with a non-homogeneous bound to iteratively solve two subproblems, thereby effectively optimizing the overall objective. This approach ensures robust security and fairness in resource allocation for ISAC systems. Finally, simulation results verify the performance gains in terms of average secrecy rate, average data rate, and beam gain.

Paper number 19:
Title: Optimal Sensor Scheduling and Selection for Continuous-Discrete Kalman Filtering with Auxiliary Dynamics
Authors: Mohamad Al Ahdab, John Leth, Zheng-Hua Tan
Abstract: We study the Continuous-Discrete Kalman Filter (CD-KF) for State-Space Models (SSMs) where continuous-time dynamics are observed via multiple sensors with discrete, irregularly timed measurements. Our focus extends to scenarios in which the measurement process is coupled with the states of an auxiliary SSM. For instance, higher measurement rates may increase energy consumption or heat generation, while a sensor's accuracy can depend on its own spatial trajectory or that of the measured target. Each sensor thus carries distinct costs and constraints associated with its measurement rate and additional constraints and costs on the auxiliary state. We model measurement occurrences as independent Poisson processes with sensor-specific rates and derive an upper bound on the mean posterior covariance matrix of the CD-KF along the mean auxiliary state. The bound is continuously differentiable with respect to the measurement rates, which enables efficient gradient-based optimization. Exploiting this bound, we propose a finite-horizon optimal control framework to optimize measurement rates and auxiliary-state dynamics jointly. We further introduce a deterministic method for scheduling measurement times from the optimized rates. Empirical results in state-space filtering and dynamic temporal Gaussian process regression demonstrate that our approach achieves improved trade-offs between resource usage and estimation accuracy.

Paper number 20:
Title: Fast and Efficient Implementation of the Maximum Likelihood Estimation for the Linear Regression with Gaussian Model Uncertainty
Authors: Ruohai Guo, Jiang Zhu, Xing Jiang, Fengzhong Qu
Abstract: The linear regression model with a random variable (RV) measurement matrix, where the mean of the random measurement matrix has full column rank, has been extensively studied. In particular, the quasiconvexity of the maximum likelihood estimation (MLE) problem was established, and the corresponding Cramer-Rao bound (CRB) was derived, leading to the development of an efficient bisection-based algorithm known as RV-ML. In contrast, this work extends the analysis to both overdetermined and underdetermined cases, allowing the mean of the random measurement matrix to be rank-deficient. A remarkable contribution is the proof that the equivalent MLE problem is convex and satisfies strong duality, strengthening previous quasiconvexity results. Moreover, it is shown that in underdetermined scenarios, the randomness in the measurement matrix can be beneficial for estimation under certain conditions. In addition, a fast and unified implementation of the MLE solution, referred to as generalized RV-ML (GRV-ML), is proposed, which handles a more general case including both underdetermined and overdetermined systems. Extensive numerical simulations are provided to validate the theoretical findings.

Paper number 21:
Title: Sensing Accuracy Optimization for Multi-UAV SAR Interferometry with Data Offloading
Authors: Mohamed-Amine Lahmeri, Pouya Fakharizadeh, Víctor Mustieles-Pérez, Martin Vossiek, Gerhard Krieger, Robert Schober
Abstract: The integration of unmanned aerial vehicles (UAVs) with radar imaging sensors has revolutionized the monitoring of dynamic and local Earth surface processes by enabling high-resolution and cost-effective remote sensing. This paper investigates the optimization of the sensing accuracy of a UAV swarm deployed to perform multi-baseline interferometric synthetic aperture radar (InSAR) sensing. In conventional single-baseline InSAR systems, only one synthetic aperture radar (SAR) antenna pair acquires two SAR images from two distinct angles to generate a digital elevation model (DEM) of the target area. However, multi-baseline InSAR extends this concept by aggregating multiple acquisitions from different angles, thus, significantly enhancing the vertical accuracy of the DEM. The heavy computations required for this process are performed on the ground and, therefore, the radar data is transmitted in real time to a ground station (GS) via a frequency-division multiple access (FDMA) air-to-ground backhaul link. This work focuses on improving the sensing precision by minimizing the height error of the averaged DEM while simultaneously ensuring sensing and communication quality-of-service (QoS). To this end, the UAV formation, velocity, and communication power allocation are jointly optimized using evolutionary algorithms (EAs). Our approach is benchmarked against established optimization methods, including genetic algorithms (GAs), simulated annealing (SA), and deep reinforcement learning (DRL) techniques. Numerical results show that the proposed solution outperforms these baseline schemes and achieves sub-decimeter vertical accuracy in several scenarios. These findings underline the potential of coordinated UAV swarms for delivering high-precision and real-time Earth observations through radar interferometry.

Paper number 22:
Title: P.808 Multilingual Speech Enhancement Testing: Approach and Results of URGENT 2025 Challenge
Authors: Marvin Sach, Yihui Fu, Kohei Saijo, Wangyou Zhang, Samuele Cornell, Robin Scheibler, Chenda Li, Anurag Kumar, Wei Wang, Yanmin Qian, Shinji Watanabe, Tim Fingscheidt
Abstract: In speech quality estimation for speech enhancement (SE) systems, subjective listening tests so far are considered as the gold standard. This should be even more true considering the large influx of new generative or hybrid methods into the field, revealing issues of some objective metrics. Efforts such as the Interspeech 2025 URGENT Speech Enhancement Challenge also involving non-English datasets add the aspect of multilinguality to the testing procedure. In this paper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective listening test method. A first novel contribution is our proposed process of localizing both text and audio components of Naderi and Cutler's implementation of crowdsourced subjective absolute category rating (ACR) listening tests involving text-to-speech (TTS). Further, we provide surprising analyses of and insights into URGENT Challenge results, tackling the reliability of (P.808) ACR subjective testing as gold standard in the age of generative AI. Particularly, it seems that for generative SE methods, subjective (ACR MOS) and objective (DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone fidelity metrics to reliably detect hallucinations. Finally, in the accepted version, we will release our localization scripts and methods for easy deployment for new multilingual speech enhancement subjective evaluations according to ITU-T P.808.

Paper number 23:
Title: Moving Beyond Marginal Carbon Intensity: A Poor Metric for Both Carbon Accounting and Grid Flexibility
Authors: Philipp Wiesner, Odej Kao
Abstract: Marginal Carbon Intensity (MCI) has been promoted as an effective metric for carbon-aware computing. Although it is already considered as impractical for carbon accounting purposes, many still view it as valuable when optimizing for grid flexibility by incentivizing electricity usage during curtailment periods. In this statement paper, we argue that MCI is neither reliable nor actionable for either purpose. We outline its fundamental limitations, including non-observability, reliance on opaque predictive models, and the lack of verifiability. Moreover, MCI fails to reflect curtailment caused by high-carbon sources and offers no insight into the quantity of available excess power. We advocate moving beyond MCI and instead call for research on more actionable metrics, such as direct reporting of excess power, explicit modeling of energy storage and grid stability, and integration with emerging granular renewable energy certificate markets.

Paper number 24:
Title: Sparse Regression Codes exploit Multi-User Diversity without CSI
Authors: V S V Sandeep, Sai Dinesh Kancharana, Arun Pachai Kannu
Abstract: We study sparse regression codes (SPARC) for multiple access channels with multiple receive antennas, in non-coherent flat fading channels. We propose a novel practical decoder, referred to as maximum likelihood matching pursuit (MLMP), which greedily finds the support of the codewords of users with partial maximum likelihood metrics. As opposed to the conventional successive-cancellation based greedy algorithms, MLMP works as a successive-combining energy detector. We also propose MLMP modifications to improve the performance at high code rates. Our studies in short block lengths show that, even without any channel state information, SPARC with MLMP decoder achieves multi-user diversity in some scenarios, giving better error performance with multiple users than that of the corresponding single-user case. We also show that SPARC with MLMP performs better than conventional sparse recovery algorithms and pilot-aided transmissions with polar codes.

Paper number 25:
Title: Inverse Optimal Control with Constraint Relaxation
Authors: Rahel Rickenbach, Amon Lahr, Melanie N. Zeilinger
Abstract: Inverse optimal control (IOC) is a promising paradigm for learning and mimicking optimal control strategies from capable demonstrators, or gaining a deeper understanding of their intentions, by estimating an unknown objective function from one or more corresponding optimal control sequences. When computing estimates from demonstrations in environments with safety-preserving inequality constraints, acknowledging their presence in the chosen IOC method is crucial given their strong influence on the final control strategy. However, solution strategies capable of considering inequality constraints, such as the inverse Karush-Kuhn-Tucker approach, rely on their correct activation and fulfillment; a restrictive assumption when dealing with noisy demonstrations. To overcome this problem, we leverage the concept of exact penalty functions for IOC and show preservation of estimation accuracy. Considering noisy demonstrations, we then illustrate how the usage of penalty functions reduces the number of unknown variables and how their approximations enhance the estimation method's capacity to account for wrong constraint activations within a polytopic-constrained environment. The proposed method is evaluated for three systems in simulation, outperforming traditional relaxation approaches for noisy demonstrations.

Paper number 26:
Title: Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty
Authors: Christos N. Efrem, Ioannis Krikidis
Abstract: We study the joint power allocation and reflecting element (RE) activation to maximize the energy efficiency (EE) in communication systems assisted by an intelligent reflecting surface (IRS), taking into account imperfections in channel state information (CSI). The robust optimization problem is mixed integer, i.e., the optimization variables are continuous (transmit power) and discrete (binary states of REs). In order to solve this challenging problem we develop two algorithms. The first one is an alternating optimization (AO) method that attains a suboptimal solution with low complexity, based on the Lambert W function and a dynamic programming (DP) algorithm. The second one is a branch-and-bound (B&B) method that uses AO as its subroutine and is formally guaranteed to achieve a globally optimal solution. Both algorithms do not require any external optimization solver for their implementation. Furthermore, numerical results show that the proposed algorithms outperform the baseline schemes, AO achieves near-optimal performance in most cases, and B&B has low computational complexity on average.

Paper number 27:
Title: U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV
Authors: Hongbo Ye, Fenghe Tang, Peiang Zhao, Zhen Huang, Dexin Zhao, Minghao Bian, S.Kevin Zhou
Abstract: Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and maintaining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at this https URL.

Paper number 28:
Title: A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification
Authors: Mingcong Li
Abstract: Solving chance-constrained optimal control problems for systems subject to non-stationary uncertainties is a significant this http URL robust model predictive control (MPC) often yields excessive conservatism by relying on static worst-case assumptions, while standard stochastic MPC methods struggle when underlying uncertainty distributions are unknown a this http URL article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a hierarchical architecture that systematically orchestrates a novel synthesis of proactive, learning-based risk assessment and reactive risk regulation. The framework employs a medium-frequency risk assessment engine, which leverages Gaussian process regression and active learning, to construct a tight, data-driven characterization of the prediction error set from operational this http URL, a low-timescale outer loop implements a self-correcting update law for an adaptive safety margin to precisely regulate the empirical risk and compensate for unmodeled this http URL dual-timescale adaptation enables the system to rigorously satisfy chance constraints with a user-defined probability, while minimizing the conservatism inherent in traditional this http URL formally establish that the interplay between these adaptive components guarantees recursive feasibility and ensures the closed-loop system satisfies the chance constraints up to a user-defined risk level with high this http URL experiments on a benchmark DC-DC converter under non-stationary parametric uncertainties demonstrate that our framework precisely achieves the target risk level, resulting in a significantly lower average cost compared to state-of-the-art robust and stochastic MPC strategies.

Paper number 29:
Title: Towards Reliable Objective Evaluation Metrics for Generative Singing Voice Separation Models
Authors: Paul A. Bereuter, Benjamin Stahl, Mark D. Plumbley, Alois Sontacchi
Abstract: Traditional Blind Source Separation Evaluation (BSS-Eval) metrics were originally designed to evaluate linear audio source separation models based on methods such as time-frequency masking. However, recent generative models may introduce nonlinear relationships between the separated and reference signals, limiting the reliability of these metrics for objective evaluation. To address this issue, we conduct a Degradation Category Rating listening test and analyze correlations between the obtained degradation mean opinion scores (DMOS) and a set of objective audio quality metrics for the task of singing voice separation. We evaluate three state-of-the-art discriminative models and two new competitive generative models. For both discriminative and generative models, intrusive embedding-based metrics show higher correlations with DMOS than conventional intrusive metrics such as BSS-Eval. For discriminative models, the highest correlation is achieved by the MSE computed on Music2Latent embeddings. When it comes to the evaluation of generative models, the strongest correlations are evident for the multi-resolution STFT loss and the MSE calculated on MERT-L12 embeddings, with the latter also providing the most balanced correlation across both model types. Our results highlight the limitations of BSS-Eval metrics for evaluating generative singing voice separation models and emphasize the need for careful selection and validation of alternative evaluation metrics for the task of singing voice separation.

Paper number 30:
Title: Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection
Authors: Buddhi Wijenayake, Athulya Ratnayake, Praveen Sumanasekara, Nichula Wasalathilaka, Mathivathanan Piratheepan, Roshan Godaliyadda, Mervyn Ekanayake, Vijitha Herath
Abstract: Remote sensing change detection is vital for monitoring environmental and urban transformations but faces challenges like manual feature extraction and sensitivity to noise. Traditional methods and early deep learning models, such as convolutional neural networks (CNNs), struggle to capture long-range dependencies and global context essential for accurate change detection in complex scenes. While Transformer-based models mitigate these issues, their computational complexity limits their applicability in high-resolution remote sensing. Building upon ChangeMamba architecture, which leverages state space models for efficient global context modeling, this paper proposes precision fusion blocks to capture channel-wise temporal variations and per-pixel differences for fine-grained change detection. An enhanced decoder pipeline, incorporating lightweight channel reduction mechanisms, preserves local details with minimal computational cost. Additionally, an optimized loss function combining Cross Entropy, Dice and Lovasz objectives addresses class imbalance and boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+, and WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and overall accuracy compared to state-of-the-art methods, highlighting the approach's robustness for remote sensing change detection. For complete transparency, the codes and pretrained models are accessible at this https URL

Paper number 31:
Title: Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing
Authors: Sameera Bharadwaja H., Siddhrath Jandial, Shashank S. Agashe, Rajesh Kumar Reddy Moore, Youngkwan Kim
Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber matching in the context of a semiconductor manufacturing equipment. Traditional TTTM approaches utilize static configuration data or depend on a golden reference which are difficult to obtain in a commercial manufacturing line. Further, existing methods do not extend very well to a heterogeneous setting, where equipment are of different make-and-model, sourced from different equipment vendors. We propose novel TTTM analysis pipelines to overcome these issues. We hypothesize that a mismatched equipment would have higher variance and/or higher number of modes in the data. Our best univariate method achieves a correlation coefficient >0.95 and >0.5 with the variance and number of modes, respectively showing that the proposed methods are effective. Also, the best multivariate method achieves a correlation coefficient >0.75 with the top-performing univariate methods, showing its effectiveness. Finally, we analyze the sensitivity of the multivariate algorithms to the algorithm hyper-parameters.

Paper number 32:
Title: AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography
Authors: Ruixi Zheng, Wei Zhang, Yijie Li, Xi Zhu, Zhou Lan, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Lauren J. O'Donnell, Fan Zhang
Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo mapping of the brain's white matter (WM) connections. Tractometry is an advanced tractography analysis technique for along-tract profiling to investigate the morphology and microstructural properties along the fiber tracts. Tractometry has become an essential tool for studying local along-tract differences between different populations (e.g., health vs disease). In this study, we propose a novel atlas-guided fine-scale tractometry method, namely AGFS-Tractometry, that leverages tract spatial information and permutation testing to enhance the along-tract statistical analysis between populations. There are two major contributions in AGFS-Tractometry. First, we create a novel atlas-guided tract profiling template that enables consistent, fine-scale, along-tract parcellation of subject-specific fiber tracts. Second, we propose a novel nonparametric permutation testing group comparison method to enable simultaneous analysis across all along-tract parcels while correcting for multiple comparisons. We perform experimental evaluations on synthetic datasets with known group differences and in vivo real data. We compare AGFS-Tractometry with two state-of-the-art tractometry methods, including Automated Fiber-tract Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in detecting local WM differences. In the real data analysis experiments, AGFS-Tractometry can identify more regions with significant differences, which are anatomically consistent with the existing literature. Overall, these demonstrate the ability of AGFS-Tractometry to detect subtle or spatially localized WM group-level differences. The created tract profiling template and related code are available at: this https URL.

Paper number 33:
Title: A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights
Authors: Obumneme Nwafor, Chioma Nwafor, Amro Zakaria, Nkechi Nwankwo
Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to meet over 90% of its drinking water needs. Desalination processes are highly energy intensive and account for approximately 15% of the UAE's electricity consumption, contributing to over 22% of the country's energy-related CO2 emissions. Moreover, these processes face significant sustainability challenges in the face of climate uncertainties such as rising seawater temperatures, salinity, and aerosol optical depth (AOD). AOD greatly affects the operational and economic performance of solar-powered desalination systems through photovoltaic soiling, membrane fouling, and water turbidity cycles. This study proposes a novel pipelined two-stage predictive modelling architecture: the first stage forecasts AOD using satellite-derived time series and meteorological data; the second stage uses the predicted AOD and other meteorological factors to predict desalination performance efficiency losses. The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations) was used to reveal key drivers of system degradation. Furthermore, this study proposes a dust-aware rule-based control logic for desalination systems based on predicted values of AOD and solar efficiency. This control logic is used to adjust the desalination plant feed water pressure, adapt maintenance scheduling, and regulate energy source switching. To enhance the practical utility of the research findings, the predictive models and rule-based controls were packaged into an interactive dashboard for scenario and predictive analytics. This provides a management decision-support system for climate-adaptive planning.

Paper number 34:
Title: Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music
Authors: Maziar Kanani, Sean O Leary, James McDermott
Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian classical music, and algorithms for structural parsing of this music, and generation of variations. The corpus comprises MIDI files and data sheets of Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian classical music. Furthermore, we apply our previously-introduced algorithm for parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much Western music, this type of non-metric music does not follow bar-centric organisation. The non-metric organisation can be captured well by our parsing algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and phrases. These grammar representations can be useful for educational and ethnomusicological purposes. We also further develop a previously-introduced method of creating melodic variations (Kanani et al., 2023b). After parsing an existing tune to produce a grammar, by applying mutations to this grammar, we generate a new grammar. Expanding this new version yields a variation of the original tune. Variations are assessed by a domain-expert listener. Additionally, we conduct a statistical analysis of mutation with different representation setups for our parsing and generation algorithms. The overarching conclusion is that the system successfully produces acceptable variations post-mutation. While our case study focuses on Iranian classical music, the methodology can be adapted for Arabic or Turkish classical music.

Paper number 35:
Title: Parsing Musical Structure to Enable Meaningful Variations
Authors: Maziar Kanani, Sean O Leary, James McDermott
Abstract: This paper presents a novel rule-based approach for generating music by varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [ 1], that is a structure representing all repetitions in the tune. The Sequitur algorithm [2 ] is used for this. The result is a grammar. We then carry out mutation on the grammar, rather than on a tune directly. There are potentially 19 types of mutations such as adding, removing, swapping or reversing parts of the grammar that can be applied to the grammars. The system employs one of the mutations randomly in this step to automatically manipulate the grammar. Following the mutation, we need to expand the grammar which returns a new tune. The output after 1 or more mutations will be a new tune related to the original tune. Our study examines how tunes change gradually over the course of multiple mutations. Edit distances, structural complexity and length of the tunes are used to show how a tune is changed after multiple mutations. In addition, the size of effect of each mutation type is analyzed. As a final point, we review the musical aspect of the output tunes. It should be noted that the study only focused on generating new pitch sequences. The study is based on an Irish traditional tune dataset and a list of integers has been used to represent each tune's pitch values.

Paper number 36:
Title: A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers
Authors: Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman
Abstract: Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA's TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA's inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at this https URL.

Paper number 37:
Title: Contrastive-KAN: A Semi-Supervised Intrusion Detection Framework for Cybersecurity with scarce Labeled Data
Authors: Mohammad Alikhani, Reza Kazemi
Abstract: In the era of the Fourth Industrial Revolution, cybersecurity and intrusion detection systems are vital for the secure and reliable operation of IoT and IIoT environments. A key challenge in this domain is the scarcity of labeled cyber-attack data, as most industrial systems operate under normal conditions. This data imbalance, combined with the high cost of annotation, hinders the effective training of machine learning models. Moreover, rapid detection of attacks is essential, especially in critical infrastructure, to prevent large-scale disruptions. To address these challenges, we propose a real-time intrusion detection system based on a semi-supervised contrastive learning framework using the Kolmogorov-Arnold Network (KAN). Our method leverages abundant unlabeled data to distinguish between normal and attack behaviors effectively. We validate our approach on three benchmark datasets: UNSW-NB15, BoT-IoT, and Gas Pipeline, using only 2.20 percent, 1.28 percent, and 8 percent of labeled samples, respectively, to simulate real-world conditions. Experimental results show that our method outperforms existing contrastive learning-based approaches. We further compare KAN with a traditional multilayer perceptron (MLP), demonstrating KAN's superior performance in both detection accuracy and robustness under limited supervision. KAN's ability to model complex relationships and its learnable activation functions are also explored and visualized, offering interpretability and potential for rule extraction. The method supports multi-class classification and proves effective in safety-critical environments where reliability is paramount.

Paper number 38:
Title: OpenGCRAM: An Open-Source Gain Cell Compiler Enabling Design-Space Exploration for AI Workloads
Authors: Xinxin Wang, Lixian Yan, Shuhan Liu, Luke Upton, Zhuoqi Cai, Yiming Tan, Shengman Li, Koustav Jana, Peijing Li, Jesse Cirimelli-Low, Thierry Tambe, Matthew Guthaus, H.-S. Philip Wong
Abstract: Gain Cell memory (GCRAM) offers higher density and lower power than SRAM, making it a promising candidate for on-chip memory in domain-specific accelerators. To support workloads with varying traffic and lifetime metrics, GCRAM also offers high bandwidth, ultra low leakage power and a wide range of retention times, which can be adjusted through transistor design (like threshold voltage and channel material) and on-the-fly by changing the operating voltage. However, designing and optimizing GCRAM sub-systems can be time-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM compiler capable of generating GCRAM bank circuit designs and DRC- and LVS-clean layouts for commercially available foundry CMOS, while also providing area, delay, and power simulations based on user-specified configurations (e.g., word size and number of words). OpenGCRAM enables fast, accurate, customizable, and optimized GCRAM block generation, reduces design time, ensure process compliance, and delivers performance-tailored memory blocks that meet diverse application requirements.

Paper number 39:
Title: Time-series forecasting for nonlinear high-dimensional system using hybrid method combining autoencoder and multi-parallelized quantum long short-term memory and gated recurrent unit
Authors: Makoto Takagi, Ryuji Kokubo, Misato Kurosawa, Tsubasa Ikami, Yasuhiro Egami, Hiroki Nagai, Takahiro Kashikawa, Koichi Kimura, Yutaka Takita, Yu Matsuda
Abstract: A time-series forecasting method for high-dimensional spatial data is proposed. The method involves optimal selection of sparse sensor positions to efficiently represent the spatial domain, time-series forecasting at these positions, and estimation of the entire spatial distribution from the forecasted values via a learned decoder. Sensor positions are selected using a method based on combinatorial optimization. Introducing multi-parallelized quantum long short-term memory (MP-QLSTM) and gated recurrent unit (MP-QGRU) improves time-series forecasting performance by extending QLSTM models using the same number of variational quantum circuits (VQCs) as the cell state dimensions. Unlike the original QLSTM, our method fully measures all qubits in each VQC, maximizing the representation capacity. MP-QLSTM and MP-QGRU achieve approximately 1.5% lower test loss than classical LSTM and GRU. The root mean squared percentage error of MP-QLSTM is 0.256% against the values measured independently using semiconductor pressure sensors, demonstrating the method's accuracy and effectiveness for high-dimensional forecasting tasks.

Paper number 40:
Title: Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition
Authors: Xiaocong Zeng, Craig Michoski, Yan Pang, Dongyang Kuang
Abstract: In this work, we address the often-overlooked issue of Timescale Dependent Label Inconsistency (TsDLI) in training neural network models for EEG-based human emotion recognition. To mitigate TsDLI and enhance model generalization and explainability, we propose two novel regularization strategies: Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods incorporate classical mathematical principles--specifically, functions of bounded variation and commute-time distances--within a graph theoretic framework. Complementing our regularizers, we introduce a suite of new evaluation metrics that better capture the alignment between temporally local predictions and their associated global emotion labels. We validate our approach through comprehensive experiments on two widely used EEG emotion datasets, DREAMER and DEAP, across a range of neural architectures including LSTM and transformer-based models. Performance is assessed using five distinct metrics encompassing both quantitative accuracy and qualitative consistency. Results consistently show that our proposed methods outperform state-of-the-art baselines, delivering superior aggregate performance and offering a principled trade-off between interpretability and predictive power under label inconsistency. Notably, LVL achieves the best aggregate rank across all benchmarked backbones and metrics, while LGCL frequently ranks the second, highlighting the effectiveness of our framework.

Paper number 41:
Title: SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging
Authors: Toktam Mohammadnejad, Jovin D'sa, Behdad Chalaki, Hossein Nourkhiz Mahjoub, Ehsan Moradi-Pari
Abstract: Merging onto a highway is a complex driving task that requires identifying a safe gap, adjusting speed, often interactions to create a merging gap, and completing the merge maneuver within a limited time window while maintaining safety and driving comfort. In this paper, we introduce a Safe Merging and Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed to facilitate safe and comfortable forced merging. By deliberately adapting cost terms to the unique challenges of forced merging and introducing a desired speed heuristic, SMART-Merge planner enables the ego vehicle to merge successfully while minimizing the merge time. We verify the efficiency and effectiveness of the proposed merge planner through high-fidelity CarMaker simulations on hundreds of highway merge scenarios. Our proposed planner achieves the success rate of 100% as well as completes the merge maneuver in the shortest amount of time compared with the baselines, demonstrating our planner's capability to handle complex forced merge tasks and provide a reliable and robust solution for autonomous highway merge. The simulation result videos are available at this https URL.

Paper number 42:
Title: Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison
Authors: Andrew Valdivia, Yueming Zhang, Hailu Xu, Amir Ghasemkhani, Xin Qin
Abstract: This paper presents a novel approach for detecting mispronunciations by analyzing deviations between a user's original speech and their voice-cloned counterpart with corrected pronunciation. We hypothesize that regions with maximal acoustic deviation between the original and cloned utterances indicate potential mispronunciations. Our method leverages recent advances in voice cloning to generate a synthetic version of the user's voice with proper pronunciation, then performs frame-by-frame comparisons to identify problematic segments. Experimental results demonstrate the effectiveness of this approach in pinpointing specific pronunciation errors without requiring predefined phonetic rules or extensive training data for each target language.

Paper number 43:
Title: MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments
Authors: Chen Cai, Ernesto Dickel Saraiva, Ya-jun Pan, Steven Liu
Abstract: This letter presents a novel coarse-to-fine motion planning framework for robotic manipulation in cluttered, unmodeled environments. The system integrates a dual-camera perception setup with a B-spline-based model predictive control (MPC) scheme. Initially, the planner generates feasible global trajectories from partial and uncertain observations. As new visual data are incrementally fused, both the environment model and motion planning are progressively refined. A vision-based cost function promotes target-driven exploration, while a refined kernel-perceptron collision detector enables efficient constraint updates for real-time planning. The framework accommodates closed-chain kinematics and supports dynamic replanning. Experiments on a multi-arm platform validate its robustness and adaptability under uncertainties and clutter.

Paper number 44:
Title: MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection
Authors: Guanghao Wu, Chen Xu, Hai Song, Chong Wang, Qixing Zhang
Abstract: Smoke is the first visible indicator of a this http URL the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image this http URL, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask this http URL, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at this https URL.

Paper number 45:
Title: Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks
Authors: Weiyi Liu, Jingzehua Xu, Guanwen Xie, Yi Li
Abstract: This paper presents a diffusion-augmented reinforcement learning (RL) approach for robust autonomous underwater vehicle (AUV) control, addressing key challenges in underwater trajectory planning and dynamic environment adaptation. The proposed method integrates three core innovations: (1) A diffusion-based trajectory generation framework that produces physically feasible multi-step trajectories, enhanced by a high-dimensional state encoding mechanism combining current observations with historical states and actions through a novel diffusion U-Net architecture, significantly improving long-horizon planning. (2) A sample-efficient hybrid learning architecture that synergizes diffusion-guided exploration with RL policy optimization, where the diffusion model generates diverse candidate actions and the RL critic selects optimal actions, achieving higher exploration efficiency and policy stability in dynamic underwater environments. Extensive simulation experiments validating the method's superior robustness and flexibility, outperforms conventional control methods in challenging marine conditions, offering enhanced adaptability and reliability for AUV operations in the underwater tasks.

Paper number 46:
Title: Distributionally Robust Optimization is a Multi-Objective Problem
Authors: Jun-ya Gotoh, Michael Jong Kim, Andrew E.B. Lim
Abstract: Distributionally Robust Optimization (DRO) is a worst-case approach to decision making when there is model uncertainty. Though formulated as a single-objective problem, we show that it is intrinsically multi-objective in that DRO solutions map out a near-Pareto-optimal frontier between expected cost and a measure of robustness called worst-case sensitivity (WCS). We take this as the starting point and explore robust decision making through a multi-objective lens. We show that WCS is a measure of spread and derive WCS for a collection of uncertainty sets commonly used in DRO. These sensitivity measures identify the errors against which the nominal expected cost is most vulnerable and the uncertainty set for the worst-case problem that most effectively mitigates it. The associated mean-sensitivity frontier is used to select its size. The multi-objective perspective provides a quantitative measure of robustness and a sensitivity-based approach to addressing important conceptual gaps in DRO -- how to choose the family and size of uncertainty sets for a given cost distribution, and how this affects the solution.

Paper number 47:
Title: FasTUSS: Faster Task-Aware Unified Source Separation
Authors: Francesco Paissan, Gordon Wichern, Yoshiki Masuyama, Ryo Aihara, François G. Germain, Kohei Saijo, Jonathan Le Roux
Abstract: Time-Frequency (TF) dual-path models are currently among the best performing audio source separation network architectures, achieving state-of-the-art performance in speech enhancement, music source separation, and cinematic audio source separation. While they are characterized by a relatively low parameter count, they still require a considerable number of operations, implying a higher execution time. This problem is exacerbated by the trend towards bigger models trained on large amounts of data to solve more general tasks, such as the recently introduced task-aware unified source separation (TUSS) model. TUSS, which aims to solve audio source separation tasks using a single, conditional model, is built upon TF-Locoformer, a TF dual-path model combining convolution and attention layers. The task definition comes in the form of a sequence of prompts that specify the number and type of sources to be extracted. In this paper, we analyze the design choices of TUSS with the goal of optimizing its performance-complexity trade-off. We derive two more efficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original model's operations by 81\% and 73\% with minor performance drops of 1.2~dB and 0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate the impact of prompt conditioning to derive a causal TUSS model.

Paper number 48:
Title: Multi-IMU Sensor Fusion for Legged Robots
Authors: Shuo Yang, John Z. Zhang, Ibrahima Sory Sow, Zachary Manchester
Abstract: This paper presents a state-estimation solution for legged robots that uses a set of low-cost, compact, and lightweight sensors to achieve low-drift pose and velocity estimation under challenging locomotion conditions. The key idea is to leverage multiple inertial measurement units on different links of the robot to correct a major error source in standard proprioceptive odometry. We fuse the inertial sensor information and joint encoder measurements in an extended Kalman filter, then combine the velocity estimate from this filter with camera data in a factor-graph-based sliding-window estimator to form a visual-inertial-leg odometry method. We validate our state estimator through comprehensive theoretical analysis and hardware experiments performed using real-world robot data collected during a variety of challenging locomotion tasks. Our algorithm consistently achieves minimal position deviation, even in scenarios involving substantial ground impact, foot slippage, and sudden body rotations. A C++ implementation, along with a large-scale dataset, is available at this https URL.

Paper number 49:
Title: LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer
Authors: Yaoxian Dong, Yifan Gao, Haoyue Li, Yanfen Cui, Xin Gao
Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal cancer guides treatment decisions, yet conventional MRI evaluation based on morphological criteria shows limited diagnostic performance. While some artificial intelligence models have been developed, they often operate as black boxes, lacking the interpretability needed for clinical trust. Moreover, these models typically evaluate nodes in isolation, overlooking the patient-level context. To address these limitations, we introduce LRMR, an LLM-Driven Relational Multi-node Ranking framework. This approach reframes the diagnostic task from a direct classification problem into a structured reasoning and ranking process. The LRMR framework operates in two stages. First, a multimodal large language model (LLM) analyzes a composite montage image of all LNs from a patient, generating a structured report that details ten distinct radiological features. Second, a text-based LLM performs pairwise comparisons of these reports between different patients, establishing a relative risk ranking based on the severity and number of adverse features. We evaluated our method on a retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies confirmed the value of our two main contributions: removing the relational ranking stage or the structured prompting stage led to a significant performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our work demonstrates that decoupling visual perception from cognitive reasoning through a two-stage LLM framework offers a powerful, interpretable, and effective new paradigm for assessing lymph node metastasis in rectal cancer.

Paper number 50:
Title: Demo: Secure Edge Server for Network Slicing and Resource Allocation in Open RAN
Authors: Adhwaa Alchaab, Ayman Younis, Dario Pompili
Abstract: Next-Generation Radio Access Networks (NGRAN) aim to support diverse vertical applications with strict security, latency, and Service-Level Agreement (SLA) requirements. These demands introduce challenges in securing the infrastructure, allocating resources dynamically, and enabling real-time reconfiguration. This demo presents SnSRIC, a secure and intelligent network slicing framework that mitigates a range of Distributed Denial-of-Service (DDoS) attacks in Open RAN environments. SnSRIC incorporates an AI-driven xApp that dynamically allocates Physical Resource Blocks (PRBs) to active users while enforcing slice-level security. The system detects anomalous behavior, distinguishes between benign and malicious devices, and uses the E2 interface to throttle rogue signaling while maintaining service continuity for legitimate users.

Paper number 51:
Title: Gaussian Noise Model of Nonlinear Distortions from Semiconductor Optical Amplifiers
Authors: Hartmut Hafermann
Abstract: A Gaussian Noise Model of the nonlinear noise power spectral density is developed for a semiconductor optical amplifier as described by the Agrawal model. A simple closed-form expression is obtained for the nonlinear noise-to-signal ratio of broadband wavelength-division multiplexed signals as a function of the Agrawal model parameters, the amplifier output power and the transmission bandwidth. The accuracy of the closed-form expression and its region of validity is assessed in numerical simulations. The error is smaller than 0.1 dB when the product of bandwidth and gain recovery time $B\times\tau_c$ exceeds 100.

Paper number 52:
Title: Canonical Bayesian Linear System Identification
Authors: Andrey Bryutkin, Matthew E. Levine, Iñigo Urteaga, Youssef Marzouk
Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data.

Paper number 53:
Title: The Utility of the Virtual Imaging Trials Methodology for Objective Characterization of AI Systems and Training Data
Authors: Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo
Abstract: The credibility of Artificial Intelligence (AI) models for medical imaging continues to be a challenge, affected by the diversity of models, the data used to train the models, and applicability of their combination to produce reproducible results for new data. In this work we aimed to explore if the emerging Virtual Imaging Trials (VIT) methodologies can provide an objective resource to approach this challenge. The study was conducted for the case example of COVID-19 diagnosis using clinical and virtual computed tomography (CT) and chest radiography (CXR) processed with convolutional neural networks (CNNs). Multiple AI models were developed and tested using 3D ResNet-like and 2D EfficientNetv2 architectures across diverse datasets. The performance differences were evaluated in terms of the area under the curve (AUC) and the DeLong method for AUC confidence intervals. The models trained on the most diverse datasets showed the highest external testing performance, with AUC values ranging from 0.73 to 0.76 for CT and 0.70 to 0.73 for CXR. Internal testing yielded higher AUC values (0.77 to 0.85 for CT and 0.77 to 1.0 for CXR), highlighting a substantial drop in performance during external validation, which underscores the importance of diverse and comprehensive training and testing data. Most notably, the VIT approach provided objective assessment of the utility of diverse models and datasets while further providing insight into the influence of dataset characteristics, patient factors, and imaging physics on AI efficacy. The VIT approach can be used to enhance model transparency and reliability, offering nuanced insights into the factors driving AI performance and bridging the gap between experimental and clinical settings.

Paper number 54:
Title: Offline and Online Use of Interval and Set-Based Approaches for Control and State Estimation: A Selection of Methodological Approaches and Their Application
Authors: Andreas Rauh, Marit Lahme, Simon Rohou, Luc Jaulin, Thach Ngoc Dinh, Tarek Raissi, Mohamed Fnadi
Abstract: Control and state estimation procedures need to be robust against imprecisely known parameters, uncertainty in initial conditions, and external disturbances. Interval methods and other set-based techniques form the basis for the implementation of powerful approaches that can be used to identify parameters of dynamic system models in the presence of the aforementioned types of uncertainty. Moreover, they are applicable to a verified feasibility and stability analysis of controllers and state estimators. In addition to these approaches which are typically used offline for analysis of system models designed with classical floating point procedures, interval and set-based methods have also been developed in recent years, which allow to directly solve the associated design tasks and to implement reliable techniques that are applicable online, i.e., during system operation. The latter approaches include set-based model predictive control, online parameter adaptation techniques for nonlinear variable-structure and backstepping controllers, interval observers, and fault diagnosis techniques. This paper provides an overview of the methodological background and reviews numerous practical applications for which interval and other set-valued approaches have been employed successfully.

Paper number 55:
Title: A Forward Reachability Perspective on Control Barrier Functions and Discount Factors in Reachability Analysis
Authors: Jason J. Choi, Donggun Lee, Boyang Li, Jonathan P. How, Koushil Sreenath, Sylvia L. Herbert, Claire J. Tomlin
Abstract: Control invariant sets are crucial for various methods that aim to design safe control policies for systems whose state constraints must be satisfied over an indefinite time horizon. In this article, we explore the connections among reachability, control invariance, and Control Barrier Functions (CBFs). Unlike prior formulations based on backward reachability concepts, we establish a strong link between these three concepts by examining the inevitable Forward Reachable Tube (FRT), which is the set of states such that every trajectory reaching the FRT must have passed through a given initial set of states. First, our findings show that the inevitable FRT is precisely this initial set itself if it is a robust control invariant set with a differentiable boundary-a property necessary to connect with CBFs whose zero-level sets are control invariant. We highlight that if the boundary is not differentiable, the FRT of the robust control invariant set may become a strict superset of the invariant set and lose invariance. Next, we formulate a differential game between the control and disturbance, where the inevitable FRT is characterized by the zero-superlevel set of the value function. By incorporating a discount factor in the cost function of the game, the barrier constraint of the CBF naturally arises in the Hamilton-Jacobi equation and determines the optimal policy. Combining these results, the value function of our FRT formulation serves as a CBF-like function, and conversely, any valid CBF is also a forward reachability value function inside the control invariant set, thereby revealing the inverse optimality of the CBF. This strong link between reachability and barrier constraints is not achievable by previous backward reachability-based formulations, and addresses an important gap in existing literature for constructing valid CBFs to ensure safety.

Paper number 56:
Title: Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation
Authors: Qing Wu, Chenhe Du, Xuanyu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei
Abstract: Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to outstanding image reconstruction results. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly reconstructs artifact-free MR images and estimates accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring any training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, facilitating optimal solutions. Specifically, we integrate a quasi-static motion model into the INR, granting its ability to correct subject's motion. To stabilize model optimization, we reformulate radial MRI reconstruction as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: this https URL

Paper number 57:
Title: Parallel Batch Scheduling With Incompatible Job Families Via Constraint Programming
Authors: Jorge A. Huertas, Pascal Van Hentenryck
Abstract: This paper addresses the incompatible case of parallel batch scheduling, where compatible jobs belong to the same family, and jobs from different families cannot be processed together in the same batch. The state-of-the-art constraint programming (CP) model for this problem relies on specific functions and global constraints only available in a well established commercial CP solver. This paper expands the literature around this problem by proposing four new CP models that can be implemented in commercial and open-source solvers: a new model that relies on automaton constraints, and three alternative models that integrate assignment and scheduling decisions with different strategies and global constraints. Extensive computational experiments on standard test cases under multiple objectives and multiple solvers demonstrate the implementation flexibility and competitive performance of the proposed models.

Paper number 58:
Title: From Real Artifacts to Virtual Reference: A Robust Framework for Translating Endoscopic Images
Authors: Junyang Wu, Fangfang Xie, Jiayuan Sun, Yun Gu, Guang-Zhong Yang
Abstract: Domain adaptation, which bridges the distributions across different modalities, plays a crucial role in multimodal medical image analysis. In endoscopic imaging, combining pre-operative data with intra-operative imaging is important for surgical planning and navigation. However, existing domain adaptation methods are hampered by distribution shift caused by in vivo artifacts, necessitating robust techniques for aligning noisy and artifact abundant patient endoscopic videos with clean virtual images reconstructed from pre-operative tomographic data for pose estimation during intraoperative guidance. This paper presents an artifact-resilient image translation method and an associated benchmark for this purpose. The method incorporates a novel ``local-global'' translation framework and a noise-resilient feature extraction strategy. For the former, it decouples the image translation process into a local step for feature denoising, and a global step for global style transfer. For feature extraction, a new contrastive learning strategy is proposed, which can extract noise-resilient features for establishing robust correspondence across domains. Detailed validation on both public and in-house clinical datasets has been conducted, demonstrating significantly improved performance compared to the current state-of-the-art.

Paper number 59:
Title: 360-Degree Video Super Resolution and Quality Enhancement Challenge: Methods and Results
Authors: Ahmed Telili, Wassim Hamidouche, Ibrahim Farhat, Hadi Amirpour, Christian Timmerer, Ibrahim Khadraoui, Jiajie Lu, The Van Le, Jeonneung Baek, Jin Young Lee, Yiying Wei, Xiaopeng Sun, Yu Gao, JianCheng Huangl, Yujie Zhong
Abstract: Omnidirectional (360-degree) video is rapidly gaining popularity due to advancements in immersive technologies like virtual reality (VR) and extended reality (XR). However, real-time streaming of such videos, especially in live mobile scenarios like unmanned aerial vehicles (UAVs), is challenged by limited bandwidth and strict latency constraints. Traditional methods, such as compression and adaptive resolution, help but often compromise video quality and introduce artifacts that degrade the viewer experience. Additionally, the unique spherical geometry of 360-degree video presents challenges not encountered in traditional 2D video. To address these issues, we initiated the 360-degree Video Super Resolution and Quality Enhancement Challenge. This competition encourages participants to develop efficient machine learning solutions to enhance the quality of low-bitrate compressed 360-degree videos, with two tracks focusing on 2x and 4x super-resolution (SR). In this paper, we outline the challenge framework, detailing the two competition tracks and highlighting the SR solutions proposed by the top-performing models. We assess these models within a unified framework, considering quality enhancement, bitrate gain, and computational efficiency. This challenge aims to drive innovation in real-time 360-degree video streaming, improving the quality and accessibility of immersive visual experiences.

Paper number 60:
Title: Orthogonality Analysis in LoRa Uplink Satellite Communications Affected by Doppler Effect
Authors: Jikang Deng, Fatma Benkhelifa, Mohamed-Slim Alouini
Abstract: This paper provides, for the first time, analytical expressions for the Long-Range (LoRa) waveform and cross-correlation in both continuous and discrete time domains under the Doppler effect in satellite communication. We propose the concept and formulas of the shared visibility window for satellites toward two ground devices. Our analysis covers cross-correlation results with varying spreading factors (SF) for no-Doppler and with-Doppler cases. We find the maximum cross-correlation with different SFs and the mean cross-correlation are immune to the Doppler effect. However, the maximum cross-correlation with the same SFs is only immune to high Doppler shift, with its value fluctuating between 0.6 and 1 under high Doppler rate. We interpret this fluctuation by introducing the relationship between transmission start time and cross-correlation. We provide a parameter analysis for orbit height, ground device distance, and inclination angle. Additionally, we analyze the bit error rate (BER) for LoRa signals and observe worse performance under high Doppler shift or interference with same SF. Increasing the SNR or the SIR improves the BER only when Doppler effect is below a frequency threshold. Notably, under Doppler effect, the performance behaviors of BER no longer align with those of maximum cross-correlation. Finally, our results lead to two recommendations: 1) To mitigate Doppler impact on cross-correlation, we recommend utilizing low SFs, high orbit height, short ground device distance, and the transmission start time with high Doppler shift; 2) To mitigate Doppler impact on BER, we recommend employing low SFs, high bandwidth, and transmission start time with high Doppler rate. These conflicting recommendations regarding transmission start time highlight the necessity of Doppler shift compensation techniques to help operate LoRa in space properly.

Paper number 61:
Title: Conformal Lyapunov Optimization: Optimal Resource Allocation under Deterministic Reliability Constraints
Authors: Francesco Binucci, Osvaldo Simeone, Paolo Banelli
Abstract: This paper introduces conformal Lyapunov optimization (CLO), a novel resource allocation framework for networked systems that optimizes average long-term objectives, while satisfying deterministic long-term reliability constraints. Unlike traditional Lyapunov optimization (LO), which addresses resource allocation tasks under average long-term constraints, CLO provides formal worst-case deterministic reliability guarantees. This is achieved by integrating the standard LO optimization framework with online conformal risk control (O-CRC), an adaptive update mechanism controlling long-term risks. The effectiveness of CLO is verified via experiments for hierarchal edge inference targeting image segmentation tasks in a networked computing architecture. Specifically, simulation results confirm that CLO can control reliability constraints, measured via the false negative rate of all the segmentation decisions made in the network, while at the same time minimizing the weighted sum of energy consumption and precision loss, with the latter accounting for the rate of false positives.

Paper number 62:
Title: Partition Map-Based Fast Block Partitioning for VVC Inter Coding
Authors: Xinmin Feng, Zhuoyuan Li, Li Li, Dong Liu, Feng Wu
Abstract: Among the new techniques of Versatile Video Coding (VVC), the quadtree with nested multi-type tree (QT+MTT) block structure yields significant coding gains by providing more flexible block partitioning patterns. However, the recursive partition search in the VVC encoder increases the encoder complexity substantially. To address this issue, we propose a partition map-based algorithm to pursue fast block partitioning in inter coding. Based on our previous work on partition map-based methods for intra coding, we analyze the characteristics of VVC inter coding, and thus improve the partition map by incorporating an MTT mask for early termination. Next, we develop a neural network that uses both spatial and temporal features to predict the partition map. It consists of several special designs including stacked top-down and bottom-up processing, quantization parameter modulation layers, and partitioning-adaptive warping. Furthermore, we present a dual-threshold decision scheme to achieve a fine-grained trade-off between complexity reduction and rate-distortion (RD) performance loss. The experimental results demonstrate that the proposed method achieves an average 51.30% encoding time saving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access configuration.

Paper number 63:
Title: Memory-less and Backscatter-less Tunnel Diode Harmonic Signatures for RFID
Authors: Christopher Saetia, Kaitlyn Graves, Serhat Tadik, Gregory D. Durgin
Abstract: Tunnel diodes have traditionally been researched for extending backscatter read-ranges for ultra-high-frequency (UHF) radio-frequency identification (RFID) tags as reflection amplifiers. This paper explores the natural harmonics that arise from biasing these diodes within their negative differential resistance regions and with no interrogating signal from a transmitting source, such as an RFID reader, to injection-lock these diodes. These harmonics are characterized for five tunnel diode boards, made with the same components and with each board's fundamental frequencies measuring at above -15 dBm at a biasing voltage of 200 mV when measured over-the-cable. The occurrence of these harmonics creates unique harmonic signatures for each board and demonstrates possible harmonic RFID applications that can help RFID readers discover and even identify RFID tags with backscatter-less and memory-less IDs generated by tunnel diodes.

Paper number 64:
Title: Deep Learning-Based CSI Feedback for Wi-Fi Systems With Temporal Correlation
Authors: Junyong Shin, Eunsung Jeon, Inhyoung Kim, Yo-Seb Jeon
Abstract: To achieve higher throughput in next-generation Wi-Fi systems, a station (STA) needs to efficiently compress channel state information (CSI) and feed it back to an access point (AP). In this paper, we propose a novel deep learning (DL)-based CSI feedback framework tailored for next-generation Wi-Fi systems. Our framework incorporates a pair of encoder and decoder neural networks to compress and reconstruct the angle parameters of the CSI. To enable an efficient finite-bit representation of the encoder output, we introduce a trainable vector quantization module, which is integrated after the encoder network and jointly trained with both the encoder and decoder networks in an end-to-end manner. Additionally, we further enhance our framework by leveraging the temporal correlation of the angle parameters. Specifically, we propose an angle-difference feedback strategy which transmits the difference between the current and previous angle parameters when the difference is sufficiently small. This strategy accounts for the periodicity of the angle parameters through proper preprocessing and mitigates error propagation effects using novel feedback methods. We also introduce a DL-based CSI refinement module for the AP, which improves the reconstruction accuracy of the angle parameters by simultaneously utilizing both the previous and current feedback information. Simulation results demonstrate that our framework outperforms the standard method employed in current Wi-Fi systems. Our results also demonstrate significant performance gains achieved by the angle-difference feedback strategy and the CSI refinement module.

Paper number 65:
Title: petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI
Authors: Pierrick Coupé, Boris Mansencal, Floréal Morandat, Sergio Morell-Ortega, Nicolas Villain, Jose V. Manjón, Vincent Planche
Abstract: INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles (T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's disease (AD) diagnosis and prognosis. Existing pipelines face limitations regarding processing time, variability in tracer types, and challenges in multimodal integration. METHODS: We developed petBrain, a novel end-to-end processing pipeline for amyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based segmentation, standardized biomarker quantification (Centiloid, CenTauR, HAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is implemented as a web-based platform, requiring no local computational infrastructure or specialized software knowledge. RESULTS: petBrain provides reliable and rapid biomarker quantification, with results comparable to existing pipelines for A and T2. It shows strong concordance with data processed in ADNI databases. The staging and quantification of A/T2/N by petBrain demonstrated good agreement with CSF/plasma biomarkers, clinical status, and cognitive performance. DISCUSSION: petBrain represents a powerful and openly accessible platform for standardized AD biomarker analysis, facilitating applications in clinical research.

Paper number 66:
Title: Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels
Authors: David Nozadze, Zurab Kiguradze, Amendra Koul, Mike Sapozhnikov
Abstract: The rise of AI workloads and growing data center demands have driven the need for ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI) shrink, even a few picoseconds of P/N skew can degrade serializer-deserializer (SerDes) performance. Traditional methods for quantifying skew fall short in capturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion Loss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD), analytically developed to assess P/N skew effects. Measured S-parameters confirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong correlation with bit error rate (BER) trends. This approach offers a robust framework for analyzing skew in next-generation ultra-high-speed interconnects.

Paper number 67:
Title: Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing
Authors: Mihir Kavishwar, Naresh Shanbhag
Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to digital architectures for accelerating machine learning and signal processing workloads. However, its energy efficiency is limited by the high energy cost of the column analog-to-digital converters (ADCs). Reducing the ADC precision is an effective approach to lowering its energy cost. However, doing so also reduces the AIMC's computational accuracy thereby making it critical to identify the minimum precision required to meet a target accuracy. Prior works overestimate the ADC precision requirements by modeling quantization error as input-independent noise, maximizing the signal-to-quantization-noise ratio (SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address these limitations by developing analytical expressions for estimating the compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we show that for a 256-dimensional binary dot product, CACTUS reduces the ADC precision requirements by 3b while achieving 6dB higher CSNR over prior methods. We also delineate operating conditions under which our proposed CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

Paper number 68:
Title: IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution
Authors: Sejin Park, Sangmin Lee, Kyong Hwan Jin, Seung-Won Jung
Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.

Paper number 69:
Title: Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)
Authors: Guohao Huo, Ruiting Dai, Hao Tang
Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and treatment planning, yet the variability in imaging quality across different MRI scanners presents significant challenges to model generalization. To address this, we propose the Edge Iterative MRI Lesion Localization System (EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to adaptively fine-tune segmentation models based on clinician feedback, thereby enhancing robustness to scanner-specific imaging characteristics. Central to this system is the Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive Encoder (M2AE) to extract multi-scale semantic features efficiently, and a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model complementary cross-modal relationships via graph structures. Additionally, we introduce a novel Voxel Refinement UpSampling Module (VRUM) that synergistically combines linear interpolation and multi-scale transposed convolutions to suppress artifacts while preserving high-frequency details, improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, representing a 98% reduction compared to mainstream 3D Transformer models, and significantly outperforms existing lightweight approaches. This work demonstrates a synergistic breakthrough in achieving high-accuracy, resource-efficient brain tumor segmentation suitable for deployment in resource-constrained clinical environments.

Paper number 70:
Title: Gain and phase type multipliers for feedback robustness
Authors: Axel Ringh, Xin Mao, Wei Chen, Li Qiu, Sei Zhen Khong
Abstract: It is known that the stability of a feedback interconnection of two linear time-invariant systems implies that the graphs of the open-loop systems are quadratically separated. This separation is defined by an object known as the multiplier. The theory of integral quadratic constraints shows that the converse also holds under certain conditions. This paper establishes that if the feedback is robustly stable against certain structured uncertainty, then there always exists a multiplier that takes a corresponding form. In particular, if the feedback is robustly stable to certain gain-type uncertainty, then there exists a corresponding multiplier that is of phase-type, i.e., its diagonal blocks are zeros. These results build on the notion of phases of matrices and systems, which was recently introduced in the field of control. Similarly, if the feedback is robustly stable to certain phase-type uncertainty, then there exists a gain-type multiplier, i.e., its off-diagonal blocks are zeros. The results are meaningfully instructive in the search for a valid multiplier for establishing robust closed-loop stability, and cover the well-known small-gain and the recent small-phase theorems.

Paper number 71:
Title: A Survey on Speech Deepfake Detection
Authors: Menglu Li, Yasaman Ahmadiadli, Xiao-Ping Zhang
Abstract: The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defences against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems.

Paper number 72:
Title: Retinex-RAWMamba: Bridging Demosaicing and Denoising for Low-Light RAW Image Enhancement
Authors: Xianmin Chen, Longfei Han, Peiliang Huang, Xiaoxu Feng, Dingwen Zhang, Junwei Han
Abstract: Low-light image enhancement, particularly in cross-domain tasks such as mapping from the raw domain to the sRGB domain, remains a significant challenge. Many deep learning-based methods have been developed to address this issue and have shown promising results in recent years. However, single-stage methods, which attempt to unify the complex mapping across both domains, leading to limited denoising performance. In contrast, existing two-stage approaches typically overlook the characteristic of demosaicing within the Image Signal Processing (ISP) pipeline, leading to color distortions under varying lighting conditions, especially in low-light scenarios. To address these issues, we propose a novel Mamba-based method customized for low light RAW images, called RAWMamba, to effectively handle raw images with different CFAs. Furthermore, we introduce a Retinex Decomposition Module (RDM) grounded in Retinex prior, which decouples illumination from reflectance to facilitate more effective denoising and automatic non-linear exposure correction, reducing the effect of manual linear illumination enhancement. By bridging demosaicing and denoising, better enhancement for low light RAW images is achieved. Experimental evaluations conducted on public datasets SID and MCR demonstrate that our proposed RAWMamba achieves state-of-the-art performance on cross-domain mapping. The code is available at this https URL.

Paper number 73:
Title: BlueME: Robust Underwater Robot-to-Robot Communication Using Compact Magnetoelectric Antennas
Authors: Mehron Talebi, Sultan Mahmud, Adam Khalifa, Md Jahidul Islam
Abstract: We present the design, development, and experimental validation of BlueME, a compact magnetoelectric (ME) antenna array system for underwater robot-to-robot communication. BlueME employs ME antennas operating at their natural mechanical resonance frequency to efficiently transmit and receive very-low-frequency (VLF) electromagnetic signals underwater. We outline the design, simulation, fabrication, and integration of the proposed system on low-power embedded platforms focusing on portable and scalable applications. For performance evaluation, we deployed BlueME on an autonomous surface vehicle (ASV) and a remotely operated vehicle (ROV) in open-water field trials. Our tests demonstrate that BlueME maintains reliable signal transmission at distances beyond 200 meters while consuming only 1 watt of power. Field trials show that the system operates effectively in challenging underwater conditions such as turbidity, obstacles, and multipath interference -- that generally affect acoustics and optics. Our analysis also examines the impact of complete submersion on system performance and identifies key deployment considerations. This work represents the first practical underwater deployment of ME antennas outside the laboratory, and implements the largest VLF ME array system to date. BlueME demonstrates significant potential for marine robotics and automation in multi-robot cooperative systems and remote sensor networks.

Paper number 74:
Title: A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation
Authors: M.M.A. Valiuddin, R.J.G. van Sloun, C.G.A. Viviers, P.H.N. de With, F. van der Sommen
Abstract: Advances in architectural design, data availability, and compute have driven remarkable progress in semantic segmentation. Yet, these models often rely on relaxed Bayesian assumptions, omitting critical uncertainty information needed for robust decision-making. The resulting reliance on point estimates has fueled interest in probabilistic segmentation, but the literature remains fragmented. In response, this review consolidates and contextualizes foundational concepts in uncertainty modeling, including the non-trivial task of distinguishing between epistemic and aleatoric uncertainty and examining their roles across four key downstream segmentation tasks, highlighting Active Learning as particularly promising. By unifying theory, terminology, and applications, we provide a coherent foundation for researchers and identify critical challenges, such as strong assumptions in spatial aggregation, lack of standardized benchmarks, and pitfalls in current uncertainty quantification methods. We identify trends such as the adoption of contemporary generative models, driven by advances in the broader field of generative modeling, with segmentation-specific innovation primarily in the conditioning mechanisms. Moreover, we observe growing interest in distribution- and sampling-free approaches to uncertainty estimation. We further propose directions for advancing uncertainty-aware segmentation in deep learning, including pragmatic strategies for disentangling different sources of uncertainty, novel uncertainty modeling approaches and improved Transformer-based backbones. In this way, we aim to support the development of more reliable, efficient, and interpretable segmentation models that effectively incorporate uncertainty into real-world applications.

Paper number 75:
Title: Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation
Authors: Zi Huang, Simon Denman, Akila Pemasiri, Clinton Fookes, Terrence Martin
Abstract: Radar signal recognition (RSR) plays a pivotal role in electronic warfare (EW), as accurately classifying radar signals is critical for informing decision-making. Recent advances in deep learning have shown significant potential in improving RSR in domains with ample annotated data. However, these methods fall short in EW scenarios where annotated radio frequency (RF) data are scarce or impractical to obtain. To address these challenges, we introduce a self-supervised learning (SSL) method which utilises masked signal modelling and RF domain adaption to perform few-shot RSR and enhance performance in environments with limited RF samples and annotations. We propose a two-step approach, first pre-training masked autoencoders (MAE) on baseband in-phase and quadrature (I/Q) signals from diverse RF domains, and then transferring the learned representations to the radar domain, where annotated data are scarce. Empirical results show that our lightweight self-supervised ResNet1D model with domain adaptation achieves up to a 17.5% improvement in 1-shot classification accuracy when pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31% improvement when pre-trained on out-of-domain signals (i.e., comm signals), compared to its baseline without using SSL. We also present reference results for several MAE designs and pre-training strategies, establishing a new benchmark for few-shot radar signal classification.

Paper number 76:
Title: Characterizing gaussian mixture of motion modes for skid-steer vehicle state estimation
Authors: Ameya Salvi, Mark Brudnak, Jonathon M. Smereka, Matthias Schmid, Venkat Krovi
Abstract: Skid-steered wheel mobile robots (SSWMRs) are characterized by the unique domination of the tire-terrain skidding for the robot to move. The lack of reliable friction models cascade into unreliable motion models, especially the reduced ordered variants used for state estimation and robot control. Ensemble modeling is an emerging research direction where the overall motion model is broken down into a family of local models to distribute the performance and resource requirement and provide a fast real-time prediction. To this end, a gaussian mixture model based modeling identification of model clusters is adopted and implemented within an interactive multiple model (IMM) based state estimation. The framework is adopted and implemented for angular velocity as the estimated state for a mid scaled skid-steered wheel mobile robot platform.

Paper number 77:
Title: LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture
Authors: Kazuki Atsuta, Kohei Honda, Hiroyuki Okuda, Tatsuya Suzuki
Abstract: This paper proposes a novel Large Vision-Language Model (LVLM) and Model Predictive Control (MPC) integration framework that delivers both task scalability and safety for Autonomous Driving (AD). LVLMs excel at high-level task planning across diverse driving scenarios. However, since these foundation models are not specifically designed for driving and their reasoning is not consistent with the feasibility of low-level motion planning, concerns remain regarding safety and smooth task switching. This paper integrates LVLMs with MPC Builder, which automatically generates MPCs on demand, based on symbolic task commands generated by the LVLM, while ensuring optimality and safety. The generated MPCs can strongly assist the execution or rejection of LVLM-driven task switching by providing feedback on the feasibility of the given tasks and generating task-switching-aware MPCs. Our approach provides a safe, flexible, and adaptable control framework, bridging the gap between cutting-edge foundation models and reliable vehicle operation. We demonstrate the effectiveness of our approach through a simulation experiment, showing that our system can safely and effectively handle highway driving while maintaining the flexibility and adaptability of LVLMs.

Paper number 78:
Title: ReverbMiipher: Generative Speech Restoration meets Reverberation Characteristics Controllability
Authors: Wataru Nakata, Yuma Koizumi, Shigeki Karita, Robin Scheibler, Haruko Ishikawa, Adriana Guevara-Rukoz, Heiga Zen, Michiel Bacchiani
Abstract: Reverberation encodes spatial information regarding the acoustic source environment, yet traditional Speech Restoration (SR) usually completely removes reverberation. We propose ReverbMiipher, an SR model extending parametric resynthesis framework, designed to denoise speech while preserving and enabling control over reverberation. ReverbMiipher incorporates a dedicated ReverbEncoder to extract a reverb feature vector from noisy input. This feature conditions a vocoder to reconstruct the speech signal, removing noise while retaining the original reverberation characteristics. A stochastic zero-vector replacement strategy during training ensures the feature specifically encodes reverberation, disentangling it from other speech attributes. This learned representation facilitates reverberation control via techniques such as interpolation between features, replacement with features from other utterances, or sampling from a latent space. Objective and subjective evaluations confirm ReverbMiipher effectively preserves reverberation, removes other artifacts, and outperforms the conventional two-stage SR and convolving simulated room impulse response approach. We further demonstrate its ability to generate novel reverberation effects through feature manipulation.

Paper number 79:
Title: Zero-Shot Hyperspectral Pansharpening Using Hysteresis-Based Tuning for Spectral Quality Control
Authors: Giuseppe Guarino, Matteo Ciotola, Gemine Vivone, Giovanni Poggi, Giuseppe Scarpa
Abstract: Hyperspectral pansharpening has received much attention in recent years due to technological and methodological advances that open the door to new application scenarios. However, research on this topic is only now gaining momentum. The most popular methods are still borrowed from the more mature field of multispectral pansharpening and often overlook the unique challenges posed by hyperspectral data fusion, such as i) the very large number of bands, ii) the overwhelming noise in selected spectral ranges, iii) the significant spectral mismatch between panchromatic and hyperspectral components, iv) a typically high resolution ratio. Imprecise data modeling especially affects spectral fidelity. Even state-of-the-art methods perform well in certain spectral ranges and much worse in others, failing to ensure consistent quality across all bands, with the risk of generating unreliable results. Here, we propose a hyperspectral pansharpening method that explicitly addresses this problem and ensures uniform spectral quality. To this end, a single lightweight neural network is used, with weights that adapt on the fly to each band. During fine-tuning, the spatial loss is turned on and off to ensure a fast convergence of the spectral loss to the desired level, according to a hysteresis-like dynamic. Furthermore, the spatial loss itself is appropriately redefined to account for nonlinear dependencies between panchromatic and spectral bands. Overall, the proposed method is fully unsupervised, with no prior training on external data, flexible, and low-complexity. Experiments on a recently published benchmarking toolbox show that it ensures excellent sharpening quality, competitive with the state-of-the-art, consistently across all bands. The software code and the full set of results are shared online on this https URL.

Paper number 80:
Title: SIC-Free Rate-Splitting Multiple Access: Constellation-Constrained Optimization and Application to Large-Scale Systems
Authors: Sibo Zhang, Bruno Clerckx, David Vargas
Abstract: Rate-Splitting Multiple Access (RSMA) has been recognized as a promising multiple access technique for future wireless communication systems. Recent research demonstrates that RSMA can maintain its superiority without relying on Successive Interference Cancellation (SIC) receivers. In practical systems, SIC-free receivers are more attractive than SIC receivers because of their low complexity and latency. This paper evaluates the theoretical limits of RSMA with and without SIC receivers under finite constellations. We first derive the constellation-constrained rate expressions for RSMA. We then design algorithms based on projected subgradient ascent to optimize the precoders and maximize the weighted sum-rate or max-min fairness among users. To apply the proposed optimization algorithms to large-scale systems, one challenge lies in the exponentially increasing computational complexity brought about by the constellation-constrained rate expressions. In light of this, we propose methods to avoid such computational burden. Numerical results show that, under optimized precoders, SIC-free RSMA leads to minor losses in both weighted sum-rate and max-min fairness in comparison to RSMA with SIC receivers, making it a viable option for future implementations.

Paper number 81:
Title: AoI-Energy-Spectrum Optimization in Post-Disaster Powered Communication Intelligent Network via Hierarchical Heterogeneous Graph Neural Network
Authors: Hanjian Liu, Jinsong Gui, Xiaoheng Deng
Abstract: This paper designs a post-disaster powered communication intelligent network (PDPCIN) to address communication disruptions caused by ground base station (GBS) failures within the post-disaster area. PDPCIN employs unmanned aerial vehicles (UAVs) to provide wireless data collection (WDC) and wireless energy transmission (WET) for affected areas and leverages low earth orbit satellites (LEO SATs) to relay UAV data to the nearest survival GBS. To ensure basic post-disaster communication while co-optimizing age of information (AoI), energy efficiency, and spectrum efficiency, intelligent synchronization-UAV (IS-UAV) architecture, AoI-based four thresholds updating (AFTU) mechanism, and Dynamic multi-LEO access (DMLA) strategy are proposed. However, three key challenges remain: time-varying task-resource imbalances, complex topology caused by multi-device scheduling, and nonlinear coupling in multidimensional metric optimization, making system optimization NP-hard. Therefore, this paper proposes a hierarchical heterogeneous graph neural networks (HHGNN) framework. It models heterogeneous device nodes and their communication relations as a hierarchical heterogeneous graph structure, integrating our defined graph sensing, exchange, and mask layer to handle the network's input, feature propagation, and output. To search appropriate number of single-LEO SATs, we propose single-LEO SAT density optimization (S-LSDO) algorithm. Finally, we compare the proposed scheme with state-of-the-art benchmarks to validate its superior collaborative optimization of AoI, energy efficiency, and spectrum efficiency. Based on this, we derive the expressions for the expected values of AoI and stagnant AoI proportion.

Paper number 82:
Title: Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition
Authors: Bingshen Mu, Kun Wei, Pengcheng Guo, Lei Xie
Abstract: Despite substantial improvements in ASR, performance tends to degrade when faced with adverse conditions such as speaker accents. Generative error correction (GER) leverages the rich linguistic knowledge and exceptional reasoning ability of LLMs, significantly outperforming typical LM methods. However, it lacks specificity in accented speech scenarios. In this study, we leverage GER to improve the accuracy of transcription predictions by addressing the two primary features of accented speech recognition. To fully leverage pronunciation information, we propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level information related to pronunciation. These two methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through LoRA fine-tuning. On the one hand, we employ a three-stage training strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge multiple mono-accent LoRA experts within a single multi-modal GER to overcome the challenges posed by accent diversity. On the other hand, multi-granularity GER leverages the N-best word-level and phoneme-level hypotheses generated by the HDMoLE model to predict the final accented speech transcriptions. Experimental results on the multi-accent English dataset demonstrate the efficacy of our proposed methods. Our methods achieve a remarkable relative WER reduction of 67.35% compared to the Whisper-large-v3 baseline.
    